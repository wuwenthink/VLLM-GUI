#!/bin/bash
# vLLM Server Startup Script
# Generated by VLLM GUI
# Model: /mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/
# Created: 2026/1/18 15:54:46

# Set environment variables
export CUDA_VISIBLE_DEVICES=0,1
# Use conda environment's python directly - no need for conda activate
source ~/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true
PYTHON_PATH=~/miniconda3/envs/vllm-tool/bin/python

# Start vLLM server
echo "Starting vLLM server..."
echo "Model: /mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/"
echo "Port: 8005"
echo "Tensor Parallel: 2"
$PYTHON_PATH -m vllm.entrypoints.openai.api_server --model "/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/" --host 0.0.0.0 --port 8005 --tensor-parallel-size 2 --gpu-memory-utilization=0.9 --max-num-seqs 256 --max-model-len 128000 --served-model-name VLLM-MODEL --chat-template "/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/chat_template.jinja" --trust-remote-code --enable-auto-tool-choice --async-scheduling --tool-call-parser deepseek-v3 --reasoning-parser deepseek-v3
