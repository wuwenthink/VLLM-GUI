[2026-01-17 17:02:15] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:56244 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:02:19] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:02:19 [loggers.py:257] Engine 000: Avg prompt throughput: 10486.5 tokens/s, Avg generation throughput: 50.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.8%
[2026-01-17 17:02:19] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:56244 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:02:22] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:56244 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:02:27] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:56244 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:02:29] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:02:29 [loggers.py:257] Engine 000: Avg prompt throughput: 15968.9 tokens/s, Avg generation throughput: 43.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 14.4%, Prefix cache hit rate: 95.9%
[2026-01-17 17:02:32] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:56244 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:02:39] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:02:39 [loggers.py:257] Engine 000: Avg prompt throughput: 5378.7 tokens/s, Avg generation throughput: 69.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 14.6%, Prefix cache hit rate: 95.9%
[2026-01-17 17:02:49] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:02:49 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 81.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 14.8%, Prefix cache hit rate: 95.9%
[2026-01-17 17:02:59] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:02:59 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 81.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 15.1%, Prefix cache hit rate: 95.9%
[2026-01-17 17:03:03] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:56244 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:03:06] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:56244 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:03:09] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:03:09 [loggers.py:257] Engine 000: Avg prompt throughput: 11294.9 tokens/s, Avg generation throughput: 50.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 15.3%, Prefix cache hit rate: 95.9%
[2026-01-17 17:03:11] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:56244 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:03:15] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:56244 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:03:18] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:56244 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:03:19] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:03:19 [loggers.py:257] Engine 000: Avg prompt throughput: 17091.0 tokens/s, Avg generation throughput: 44.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 15.4%, Prefix cache hit rate: 96.0%
[2026-01-17 17:03:29] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:03:29 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 20.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.0%
[2026-01-17 17:03:39] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:03:39 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.0%
[2026-01-17 17:04:32] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:33290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:04:36] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:33290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:04:39] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:04:39 [loggers.py:257] Engine 000: Avg prompt throughput: 11519.4 tokens/s, Avg generation throughput: 23.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.1%
[2026-01-17 17:04:42] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:33290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:04:45] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:33290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:04:49] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:33290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:04:49] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:04:49 [loggers.py:257] Engine 000: Avg prompt throughput: 11566.7 tokens/s, Avg generation throughput: 38.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.1%
[2026-01-17 17:04:51] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:33290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:04:53] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:33290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:04:59] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:04:59 [loggers.py:257] Engine 000: Avg prompt throughput: 17432.5 tokens/s, Avg generation throughput: 49.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.2%
[2026-01-17 17:04:59] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:33290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:05:03] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:33290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:05:09] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:05:09 [loggers.py:257] Engine 000: Avg prompt throughput: 11671.6 tokens/s, Avg generation throughput: 45.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.2%
[2026-01-17 17:05:19] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:05:19 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.2%
[2026-01-17 17:05:31] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:51570 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:05:34] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:51570 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:05:37] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:51570 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:05:39] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:05:39 [loggers.py:257] Engine 000: Avg prompt throughput: 17657.1 tokens/s, Avg generation throughput: 35.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.3%
[2026-01-17 17:05:41] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:51570 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:05:46] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:51570 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:05:49] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:05:49 [loggers.py:257] Engine 000: Avg prompt throughput: 11827.6 tokens/s, Avg generation throughput: 22.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.4%
[2026-01-17 17:05:51] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:51570 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:05:56] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:51570 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:05:59] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:05:59 [loggers.py:257] Engine 000: Avg prompt throughput: 11865.0 tokens/s, Avg generation throughput: 36.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 16.0%, Prefix cache hit rate: 96.4%
[2026-01-17 17:06:01] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:51570 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:06:07] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:51570 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:06:09] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:06:09 [loggers.py:257] Engine 000: Avg prompt throughput: 11950.1 tokens/s, Avg generation throughput: 61.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 16.1%, Prefix cache hit rate: 96.4%
[2026-01-17 17:06:13] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:51570 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:06:19] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:06:19 [loggers.py:257] Engine 000: Avg prompt throughput: 5997.2 tokens/s, Avg generation throughput: 59.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.5%
[2026-01-17 17:06:29] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:06:29 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.5%
[2026-01-17 17:10:09] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:42998 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:10:19] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:10:19 [loggers.py:257] Engine 000: Avg prompt throughput: 6030.2 tokens/s, Avg generation throughput: 10.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.5%
[2026-01-17 17:10:29] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:10:29 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.5%
[2026-01-17 17:11:42] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:60218 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:11:48] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:60218 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:11:49] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:11:49 [loggers.py:257] Engine 000: Avg prompt throughput: 12116.1 tokens/s, Avg generation throughput: 25.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 16.4%, Prefix cache hit rate: 96.5%
[2026-01-17 17:11:53] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:60218 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:11:59] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:11:59 [loggers.py:257] Engine 000: Avg prompt throughput: 6097.0 tokens/s, Avg generation throughput: 25.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.5%
[2026-01-17 17:12:09] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:12:09 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.5%
[2026-01-17 17:13:57] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:32768 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:13:59] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:32768 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:13:59] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:13:59 [loggers.py:257] Engine 000: Avg prompt throughput: 12216.4 tokens/s, Avg generation throughput: 8.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 16.4%, Prefix cache hit rate: 96.6%
[2026-01-17 17:14:09] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:14:09 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 13.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.6%
[2026-01-17 17:14:19] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:14:19 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.6%
[2026-01-17 17:16:03] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:54776 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:16:09] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:16:09 [loggers.py:257] Engine 000: Avg prompt throughput: 6125.6 tokens/s, Avg generation throughput: 30.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.6%
[2026-01-17 17:16:19] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:16:19 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.6%
[2026-01-17 17:17:39] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:43186 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:17:45] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:43186 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:17:48] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:43186 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:17:49] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:17:49 [loggers.py:257] Engine 000: Avg prompt throughput: 18507.9 tokens/s, Avg generation throughput: 43.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 16.7%, Prefix cache hit rate: 96.7%
[2026-01-17 17:17:59] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:17:59 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.7%
[2026-01-17 17:18:09] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:18:09 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.7%
[2026-01-17 17:19:22] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:59024 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:19:28] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:59024 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:19:29] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:19:29 [loggers.py:257] Engine 000: Avg prompt throughput: 12429.4 tokens/s, Avg generation throughput: 45.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 16.8%, Prefix cache hit rate: 96.7%
[2026-01-17 17:19:33] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:59024 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:19:39] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:19:39 [loggers.py:257] Engine 000: Avg prompt throughput: 6251.4 tokens/s, Avg generation throughput: 46.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.7%
[2026-01-17 17:19:43] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:36880 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:19:49] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:19:49 [loggers.py:257] Engine 000: Avg prompt throughput: 6313.7 tokens/s, Avg generation throughput: 46.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 17.1%, Prefix cache hit rate: 96.7%
[2026-01-17 17:19:57] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:53516 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:19:59] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:19:59 [loggers.py:257] Engine 000: Avg prompt throughput: 6365.1 tokens/s, Avg generation throughput: 26.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 17.2%, Prefix cache hit rate: 96.8%
[2026-01-17 17:20:09] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:20:09 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.8%
[2026-01-17 17:20:11] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:46358 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:20:15] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:46358 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:20:17] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:46358 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:20:19] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:20:19 [loggers.py:257] Engine 000: Avg prompt throughput: 19374.3 tokens/s, Avg generation throughput: 38.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.8%
[2026-01-17 17:20:29] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:20:29 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.8%
[2026-01-17 17:22:33] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:43774 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:22:39] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:22:39 [loggers.py:257] Engine 000: Avg prompt throughput: 6489.3 tokens/s, Avg generation throughput: 14.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.8%
[2026-01-17 17:22:41] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:40408 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:22:49] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:22:49 [loggers.py:257] Engine 000: Avg prompt throughput: 6535.3 tokens/s, Avg generation throughput: 31.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.8%
[2026-01-17 17:22:59] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:22:59 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.8%
[2026-01-17 17:40:34] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:44204 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:40:36] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:44204 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:40:39] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:40:39 [loggers.py:257] Engine 000: Avg prompt throughput: 13466.5 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.6%, Prefix cache hit rate: 96.8%
[2026-01-17 17:40:49] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:40:49 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 76.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.8%, Prefix cache hit rate: 96.8%
[2026-01-17 17:40:59] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:40:59 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 77.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 19.0%, Prefix cache hit rate: 96.8%
[2026-01-17 17:41:09] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:41:09 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 76.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 19.2%, Prefix cache hit rate: 96.8%
[2026-01-17 17:41:19] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:41:19 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 76.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 19.4%, Prefix cache hit rate: 96.8%
[2026-01-17 17:41:25] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:44204 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:41:29] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:41:29 [loggers.py:257] Engine 000: Avg prompt throughput: 7279.8 tokens/s, Avg generation throughput: 50.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.8%
[2026-01-17 17:41:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:44204 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:41:39] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:41:39 [loggers.py:257] Engine 000: Avg prompt throughput: 7295.4 tokens/s, Avg generation throughput: 15.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.8%
[2026-01-17 17:41:49] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:41:49 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.8%
[2026-01-17 20:52:39] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:47834 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 20:52:46] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:47834 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 20:52:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 20:52:50 [loggers.py:257] Engine 000: Avg prompt throughput: 5568.3 tokens/s, Avg generation throughput: 32.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.5%
[2026-01-17 20:52:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:47834 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 20:52:55] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:47834 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 20:53:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 20:53:00 [loggers.py:257] Engine 000: Avg prompt throughput: 5940.6 tokens/s, Avg generation throughput: 46.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.6%
[2026-01-17 20:53:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 20:53:10 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.6%
[2026-01-17 20:54:59] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:46410 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 20:55:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 20:55:00 [loggers.py:257] Engine 000: Avg prompt throughput: 2997.3 tokens/s, Avg generation throughput: 7.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.1%, Prefix cache hit rate: 96.6%
[2026-01-17 20:55:01] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:46410 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 20:55:05] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:46410 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 20:55:07] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:46410 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 20:55:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 20:55:10 [loggers.py:257] Engine 000: Avg prompt throughput: 9068.7 tokens/s, Avg generation throughput: 29.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.6%
[2026-01-17 20:55:12] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:46410 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 20:55:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 20:55:20 [loggers.py:257] Engine 000: Avg prompt throughput: 3041.8 tokens/s, Avg generation throughput: 10.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.6%
[2026-01-17 20:55:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 20:55:30 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.6%
[2026-01-17 20:57:08] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45946 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 20:57:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 20:57:10 [loggers.py:257] Engine 000: Avg prompt throughput: 3526.7 tokens/s, Avg generation throughput: 7.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.6%
[2026-01-17 20:57:11] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45946 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 20:57:14] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45946 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 20:57:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 20:57:20 [loggers.py:257] Engine 000: Avg prompt throughput: 7091.3 tokens/s, Avg generation throughput: 41.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.6%
[2026-01-17 20:57:21] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45946 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 20:57:25] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45946 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 20:57:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 20:57:30 [loggers.py:257] Engine 000: Avg prompt throughput: 7181.4 tokens/s, Avg generation throughput: 28.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.6%
[2026-01-17 20:57:37] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:58068 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 20:57:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 20:57:40 [loggers.py:257] Engine 000: Avg prompt throughput: 3654.4 tokens/s, Avg generation throughput: 18.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.6%
[2026-01-17 20:57:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:58068 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 20:57:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 20:57:50 [loggers.py:257] Engine 000: Avg prompt throughput: 3685.7 tokens/s, Avg generation throughput: 20.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.6%
[2026-01-17 20:58:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 20:58:00 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.6%
[2026-01-17 21:45:29] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:39612 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:45:36] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:39612 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:45:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:45:40 [loggers.py:257] Engine 000: Avg prompt throughput: 4060.4 tokens/s, Avg generation throughput: 34.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.4%
[2026-01-17 21:45:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:45:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.4%
[2026-01-17 21:46:49] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:36694 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:46:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:46:50 [loggers.py:257] Engine 000: Avg prompt throughput: 2058.0 tokens/s, Avg generation throughput: 5.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.6%, Prefix cache hit rate: 96.5%
[2026-01-17 21:46:55] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:36694 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:47:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:47:00 [loggers.py:257] Engine 000: Avg prompt throughput: 2130.7 tokens/s, Avg generation throughput: 31.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.5%
[2026-01-17 21:47:02] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:44270 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:47:09] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:44270 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:47:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:47:10 [loggers.py:257] Engine 000: Avg prompt throughput: 4495.5 tokens/s, Avg generation throughput: 20.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.5%
[2026-01-17 21:47:13] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:44270 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:47:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:47:20 [loggers.py:257] Engine 000: Avg prompt throughput: 2267.2 tokens/s, Avg generation throughput: 23.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.5%
[2026-01-17 21:47:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:47:30 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.5%
[2026-01-17 21:47:55] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:52584 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:48:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:48:00 [loggers.py:257] Engine 000: Avg prompt throughput: 2016.9 tokens/s, Avg generation throughput: 16.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.5%
[2026-01-17 21:48:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:48:10 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.5%
[2026-01-17 21:48:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:44388 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:48:35] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:44388 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:48:39] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:44388 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:48:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:48:40 [loggers.py:257] Engine 000: Avg prompt throughput: 6108.6 tokens/s, Avg generation throughput: 20.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.5%, Prefix cache hit rate: 96.5%
[2026-01-17 21:48:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:48:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.5%
[2026-01-17 21:49:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:49:00 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.5%
[2026-01-17 21:49:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:56034 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:49:28] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:56034 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:49:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:49:30 [loggers.py:257] Engine 000: Avg prompt throughput: 4696.5 tokens/s, Avg generation throughput: 20.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.3%
[2026-01-17 21:49:33] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:56034 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:49:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:49:40 [loggers.py:257] Engine 000: Avg prompt throughput: 2520.4 tokens/s, Avg generation throughput: 35.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.3%
[2026-01-17 21:49:41] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:56034 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:49:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:49:50 [loggers.py:257] Engine 000: Avg prompt throughput: 2555.3 tokens/s, Avg generation throughput: 25.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.3%
[2026-01-17 21:49:51] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:47050 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:50:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:50:00 [loggers.py:257] Engine 000: Avg prompt throughput: 2608.8 tokens/s, Avg generation throughput: 53.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.3%
[2026-01-17 21:50:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:50:10 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.3%
[2026-01-17 21:53:44] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:38958 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:53:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:53:50 [loggers.py:257] Engine 000: Avg prompt throughput: 2015.6 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.3%
[2026-01-17 21:54:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:54:00 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.3%
[2026-01-17 21:54:28] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:51468 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:54:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:54:30 [loggers.py:257] Engine 000: Avg prompt throughput: 2018.8 tokens/s, Avg generation throughput: 17.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.5%, Prefix cache hit rate: 96.3%
[2026-01-17 21:54:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:54:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 2.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.3%
[2026-01-17 21:54:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:54:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.3%
[2026-01-17 21:55:04] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:39182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:55:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:55:10 [loggers.py:257] Engine 000: Avg prompt throughput: 2027.2 tokens/s, Avg generation throughput: 11.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.3%
[2026-01-17 21:55:11] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:39194 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:55:16] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:39194 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:55:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:55:20 [loggers.py:257] Engine 000: Avg prompt throughput: 4199.3 tokens/s, Avg generation throughput: 32.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.3%
[2026-01-17 21:55:23] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:39194 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:55:29] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:39194 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:55:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:55:30 [loggers.py:257] Engine 000: Avg prompt throughput: 4321.3 tokens/s, Avg generation throughput: 20.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.9%, Prefix cache hit rate: 96.3%
[2026-01-17 21:55:34] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:39194 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:55:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:55:40 [loggers.py:257] Engine 000: Avg prompt throughput: 2411.6 tokens/s, Avg generation throughput: 23.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.3%
[2026-01-17 21:55:41] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:39194 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:55:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:55:50 [loggers.py:257] Engine 000: Avg prompt throughput: 2434.1 tokens/s, Avg generation throughput: 21.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.3%
[2026-01-17 21:55:52] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45676 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:56:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:56:00 [loggers.py:257] Engine 000: Avg prompt throughput: 2445.0 tokens/s, Avg generation throughput: 13.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.3%
[2026-01-17 21:56:03] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:39262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:56:09] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:39262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:56:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:56:10 [loggers.py:257] Engine 000: Avg prompt throughput: 2652.8 tokens/s, Avg generation throughput: 12.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.3%
[2026-01-17 21:56:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:56:20 [loggers.py:257] Engine 000: Avg prompt throughput: 3709.9 tokens/s, Avg generation throughput: 79.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.2%, Prefix cache hit rate: 96.2%
[2026-01-17 21:56:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:56:30 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 86.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.4%, Prefix cache hit rate: 96.2%
[2026-01-17 21:56:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:56:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 85.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.7%, Prefix cache hit rate: 96.2%
[2026-01-17 21:56:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:56:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 85.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.9%, Prefix cache hit rate: 96.2%
[2026-01-17 21:57:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:57:00 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 85.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 11.1%, Prefix cache hit rate: 96.2%
[2026-01-17 21:57:08] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:39262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:57:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:57:10 [loggers.py:257] Engine 000: Avg prompt throughput: 4162.9 tokens/s, Avg generation throughput: 55.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 11.2%, Prefix cache hit rate: 96.2%
[2026-01-17 21:57:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:57:20 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 34.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.2%
[2026-01-17 21:57:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:57:30 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.2%
[2026-01-17 21:58:42] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:58422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:58:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:58:50 [loggers.py:257] Engine 000: Avg prompt throughput: 4187.9 tokens/s, Avg generation throughput: 45.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.2%
[2026-01-17 21:58:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:58422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:59:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:59:00 [loggers.py:257] Engine 000: Avg prompt throughput: 4245.4 tokens/s, Avg generation throughput: 79.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 11.6%, Prefix cache hit rate: 96.2%
[2026-01-17 21:59:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:59:10 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 84.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 11.9%, Prefix cache hit rate: 96.2%
[2026-01-17 21:59:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:59:20 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 84.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.1%, Prefix cache hit rate: 96.2%
[2026-01-17 21:59:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:59:30 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 84.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.3%, Prefix cache hit rate: 96.2%
[2026-01-17 21:59:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:59:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 83.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.5%, Prefix cache hit rate: 96.2%
[2026-01-17 21:59:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:59:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 83.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.8%, Prefix cache hit rate: 96.2%
[2026-01-17 22:00:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:00:00 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 83.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 13.0%, Prefix cache hit rate: 96.2%
[2026-01-17 22:00:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:00:10 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 82.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 13.2%, Prefix cache hit rate: 96.2%
[2026-01-17 22:00:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:00:20 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 82.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 13.4%, Prefix cache hit rate: 96.2%
[2026-01-17 22:00:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:00:30 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 82.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 13.7%, Prefix cache hit rate: 96.2%
[2026-01-17 22:00:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:00:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 81.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 13.9%, Prefix cache hit rate: 96.2%
[2026-01-17 22:00:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:00:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 81.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 14.1%, Prefix cache hit rate: 96.2%
[2026-01-17 22:01:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:01:00 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 76.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.2%
[2026-01-17 22:01:02] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:58422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:01:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:01:10 [loggers.py:257] Engine 000: Avg prompt throughput: 5375.7 tokens/s, Avg generation throughput: 40.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.1%
[2026-01-17 22:01:12] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:58422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:01:17] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:58422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:01:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:01:20 [loggers.py:257] Engine 000: Avg prompt throughput: 10821.3 tokens/s, Avg generation throughput: 22.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.1%
[2026-01-17 22:01:21] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:58422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:01:26] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:58422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:01:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:01:30 [loggers.py:257] Engine 000: Avg prompt throughput: 10888.0 tokens/s, Avg generation throughput: 42.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 14.7%, Prefix cache hit rate: 96.2%
[2026-01-17 22:01:36] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:49988 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:01:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:01:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 11.7%, Prefix cache hit rate: 95.9%
[2026-01-17 22:01:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:01:50 [loggers.py:257] Engine 000: Avg prompt throughput: 5428.9 tokens/s, Avg generation throughput: 60.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 14.8%, Prefix cache hit rate: 95.9%
[2026-01-17 22:01:56] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:49988 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:02:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:02:00 [loggers.py:257] Engine 000: Avg prompt throughput: 5533.3 tokens/s, Avg generation throughput: 68.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 14.9%, Prefix cache hit rate: 95.9%
[2026-01-17 22:02:01] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:49988 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:02:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:02:10 [loggers.py:257] Engine 000: Avg prompt throughput: 5591.2 tokens/s, Avg generation throughput: 43.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.9%
[2026-01-17 22:02:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:02:20 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.9%
[2026-01-17 22:07:28] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:37262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:07:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:07:30 [loggers.py:257] Engine 000: Avg prompt throughput: 5614.3 tokens/s, Avg generation throughput: 14.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 15.1%, Prefix cache hit rate: 95.9%
[2026-01-17 22:07:33] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:37262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:07:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:07:40 [loggers.py:257] Engine 000: Avg prompt throughput: 5658.0 tokens/s, Avg generation throughput: 69.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 15.4%, Prefix cache hit rate: 96.0%
[2026-01-17 22:07:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:07:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 80.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 15.6%, Prefix cache hit rate: 96.0%
[2026-01-17 22:08:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:08:00 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 80.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 15.8%, Prefix cache hit rate: 96.0%
[2026-01-17 22:08:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:08:10 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 80.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 16.0%, Prefix cache hit rate: 96.0%
[2026-01-17 22:08:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:08:20 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 80.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 16.2%, Prefix cache hit rate: 96.0%
[2026-01-17 22:08:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:08:30 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 79.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 16.4%, Prefix cache hit rate: 96.0%
[2026-01-17 22:08:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:08:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 16.6%, Prefix cache hit rate: 96.0%
[2026-01-17 22:08:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:08:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 16.9%, Prefix cache hit rate: 96.0%
[2026-01-17 22:09:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:09:00 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 17.1%, Prefix cache hit rate: 96.0%
[2026-01-17 22:09:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:09:10 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 17.3%, Prefix cache hit rate: 96.0%
[2026-01-17 22:09:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:09:20 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 17.5%, Prefix cache hit rate: 96.0%
[2026-01-17 22:09:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:09:30 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 17.7%, Prefix cache hit rate: 96.0%
[2026-01-17 22:09:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:09:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 77.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 17.9%, Prefix cache hit rate: 96.0%
[2026-01-17 22:09:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:09:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 77.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.1%, Prefix cache hit rate: 96.0%
[2026-01-17 22:10:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:10:00 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 77.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.3%, Prefix cache hit rate: 96.0%
[2026-01-17 22:10:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:10:10 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 77.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.5%, Prefix cache hit rate: 96.0%
[2026-01-17 22:10:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:10:20 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 77.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.7%, Prefix cache hit rate: 96.0%
[2026-01-17 22:10:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:10:30 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 76.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.9%, Prefix cache hit rate: 96.0%
[2026-01-17 22:10:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:10:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 76.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 19.2%, Prefix cache hit rate: 96.0%
[2026-01-17 22:10:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:10:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 76.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 19.4%, Prefix cache hit rate: 96.0%
[2026-01-17 22:11:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:11:00 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 75.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 19.6%, Prefix cache hit rate: 96.0%
[2026-01-17 22:11:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:37262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:11:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:11:10 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.0%
[2026-01-17 22:11:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:11:20 [loggers.py:257] Engine 000: Avg prompt throughput: 7435.2 tokens/s, Avg generation throughput: 29.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.8%
[2026-01-17 22:11:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:37262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:11:25] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:37262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:11:28] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:37262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:11:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:11:30 [loggers.py:257] Engine 000: Avg prompt throughput: 22857.6 tokens/s, Avg generation throughput: 47.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 20.6%, Prefix cache hit rate: 95.9%
[2026-01-17 22:11:33] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:37262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:11:36] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:37262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:11:39] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:37262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:11:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:11:40 [loggers.py:257] Engine 000: Avg prompt throughput: 23229.1 tokens/s, Avg generation throughput: 39.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 21.0%, Prefix cache hit rate: 96.0%
[2026-01-17 22:11:42] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:37262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:11:45] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:37262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:11:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:11:50 [loggers.py:257] Engine 000: Avg prompt throughput: 15855.8 tokens/s, Avg generation throughput: 48.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 21.6%, Prefix cache hit rate: 96.0%
[2026-01-17 22:12:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:12:00 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 73.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 21.8%, Prefix cache hit rate: 96.0%
[2026-01-17 22:12:09] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:37262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:12:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:12:10 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 49.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.0%
[2026-01-17 22:12:16] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:37262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:12:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:12:20 [loggers.py:257] Engine 000: Avg prompt throughput: 16349.5 tokens/s, Avg generation throughput: 60.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 22.1%, Prefix cache hit rate: 96.0%
[2026-01-17 22:12:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:12:30 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 30.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.0%
[2026-01-17 22:12:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:12:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.0%
[2026-01-17 22:29:23] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:36928 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:29:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:29:30 [loggers.py:257] Engine 000: Avg prompt throughput: 8233.4 tokens/s, Avg generation throughput: 45.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 22.3%, Prefix cache hit rate: 96.1%
[2026-01-17 22:29:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:29:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 73.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 22.5%, Prefix cache hit rate: 96.1%
[2026-01-17 22:29:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:29:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 73.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 22.7%, Prefix cache hit rate: 96.1%
[2026-01-17 22:30:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:30:00 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 72.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 22.9%, Prefix cache hit rate: 96.1%
[2026-01-17 22:30:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:30:10 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 72.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 23.0%, Prefix cache hit rate: 96.1%
[2026-01-17 22:30:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:30:20 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 72.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 23.2%, Prefix cache hit rate: 96.1%
[2026-01-17 22:30:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:30:30 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 72.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 23.4%, Prefix cache hit rate: 96.1%
[2026-01-17 22:30:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:30:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 71.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 23.6%, Prefix cache hit rate: 96.1%
[2026-01-17 22:30:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:30:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 71.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 23.8%, Prefix cache hit rate: 96.1%
[2026-01-17 22:31:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:31:00 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 71.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 24.0%, Prefix cache hit rate: 96.1%
[2026-01-17 22:31:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:31:10 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 71.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 24.2%, Prefix cache hit rate: 96.1%
[2026-01-17 22:31:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:31:20 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 71.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 24.4%, Prefix cache hit rate: 96.1%
[2026-01-17 22:31:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:31:30 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 70.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 24.6%, Prefix cache hit rate: 96.1%
[2026-01-17 22:31:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:31:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 70.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 24.8%, Prefix cache hit rate: 96.1%
[2026-01-17 22:31:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:31:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 70.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 25.0%, Prefix cache hit rate: 96.1%
[2026-01-17 22:32:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:32:00 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 70.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 25.2%, Prefix cache hit rate: 96.1%
[2026-01-17 22:32:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:32:10 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 70.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 25.3%, Prefix cache hit rate: 96.1%
[2026-01-17 22:32:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:32:20 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 70.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 25.5%, Prefix cache hit rate: 96.1%
[2026-01-17 22:32:23] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:36928 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:32:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:32:30 [loggers.py:257] Engine 000: Avg prompt throughput: 9552.3 tokens/s, Avg generation throughput: 18.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.0%
[2026-01-17 22:32:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:36928 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:32:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:32:40 [loggers.py:257] Engine 000: Avg prompt throughput: 9573.5 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.0%
[2026-01-17 22:32:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:32:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.0%
[2026-01-17 22:34:27] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:34:27 [serving_chat.py:331] Error in preprocessing prompt inputs
[2026-01-17 22:34:27] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:34:27 [serving_chat.py:331] Traceback (most recent call last):
[2026-01-17 22:34:27] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:34:27 [serving_chat.py:331]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/serving_chat.py", line 309, in create_chat_completion
[2026-01-17 22:34:27] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:34:27 [serving_chat.py:331]     conversation, engine_prompts = await self._preprocess_chat(
[2026-01-17 22:34:27] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:34:27 [serving_chat.py:331]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-17 22:34:27] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:34:27 [serving_chat.py:331]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/serving_engine.py", line 1254, in _preprocess_chat
[2026-01-17 22:34:27] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:34:27 [serving_chat.py:331]     prompt_inputs = await self._tokenize_prompt_input_async(
[2026-01-17 22:34:27] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:34:27 [serving_chat.py:331]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-17 22:34:27] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:34:27 [serving_chat.py:331]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/serving_engine.py", line 1095, in _tokenize_prompt_input_async
[2026-01-17 22:34:27] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:34:27 [serving_chat.py:331]     async for result in self._tokenize_prompt_inputs_async(
[2026-01-17 22:34:27] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:34:27 [serving_chat.py:331]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/serving_engine.py", line 1116, in _tokenize_prompt_inputs_async
[2026-01-17 22:34:27] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:34:27 [serving_chat.py:331]     yield await self._normalize_prompt_text_to_input(
[2026-01-17 22:34:27] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:34:27 [serving_chat.py:331]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-17 22:34:27] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:34:27 [serving_chat.py:331]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/serving_engine.py", line 980, in _normalize_prompt_text_to_input
[2026-01-17 22:34:27] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:34:27 [serving_chat.py:331]     return self._validate_input(request, input_ids, input_text)
[2026-01-17 22:34:27] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:34:27 [serving_chat.py:331]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-17 22:34:27] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:34:27 [serving_chat.py:331]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/serving_engine.py", line 1073, in _validate_input
[2026-01-17 22:34:27] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:34:27 [serving_chat.py:331]     raise VLLMValidationError(
[2026-01-17 22:34:27] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:34:27 [serving_chat.py:331] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 32000. This model's maximum context length is 128000 tokens and your request has 96360 input tokens (32000 > 128000 - 96360). (parameter=max_tokens, value=32000)
[2026-01-17 22:34:27] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:38672 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
[2026-01-17 22:34:28] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:38672 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:34:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:34:50 [loggers.py:257] Engine 000: Avg prompt throughput: 7745.9 tokens/s, Avg generation throughput: 61.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 21.0%, Prefix cache hit rate: 95.5%
[2026-01-17 22:34:54] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:38672 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:35:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:35:00 [loggers.py:257] Engine 000: Avg prompt throughput: 1971.0 tokens/s, Avg generation throughput: 75.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.4%, Prefix cache hit rate: 95.5%
[2026-01-17 22:35:02] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:38672 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:35:05] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:38672 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:35:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:35:10 [loggers.py:257] Engine 000: Avg prompt throughput: 4219.4 tokens/s, Avg generation throughput: 68.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.0%, Prefix cache hit rate: 95.5%
[2026-01-17 22:35:11] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:38672 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:35:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:35:20 [loggers.py:257] Engine 000: Avg prompt throughput: 336.6 tokens/s, Avg generation throughput: 92.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.4%
[2026-01-17 22:35:21] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:38672 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:35:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:35:30 [loggers.py:257] Engine 000: Avg prompt throughput: 1982.4 tokens/s, Avg generation throughput: 18.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.4%
[2026-01-17 22:35:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:35:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.4%
[2026-01-17 22:36:08] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:36:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:36:10 [loggers.py:257] Engine 000: Avg prompt throughput: 1994.4 tokens/s, Avg generation throughput: 11.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.4%, Prefix cache hit rate: 95.4%
[2026-01-17 22:36:12] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:36:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:36:20 [loggers.py:257] Engine 000: Avg prompt throughput: 4251.3 tokens/s, Avg generation throughput: 44.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 11.5%, Prefix cache hit rate: 95.3%
[2026-01-17 22:36:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:36:30 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 85.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 11.8%, Prefix cache hit rate: 95.3%
[2026-01-17 22:36:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:36:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 84.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.0%, Prefix cache hit rate: 95.3%
[2026-01-17 22:36:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:36:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 84.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.2%, Prefix cache hit rate: 95.3%
[2026-01-17 22:37:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:37:00 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 84.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.4%, Prefix cache hit rate: 95.3%
[2026-01-17 22:37:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:37:10 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 83.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.7%, Prefix cache hit rate: 95.3%
[2026-01-17 22:37:16] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:37:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:37:20 [loggers.py:257] Engine 000: Avg prompt throughput: 4724.9 tokens/s, Avg generation throughput: 62.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.8%, Prefix cache hit rate: 95.3%
[2026-01-17 22:37:22] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:37:24] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:37:27] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:37:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:37:30 [loggers.py:257] Engine 000: Avg prompt throughput: 14812.9 tokens/s, Avg generation throughput: 32.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.3%
[2026-01-17 22:37:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:37:33] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:37:36] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:37:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:37:40 [loggers.py:257] Engine 000: Avg prompt throughput: 15870.5 tokens/s, Avg generation throughput: 55.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 14.6%, Prefix cache hit rate: 95.3%
[2026-01-17 22:37:45] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:37:49] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:37:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:37:50 [loggers.py:257] Engine 000: Avg prompt throughput: 10980.7 tokens/s, Avg generation throughput: 41.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 14.9%, Prefix cache hit rate: 95.4%
[2026-01-17 22:37:52] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:37:54] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:38:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:38:00 [loggers.py:257] Engine 000: Avg prompt throughput: 11238.4 tokens/s, Avg generation throughput: 57.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 15.3%, Prefix cache hit rate: 95.4%
[2026-01-17 22:38:08] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:38:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:38:10 [loggers.py:257] Engine 000: Avg prompt throughput: 5737.2 tokens/s, Avg generation throughput: 58.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.4%
[2026-01-17 22:38:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:38:13] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:38:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:38:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:38:20 [loggers.py:257] Engine 000: Avg prompt throughput: 11585.6 tokens/s, Avg generation throughput: 53.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.4%
[2026-01-17 22:38:23] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:38:26] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:38:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:38:30 [loggers.py:257] Engine 000: Avg prompt throughput: 17876.2 tokens/s, Avg generation throughput: 52.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 16.4%, Prefix cache hit rate: 95.5%
[2026-01-17 22:38:39] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:38:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:38:40 [loggers.py:257] Engine 000: Avg prompt throughput: 6160.6 tokens/s, Avg generation throughput: 64.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 16.6%, Prefix cache hit rate: 95.5%
[2026-01-17 22:38:42] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:38:45] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:38:47] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:38:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:38:50 [loggers.py:257] Engine 000: Avg prompt throughput: 18688.5 tokens/s, Avg generation throughput: 41.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 17.0%, Prefix cache hit rate: 95.5%
[2026-01-17 22:38:53] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:38:55] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:38:58] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:39:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:39:00 [loggers.py:257] Engine 000: Avg prompt throughput: 19263.1 tokens/s, Avg generation throughput: 37.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 17.7%, Prefix cache hit rate: 95.6%
[2026-01-17 22:39:04] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:39:07] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:39:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:39:10 [loggers.py:257] Engine 000: Avg prompt throughput: 13255.6 tokens/s, Avg generation throughput: 47.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.6%
[2026-01-17 22:39:11] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:39:14] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:39:17] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:39:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:39:20 [loggers.py:257] Engine 000: Avg prompt throughput: 20057.0 tokens/s, Avg generation throughput: 43.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.2%, Prefix cache hit rate: 95.6%
[2026-01-17 22:39:23] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:39:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:39:30 [loggers.py:257] Engine 000: Avg prompt throughput: 6759.1 tokens/s, Avg generation throughput: 66.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.3%, Prefix cache hit rate: 95.7%
[2026-01-17 22:39:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:39:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 77.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.5%, Prefix cache hit rate: 95.7%
[2026-01-17 22:39:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:39:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 77.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.7%, Prefix cache hit rate: 95.7%
[2026-01-17 22:39:52] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:40:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:40:00 [loggers.py:257] Engine 000: Avg prompt throughput: 6968.2 tokens/s, Avg generation throughput: 59.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.9%, Prefix cache hit rate: 95.7%
[2026-01-17 22:40:02] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:40:05] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:40:08] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:40:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:40:10 [loggers.py:257] Engine 000: Avg prompt throughput: 21102.2 tokens/s, Avg generation throughput: 34.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.7%
[2026-01-17 22:40:11] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:40:15] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:40:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:40:20 [loggers.py:257] Engine 000: Avg prompt throughput: 14347.6 tokens/s, Avg generation throughput: 52.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 19.4%, Prefix cache hit rate: 95.8%
[2026-01-17 22:40:22] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:40:28] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:40:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:40:30 [loggers.py:257] Engine 000: Avg prompt throughput: 14454.7 tokens/s, Avg generation throughput: 49.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 19.5%, Prefix cache hit rate: 95.8%
[2026-01-17 22:40:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:40:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.8%
[2026-01-17 22:40:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:40:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.8%
[2026-01-17 22:43:29] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:43:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:43:30 [loggers.py:257] Engine 000: Avg prompt throughput: 7258.3 tokens/s, Avg generation throughput: 2.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 19.5%, Prefix cache hit rate: 95.8%
[2026-01-17 22:43:32] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:43:36] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:43:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:43:40 [loggers.py:257] Engine 000: Avg prompt throughput: 14900.4 tokens/s, Avg generation throughput: 48.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 20.2%, Prefix cache hit rate: 95.8%
[2026-01-17 22:43:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:43:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 74.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 20.4%, Prefix cache hit rate: 95.8%
[2026-01-17 22:44:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:44:00 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 74.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 20.6%, Prefix cache hit rate: 95.8%
[2026-01-17 22:44:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:44:10 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 74.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 20.9%, Prefix cache hit rate: 95.8%
[2026-01-17 22:44:15] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:44:19] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:44:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:44:20 [loggers.py:257] Engine 000: Avg prompt throughput: 15669.2 tokens/s, Avg generation throughput: 41.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 21.2%, Prefix cache hit rate: 95.8%
[2026-01-17 22:44:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:44:30 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 74.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 21.4%, Prefix cache hit rate: 95.8%
[2026-01-17 22:44:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:44:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 74.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 21.6%, Prefix cache hit rate: 95.8%
[2026-01-17 22:44:44] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:44:48] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:44:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:44:50 [loggers.py:257] Engine 000: Avg prompt throughput: 16196.9 tokens/s, Avg generation throughput: 43.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.9%
[2026-01-17 22:44:51] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:45:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:45:00 [loggers.py:257] Engine 000: Avg prompt throughput: 8204.7 tokens/s, Avg generation throughput: 61.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 22.2%, Prefix cache hit rate: 95.9%
[2026-01-17 22:45:09] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:45:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:45:10 [loggers.py:257] Engine 000: Avg prompt throughput: 8320.9 tokens/s, Avg generation throughput: 56.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 22.4%, Prefix cache hit rate: 95.9%
[2026-01-17 22:45:16] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:45:19] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:45:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:45:20 [loggers.py:257] Engine 000: Avg prompt throughput: 16763.9 tokens/s, Avg generation throughput: 46.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 22.6%, Prefix cache hit rate: 95.9%
[2026-01-17 22:45:22] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:45:25] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:45:28] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:45:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:45:30 [loggers.py:257] Engine 000: Avg prompt throughput: 25492.4 tokens/s, Avg generation throughput: 34.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 23.0%, Prefix cache hit rate: 96.0%
[2026-01-17 22:45:35] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:45:38] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:45:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:45:40 [loggers.py:257] Engine 000: Avg prompt throughput: 17198.3 tokens/s, Avg generation throughput: 46.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 23.2%, Prefix cache hit rate: 96.0%
[2026-01-17 22:45:45] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:45:48] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:45:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:45:50 [loggers.py:257] Engine 000: Avg prompt throughput: 17357.9 tokens/s, Avg generation throughput: 45.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 23.4%, Prefix cache hit rate: 96.1%
[2026-01-17 22:45:51] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:45:56] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:46:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:46:00 [loggers.py:257] Engine 000: Avg prompt throughput: 17500.7 tokens/s, Avg generation throughput: 47.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 23.7%, Prefix cache hit rate: 96.1%
[2026-01-17 22:46:02] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:46:08] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:46:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:46:10 [loggers.py:257] Engine 000: Avg prompt throughput: 17618.6 tokens/s, Avg generation throughput: 44.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 23.8%, Prefix cache hit rate: 96.1%
[2026-01-17 22:46:11] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:46:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:46:20 [loggers.py:257] Engine 000: Avg prompt throughput: 8884.3 tokens/s, Avg generation throughput: 58.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 24.1%, Prefix cache hit rate: 96.1%
[2026-01-17 22:46:24] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:46:27] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:46:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:46:30 [loggers.py:257] Engine 000: Avg prompt throughput: 17922.7 tokens/s, Avg generation throughput: 43.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.2%
[2026-01-17 22:46:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:46:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.2%
[2026-01-17 22:48:55] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:48:59] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:49:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:49:00 [loggers.py:257] Engine 000: Avg prompt throughput: 18114.1 tokens/s, Avg generation throughput: 18.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 24.6%, Prefix cache hit rate: 96.2%
[2026-01-17 22:49:04] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:49:09] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:49:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:49:10 [loggers.py:257] Engine 000: Avg prompt throughput: 18803.6 tokens/s, Avg generation throughput: 39.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 25.5%, Prefix cache hit rate: 96.2%
[2026-01-17 22:49:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:49:20 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 70.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 25.7%, Prefix cache hit rate: 96.2%
[2026-01-17 22:49:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:49:30 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 69.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 25.9%, Prefix cache hit rate: 96.2%
[2026-01-17 22:49:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:49:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 69.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 26.1%, Prefix cache hit rate: 96.2%
[2026-01-17 22:49:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:49:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 69.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 26.3%, Prefix cache hit rate: 96.2%
[2026-01-17 22:49:52] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:49:52 [serving_chat.py:331] Error in preprocessing prompt inputs
[2026-01-17 22:49:52] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:49:52 [serving_chat.py:331] Traceback (most recent call last):
[2026-01-17 22:49:52] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:49:52 [serving_chat.py:331]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/serving_chat.py", line 309, in create_chat_completion
[2026-01-17 22:49:52] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:49:52 [serving_chat.py:331]     conversation, engine_prompts = await self._preprocess_chat(
[2026-01-17 22:49:52] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:49:52 [serving_chat.py:331]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-17 22:49:52] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:49:52 [serving_chat.py:331]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/serving_engine.py", line 1254, in _preprocess_chat
[2026-01-17 22:49:52] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:49:52 [serving_chat.py:331]     prompt_inputs = await self._tokenize_prompt_input_async(
[2026-01-17 22:49:52] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:49:52 [serving_chat.py:331]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-17 22:49:52] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:49:52 [serving_chat.py:331]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/serving_engine.py", line 1095, in _tokenize_prompt_input_async
[2026-01-17 22:49:52] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:49:52 [serving_chat.py:331]     async for result in self._tokenize_prompt_inputs_async(
[2026-01-17 22:49:52] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:49:52 [serving_chat.py:331]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/serving_engine.py", line 1116, in _tokenize_prompt_inputs_async
[2026-01-17 22:49:52] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:49:52 [serving_chat.py:331]     yield await self._normalize_prompt_text_to_input(
[2026-01-17 22:49:52] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:49:52 [serving_chat.py:331]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-17 22:49:52] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:49:52 [serving_chat.py:331]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/serving_engine.py", line 980, in _normalize_prompt_text_to_input
[2026-01-17 22:49:52] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:49:52 [serving_chat.py:331]     return self._validate_input(request, input_ids, input_text)
[2026-01-17 22:49:52] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:49:52 [serving_chat.py:331]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-17 22:49:52] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:49:52 [serving_chat.py:331]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/serving_engine.py", line 1073, in _validate_input
[2026-01-17 22:49:52] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:49:52 [serving_chat.py:331]     raise VLLMValidationError(
[2026-01-17 22:49:52] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:49:52 [serving_chat.py:331] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 32000. This model's maximum context length is 128000 tokens and your request has 97587 input tokens (32000 > 128000 - 97587). (parameter=max_tokens, value=32000)
[2026-01-17 22:49:52] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
[2026-01-17 22:49:53] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:50:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:50:00 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 15.5%, Prefix cache hit rate: 95.8%
[2026-01-17 22:50:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:50:10 [loggers.py:257] Engine 000: Avg prompt throughput: 7869.3 tokens/s, Avg generation throughput: 17.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 21.2%, Prefix cache hit rate: 95.8%
[2026-01-17 22:50:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:50:20 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 73.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 21.4%, Prefix cache hit rate: 95.8%
[2026-01-17 22:50:24] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:50:29] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:50:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:50:30 [loggers.py:257] Engine 000: Avg prompt throughput: 4084.6 tokens/s, Avg generation throughput: 60.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.7%, Prefix cache hit rate: 95.8%
[2026-01-17 22:50:32] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:50:36] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:50:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:50:40 [loggers.py:257] Engine 000: Avg prompt throughput: 4376.0 tokens/s, Avg generation throughput: 52.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.1%, Prefix cache hit rate: 95.8%
[2026-01-17 22:50:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:50:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 91.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.4%, Prefix cache hit rate: 95.8%
[2026-01-17 22:50:53] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:50:56] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:50:58] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:51:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:51:00 [loggers.py:257] Engine 000: Avg prompt throughput: 7192.9 tokens/s, Avg generation throughput: 48.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.6%, Prefix cache hit rate: 95.8%
[2026-01-17 22:51:02] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:51:05] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:51:08] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:51:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:51:10 [loggers.py:257] Engine 000: Avg prompt throughput: 7843.4 tokens/s, Avg generation throughput: 44.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.2%, Prefix cache hit rate: 95.8%
[2026-01-17 22:51:19] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:51:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:51:20 [loggers.py:257] Engine 000: Avg prompt throughput: 2738.7 tokens/s, Avg generation throughput: 74.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.4%, Prefix cache hit rate: 95.8%
[2026-01-17 22:51:22] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:51:25] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:51:28] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:51:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:51:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:51:30 [loggers.py:257] Engine 000: Avg prompt throughput: 8501.3 tokens/s, Avg generation throughput: 37.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.8%
[2026-01-17 22:51:33] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:51:35] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:51:39] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:51:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:51:40 [loggers.py:257] Engine 000: Avg prompt throughput: 11819.0 tokens/s, Avg generation throughput: 45.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.2%, Prefix cache hit rate: 95.9%
[2026-01-17 22:51:42] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:51:45] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:51:49] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:51:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:51:50 [loggers.py:257] Engine 000: Avg prompt throughput: 9254.0 tokens/s, Avg generation throughput: 51.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.3%, Prefix cache hit rate: 95.9%
[2026-01-17 22:51:54] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:52:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:52:00 [loggers.py:257] Engine 000: Avg prompt throughput: 1232.7 tokens/s, Avg generation throughput: 77.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.4%, Prefix cache hit rate: 95.8%
[2026-01-17 22:52:05] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:52:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:52:10 [loggers.py:257] Engine 000: Avg prompt throughput: 1992.1 tokens/s, Avg generation throughput: 64.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.8%
[2026-01-17 22:52:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:52:20 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.8%
[2026-01-17 22:54:09] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:34668 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:54:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:54:10 [loggers.py:257] Engine 000: Avg prompt throughput: 2003.4 tokens/s, Avg generation throughput: 4.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.4%, Prefix cache hit rate: 95.8%
[2026-01-17 22:54:13] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:34668 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:54:19] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:34668 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:54:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:54:20 [loggers.py:257] Engine 000: Avg prompt throughput: 4119.4 tokens/s, Avg generation throughput: 46.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.7%, Prefix cache hit rate: 95.8%
[2026-01-17 22:54:22] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:34668 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:54:25] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:34668 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:54:28] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:34668 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:54:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:54:30 [loggers.py:257] Engine 000: Avg prompt throughput: 6494.5 tokens/s, Avg generation throughput: 35.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.8%
[2026-01-17 22:54:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:54:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.8%
[2026-01-17 22:54:57] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:39328 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:55:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:55:00 [loggers.py:257] Engine 000: Avg prompt throughput: 2221.1 tokens/s, Avg generation throughput: 8.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.8%
[2026-01-17 22:55:03] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:39328 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:55:07] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:39328 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:55:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:55:10 [loggers.py:257] Engine 000: Avg prompt throughput: 4485.3 tokens/s, Avg generation throughput: 45.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.8%
[2026-01-17 22:55:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:55:20 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.8%
[2026-01-17 23:01:35] [INFO] [0;36m(Worker_TP1 pid=53925)[0;0m INFO 01-17 23:01:35 [multiproc_executor.py:707] Parent process exited, terminating worker
[2026-01-17 23:01:35] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 23:01:35 [core_client.py:610] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.
[2026-01-17 23:01:35] [INFO] [0;36m(Worker_TP0 pid=53924)[0;0m INFO 01-17 23:01:35 [multiproc_executor.py:707] Parent process exited, terminating worker
[2026-01-17 23:01:35] [INFO] [0;36m(Worker_TP1 pid=53925)[0;0m INFO 01-17 23:01:35 [multiproc_executor.py:751] WorkerProc shutting down.
[2026-01-17 23:01:35] [INFO] [0;36m(Worker_TP0 pid=53924)[0;0m INFO 01-17 23:01:35 [multiproc_executor.py:751] WorkerProc shutting down.
[2026-01-17 23:01:35] [INFO] [0;36m(Worker_TP1 pid=53925)[0;0m /mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py:147: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.
[2026-01-17 23:01:35] [INFO] [0;36m(Worker_TP1 pid=53925)[0;0m   warnings.warn('resource_tracker: process died unexpectedly, '
[2026-01-17 23:01:35] [INFO] [0;36m(Worker_TP0 pid=53924)[0;0m /mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py:147: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.
[2026-01-17 23:01:35] [INFO] [0;36m(Worker_TP0 pid=53924)[0;0m   warnings.warn('resource_tracker: process died unexpectedly, '
[2026-01-17 23:01:35] [ERROR] Traceback (most recent call last):
[2026-01-17 23:01:35] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-17 23:01:35] [INFO] cache[rtype].remove(name)
[2026-01-17 23:01:35] [INFO] KeyError: '/psm_24685522'
[2026-01-17 23:01:35] [ERROR] Traceback (most recent call last):
[2026-01-17 23:01:35] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-17 23:01:35] [INFO] cache[rtype].remove(name)
[2026-01-17 23:01:35] [INFO] KeyError: '/psm_e3bc7a23'
[2026-01-17 23:01:35] [ERROR] Traceback (most recent call last):
[2026-01-17 23:01:35] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-17 23:01:35] [INFO] cache[rtype].remove(name)
[2026-01-17 23:01:35] [INFO] KeyError: '/mp-blwhkhtg'
[2026-01-17 23:01:35] [INFO] 服务已停止
[2026-01-17 23:01:36] [ERROR] Traceback (most recent call last):
[2026-01-17 23:01:36] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-17 23:01:36] [INFO] cache[rtype].remove(name)
[2026-01-17 23:01:36] [INFO] KeyError: '/psm_0582c16f'
[2026-01-17 23:01:36] [ERROR] Traceback (most recent call last):
[2026-01-17 23:01:36] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-17 23:01:36] [INFO] cache[rtype].remove(name)
[2026-01-17 23:01:36] [INFO] KeyError: '/mp-fu6kqlw8'
[2026-01-17 23:01:36] [INFO] nvitop监控已启动
[2026-01-17 23:01:36] [INFO] nvitop监控已启动
[2026-01-17 23:01:41] [INFO] nvitop监控已启动
[2026-01-17 23:02:47] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-17 23:02:47] [INFO] nvitop监控已启动
[2026-01-17 23:02:47] [INFO] nvitop监控已停止
[2026-01-17 23:03:03] [INFO] nvitop监控已启动
[2026-01-17 23:03:03] [INFO] 服务已停止
[2026-01-17 23:03:03] [INFO] nvitop监控已启动
[2026-01-17 23:03:04] [INFO] nvitop监控已启动
[2026-01-17 23:03:06] [SUCCESS] 正在关闭服务器...
[2026-01-17 23:03:37] [INFO] VLLM GUI 服务器启动，端口: 5000
[2026-01-17 23:03:38] [INFO] nvitop监控已启动
[2026-01-17 23:03:38] [INFO] nvitop监控已启动
[2026-01-17 23:03:39] [INFO] 从conda info --base自动检测到conda路径: /mnt/AI-Acer4T/miniconda3
[2026-01-17 23:04:21] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-17 23:04:21] [INFO] nvitop监控已启动
[2026-01-17 23:04:21] [INFO] nvitop监控已停止
[2026-01-17 23:04:45] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:04:45 [api_server.py:1278] vLLM API server version 0.14.0rc1.dev492+g19504ac07
[2026-01-17 23:04:45] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:04:45 [utils.py:254] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/minimax/MiniMax-M2.1-NVFP4-TYW', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/AI-Acer4T/AI-Chat/models/minimax/MiniMax-M2.1-NVFP4-TYW/chat_template.jinja', 'enable_auto_tool_choice': True, 'tool_call_parser': 'minimax_m2', 'model': '/mnt/AI-Acer4T/AI-Chat/models/minimax/MiniMax-M2.1-NVFP4-TYW', 'trust_remote_code': True, 'max_model_len': 128000, 'quantization': 'nvfp4', 'served_model_name': ['VLLM-MODEL'], 'reasoning_parser': 'minimax_m2_append_think', 'tensor_parallel_size': 2, 'all2all_backend': 'pplx', 'gpu_memory_utilization': 0.95, 'kv_cache_dtype': 'fp8', 'max_num_seqs': 256, 'enable_chunked_prefill': True, 'async_scheduling': True}
[2026-01-17 23:04:45] [INFO] [0;36m(APIServer pid=253022)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-17 23:04:45] [INFO] [0;36m(APIServer pid=253022)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-17 23:04:46] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:04:46 [model.py:528] Resolved architecture: MiniMaxM2ForCausalLM
[2026-01-17 23:04:46] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:04:46 [model.py:1543] Using max model len 128000
[2026-01-17 23:04:46] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:04:46 [cache.py:206] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor.
[2026-01-17 23:04:46] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:04:46 [scheduler.py:231] Chunked prefill is enabled with max_num_batched_tokens=8192.
[2026-01-17 23:04:46] [WARNING] [0;36m(APIServer pid=253022)[0;0m WARNING 01-17 23:04:46 [modelopt.py:1018] Detected ModelOpt NVFP4 checkpoint. Please note that the format is experimental and could change in future.
[2026-01-17 23:04:46] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:04:46 [vllm.py:640] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-17 23:04:46] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:04:46 [vllm.py:645] Asynchronous scheduling is enabled.
[2026-01-17 23:04:47] [INFO] [0;36m(APIServer pid=253022)[0;0m The tokenizer you are loading from '/mnt/AI-Acer4T/AI-Chat/models/minimax/MiniMax-M2.1-NVFP4-TYW' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[2026-01-17 23:05:09] [INFO] [0;36m(EngineCore_DP0 pid=253248)[0;0m INFO 01-17 23:05:09 [core.py:97] Initializing a V1 LLM engine (v0.14.0rc1.dev492+g19504ac07) with config: model='/mnt/AI-Acer4T/AI-Chat/models/minimax/MiniMax-M2.1-NVFP4-TYW', speculative_config=None, tokenizer='/mnt/AI-Acer4T/AI-Chat/models/minimax/MiniMax-M2.1-NVFP4-TYW', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=modelopt_fp4, enforce_eager=False, kv_cache_dtype=fp8, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='minimax_m2_append_think', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-17 23:05:09] [WARNING] [0;36m(EngineCore_DP0 pid=253248)[0;0m WARNING 01-17 23:05:09 [multiproc_executor.py:880] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-17 23:05:32] [INFO] INFO 01-17 23:05:32 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:34341 backend=nccl
[2026-01-17 23:05:32] [INFO] INFO 01-17 23:05:32 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:34341 backend=nccl
[2026-01-17 23:05:33] [INFO] INFO 01-17 23:05:33 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-17 23:05:33] [WARNING] WARNING 01-17 23:05:33 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-17 23:05:33] [WARNING] WARNING 01-17 23:05:33 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-17 23:05:33] [INFO] INFO 01-17 23:05:33 [parallel_state.py:1425] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[2026-01-17 23:05:33] [INFO] INFO 01-17 23:05:33 [parallel_state.py:1425] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
[2026-01-17 23:05:35] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m INFO 01-17 23:05:35 [gpu_model_runner.py:3789] Starting to load model /mnt/AI-Acer4T/AI-Chat/models/minimax/MiniMax-M2.1-NVFP4-TYW...
[2026-01-17 23:05:35] [INFO] [0;36m(Worker_TP1 pid=253471)[0;0m INFO 01-17 23:05:35 [modelopt.py:1142] Using flashinfer-cutlass for NVFP4 GEMM
[2026-01-17 23:05:35] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m INFO 01-17 23:05:35 [modelopt.py:1142] Using flashinfer-cutlass for NVFP4 GEMM
[2026-01-17 23:05:39] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m INFO 01-17 23:05:39 [cuda.py:351] Using FLASHINFER attention backend out of potential backends: ('FLASHINFER', 'TRITON_ATTN')
[2026-01-17 23:05:39] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m INFO 01-17 23:05:39 [nvfp4.py:110] Using vLLM CUTASS backend for NvFp4 MoE
[2026-01-17 23:05:39] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:05:39] [INFO] Loading safetensors checkpoint shards:   0% Completed | 0/27 [00:00<?, ?it/s]
[2026-01-17 23:05:40] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:05:40] [INFO] Loading safetensors checkpoint shards:   4% Completed | 1/27 [00:00<00:24,  1.07it/s]
[2026-01-17 23:05:41] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:05:41] [INFO] Loading safetensors checkpoint shards:   7% Completed | 2/27 [00:01<00:24,  1.02it/s]
[2026-01-17 23:05:42] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:05:42] [INFO] Loading safetensors checkpoint shards:  11% Completed | 3/27 [00:02<00:22,  1.09it/s]
[2026-01-17 23:05:43] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:05:43] [INFO] Loading safetensors checkpoint shards:  15% Completed | 4/27 [00:03<00:22,  1.04it/s]
[2026-01-17 23:05:44] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:05:44] [INFO] Loading safetensors checkpoint shards:  19% Completed | 5/27 [00:04<00:21,  1.02it/s]
[2026-01-17 23:05:45] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:05:45] [INFO] Loading safetensors checkpoint shards:  22% Completed | 6/27 [00:05<00:21,  1.00s/it]
[2026-01-17 23:05:46] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:05:46] [INFO] Loading safetensors checkpoint shards:  26% Completed | 7/27 [00:06<00:20,  1.01s/it]
[2026-01-17 23:05:48] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:05:48] [INFO] Loading safetensors checkpoint shards:  30% Completed | 8/27 [00:08<00:24,  1.27s/it]
[2026-01-17 23:05:49] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:05:49] [INFO] Loading safetensors checkpoint shards:  33% Completed | 9/27 [00:09<00:21,  1.20s/it]
[2026-01-17 23:05:50] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:05:50] [INFO] Loading safetensors checkpoint shards:  37% Completed | 10/27 [00:10<00:19,  1.15s/it]
[2026-01-17 23:05:51] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:05:51] [INFO] Loading safetensors checkpoint shards:  41% Completed | 11/27 [00:11<00:17,  1.11s/it]
[2026-01-17 23:05:52] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:05:52] [INFO] Loading safetensors checkpoint shards:  44% Completed | 12/27 [00:12<00:16,  1.09s/it]
[2026-01-17 23:05:53] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:05:53] [INFO] Loading safetensors checkpoint shards:  48% Completed | 13/27 [00:13<00:15,  1.07s/it]
[2026-01-17 23:05:54] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:05:54] [INFO] Loading safetensors checkpoint shards:  52% Completed | 14/27 [00:14<00:13,  1.07s/it]
[2026-01-17 23:05:55] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:05:55] [INFO] Loading safetensors checkpoint shards:  56% Completed | 15/27 [00:16<00:12,  1.06s/it]
[2026-01-17 23:05:56] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:05:56] [INFO] Loading safetensors checkpoint shards:  59% Completed | 16/27 [00:17<00:11,  1.05s/it]
[2026-01-17 23:05:57] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:05:57] [INFO] Loading safetensors checkpoint shards:  63% Completed | 17/27 [00:18<00:10,  1.05s/it]
[2026-01-17 23:05:58] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:05:58] [INFO] Loading safetensors checkpoint shards:  67% Completed | 18/27 [00:19<00:09,  1.04s/it]
[2026-01-17 23:05:59] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:05:59] [INFO] Loading safetensors checkpoint shards:  70% Completed | 19/27 [00:20<00:08,  1.05s/it]
[2026-01-17 23:06:01] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:06:01] [INFO] Loading safetensors checkpoint shards:  74% Completed | 20/27 [00:21<00:07,  1.04s/it]
[2026-01-17 23:06:02] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:06:02] [INFO] Loading safetensors checkpoint shards:  78% Completed | 21/27 [00:22<00:06,  1.04s/it]
[2026-01-17 23:06:03] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:06:03] [INFO] Loading safetensors checkpoint shards:  81% Completed | 22/27 [00:23<00:05,  1.04s/it]
[2026-01-17 23:06:04] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:06:04] [INFO] Loading safetensors checkpoint shards:  85% Completed | 23/27 [00:24<00:04,  1.04s/it]
[2026-01-17 23:06:05] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:06:05] [INFO] Loading safetensors checkpoint shards:  89% Completed | 24/27 [00:25<00:03,  1.04s/it]
[2026-01-17 23:06:06] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:06:06] [INFO] Loading safetensors checkpoint shards:  93% Completed | 25/27 [00:26<00:02,  1.04s/it]
[2026-01-17 23:06:06] [WARNING] [0;36m(Worker_TP1 pid=253471)[0;0m WARNING 01-17 23:06:06 [kv_cache.py:90] Checkpoint does not provide a q scaling factor. Setting it to k_scale. This only matters for FP8 Attention backends (flash-attn or flashinfer).
[2026-01-17 23:06:06] [WARNING] [0;36m(Worker_TP1 pid=253471)[0;0m WARNING 01-17 23:06:06 [kv_cache.py:104] Using KV cache scaling factor 1.0 for fp8_e4m3. If this is unintended, verify that k/v_scale scaling factors are properly set in the checkpoint.
[2026-01-17 23:06:06] [WARNING] [0;36m(Worker_TP1 pid=253471)[0;0m WARNING 01-17 23:06:06 [kv_cache.py:143] Using uncalibrated q_scale 1.0 and/or prob_scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure q/prob scaling factors are available in the fp8 checkpoint.
[2026-01-17 23:06:07] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:06:07] [INFO] Loading safetensors checkpoint shards:  96% Completed | 26/27 [00:27<00:01,  1.00s/it]
[2026-01-17 23:06:07] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:06:07] [INFO] Loading safetensors checkpoint shards: 100% Completed | 27/27 [00:27<00:00,  1.34it/s]
[2026-01-17 23:06:07] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:06:07] [INFO] Loading safetensors checkpoint shards: 100% Completed | 27/27 [00:27<00:00,  1.02s/it]
[2026-01-17 23:06:07] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:06:07] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m INFO 01-17 23:06:07 [default_loader.py:291] Loading weights took 27.51 seconds
[2026-01-17 23:06:07] [WARNING] [0;36m(Worker_TP0 pid=253470)[0;0m WARNING 01-17 23:06:07 [kv_cache.py:90] Checkpoint does not provide a q scaling factor. Setting it to k_scale. This only matters for FP8 Attention backends (flash-attn or flashinfer).
[2026-01-17 23:06:07] [WARNING] [0;36m(Worker_TP0 pid=253470)[0;0m WARNING 01-17 23:06:07 [kv_cache.py:104] Using KV cache scaling factor 1.0 for fp8_e4m3. If this is unintended, verify that k/v_scale scaling factors are properly set in the checkpoint.
[2026-01-17 23:06:07] [WARNING] [0;36m(Worker_TP0 pid=253470)[0;0m WARNING 01-17 23:06:07 [kv_cache.py:143] Using uncalibrated q_scale 1.0 and/or prob_scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure q/prob scaling factors are available in the fp8 checkpoint.
[2026-01-17 23:06:07] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m INFO 01-17 23:06:07 [gpu_model_runner.py:3886] Model loading took 60.93 GiB memory and 32.096454 seconds
[2026-01-17 23:06:23] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m INFO 01-17 23:06:23 [backends.py:644] Using cache directory: /home/wuwen/.cache/vllm/torch_compile_cache/468f724d39/rank_0_0/backbone for vLLM's torch.compile
[2026-01-17 23:06:23] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m INFO 01-17 23:06:23 [backends.py:704] Dynamo bytecode transform time: 15.28 s
[2026-01-17 23:06:39] [INFO] [0;36m(Worker_TP1 pid=253471)[0;0m INFO 01-17 23:06:39 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 5.330 s
[2026-01-17 23:06:39] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m INFO 01-17 23:06:39 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 4.113 s
[2026-01-17 23:06:39] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m INFO 01-17 23:06:39 [monitor.py:34] torch.compile takes 19.39 s in total
[2026-01-17 23:06:40] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m INFO 01-17 23:06:40 [gpu_worker.py:358] Available KV cache memory: 26.74 GiB
[2026-01-17 23:06:41] [INFO] [0;36m(EngineCore_DP0 pid=253248)[0;0m INFO 01-17 23:06:41 [kv_cache_utils.py:1305] GPU KV cache size: 452,176 tokens
[2026-01-17 23:06:41] [INFO] [0;36m(EngineCore_DP0 pid=253248)[0;0m INFO 01-17 23:06:41 [kv_cache_utils.py:1310] Maximum concurrency for 128,000 tokens per request: 3.53x
[2026-01-17 23:06:41] [INFO] [0;36m(Worker_TP1 pid=253471)[0;0m 2026-01-17 23:06:41,137 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[2026-01-17 23:06:41] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m 2026-01-17 23:06:41,137 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[2026-01-17 23:06:41] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m 2026-01-17 23:06:41,695 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[2026-01-17 23:06:41] [INFO] [0;36m(Worker_TP1 pid=253471)[0;0m 2026-01-17 23:06:41,695 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[2026-01-17 23:06:41] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m INFO 01-17 23:06:41 [kernel_warmup.py:64] Warming up FlashInfer attention.
[2026-01-17 23:06:41] [INFO] [0;36m(Worker_TP1 pid=253471)[0;0m INFO 01-17 23:06:41 [kernel_warmup.py:64] Warming up FlashInfer attention.
[2026-01-17 23:06:42] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:06:42] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
[2026-01-17 23:06:42] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:08,  5.81it/s]
[2026-01-17 23:06:42] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:08,  5.96it/s]
[2026-01-17 23:06:43] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:07,  6.04it/s]
[2026-01-17 23:06:43] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:00<00:07,  6.06it/s]
[2026-01-17 23:06:43] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:00<00:07,  6.08it/s]
[2026-01-17 23:06:43] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:07,  6.06it/s]
[2026-01-17 23:06:43] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▎        | 7/51 [00:01<00:07,  5.88it/s]
[2026-01-17 23:06:43] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:01<00:07,  5.89it/s]
[2026-01-17 23:06:44] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:01<00:07,  5.94it/s]
[2026-01-17 23:06:44] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|█▉        | 10/51 [00:01<00:06,  5.97it/s]
[2026-01-17 23:06:44] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:01<00:06,  6.00it/s]
[2026-01-17 23:06:44] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:02<00:06,  6.01it/s]
[2026-01-17 23:06:44] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:02<00:06,  6.03it/s]
[2026-01-17 23:06:44] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:02<00:06,  6.04it/s]
[2026-01-17 23:06:45] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:02<00:05,  6.04it/s]
[2026-01-17 23:06:45] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 16/51 [00:02<00:05,  6.05it/s]
[2026-01-17 23:06:45] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:02<00:05,  6.02it/s]
[2026-01-17 23:06:45] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:02<00:05,  6.01it/s]
[2026-01-17 23:06:45] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 19/51 [00:03<00:05,  5.98it/s]
[2026-01-17 23:06:45] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:03<00:05,  5.96it/s]
[2026-01-17 23:06:46] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:03<00:05,  5.93it/s]
[2026-01-17 23:06:46] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 22/51 [00:03<00:04,  5.95it/s]
[2026-01-17 23:06:46] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:03<00:04,  5.98it/s]
[2026-01-17 23:06:46] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:04<00:04,  5.99it/s]
[2026-01-17 23:06:46] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 25/51 [00:04<00:04,  5.99it/s]
[2026-01-17 23:06:46] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:04<00:04,  6.01it/s]
[2026-01-17 23:06:47] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:04<00:04,  6.00it/s]
[2026-01-17 23:06:47] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:04<00:03,  5.98it/s]
[2026-01-17 23:06:47] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:04<00:03,  5.96it/s]
[2026-01-17 23:06:47] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:05<00:03,  5.96it/s]
[2026-01-17 23:06:47] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 31/51 [00:05<00:03,  5.92it/s]
[2026-01-17 23:06:47] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:05<00:03,  5.93it/s]
[2026-01-17 23:06:48] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:05<00:03,  5.92it/s]
[2026-01-17 23:06:48] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:05<00:02,  5.92it/s]
[2026-01-17 23:06:48] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:05<00:02,  5.92it/s]
[2026-01-17 23:06:48] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:06<00:02,  5.92it/s]
[2026-01-17 23:06:48] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 37/51 [00:06<00:02,  5.93it/s]
[2026-01-17 23:06:49] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:06<00:02,  5.92it/s]
[2026-01-17 23:06:49] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▋  | 39/51 [00:06<00:02,  5.94it/s]
[2026-01-17 23:06:49] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:06<00:01,  5.92it/s]
[2026-01-17 23:06:49] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:06<00:01,  5.93it/s]
[2026-01-17 23:06:49] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:07<00:01,  5.91it/s]
[2026-01-17 23:06:49] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 43/51 [00:07<00:01,  5.87it/s]
[2026-01-17 23:06:50] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:07<00:01,  5.11it/s]
[2026-01-17 23:06:50] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 45/51 [00:07<00:01,  5.32it/s]
[2026-01-17 23:06:50] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|█████████ | 46/51 [00:07<00:00,  5.51it/s]
[2026-01-17 23:06:50] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:07<00:00,  5.62it/s]
[2026-01-17 23:06:50] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 48/51 [00:08<00:00,  5.73it/s]
[2026-01-17 23:06:50] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▌| 49/51 [00:08<00:00,  5.80it/s]
[2026-01-17 23:06:51] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:08<00:00,  5.86it/s]
[2026-01-17 23:06:51] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:09<00:00,  3.34it/s]
[2026-01-17 23:06:51] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:09<00:00,  5.62it/s]
[2026-01-17 23:06:51] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:06:51] [INFO] Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
[2026-01-17 23:06:51] [INFO] Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:06,  5.50it/s]
[2026-01-17 23:06:52] [INFO] Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:00<00:05,  5.62it/s]
[2026-01-17 23:06:52] [INFO] Capturing CUDA graphs (decode, FULL):   9%|▊         | 3/35 [00:00<00:05,  5.65it/s]
[2026-01-17 23:06:52] [INFO] Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:07,  4.38it/s]
[2026-01-17 23:06:52] [INFO] Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:01<00:06,  4.78it/s]
[2026-01-17 23:06:52] [INFO] Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:01<00:05,  5.02it/s]
[2026-01-17 23:06:53] [INFO] Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:01<00:05,  5.24it/s]
[2026-01-17 23:06:53] [INFO] Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:01<00:05,  5.34it/s]
[2026-01-17 23:06:53] [INFO] Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:01<00:04,  5.44it/s]
[2026-01-17 23:06:53] [INFO] Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:01<00:04,  5.52it/s]
[2026-01-17 23:06:53] [INFO] Capturing CUDA graphs (decode, FULL):  31%|███▏      | 11/35 [00:02<00:04,  5.55it/s]
[2026-01-17 23:06:53] [INFO] Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:02<00:04,  5.50it/s]
[2026-01-17 23:06:54] [INFO] Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:02<00:03,  5.55it/s]
[2026-01-17 23:06:54] [INFO] Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:02<00:03,  5.55it/s]
[2026-01-17 23:06:54] [INFO] Capturing CUDA graphs (decode, FULL):  43%|████▎     | 15/35 [00:02<00:03,  5.56it/s]
[2026-01-17 23:06:54] [INFO] Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:02<00:03,  5.59it/s]
[2026-01-17 23:06:54] [INFO] Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:03<00:03,  5.59it/s]
[2026-01-17 23:06:55] [INFO] Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:03<00:03,  5.60it/s]
[2026-01-17 23:06:55] [INFO] Capturing CUDA graphs (decode, FULL):  54%|█████▍    | 19/35 [00:03<00:03,  4.44it/s]
[2026-01-17 23:06:55] [INFO] Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:03<00:03,  4.73it/s]
[2026-01-17 23:06:55] [INFO] Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:04<00:02,  4.98it/s]
[2026-01-17 23:06:55] [INFO] Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:04<00:02,  5.14it/s]
[2026-01-17 23:06:56] [INFO] Capturing CUDA graphs (decode, FULL):  66%|██████▌   | 23/35 [00:04<00:02,  5.28it/s]
[2026-01-17 23:06:56] [INFO] Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:04<00:02,  5.39it/s]
[2026-01-17 23:06:56] [INFO] Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:04<00:01,  5.46it/s]
[2026-01-17 23:06:56] [INFO] Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:04<00:01,  5.49it/s]
[2026-01-17 23:06:56] [INFO] Capturing CUDA graphs (decode, FULL):  77%|███████▋  | 27/35 [00:05<00:01,  5.53it/s]
[2026-01-17 23:06:57] [INFO] Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:05<00:01,  5.54it/s]
[2026-01-17 23:06:57] [INFO] Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:05<00:01,  5.54it/s]
[2026-01-17 23:06:57] [INFO] Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:05<00:00,  5.54it/s]
[2026-01-17 23:06:57] [INFO] Capturing CUDA graphs (decode, FULL):  89%|████████▊ | 31/35 [00:05<00:00,  5.56it/s]
[2026-01-17 23:06:57] [INFO] Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:05<00:00,  5.60it/s]
[2026-01-17 23:06:57] [INFO] Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:06<00:00,  5.62it/s]
[2026-01-17 23:06:58] [INFO] Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:06<00:00,  5.65it/s]
[2026-01-17 23:06:58] [INFO] Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:06<00:00,  5.61it/s]
[2026-01-17 23:06:58] [INFO] Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:06<00:00,  5.37it/s]
[2026-01-17 23:06:58] [INFO] [0;36m(Worker_TP1 pid=253471)[0;0m INFO 01-17 23:06:58 [custom_all_reduce.py:216] Registering 15958 cuda graph addresses
[2026-01-17 23:06:58] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m INFO 01-17 23:06:58 [custom_all_reduce.py:216] Registering 15958 cuda graph addresses
[2026-01-17 23:06:59] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m INFO 01-17 23:06:59 [gpu_model_runner.py:4837] Graph capturing finished in 17 secs, took -0.07 GiB
[2026-01-17 23:06:59] [INFO] [0;36m(EngineCore_DP0 pid=253248)[0;0m INFO 01-17 23:06:59 [core.py:273] init engine (profile, create kv cache, warmup model) took 51.09 seconds
[2026-01-17 23:06:59] [INFO] [0;36m(EngineCore_DP0 pid=253248)[0;0m The tokenizer you are loading from '/mnt/AI-Acer4T/AI-Chat/models/minimax/MiniMax-M2.1-NVFP4-TYW' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[2026-01-17 23:06:59] [INFO] [0;36m(EngineCore_DP0 pid=253248)[0;0m INFO 01-17 23:06:59 [core.py:186] Batch queue is enabled with size 2
[2026-01-17 23:06:59] [INFO] [0;36m(EngineCore_DP0 pid=253248)[0;0m INFO 01-17 23:06:59 [vllm.py:645] Asynchronous scheduling is enabled.
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [api_server.py:1020] Supported tasks: ['generate']
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m The tokenizer you are loading from '/mnt/AI-Acer4T/AI-Chat/models/minimax/MiniMax-M2.1-NVFP4-TYW' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[2026-01-17 23:07:00] [WARNING] [0;36m(APIServer pid=253022)[0;0m WARNING 01-17 23:07:00 [model.py:1356] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [serving_responses.py:224] Using default chat sampling params from model: {'top_k': 40, 'top_p': 0.95}
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [serving_engine.py:271] "auto" tool choice has been enabled.
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [serving_engine.py:271] "auto" tool choice has been enabled.
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [serving_chat.py:146] Using default chat sampling params from model: {'top_k': 40, 'top_p': 0.95}
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [serving_chat.py:182] Warming up chat template processing...
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [chat_utils.py:599] Detected the chat template content format to be 'openai'. You can set `--chat-template-content-format` to override this.
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [serving_chat.py:218] Chat template warmup completed in 43.2ms
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [serving_completion.py:78] Using default completion sampling params from model: {'top_k': 40, 'top_p': 0.95}
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [serving_engine.py:271] "auto" tool choice has been enabled.
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [serving_chat.py:146] Using default chat sampling params from model: {'top_k': 40, 'top_p': 0.95}
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [api_server.py:1352] Starting vLLM API server 0 on http://0.0.0.0:8005
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:38] Available routes are:
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /docs, Methods: HEAD, GET
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /tokenize, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /detokenize, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /pause, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /resume, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /is_paused, Methods: GET
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /metrics, Methods: GET
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /health, Methods: GET
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /load, Methods: GET
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /v1/models, Methods: GET
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /version, Methods: GET
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /v1/responses, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /v1/messages, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /v1/completions, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /ping, Methods: GET
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /ping, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /invocations, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /classify, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /score, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /v1/score, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /rerank, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /v1/rerank, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /v2/rerank, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /pooling, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     Started server process [253022]
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     Waiting for application startup.
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     Application startup complete.
[2026-01-17 23:08:21] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:08:25] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:08:28] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:08:30] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:08:30 [loggers.py:257] Engine 000: Avg prompt throughput: 4644.2 tokens/s, Avg generation throughput: 22.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.4%, Prefix cache hit rate: 49.8%
[2026-01-17 23:08:35] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:08:39] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:08:40] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:08:40 [loggers.py:257] Engine 000: Avg prompt throughput: 9386.6 tokens/s, Avg generation throughput: 43.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 66.4%
[2026-01-17 23:08:47] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:08:50] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:08:50] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:08:50 [loggers.py:257] Engine 000: Avg prompt throughput: 13396.1 tokens/s, Avg generation throughput: 23.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 75.5%
[2026-01-17 23:08:54] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:09:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:09:00 [loggers.py:257] Engine 000: Avg prompt throughput: 14229.8 tokens/s, Avg generation throughput: 47.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 82.6%
[2026-01-17 23:09:02] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:09:07] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:09:10] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:09:10 [loggers.py:257] Engine 000: Avg prompt throughput: 14602.7 tokens/s, Avg generation throughput: 47.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 87.0%
[2026-01-17 23:09:12] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:09:15] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:09:19] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:09:20] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:09:20 [loggers.py:257] Engine 000: Avg prompt throughput: 22113.2 tokens/s, Avg generation throughput: 31.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 16.4%, Prefix cache hit rate: 90.5%
[2026-01-17 23:09:30] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:09:30 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 75.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 16.6%, Prefix cache hit rate: 90.5%
[2026-01-17 23:09:35] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:09:40] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:09:40 [loggers.py:257] Engine 000: Avg prompt throughput: 7563.0 tokens/s, Avg generation throughput: 34.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 91.2%
[2026-01-17 23:09:44] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:09:50] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:09:50 [loggers.py:257] Engine 000: Avg prompt throughput: 7594.4 tokens/s, Avg generation throughput: 50.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 16.9%, Prefix cache hit rate: 91.9%
[2026-01-17 23:09:56] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:09:59] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:10:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:10:00 [loggers.py:257] Engine 000: Avg prompt throughput: 15301.5 tokens/s, Avg generation throughput: 37.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 17.0%, Prefix cache hit rate: 92.9%
[2026-01-17 23:10:05] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:10:07] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:10:10] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:10:10 [loggers.py:257] Engine 000: Avg prompt throughput: 15421.3 tokens/s, Avg generation throughput: 43.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 17.1%, Prefix cache hit rate: 93.8%
[2026-01-17 23:10:15] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:10:20] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:10:20 [loggers.py:257] Engine 000: Avg prompt throughput: 7754.2 tokens/s, Avg generation throughput: 27.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 94.1%
[2026-01-17 23:10:21] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:10:24] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:10:26] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:10:29] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:10:30] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:10:30 [loggers.py:257] Engine 000: Avg prompt throughput: 31471.2 tokens/s, Avg generation throughput: 34.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 17.6%, Prefix cache hit rate: 95.1%
[2026-01-17 23:10:32] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:10:35] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:10:40] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:10:40] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:10:40 [loggers.py:257] Engine 000: Avg prompt throughput: 24183.2 tokens/s, Avg generation throughput: 24.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.0%, Prefix cache hit rate: 95.7%
[2026-01-17 23:10:43] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:10:50] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:10:50 [loggers.py:257] Engine 000: Avg prompt throughput: 8251.8 tokens/s, Avg generation throughput: 58.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.4%, Prefix cache hit rate: 95.8%
[2026-01-17 23:11:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:11:00 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 73.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.5%, Prefix cache hit rate: 95.8%
[2026-01-17 23:11:02] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:11:06] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:11:10] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:11:10 [loggers.py:257] Engine 000: Avg prompt throughput: 16835.3 tokens/s, Avg generation throughput: 43.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.8%, Prefix cache hit rate: 96.0%
[2026-01-17 23:11:15] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:11:20] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:11:20 [loggers.py:257] Engine 000: Avg prompt throughput: 8506.5 tokens/s, Avg generation throughput: 51.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.9%, Prefix cache hit rate: 96.1%
[2026-01-17 23:11:27] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:11:30] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:11:30 [loggers.py:257] Engine 000: Avg prompt throughput: 8568.4 tokens/s, Avg generation throughput: 48.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.3%
[2026-01-17 23:11:31] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:11:36] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:11:39] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:11:40] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:11:40 [loggers.py:257] Engine 000: Avg prompt throughput: 25852.0 tokens/s, Avg generation throughput: 31.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 19.1%, Prefix cache hit rate: 96.6%
[2026-01-17 23:11:42] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:11:46] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:11:50] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:11:50 [loggers.py:257] Engine 000: Avg prompt throughput: 17338.6 tokens/s, Avg generation throughput: 34.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.8%
[2026-01-17 23:11:51] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:12:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:12:00 [loggers.py:257] Engine 000: Avg prompt throughput: 8707.4 tokens/s, Avg generation throughput: 50.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.9%
[2026-01-17 23:12:10] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:12:10 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.9%
[2026-01-17 23:13:05] [INFO] VLLM GUI 服务器启动，端口: 5002
[2026-01-17 23:13:06] [INFO] nvitop监控已启动
[2026-01-17 23:13:07] [INFO] nvitop监控已启动
[2026-01-17 23:13:08] [INFO] 从conda info --base自动检测到conda路径: /mnt/AI-Acer4T/miniconda3
[2026-01-17 23:13:20] [SUCCESS] 方案已保存: vllm_配置_2026/1/17
[2026-01-17 23:13:40] [SUCCESS] 方案已删除: vllm_配置_2026/1/17
[2026-01-17 23:16:32] [SUCCESS] 方案已保存: MiniMax-M2.1-NVFP4-TYW-TP2
[2026-01-17 23:17:34] [SUCCESS] 方案已保存: MiniMax-M2.1-FP8-INT4-AWQ-M-TP2
[2026-01-17 23:18:11] [SUCCESS] 方案已保存: MiniMax-M2.1-NVFP4-TP2
[2026-01-17 23:18:54] [SUCCESS] 方案已保存: MiniMax-M2.1-AWQ-TP2
[2026-01-17 23:19:03] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-17 23:19:03] [INFO] nvitop监控已启动
[2026-01-17 23:19:03] [INFO] nvitop监控已停止
[2026-01-17 23:19:28] [ERROR] Traceback (most recent call last):
[2026-01-17 23:19:28] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/bin/vllm", line 7, in <module>
[2026-01-17 23:19:28] [INFO] sys.exit(main())
[2026-01-17 23:19:28] [INFO] ^^^^^^
[2026-01-17 23:19:28] [INFO] File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/main.py", line 70, in main
[2026-01-17 23:19:28] [INFO] cmds[args.subparser].validate(args)
[2026-01-17 23:19:28] [INFO] File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/serve.py", line 63, in validate
[2026-01-17 23:19:28] [INFO] validate_parsed_serve_args(args)
[2026-01-17 23:19:28] [INFO] File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/cli_args.py", line 307, in validate_parsed_serve_args
[2026-01-17 23:19:28] [INFO] validate_chat_template(args.chat_template)
[2026-01-17 23:19:28] [INFO] File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/chat_utils.py", line 1199, in validate_chat_template
[2026-01-17 23:19:28] [INFO] raise ValueError(
[2026-01-17 23:19:28] [INFO] ValueError: The supplied chat template string (/mnt/AI-Acer4T/AI-Chat/models/minimax/minimax/MiniMax-M2.1-AWQ/chat_template.jinja) appears path-like, but doesn't exist! Tried: /mnt/AI-Acer4T/AI-Chat/models/minimax/minimax/MiniMax-M2.1-AWQ/chat_template.jinja and /mnt/AI-Acer4T/AI-Chat/models/minimax/minimax/MiniMax-M2.1-AWQ/chat_template.jinja
[2026-01-17 23:19:29] [INFO] nvitop监控已启动
[2026-01-17 23:21:06] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-17 23:21:06] [INFO] nvitop监控已启动
[2026-01-17 23:21:06] [INFO] nvitop监控已停止
[2026-01-17 23:21:18] [INFO] nvitop监控已启动
[2026-01-17 23:21:18] [INFO] 服务已停止
[2026-01-17 23:21:19] [INFO] nvitop监控已启动
[2026-01-17 23:21:35] [INFO] [0;36m(APIServer pid=253022)[0;0m [0;36m(Worker_TP1 pid=253471)[0;0m INFO 01-17 23:21:35 [multiproc_executor.py:707] Parent process exited, terminating worker
[2026-01-17 23:21:35] [ERROR] ERROR 01-17 23:21:35 [core_client.py:610] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.
[2026-01-17 23:21:35] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m INFO 01-17 23:21:35 [multiproc_executor.py:707] Parent process exited, terminating worker
[2026-01-17 23:21:35] [INFO] [0;36m(Worker_TP1 pid=253471)[0;0m INFO 01-17 23:21:35 [multiproc_executor.py:751] WorkerProc shutting down.
[2026-01-17 23:21:35] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m INFO 01-17 23:21:35 [multiproc_executor.py:751] WorkerProc shutting down.
[2026-01-17 23:21:35] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m /mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py:147: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.
[2026-01-17 23:21:35] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m   warnings.warn('resource_tracker: process died unexpectedly, '
[2026-01-17 23:21:35] [INFO] [0;36m(Worker_TP1 pid=253471)[0;0m /mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py:147: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.
[2026-01-17 23:21:35] [INFO] [0;36m(Worker_TP1 pid=253471)[0;0m   warnings.warn('resource_tracker: process died unexpectedly, '
[2026-01-17 23:21:35] [ERROR] Traceback (most recent call last):
[2026-01-17 23:21:35] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-17 23:21:35] [INFO] cache[rtype].remove(name)
[2026-01-17 23:21:35] [INFO] KeyError: '/psm_9655ef00'
[2026-01-17 23:21:35] [ERROR] Traceback (most recent call last):
[2026-01-17 23:21:35] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-17 23:21:35] [INFO] cache[rtype].remove(name)
[2026-01-17 23:21:35] [INFO] KeyError: '/psm_04df7afb'
[2026-01-17 23:21:35] [INFO] 服务已停止
[2026-01-17 23:21:35] [INFO] nvitop监控已启动
[2026-01-17 23:21:35] [ERROR] Traceback (most recent call last):
[2026-01-17 23:21:35] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-17 23:21:35] [INFO] cache[rtype].remove(name)
[2026-01-17 23:21:35] [INFO] KeyError: '/psm_1d9c31ae'
[2026-01-17 23:21:36] [ERROR] Traceback (most recent call last):
[2026-01-17 23:21:36] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-17 23:21:36] [ERROR] Traceback (most recent call last):
[2026-01-17 23:21:36] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-17 23:21:36] [INFO] cache[rtype].remove(name)
[2026-01-17 23:21:36] [INFO] KeyError: '/mp-gpyjq2ya'
[2026-01-17 23:21:36] [INFO] cache[rtype].remove(name)
[2026-01-17 23:21:36] [INFO] KeyError: '/mp-37a58tmd'
[2026-01-17 23:21:36] [INFO] nvitop监控已启动
[2026-01-17 23:21:37] [SUCCESS] 正在关闭服务器...
[2026-01-17 23:21:47] [INFO] VLLM GUI 服务器启动，端口: 5002
[2026-01-17 23:21:47] [INFO] nvitop监控已启动
[2026-01-17 23:21:48] [INFO] nvitop监控已启动
[2026-01-17 23:21:49] [INFO] 从conda info --base自动检测到conda路径: /mnt/AI-Acer4T/miniconda3
[2026-01-17 23:21:54] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-17 23:21:54] [INFO] nvitop监控已启动
[2026-01-17 23:21:54] [INFO] nvitop监控已停止
[2026-01-17 23:22:19] [ERROR] Traceback (most recent call last):
[2026-01-17 23:22:19] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/bin/vllm", line 7, in <module>
[2026-01-17 23:22:19] [INFO] sys.exit(main())
[2026-01-17 23:22:19] [INFO] ^^^^^^
[2026-01-17 23:22:19] [INFO] File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/main.py", line 70, in main
[2026-01-17 23:22:19] [INFO] cmds[args.subparser].validate(args)
[2026-01-17 23:22:19] [INFO] File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/serve.py", line 63, in validate
[2026-01-17 23:22:19] [INFO] validate_parsed_serve_args(args)
[2026-01-17 23:22:19] [INFO] File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/cli_args.py", line 307, in validate_parsed_serve_args
[2026-01-17 23:22:19] [INFO] validate_chat_template(args.chat_template)
[2026-01-17 23:22:19] [INFO] File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/chat_utils.py", line 1199, in validate_chat_template
[2026-01-17 23:22:19] [INFO] raise ValueError(
[2026-01-17 23:22:19] [INFO] ValueError: The supplied chat template string (/mnt/AI-Acer4T/AI-Chat/models/minimax/minimax/MiniMax-M2.1-AWQ/chat_template.jinja) appears path-like, but doesn't exist! Tried: /mnt/AI-Acer4T/AI-Chat/models/minimax/minimax/MiniMax-M2.1-AWQ/chat_template.jinja and /mnt/AI-Acer4T/AI-Chat/models/minimax/minimax/MiniMax-M2.1-AWQ/chat_template.jinja
[2026-01-17 23:22:21] [INFO] nvitop监控已启动
[2026-01-17 23:22:55] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-17 23:22:55] [INFO] nvitop监控已启动
[2026-01-17 23:22:55] [INFO] nvitop监控已停止
[2026-01-17 23:23:21] [ERROR] Traceback (most recent call last):
[2026-01-17 23:23:21] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/bin/vllm", line 7, in <module>
[2026-01-17 23:23:21] [INFO] sys.exit(main())
[2026-01-17 23:23:21] [INFO] ^^^^^^
[2026-01-17 23:23:21] [INFO] File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/main.py", line 70, in main
[2026-01-17 23:23:21] [INFO] cmds[args.subparser].validate(args)
[2026-01-17 23:23:21] [INFO] File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/serve.py", line 63, in validate
[2026-01-17 23:23:21] [INFO] validate_parsed_serve_args(args)
[2026-01-17 23:23:21] [INFO] File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/cli_args.py", line 307, in validate_parsed_serve_args
[2026-01-17 23:23:21] [INFO] validate_chat_template(args.chat_template)
[2026-01-17 23:23:21] [INFO] File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/chat_utils.py", line 1199, in validate_chat_template
[2026-01-17 23:23:21] [INFO] raise ValueError(
[2026-01-17 23:23:21] [INFO] ValueError: The supplied chat template string (/mnt/AI-Acer4T/AI-Chat/models/minimax/minimax/MiniMax-M2.1-AWQ/chat_template.jinja) appears path-like, but doesn't exist! Tried: /mnt/AI-Acer4T/AI-Chat/models/minimax/minimax/MiniMax-M2.1-AWQ/chat_template.jinja and /mnt/AI-Acer4T/AI-Chat/models/minimax/minimax/MiniMax-M2.1-AWQ/chat_template.jinja
[2026-01-17 23:23:22] [INFO] nvitop监控已启动
[2026-01-17 23:23:41] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-17 23:23:41] [INFO] nvitop监控已启动
[2026-01-17 23:23:41] [INFO] nvitop监控已停止
[2026-01-17 23:24:03] [INFO] [0;36m(APIServer pid=262303)[0;0m INFO 01-17 23:24:03 [api_server.py:1278] vLLM API server version 0.14.0rc1.dev492+g19504ac07
[2026-01-17 23:24:03] [INFO] [0;36m(APIServer pid=262303)[0;0m INFO 01-17 23:24:03 [utils.py:254] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/minimax/MiniMax-M2.1-FP8-INT4-AWQ-M', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/AI-Acer4T/AI-Chat/models/minimax/MiniMax-M2.1-FP8-INT4-AWQ-M/chat_template.jinja', 'enable_auto_tool_choice': True, 'tool_call_parser': 'minimax_m2', 'model': '/mnt/AI-Acer4T/AI-Chat/models/minimax/MiniMax-M2.1-FP8-INT4-AWQ-M', 'trust_remote_code': True, 'max_model_len': 128000, 'quantization': 'awq', 'served_model_name': ['VLLM-MODEL'], 'reasoning_parser': 'minimax_m2_append_think', 'tensor_parallel_size': 2, 'all2all_backend': 'pplx', 'kv_cache_dtype': 'fp8', 'max_num_seqs': 256, 'enable_chunked_prefill': True, 'async_scheduling': True}
[2026-01-17 23:24:03] [INFO] [0;36m(APIServer pid=262303)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-17 23:24:03] [INFO] [0;36m(APIServer pid=262303)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-17 23:24:03] [INFO] [0;36m(APIServer pid=262303)[0;0m Config sanitized in configuration_minimax_m2.py
[2026-01-17 23:24:03] [INFO] [0;36m(APIServer pid=262303)[0;0m INFO 01-17 23:24:03 [model.py:528] Resolved architecture: MiniMaxM2ForCausalLM
[2026-01-17 23:24:03] [ERROR] [0;36m(APIServer pid=262303)[0;0m ERROR 01-17 23:24:03 [repo_utils.py:65] Error retrieving safetensors: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/mnt/AI-Acer4T/AI-Chat/models/minimax/MiniMax-M2.1-FP8-INT4-AWQ-M'. Use `repo_type` argument if needed., retrying 1 of 2
[2026-01-17 23:24:05] [ERROR] [0;36m(APIServer pid=262303)[0;0m ERROR 01-17 23:24:05 [repo_utils.py:63] Error retrieving safetensors: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/mnt/AI-Acer4T/AI-Chat/models/minimax/MiniMax-M2.1-FP8-INT4-AWQ-M'. Use `repo_type` argument if needed.
[2026-01-17 23:24:05] [INFO] [0;36m(APIServer pid=262303)[0;0m INFO 01-17 23:24:05 [model.py:1864] Downcasting torch.float32 to torch.bfloat16.
[2026-01-17 23:24:05] [INFO] [0;36m(APIServer pid=262303)[0;0m INFO 01-17 23:24:05 [model.py:1543] Using max model len 128000
[2026-01-17 23:24:06] [ERROR] [0;36m(APIServer pid=262303)[0;0m Traceback (most recent call last):
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/bin/vllm", line 7, in <module>
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m     sys.exit(main())
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m              ^^^^^^
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m     args.dispatch_function(args)
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m     uvloop.run(run_server(args))
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m     return __asyncio.run(
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m            ^^^^^^^^^^^^^^
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m     return runner.run(main)
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m     return self._loop.run_until_complete(task)
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m     return await main
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m            ^^^^^^^^^^
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 1325, in run_server
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 1344, in run_server_worker
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m     async with build_async_engine_client(
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m     return await anext(self.gen)
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 173, in build_async_engine_client
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m     return await anext(self.gen)
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 199, in build_async_engine_client_from_engine_args
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/engine/arg_utils.py", line 1365, in create_engine_config
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m     model_config = self.create_model_config()
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/engine/arg_utils.py", line 1220, in create_model_config
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m     return ModelConfig(
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m            ^^^^^^^^^^^^
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/pydantic/_internal/_dataclasses.py", line 121, in __init__
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m   Value error, Quantization method specified in the model config (compressed-tensors) does not match the quantization method specified in the `quantization` argument (awq). [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error
[2026-01-17 23:24:07] [INFO] nvitop监控已启动
[2026-01-17 23:25:37] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-17 23:25:37] [INFO] nvitop监控已启动
[2026-01-17 23:25:37] [INFO] nvitop监控已停止
[2026-01-17 23:26:02] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:26:02 [api_server.py:1278] vLLM API server version 0.14.0rc1.dev492+g19504ac07
[2026-01-17 23:26:02] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:26:02 [utils.py:254] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/minimax/MiniMax-M2.1-FP8-INT4-AWQ-M', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/AI-Acer4T/AI-Chat/models/minimax/MiniMax-M2.1-FP8-INT4-AWQ-M/chat_template.jinja', 'enable_auto_tool_choice': True, 'tool_call_parser': 'minimax_m2', 'model': '/mnt/AI-Acer4T/AI-Chat/models/minimax/MiniMax-M2.1-FP8-INT4-AWQ-M', 'trust_remote_code': True, 'max_model_len': 128000, 'served_model_name': ['VLLM-MODEL'], 'reasoning_parser': 'minimax_m2_append_think', 'tensor_parallel_size': 2, 'all2all_backend': 'pplx', 'kv_cache_dtype': 'fp8', 'max_num_seqs': 256, 'enable_chunked_prefill': True, 'async_scheduling': True}
[2026-01-17 23:26:02] [INFO] [0;36m(APIServer pid=262986)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-17 23:26:02] [INFO] [0;36m(APIServer pid=262986)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-17 23:26:02] [INFO] [0;36m(APIServer pid=262986)[0;0m Config sanitized in configuration_minimax_m2.py
[2026-01-17 23:26:03] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:26:03 [model.py:528] Resolved architecture: MiniMaxM2ForCausalLM
[2026-01-17 23:26:03] [ERROR] [0;36m(APIServer pid=262986)[0;0m ERROR 01-17 23:26:03 [repo_utils.py:65] Error retrieving safetensors: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/mnt/AI-Acer4T/AI-Chat/models/minimax/MiniMax-M2.1-FP8-INT4-AWQ-M'. Use `repo_type` argument if needed., retrying 1 of 2
[2026-01-17 23:26:05] [ERROR] [0;36m(APIServer pid=262986)[0;0m ERROR 01-17 23:26:05 [repo_utils.py:63] Error retrieving safetensors: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/mnt/AI-Acer4T/AI-Chat/models/minimax/MiniMax-M2.1-FP8-INT4-AWQ-M'. Use `repo_type` argument if needed.
[2026-01-17 23:26:05] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:26:05 [model.py:1864] Downcasting torch.float32 to torch.bfloat16.
[2026-01-17 23:26:05] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:26:05 [model.py:1543] Using max model len 128000
[2026-01-17 23:26:05] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:26:05 [cache.py:206] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor.
[2026-01-17 23:26:05] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:26:05 [scheduler.py:231] Chunked prefill is enabled with max_num_batched_tokens=8192.
[2026-01-17 23:26:05] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:26:05 [vllm.py:640] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-17 23:26:05] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:26:05 [vllm.py:645] Asynchronous scheduling is enabled.
[2026-01-17 23:26:30] [INFO] [0;36m(EngineCore_DP0 pid=263286)[0;0m INFO 01-17 23:26:30 [core.py:97] Initializing a V1 LLM engine (v0.14.0rc1.dev492+g19504ac07) with config: model='/mnt/AI-Acer4T/AI-Chat/models/minimax/MiniMax-M2.1-FP8-INT4-AWQ-M', speculative_config=None, tokenizer='/mnt/AI-Acer4T/AI-Chat/models/minimax/MiniMax-M2.1-FP8-INT4-AWQ-M', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=fp8, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='minimax_m2_append_think', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['+quant_fp8', 'none', '+quant_fp8'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': True, 'fuse_act_quant': True, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-17 23:26:30] [WARNING] [0;36m(EngineCore_DP0 pid=263286)[0;0m WARNING 01-17 23:26:30 [multiproc_executor.py:880] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-17 23:26:55] [INFO] INFO 01-17 23:26:55 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:57891 backend=nccl
[2026-01-17 23:26:55] [INFO] INFO 01-17 23:26:55 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:57891 backend=nccl
[2026-01-17 23:26:55] [INFO] INFO 01-17 23:26:55 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-17 23:26:56] [WARNING] WARNING 01-17 23:26:56 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-17 23:26:56] [WARNING] WARNING 01-17 23:26:56 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-17 23:26:56] [INFO] INFO 01-17 23:26:56 [parallel_state.py:1425] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[2026-01-17 23:26:56] [INFO] INFO 01-17 23:26:56 [parallel_state.py:1425] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
[2026-01-17 23:26:57] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m INFO 01-17 23:26:57 [gpu_model_runner.py:3789] Starting to load model /mnt/AI-Acer4T/AI-Chat/models/minimax/MiniMax-M2.1-FP8-INT4-AWQ-M...
[2026-01-17 23:27:01] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m INFO 01-17 23:27:01 [cuda.py:351] Using FLASHINFER attention backend out of potential backends: ('FLASHINFER', 'TRITON_ATTN')
[2026-01-17 23:27:01] [INFO] [0;36m(Worker_TP1 pid=263552)[0;0m INFO 01-17 23:27:01 [compressed_tensors_moe.py:184] Using CompressedTensorsWNA16MarlinMoEMethod
[2026-01-17 23:27:01] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m INFO 01-17 23:27:01 [compressed_tensors_moe.py:184] Using CompressedTensorsWNA16MarlinMoEMethod
[2026-01-17 23:27:02] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:02] [INFO] Loading safetensors checkpoint shards:   0% Completed | 0/125 [00:00<?, ?it/s]
[2026-01-17 23:27:04] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:04] [INFO] Loading safetensors checkpoint shards:   1% Completed | 1/125 [00:02<05:57,  2.88s/it]
[2026-01-17 23:27:05] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:05] [INFO] Loading safetensors checkpoint shards:   2% Completed | 2/125 [00:03<03:31,  1.72s/it]
[2026-01-17 23:27:07] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:07] [INFO] Loading safetensors checkpoint shards:   2% Completed | 3/125 [00:05<03:35,  1.77s/it]
[2026-01-17 23:27:08] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:08] [INFO] Loading safetensors checkpoint shards:   3% Completed | 4/125 [00:06<02:51,  1.42s/it]
[2026-01-17 23:27:10] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:10] [INFO] Loading safetensors checkpoint shards:   4% Completed | 5/125 [00:08<03:05,  1.55s/it]
[2026-01-17 23:27:11] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:11] [INFO] Loading safetensors checkpoint shards:   5% Completed | 6/125 [00:09<02:35,  1.31s/it]
[2026-01-17 23:27:12] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:12] [INFO] Loading safetensors checkpoint shards:   6% Completed | 7/125 [00:09<02:17,  1.16s/it]
[2026-01-17 23:27:13] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:13] [INFO] Loading safetensors checkpoint shards:   6% Completed | 8/125 [00:11<02:43,  1.40s/it]
[2026-01-17 23:27:14] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:14] [INFO] Loading safetensors checkpoint shards:   7% Completed | 9/125 [00:12<02:28,  1.28s/it]
[2026-01-17 23:27:16] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:16] [INFO] Loading safetensors checkpoint shards:   8% Completed | 10/125 [00:14<02:52,  1.50s/it]
[2026-01-17 23:27:17] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:17] [INFO] Loading safetensors checkpoint shards:   9% Completed | 11/125 [00:15<02:27,  1.29s/it]
[2026-01-17 23:27:19] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:19] [INFO] Loading safetensors checkpoint shards:  10% Completed | 12/125 [00:17<02:45,  1.46s/it]
[2026-01-17 23:27:20] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:20] [INFO] Loading safetensors checkpoint shards:  10% Completed | 13/125 [00:18<02:26,  1.31s/it]
[2026-01-17 23:27:22] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:22] [INFO] Loading safetensors checkpoint shards:  11% Completed | 14/125 [00:20<02:50,  1.54s/it]
[2026-01-17 23:27:23] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:23] [INFO] Loading safetensors checkpoint shards:  12% Completed | 15/125 [00:21<02:32,  1.39s/it]
[2026-01-17 23:27:25] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:25] [INFO] Loading safetensors checkpoint shards:  13% Completed | 16/125 [00:23<02:54,  1.60s/it]
[2026-01-17 23:27:26] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:26] [INFO] Loading safetensors checkpoint shards:  14% Completed | 17/125 [00:24<02:28,  1.37s/it]
[2026-01-17 23:27:28] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:28] [INFO] Loading safetensors checkpoint shards:  14% Completed | 18/125 [00:26<02:38,  1.49s/it]
[2026-01-17 23:27:30] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:30] [INFO] Loading safetensors checkpoint shards:  15% Completed | 19/125 [00:28<02:50,  1.60s/it]
[2026-01-17 23:27:31] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:31] [INFO] Loading safetensors checkpoint shards:  16% Completed | 20/125 [00:29<02:29,  1.42s/it]
[2026-01-17 23:27:33] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:33] [INFO] Loading safetensors checkpoint shards:  17% Completed | 21/125 [00:31<02:49,  1.63s/it]
[2026-01-17 23:27:34] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:34] [INFO] Loading safetensors checkpoint shards:  18% Completed | 22/125 [00:32<02:27,  1.44s/it]
[2026-01-17 23:27:36] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:36] [INFO] Loading safetensors checkpoint shards:  18% Completed | 23/125 [00:34<02:43,  1.60s/it]
[2026-01-17 23:27:37] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:37] [INFO] Loading safetensors checkpoint shards:  19% Completed | 24/125 [00:35<02:19,  1.38s/it]
[2026-01-17 23:27:39] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:39] [INFO] Loading safetensors checkpoint shards:  20% Completed | 25/125 [00:37<02:36,  1.57s/it]
[2026-01-17 23:27:40] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:40] [INFO] Loading safetensors checkpoint shards:  21% Completed | 26/125 [00:38<02:19,  1.41s/it]
[2026-01-17 23:27:42] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:42] [INFO] Loading safetensors checkpoint shards:  22% Completed | 27/125 [00:40<02:38,  1.61s/it]
[2026-01-17 23:27:43] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:43] [INFO] Loading safetensors checkpoint shards:  22% Completed | 28/125 [00:41<02:18,  1.42s/it]
[2026-01-17 23:27:45] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:45] [INFO] Loading safetensors checkpoint shards:  23% Completed | 29/125 [00:43<02:36,  1.63s/it]
[2026-01-17 23:27:46] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:46] [INFO] Loading safetensors checkpoint shards:  24% Completed | 30/125 [00:44<02:14,  1.42s/it]
[2026-01-17 23:27:48] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:48] [INFO] Loading safetensors checkpoint shards:  25% Completed | 31/125 [00:46<02:24,  1.54s/it]
[2026-01-17 23:27:50] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:50] [INFO] Loading safetensors checkpoint shards:  26% Completed | 32/125 [00:48<02:39,  1.71s/it]
[2026-01-17 23:27:51] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:51] [INFO] Loading safetensors checkpoint shards:  26% Completed | 33/125 [00:49<02:15,  1.47s/it]
[2026-01-17 23:27:53] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:53] [INFO] Loading safetensors checkpoint shards:  27% Completed | 34/125 [00:51<02:31,  1.66s/it]
[2026-01-17 23:27:54] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:54] [INFO] Loading safetensors checkpoint shards:  28% Completed | 35/125 [00:52<02:15,  1.51s/it]
[2026-01-17 23:27:56] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:56] [INFO] Loading safetensors checkpoint shards:  29% Completed | 36/125 [00:54<02:34,  1.74s/it]
[2026-01-17 23:27:57] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:57] [INFO] Loading safetensors checkpoint shards:  30% Completed | 37/125 [00:55<02:13,  1.52s/it]
[2026-01-17 23:27:59] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:59] [INFO] Loading safetensors checkpoint shards:  30% Completed | 38/125 [00:57<02:27,  1.70s/it]
[2026-01-17 23:28:00] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:00] [INFO] Loading safetensors checkpoint shards:  31% Completed | 39/125 [00:58<02:05,  1.46s/it]
[2026-01-17 23:28:02] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:02] [INFO] Loading safetensors checkpoint shards:  32% Completed | 40/125 [01:00<02:11,  1.55s/it]
[2026-01-17 23:28:03] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:03] [INFO] Loading safetensors checkpoint shards:  33% Completed | 41/125 [01:01<01:54,  1.36s/it]
[2026-01-17 23:28:05] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:05] [INFO] Loading safetensors checkpoint shards:  34% Completed | 42/125 [01:03<02:06,  1.53s/it]
[2026-01-17 23:28:06] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:06] [INFO] Loading safetensors checkpoint shards:  34% Completed | 43/125 [01:04<01:50,  1.34s/it]
[2026-01-17 23:28:07] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:07] [INFO] Loading safetensors checkpoint shards:  35% Completed | 44/125 [01:05<01:37,  1.20s/it]
[2026-01-17 23:28:09] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:09] [INFO] Loading safetensors checkpoint shards:  36% Completed | 45/125 [01:07<01:53,  1.42s/it]
[2026-01-17 23:28:09] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:09] [INFO] Loading safetensors checkpoint shards:  37% Completed | 46/125 [01:07<01:39,  1.26s/it]
[2026-01-17 23:28:11] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:11] [INFO] Loading safetensors checkpoint shards:  38% Completed | 47/125 [01:09<01:53,  1.46s/it]
[2026-01-17 23:28:12] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:12] [INFO] Loading safetensors checkpoint shards:  38% Completed | 48/125 [01:10<01:39,  1.29s/it]
[2026-01-17 23:28:14] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:14] [INFO] Loading safetensors checkpoint shards:  39% Completed | 49/125 [01:12<01:52,  1.48s/it]
[2026-01-17 23:28:15] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:15] [INFO] Loading safetensors checkpoint shards:  40% Completed | 50/125 [01:13<01:37,  1.30s/it]
[2026-01-17 23:28:17] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:17] [INFO] Loading safetensors checkpoint shards:  41% Completed | 51/125 [01:15<01:49,  1.49s/it]
[2026-01-17 23:28:18] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:18] [INFO] Loading safetensors checkpoint shards:  42% Completed | 52/125 [01:16<01:35,  1.31s/it]
[2026-01-17 23:28:20] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:20] [INFO] Loading safetensors checkpoint shards:  42% Completed | 53/125 [01:18<01:47,  1.49s/it]
[2026-01-17 23:28:21] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:21] [INFO] Loading safetensors checkpoint shards:  43% Completed | 54/125 [01:19<01:31,  1.29s/it]
[2026-01-17 23:28:22] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:22] [INFO] Loading safetensors checkpoint shards:  44% Completed | 55/125 [01:20<01:41,  1.45s/it]
[2026-01-17 23:28:23] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:23] [INFO] Loading safetensors checkpoint shards:  45% Completed | 56/125 [01:21<01:29,  1.30s/it]
[2026-01-17 23:28:25] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:25] [INFO] Loading safetensors checkpoint shards:  46% Completed | 57/125 [01:23<01:40,  1.48s/it]
[2026-01-17 23:28:26] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:26] [INFO] Loading safetensors checkpoint shards:  46% Completed | 58/125 [01:24<01:26,  1.30s/it]
[2026-01-17 23:28:27] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:27] [INFO] Loading safetensors checkpoint shards:  47% Completed | 59/125 [01:25<01:16,  1.16s/it]
[2026-01-17 23:28:29] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:29] [INFO] Loading safetensors checkpoint shards:  48% Completed | 60/125 [01:27<01:30,  1.40s/it]
[2026-01-17 23:28:31] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:31] [INFO] Loading safetensors checkpoint shards:  49% Completed | 61/125 [01:29<01:40,  1.57s/it]
[2026-01-17 23:28:32] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:32] [INFO] Loading safetensors checkpoint shards:  50% Completed | 62/125 [01:30<01:27,  1.38s/it]
[2026-01-17 23:28:33] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:33] [INFO] Loading safetensors checkpoint shards:  50% Completed | 63/125 [01:31<01:17,  1.25s/it]
[2026-01-17 23:28:35] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:35] [INFO] Loading safetensors checkpoint shards:  51% Completed | 64/125 [01:33<01:29,  1.46s/it]
[2026-01-17 23:28:36] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:36] [INFO] Loading safetensors checkpoint shards:  52% Completed | 65/125 [01:34<01:18,  1.31s/it]
[2026-01-17 23:28:37] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:37] [INFO] Loading safetensors checkpoint shards:  53% Completed | 66/125 [01:35<01:10,  1.19s/it]
[2026-01-17 23:28:39] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:39] [INFO] Loading safetensors checkpoint shards:  54% Completed | 67/125 [01:36<01:21,  1.40s/it]
[2026-01-17 23:28:39] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:39] [INFO] Loading safetensors checkpoint shards:  54% Completed | 68/125 [01:37<01:10,  1.23s/it]
[2026-01-17 23:28:41] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:41] [INFO] Loading safetensors checkpoint shards:  55% Completed | 69/125 [01:39<01:18,  1.40s/it]
[2026-01-17 23:28:42] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:42] [INFO] Loading safetensors checkpoint shards:  56% Completed | 70/125 [01:40<01:07,  1.23s/it]
[2026-01-17 23:28:44] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:44] [INFO] Loading safetensors checkpoint shards:  57% Completed | 71/125 [01:42<01:14,  1.39s/it]
[2026-01-17 23:28:45] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:45] [INFO] Loading safetensors checkpoint shards:  58% Completed | 72/125 [01:43<01:04,  1.21s/it]
[2026-01-17 23:28:46] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:46] [INFO] Loading safetensors checkpoint shards:  58% Completed | 73/125 [01:44<01:11,  1.38s/it]
[2026-01-17 23:28:47] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:47] [INFO] Loading safetensors checkpoint shards:  59% Completed | 74/125 [01:45<01:01,  1.21s/it]
[2026-01-17 23:28:49] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:49] [INFO] Loading safetensors checkpoint shards:  60% Completed | 75/125 [01:47<01:11,  1.43s/it]
[2026-01-17 23:28:50] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:50] [INFO] Loading safetensors checkpoint shards:  61% Completed | 76/125 [01:48<01:02,  1.27s/it]
[2026-01-17 23:28:52] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:52] [INFO] Loading safetensors checkpoint shards:  62% Completed | 77/125 [01:50<01:08,  1.43s/it]
[2026-01-17 23:28:54] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:54] [INFO] Loading safetensors checkpoint shards:  62% Completed | 78/125 [01:52<01:13,  1.57s/it]
[2026-01-17 23:28:55] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:55] [INFO] Loading safetensors checkpoint shards:  63% Completed | 79/125 [01:53<01:02,  1.36s/it]
[2026-01-17 23:28:58] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:58] [INFO] Loading safetensors checkpoint shards:  64% Completed | 80/125 [01:56<01:24,  1.88s/it]
[2026-01-17 23:28:59] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:59] [INFO] Loading safetensors checkpoint shards:  65% Completed | 81/125 [01:57<01:09,  1.59s/it]
[2026-01-17 23:29:01] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:01] [INFO] Loading safetensors checkpoint shards:  66% Completed | 82/125 [01:58<01:12,  1.69s/it]
[2026-01-17 23:29:01] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:01] [INFO] Loading safetensors checkpoint shards:  66% Completed | 83/125 [01:59<01:01,  1.47s/it]
[2026-01-17 23:29:03] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:03] [INFO] Loading safetensors checkpoint shards:  67% Completed | 84/125 [02:01<01:06,  1.63s/it]
[2026-01-17 23:29:04] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:04] [INFO] Loading safetensors checkpoint shards:  68% Completed | 85/125 [02:02<00:56,  1.42s/it]
[2026-01-17 23:29:06] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:06] [INFO] Loading safetensors checkpoint shards:  69% Completed | 86/125 [02:04<01:01,  1.58s/it]
[2026-01-17 23:29:07] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:07] [INFO] Loading safetensors checkpoint shards:  70% Completed | 87/125 [02:05<00:52,  1.39s/it]
[2026-01-17 23:29:09] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:09] [INFO] Loading safetensors checkpoint shards:  70% Completed | 88/125 [02:07<00:57,  1.56s/it]
[2026-01-17 23:29:10] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:10] [INFO] Loading safetensors checkpoint shards:  71% Completed | 89/125 [02:08<00:49,  1.38s/it]
[2026-01-17 23:29:12] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:12] [INFO] Loading safetensors checkpoint shards:  72% Completed | 90/125 [02:10<00:54,  1.56s/it]
[2026-01-17 23:29:14] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:14] [INFO] Loading safetensors checkpoint shards:  73% Completed | 91/125 [02:12<00:57,  1.69s/it]
[2026-01-17 23:29:15] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:15] [INFO] Loading safetensors checkpoint shards:  74% Completed | 92/125 [02:13<00:48,  1.46s/it]
[2026-01-17 23:29:17] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:17] [INFO] Loading safetensors checkpoint shards:  74% Completed | 93/125 [02:15<00:51,  1.62s/it]
[2026-01-17 23:29:18] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:18] [INFO] Loading safetensors checkpoint shards:  75% Completed | 94/125 [02:16<00:43,  1.42s/it]
[2026-01-17 23:29:20] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:20] [INFO] Loading safetensors checkpoint shards:  76% Completed | 95/125 [02:18<00:45,  1.53s/it]
[2026-01-17 23:29:21] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:21] [INFO] Loading safetensors checkpoint shards:  77% Completed | 96/125 [02:19<00:38,  1.32s/it]
[2026-01-17 23:29:22] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:22] [INFO] Loading safetensors checkpoint shards:  78% Completed | 97/125 [02:20<00:40,  1.45s/it]
[2026-01-17 23:29:23] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:23] [INFO] Loading safetensors checkpoint shards:  78% Completed | 98/125 [02:21<00:34,  1.26s/it]
[2026-01-17 23:29:25] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:25] [INFO] Loading safetensors checkpoint shards:  79% Completed | 99/125 [02:23<00:36,  1.41s/it]
[2026-01-17 23:29:26] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:26] [INFO] Loading safetensors checkpoint shards:  80% Completed | 100/125 [02:24<00:30,  1.24s/it]
[2026-01-17 23:29:28] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:28] [INFO] Loading safetensors checkpoint shards:  81% Completed | 101/125 [02:26<00:33,  1.39s/it]
[2026-01-17 23:29:29] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:29] [INFO] Loading safetensors checkpoint shards:  82% Completed | 102/125 [02:26<00:28,  1.25s/it]
[2026-01-17 23:29:29] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:29] [INFO] Loading safetensors checkpoint shards:  82% Completed | 103/125 [02:27<00:25,  1.16s/it]
[2026-01-17 23:29:31] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:31] [INFO] Loading safetensors checkpoint shards:  83% Completed | 104/125 [02:29<00:29,  1.40s/it]
[2026-01-17 23:29:32] [INFO] Executed command in new terminal (gnome-terminal): nvitop
[2026-01-17 23:29:32] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:32] [INFO] Loading safetensors checkpoint shards:  84% Completed | 105/125 [02:30<00:25,  1.27s/it]
[2026-01-17 23:29:34] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:34] [INFO] Loading safetensors checkpoint shards:  85% Completed | 106/125 [02:32<00:28,  1.49s/it]
[2026-01-17 23:29:35] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:35] [INFO] Loading safetensors checkpoint shards:  86% Completed | 107/125 [02:33<00:23,  1.33s/it]
[2026-01-17 23:29:37] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:37] [INFO] Loading safetensors checkpoint shards:  86% Completed | 108/125 [02:35<00:26,  1.53s/it]
[2026-01-17 23:29:38] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:38] [INFO] Loading safetensors checkpoint shards:  87% Completed | 109/125 [02:36<00:21,  1.36s/it]
[2026-01-17 23:29:40] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:40] [INFO] Loading safetensors checkpoint shards:  88% Completed | 110/125 [02:38<00:23,  1.53s/it]
[2026-01-17 23:29:41] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:41] [INFO] Loading safetensors checkpoint shards:  89% Completed | 111/125 [02:39<00:19,  1.36s/it]
[2026-01-17 23:29:43] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:43] [INFO] Loading safetensors checkpoint shards:  90% Completed | 112/125 [02:41<00:20,  1.55s/it]
[2026-01-17 23:29:44] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:44] [INFO] Loading safetensors checkpoint shards:  90% Completed | 113/125 [02:42<00:16,  1.34s/it]
[2026-01-17 23:29:46] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:46] [INFO] Loading safetensors checkpoint shards:  91% Completed | 114/125 [02:44<00:16,  1.46s/it]
[2026-01-17 23:29:48] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:48] [INFO] Loading safetensors checkpoint shards:  92% Completed | 115/125 [02:46<00:15,  1.55s/it]
[2026-01-17 23:29:48] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:48] [INFO] Loading safetensors checkpoint shards:  93% Completed | 116/125 [02:46<00:12,  1.34s/it]
[2026-01-17 23:29:50] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:50] [INFO] Loading safetensors checkpoint shards:  94% Completed | 117/125 [02:48<00:11,  1.48s/it]
[2026-01-17 23:29:51] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:51] [INFO] Loading safetensors checkpoint shards:  94% Completed | 118/125 [02:49<00:09,  1.29s/it]
[2026-01-17 23:29:53] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:53] [INFO] Loading safetensors checkpoint shards:  95% Completed | 119/125 [02:51<00:08,  1.43s/it]
[2026-01-17 23:29:54] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:54] [INFO] Loading safetensors checkpoint shards:  96% Completed | 120/125 [02:52<00:06,  1.25s/it]
[2026-01-17 23:29:55] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:55] [INFO] Loading safetensors checkpoint shards:  97% Completed | 121/125 [02:53<00:05,  1.41s/it]
[2026-01-17 23:29:56] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:56] [INFO] Loading safetensors checkpoint shards:  98% Completed | 122/125 [02:54<00:03,  1.24s/it]
[2026-01-17 23:29:58] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:58] [INFO] Loading safetensors checkpoint shards:  98% Completed | 123/125 [02:56<00:02,  1.40s/it]
[2026-01-17 23:29:59] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:59] [INFO] Loading safetensors checkpoint shards:  99% Completed | 124/125 [02:57<00:01,  1.24s/it]
[2026-01-17 23:30:00] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:30:00] [INFO] Loading safetensors checkpoint shards: 100% Completed | 125/125 [02:58<00:00,  1.20s/it]
[2026-01-17 23:30:00] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:30:00] [INFO] Loading safetensors checkpoint shards: 100% Completed | 125/125 [02:58<00:00,  1.43s/it]
[2026-01-17 23:30:00] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:30:00] [WARNING] [0;36m(Worker_TP1 pid=263552)[0;0m WARNING 01-17 23:30:00 [kv_cache.py:90] Checkpoint does not provide a q scaling factor. Setting it to k_scale. This only matters for FP8 Attention backends (flash-attn or flashinfer).
[2026-01-17 23:30:00] [WARNING] [0;36m(Worker_TP1 pid=263552)[0;0m WARNING 01-17 23:30:00 [kv_cache.py:104] Using KV cache scaling factor 1.0 for fp8_e4m3. If this is unintended, verify that k/v_scale scaling factors are properly set in the checkpoint.
[2026-01-17 23:30:00] [WARNING] [0;36m(Worker_TP1 pid=263552)[0;0m WARNING 01-17 23:30:00 [kv_cache.py:143] Using uncalibrated q_scale 1.0 and/or prob_scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure q/prob scaling factors are available in the fp8 checkpoint.
[2026-01-17 23:30:00] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m INFO 01-17 23:30:00 [default_loader.py:291] Loading weights took 178.48 seconds
[2026-01-17 23:30:00] [WARNING] [0;36m(Worker_TP0 pid=263551)[0;0m WARNING 01-17 23:30:00 [kv_cache.py:90] Checkpoint does not provide a q scaling factor. Setting it to k_scale. This only matters for FP8 Attention backends (flash-attn or flashinfer).
[2026-01-17 23:30:00] [WARNING] [0;36m(Worker_TP0 pid=263551)[0;0m WARNING 01-17 23:30:00 [kv_cache.py:104] Using KV cache scaling factor 1.0 for fp8_e4m3. If this is unintended, verify that k/v_scale scaling factors are properly set in the checkpoint.
[2026-01-17 23:30:00] [WARNING] [0;36m(Worker_TP0 pid=263551)[0;0m WARNING 01-17 23:30:00 [kv_cache.py:143] Using uncalibrated q_scale 1.0 and/or prob_scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure q/prob scaling factors are available in the fp8 checkpoint.
[2026-01-17 23:30:02] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m INFO 01-17 23:30:02 [gpu_model_runner.py:3886] Model loading took 61.47 GiB memory and 184.997695 seconds
[2026-01-17 23:30:20] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m INFO 01-17 23:30:20 [backends.py:644] Using cache directory: /home/wuwen/.cache/vllm/torch_compile_cache/5655bae0ef/rank_0_0/backbone for vLLM's torch.compile
[2026-01-17 23:30:20] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m INFO 01-17 23:30:20 [backends.py:704] Dynamo bytecode transform time: 16.95 s
[2026-01-17 23:30:37] [INFO] [0;36m(Worker_TP1 pid=263552)[0;0m INFO 01-17 23:30:37 [backends.py:261] Cache the graph of compile range (1, 8192) for later use
[2026-01-17 23:30:37] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m INFO 01-17 23:30:37 [backends.py:261] Cache the graph of compile range (1, 8192) for later use
[2026-01-17 23:30:38] [INFO] [0;36m(Worker_TP1 pid=263552)[0;0m /mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:312: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
[2026-01-17 23:30:38] [INFO] [0;36m(Worker_TP1 pid=263552)[0;0m   warnings.warn(
[2026-01-17 23:30:38] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m /mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:312: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
[2026-01-17 23:30:38] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m   warnings.warn(
[2026-01-17 23:31:02] [INFO] [0;36m(EngineCore_DP0 pid=263286)[0;0m INFO 01-17 23:31:02 [shm_broadcast.py:542] No available shared memory broadcast block found in 60 seconds. This typically happens when some processes are hanging or doing some time-consuming work (e.g. compilation, weight/kv cache quantization).
[2026-01-17 23:32:02] [INFO] [0;36m(EngineCore_DP0 pid=263286)[0;0m INFO 01-17 23:32:02 [shm_broadcast.py:542] No available shared memory broadcast block found in 60 seconds. This typically happens when some processes are hanging or doing some time-consuming work (e.g. compilation, weight/kv cache quantization).
[2026-01-17 23:32:30] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m INFO 01-17 23:32:30 [backends.py:278] Compiling a graph for compile range (1, 8192) takes 115.33 s
[2026-01-17 23:32:30] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m INFO 01-17 23:32:30 [monitor.py:34] torch.compile takes 132.28 s in total
[2026-01-17 23:32:31] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m INFO 01-17 23:32:31 [gpu_worker.py:358] Available KV cache memory: 22.04 GiB
[2026-01-17 23:32:32] [INFO] [0;36m(EngineCore_DP0 pid=263286)[0;0m INFO 01-17 23:32:32 [kv_cache_utils.py:1305] GPU KV cache size: 372,768 tokens
[2026-01-17 23:32:32] [INFO] [0;36m(EngineCore_DP0 pid=263286)[0;0m INFO 01-17 23:32:32 [kv_cache_utils.py:1310] Maximum concurrency for 128,000 tokens per request: 2.91x
[2026-01-17 23:32:32] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m 2026-01-17 23:32:32,361 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[2026-01-17 23:32:32] [INFO] [0;36m(Worker_TP1 pid=263552)[0;0m 2026-01-17 23:32:32,361 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[2026-01-17 23:32:32] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m 2026-01-17 23:32:32,517 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[2026-01-17 23:32:32] [INFO] [0;36m(Worker_TP1 pid=263552)[0;0m 2026-01-17 23:32:32,517 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[2026-01-17 23:32:32] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m INFO 01-17 23:32:32 [kernel_warmup.py:64] Warming up FlashInfer attention.
[2026-01-17 23:32:32] [INFO] [0;36m(Worker_TP1 pid=263552)[0;0m INFO 01-17 23:32:32 [kernel_warmup.py:64] Warming up FlashInfer attention.
[2026-01-17 23:32:33] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:32:34] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
[2026-01-17 23:32:34] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:05,  9.12it/s]
[2026-01-17 23:32:34] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:05,  9.27it/s]
[2026-01-17 23:32:34] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:05,  9.32it/s]
[2026-01-17 23:32:34] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:00<00:04,  9.48it/s]
[2026-01-17 23:32:34] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:00<00:04,  9.52it/s]
[2026-01-17 23:32:34] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:04,  9.33it/s]
[2026-01-17 23:32:34] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▎        | 7/51 [00:00<00:04,  8.92it/s]
[2026-01-17 23:32:34] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:00<00:04,  9.10it/s]
[2026-01-17 23:32:35] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:00<00:04,  9.19it/s]
[2026-01-17 23:32:35] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|█▉        | 10/51 [00:01<00:04,  9.21it/s]
[2026-01-17 23:32:35] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:01<00:04,  9.23it/s]
[2026-01-17 23:32:35] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:01<00:04,  9.10it/s]
[2026-01-17 23:32:35] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:01<00:04,  9.13it/s]
[2026-01-17 23:32:35] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:01<00:04,  9.21it/s]
[2026-01-17 23:32:35] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:01<00:03,  9.34it/s]
[2026-01-17 23:32:35] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 16/51 [00:01<00:03,  9.44it/s]
[2026-01-17 23:32:35] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:01<00:03,  9.45it/s]
[2026-01-17 23:32:36] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:01<00:03,  9.47it/s]
[2026-01-17 23:32:36] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 19/51 [00:02<00:03,  9.43it/s]
[2026-01-17 23:32:36] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:02<00:03,  9.39it/s]
[2026-01-17 23:32:36] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:02<00:03,  9.36it/s]
[2026-01-17 23:32:36] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 22/51 [00:02<00:03,  9.38it/s]
[2026-01-17 23:32:36] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:02<00:02,  9.38it/s]
[2026-01-17 23:32:36] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:02<00:02,  9.43it/s]
[2026-01-17 23:32:36] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 25/51 [00:02<00:02,  9.35it/s]
[2026-01-17 23:32:36] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:02<00:02,  9.37it/s]
[2026-01-17 23:32:36] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:02<00:02,  9.34it/s]
[2026-01-17 23:32:37] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:03<00:02,  9.31it/s]
[2026-01-17 23:32:37] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:03<00:02,  9.30it/s]
[2026-01-17 23:32:37] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:03<00:02,  9.28it/s]
[2026-01-17 23:32:37] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 31/51 [00:03<00:02,  9.19it/s]
[2026-01-17 23:32:37] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:03<00:02,  9.23it/s]
[2026-01-17 23:32:37] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:03<00:01,  9.23it/s]
[2026-01-17 23:32:37] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:03<00:01,  9.25it/s]
[2026-01-17 23:32:37] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:03<00:01,  9.26it/s]
[2026-01-17 23:32:37] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:03<00:01,  9.25it/s]
[2026-01-17 23:32:38] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 37/51 [00:03<00:01,  9.29it/s]
[2026-01-17 23:32:38] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:04<00:01,  9.29it/s]
[2026-01-17 23:32:38] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▋  | 39/51 [00:04<00:01,  9.28it/s]
[2026-01-17 23:32:38] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:04<00:01,  9.29it/s]
[2026-01-17 23:32:38] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:04<00:01,  9.28it/s]
[2026-01-17 23:32:38] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:04<00:00,  9.24it/s]
[2026-01-17 23:32:38] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 43/51 [00:04<00:00,  9.18it/s]
[2026-01-17 23:32:38] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:04<00:00,  9.15it/s]
[2026-01-17 23:32:38] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 45/51 [00:04<00:00,  9.23it/s]
[2026-01-17 23:32:39] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|█████████ | 46/51 [00:04<00:00,  9.26it/s]
[2026-01-17 23:32:39] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:05<00:00,  9.12it/s]
[2026-01-17 23:32:39] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 48/51 [00:05<00:00,  9.10it/s]
[2026-01-17 23:32:39] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▌| 49/51 [00:05<00:00,  9.18it/s]
[2026-01-17 23:32:39] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:05<00:00,  9.13it/s]
[2026-01-17 23:32:39] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:05<00:00,  4.42it/s]
[2026-01-17 23:32:39] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:05<00:00,  8.65it/s]
[2026-01-17 23:32:39] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:32:39] [INFO] Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
[2026-01-17 23:32:40] [INFO] Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:03,  8.87it/s]
[2026-01-17 23:32:40] [INFO] Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:00<00:03,  8.94it/s]
[2026-01-17 23:32:40] [INFO] Capturing CUDA graphs (decode, FULL):   9%|▊         | 3/35 [00:00<00:03,  9.02it/s]
[2026-01-17 23:32:40] [INFO] Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:03,  9.03it/s]
[2026-01-17 23:32:40] [INFO] Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:00<00:03,  9.12it/s]
[2026-01-17 23:32:40] [INFO] Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:00<00:03,  8.98it/s]
[2026-01-17 23:32:40] [INFO] Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:00<00:03,  8.91it/s]
[2026-01-17 23:32:40] [INFO] Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:00<00:03,  8.92it/s]
[2026-01-17 23:32:40] [INFO] Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:01<00:02,  9.00it/s]
[2026-01-17 23:32:41] [INFO] Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:01<00:02,  9.05it/s]
[2026-01-17 23:32:41] [INFO] Capturing CUDA graphs (decode, FULL):  31%|███▏      | 11/35 [00:01<00:02,  9.09it/s]
[2026-01-17 23:32:41] [INFO] Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:01<00:02,  9.09it/s]
[2026-01-17 23:32:41] [INFO] Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:01<00:02,  9.11it/s]
[2026-01-17 23:32:41] [INFO] Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:01<00:02,  9.09it/s]
[2026-01-17 23:32:41] [INFO] Capturing CUDA graphs (decode, FULL):  43%|████▎     | 15/35 [00:01<00:02,  9.12it/s]
[2026-01-17 23:32:41] [INFO] Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:01<00:02,  9.11it/s]
[2026-01-17 23:32:41] [INFO] Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:01<00:01,  9.07it/s]
[2026-01-17 23:32:41] [INFO] Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:01<00:01,  9.03it/s]
[2026-01-17 23:32:42] [INFO] Capturing CUDA graphs (decode, FULL):  54%|█████▍    | 19/35 [00:02<00:01,  9.08it/s]
[2026-01-17 23:32:42] [INFO] Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:02<00:01,  9.10it/s]
[2026-01-17 23:32:42] [INFO] Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:02<00:01,  9.09it/s]
[2026-01-17 23:32:42] [INFO] Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:02<00:01,  9.12it/s]
[2026-01-17 23:32:42] [INFO] Capturing CUDA graphs (decode, FULL):  66%|██████▌   | 23/35 [00:02<00:01,  9.08it/s]
[2026-01-17 23:32:42] [INFO] Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:02<00:01,  9.06it/s]
[2026-01-17 23:32:42] [INFO] Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:02<00:01,  9.04it/s]
[2026-01-17 23:32:42] [INFO] Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:02<00:00,  9.08it/s]
[2026-01-17 23:32:42] [INFO] Capturing CUDA graphs (decode, FULL):  77%|███████▋  | 27/35 [00:02<00:00,  9.14it/s]
[2026-01-17 23:32:43] [INFO] Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:03<00:00,  9.12it/s]
[2026-01-17 23:32:43] [INFO] Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:03<00:00,  9.16it/s]
[2026-01-17 23:32:43] [INFO] Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:03<00:00,  9.18it/s]
[2026-01-17 23:32:43] [INFO] Capturing CUDA graphs (decode, FULL):  89%|████████▊ | 31/35 [00:03<00:00,  9.21it/s]
[2026-01-17 23:32:43] [INFO] Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:03<00:00,  9.22it/s]
[2026-01-17 23:32:43] [INFO] Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:03<00:00,  9.20it/s]
[2026-01-17 23:32:43] [INFO] Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:03<00:00,  9.25it/s]
[2026-01-17 23:32:43] [INFO] Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:03<00:00,  9.03it/s]
[2026-01-17 23:32:43] [INFO] Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:03<00:00,  9.08it/s]
[2026-01-17 23:32:43] [INFO] [0;36m(Worker_TP1 pid=263552)[0;0m INFO 01-17 23:32:43 [custom_all_reduce.py:216] Registering 15958 cuda graph addresses
[2026-01-17 23:32:43] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m INFO 01-17 23:32:43 [custom_all_reduce.py:216] Registering 15958 cuda graph addresses
[2026-01-17 23:32:44] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m INFO 01-17 23:32:44 [gpu_model_runner.py:4837] Graph capturing finished in 11 secs, took 0.66 GiB
[2026-01-17 23:32:44] [INFO] [0;36m(EngineCore_DP0 pid=263286)[0;0m INFO 01-17 23:32:44 [core.py:273] init engine (profile, create kv cache, warmup model) took 161.61 seconds
[2026-01-17 23:32:45] [INFO] [0;36m(EngineCore_DP0 pid=263286)[0;0m INFO 01-17 23:32:45 [core.py:186] Batch queue is enabled with size 2
[2026-01-17 23:32:45] [INFO] [0;36m(EngineCore_DP0 pid=263286)[0;0m INFO 01-17 23:32:45 [vllm.py:645] Asynchronous scheduling is enabled.
[2026-01-17 23:32:45] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:45 [api_server.py:1020] Supported tasks: ['generate']
[2026-01-17 23:32:45] [INFO] [0;36m(APIServer pid=262986)[0;0m Config sanitized in configuration_minimax_m2.py
[2026-01-17 23:32:46] [WARNING] [0;36m(APIServer pid=262986)[0;0m WARNING 01-17 23:32:46 [model.py:1356] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [serving_responses.py:224] Using default chat sampling params from model: {'top_k': 40, 'top_p': 0.95}
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [serving_engine.py:271] "auto" tool choice has been enabled.
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [serving_engine.py:271] "auto" tool choice has been enabled.
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [serving_chat.py:146] Using default chat sampling params from model: {'top_k': 40, 'top_p': 0.95}
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [serving_chat.py:182] Warming up chat template processing...
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [chat_utils.py:599] Detected the chat template content format to be 'openai'. You can set `--chat-template-content-format` to override this.
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [serving_chat.py:218] Chat template warmup completed in 45.9ms
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [serving_completion.py:78] Using default completion sampling params from model: {'top_k': 40, 'top_p': 0.95}
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [serving_engine.py:271] "auto" tool choice has been enabled.
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [serving_chat.py:146] Using default chat sampling params from model: {'top_k': 40, 'top_p': 0.95}
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [api_server.py:1352] Starting vLLM API server 0 on http://0.0.0.0:8005
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:38] Available routes are:
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /docs, Methods: GET, HEAD
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /tokenize, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /detokenize, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /pause, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /resume, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /is_paused, Methods: GET
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /metrics, Methods: GET
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /health, Methods: GET
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /load, Methods: GET
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /v1/models, Methods: GET
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /version, Methods: GET
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /v1/responses, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /v1/messages, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /v1/completions, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /ping, Methods: GET
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /ping, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /invocations, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /classify, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /score, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /v1/score, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /rerank, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /v1/rerank, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /v2/rerank, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /pooling, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     Started server process [262986]
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     Waiting for application startup.
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     Application startup complete.
[2026-01-17 23:34:01] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:34:22] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:34:26] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:34:26 [loggers.py:257] Engine 000: Avg prompt throughput: 17525.8 tokens/s, Avg generation throughput: 34.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 49.8%
[2026-01-17 23:34:28] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:34:32] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:34:36] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:34:36 [loggers.py:257] Engine 000: Avg prompt throughput: 17688.1 tokens/s, Avg generation throughput: 42.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 23.9%, Prefix cache hit rate: 74.8%
[2026-01-17 23:34:43] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:34:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:34:46 [loggers.py:257] Engine 000: Avg prompt throughput: 8935.5 tokens/s, Avg generation throughput: 48.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 79.8%
[2026-01-17 23:34:47] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:34:50] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:34:53] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:34:56] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:34:56 [loggers.py:257] Engine 000: Avg prompt throughput: 27147.6 tokens/s, Avg generation throughput: 35.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 87.2%
[2026-01-17 23:34:58] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:35:02] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:35:06] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:35:06 [loggers.py:257] Engine 000: Avg prompt throughput: 18389.5 tokens/s, Avg generation throughput: 38.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 24.8%, Prefix cache hit rate: 89.7%
[2026-01-17 23:35:09] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:35:16] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:35:16] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:35:16 [loggers.py:257] Engine 000: Avg prompt throughput: 9263.4 tokens/s, Avg generation throughput: 40.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 90.6%
[2026-01-17 23:35:22] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:35:26] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:35:26 [loggers.py:257] Engine 000: Avg prompt throughput: 18597.7 tokens/s, Avg generation throughput: 35.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 92.1%
[2026-01-17 23:35:26] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:35:36] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:35:36 [loggers.py:257] Engine 000: Avg prompt throughput: 9336.0 tokens/s, Avg generation throughput: 67.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 92.6%
[2026-01-17 23:35:38] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:35:43] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:35:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:35:46 [loggers.py:257] Engine 000: Avg prompt throughput: 18824.7 tokens/s, Avg generation throughput: 36.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 93.5%
[2026-01-17 23:35:47] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:35:51] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:35:56] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:35:56] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:35:56 [loggers.py:257] Engine 000: Avg prompt throughput: 28410.5 tokens/s, Avg generation throughput: 27.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 25.5%, Prefix cache hit rate: 94.5%
[2026-01-17 23:35:59] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:36:03] [ERROR] [0;36m(APIServer pid=262986)[0;0m ERROR 01-17 23:36:03 [serving_chat.py:331] Error in preprocessing prompt inputs
[2026-01-17 23:36:03] [ERROR] [0;36m(APIServer pid=262986)[0;0m ERROR 01-17 23:36:03 [serving_chat.py:331] Traceback (most recent call last):
[2026-01-17 23:36:03] [ERROR] [0;36m(APIServer pid=262986)[0;0m ERROR 01-17 23:36:03 [serving_chat.py:331]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/serving_chat.py", line 309, in create_chat_completion
[2026-01-17 23:36:03] [ERROR] [0;36m(APIServer pid=262986)[0;0m ERROR 01-17 23:36:03 [serving_chat.py:331]     conversation, engine_prompts = await self._preprocess_chat(
[2026-01-17 23:36:03] [ERROR] [0;36m(APIServer pid=262986)[0;0m ERROR 01-17 23:36:03 [serving_chat.py:331]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-17 23:36:03] [ERROR] [0;36m(APIServer pid=262986)[0;0m ERROR 01-17 23:36:03 [serving_chat.py:331]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/serving_engine.py", line 1254, in _preprocess_chat
[2026-01-17 23:36:03] [ERROR] [0;36m(APIServer pid=262986)[0;0m ERROR 01-17 23:36:03 [serving_chat.py:331]     prompt_inputs = await self._tokenize_prompt_input_async(
[2026-01-17 23:36:03] [ERROR] [0;36m(APIServer pid=262986)[0;0m ERROR 01-17 23:36:03 [serving_chat.py:331]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-17 23:36:03] [ERROR] [0;36m(APIServer pid=262986)[0;0m ERROR 01-17 23:36:03 [serving_chat.py:331]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/serving_engine.py", line 1095, in _tokenize_prompt_input_async
[2026-01-17 23:36:03] [ERROR] [0;36m(APIServer pid=262986)[0;0m ERROR 01-17 23:36:03 [serving_chat.py:331]     async for result in self._tokenize_prompt_inputs_async(
[2026-01-17 23:36:03] [ERROR] [0;36m(APIServer pid=262986)[0;0m ERROR 01-17 23:36:03 [serving_chat.py:331]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/serving_engine.py", line 1116, in _tokenize_prompt_inputs_async
[2026-01-17 23:36:03] [ERROR] [0;36m(APIServer pid=262986)[0;0m ERROR 01-17 23:36:03 [serving_chat.py:331]     yield await self._normalize_prompt_text_to_input(
[2026-01-17 23:36:03] [ERROR] [0;36m(APIServer pid=262986)[0;0m ERROR 01-17 23:36:03 [serving_chat.py:331]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-17 23:36:03] [ERROR] [0;36m(APIServer pid=262986)[0;0m ERROR 01-17 23:36:03 [serving_chat.py:331]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/serving_engine.py", line 980, in _normalize_prompt_text_to_input
[2026-01-17 23:36:03] [ERROR] [0;36m(APIServer pid=262986)[0;0m ERROR 01-17 23:36:03 [serving_chat.py:331]     return self._validate_input(request, input_ids, input_text)
[2026-01-17 23:36:03] [ERROR] [0;36m(APIServer pid=262986)[0;0m ERROR 01-17 23:36:03 [serving_chat.py:331]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-17 23:36:03] [ERROR] [0;36m(APIServer pid=262986)[0;0m ERROR 01-17 23:36:03 [serving_chat.py:331]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/serving_engine.py", line 1073, in _validate_input
[2026-01-17 23:36:03] [ERROR] [0;36m(APIServer pid=262986)[0;0m ERROR 01-17 23:36:03 [serving_chat.py:331]     raise VLLMValidationError(
[2026-01-17 23:36:03] [ERROR] [0;36m(APIServer pid=262986)[0;0m ERROR 01-17 23:36:03 [serving_chat.py:331] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 32000. This model's maximum context length is 128000 tokens and your request has 96104 input tokens (32000 > 128000 - 96104). (parameter=max_tokens, value=32000)
[2026-01-17 23:36:03] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
[2026-01-17 23:36:04] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:36:06] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:36:06 [loggers.py:257] Engine 000: Avg prompt throughput: 9580.3 tokens/s, Avg generation throughput: 20.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.6%, Prefix cache hit rate: 91.1%
[2026-01-17 23:36:16] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:36:16 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 19.6%, Prefix cache hit rate: 91.1%
[2026-01-17 23:36:26] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:36:26 [loggers.py:257] Engine 000: Avg prompt throughput: 7320.8 tokens/s, Avg generation throughput: 62.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 91.1%
[2026-01-17 23:36:27] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:36:30] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:36:33] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:36:36] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:36:36] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:36:36 [loggers.py:257] Engine 000: Avg prompt throughput: 7155.9 tokens/s, Avg generation throughput: 35.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 91.4%
[2026-01-17 23:36:40] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:36:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:36:46 [loggers.py:257] Engine 000: Avg prompt throughput: 5662.6 tokens/s, Avg generation throughput: 54.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 91.4%
[2026-01-17 23:36:47] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:36:51] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:36:56] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:36:56 [loggers.py:257] Engine 000: Avg prompt throughput: 3495.6 tokens/s, Avg generation throughput: 87.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 91.3%
[2026-01-17 23:37:03] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:37:06] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:37:06 [loggers.py:257] Engine 000: Avg prompt throughput: 2404.9 tokens/s, Avg generation throughput: 88.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.5%, Prefix cache hit rate: 91.3%
[2026-01-17 23:37:08] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:37:13] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:37:16] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:37:16 [loggers.py:257] Engine 000: Avg prompt throughput: 5858.1 tokens/s, Avg generation throughput: 43.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 91.3%
[2026-01-17 23:37:17] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:37:20] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:37:26] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:37:26] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:37:26 [loggers.py:257] Engine 000: Avg prompt throughput: 6119.9 tokens/s, Avg generation throughput: 59.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 91.5%
[2026-01-17 23:37:30] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:37:33] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:37:36] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:37:36 [loggers.py:257] Engine 000: Avg prompt throughput: 9421.5 tokens/s, Avg generation throughput: 57.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 91.8%
[2026-01-17 23:37:38] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:37:43] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:37:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:37:46 [loggers.py:257] Engine 000: Avg prompt throughput: 6582.8 tokens/s, Avg generation throughput: 39.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 91.9%
[2026-01-17 23:37:47] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:37:56] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:37:56 [loggers.py:257] Engine 000: Avg prompt throughput: 3803.9 tokens/s, Avg generation throughput: 44.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 91.9%
[2026-01-17 23:38:06] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:38:06 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 91.9%
[2026-01-17 23:39:22] [INFO] VLLM GUI 服务器启动，端口: 5003
[2026-01-17 23:39:23] [INFO] nvitop监控已启动
[2026-01-17 23:39:24] [INFO] nvitop监控已启动
[2026-01-17 23:39:25] [INFO] 从conda info --base自动检测到conda路径: /mnt/AI-Acer4T/miniconda3
[2026-01-17 23:39:27] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-17 23:39:27] [INFO] nvitop监控已启动
[2026-01-17 23:39:27] [INFO] nvitop监控已停止
[2026-01-17 23:39:38] [WARNING] WARNING 01-17 23:39:38 [cuda.py:569] Detected different devices in the system: NVIDIA RTX PRO 6000 Blackwell Workstation Edition, NVIDIA RTX PRO 6000 Blackwell Workstation Edition, NVIDIA GeForce RTX 5060 Ti. Please make sure to set `CUDA_DEVICE_ORDER=PCI_BUS_ID` to avoid unexpected behavior.
[2026-01-17 23:39:39] [INFO] nvitop监控已启动
[2026-01-17 23:39:39] [INFO] 服务已停止
[2026-01-17 23:39:40] [INFO] nvitop监控已启动
[2026-01-17 23:39:41] [SUCCESS] 正在关闭服务器...
[2026-01-17 23:39:48] [INFO] [0;36m(Worker_TP1 pid=263552)[0;0m [0;36m(Worker_TP0 pid=263551)[0;0m INFO 01-17 23:39:48 [multiproc_executor.py:707] Parent process exited, terminating worker
[2026-01-17 23:39:48] [ERROR] [0;36m(APIServer pid=262986)[0;0m ERROR 01-17 23:39:48 [core_client.py:610] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.
[2026-01-17 23:39:48] [INFO] INFO 01-17 23:39:48 [multiproc_executor.py:707] Parent process exited, terminating worker
[2026-01-17 23:39:48] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m INFO 01-17 23:39:48 [multiproc_executor.py:751] WorkerProc shutting down.
[2026-01-17 23:39:48] [INFO] [0;36m(Worker_TP1 pid=263552)[0;0m INFO 01-17 23:39:48 [multiproc_executor.py:751] WorkerProc shutting down.
[2026-01-17 23:39:48] [INFO] [0;36m(Worker_TP1 pid=263552)[0;0m [0;36m(Worker_TP0 pid=263551)[0;0m /mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py:147: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.
[2026-01-17 23:39:48] [INFO] /mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py:147: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.
[2026-01-17 23:39:48] [INFO] [0;36m(Worker_TP1 pid=263552)[0;0m [0;36m(Worker_TP0 pid=263551)[0;0m   warnings.warn('resource_tracker: process died unexpectedly, '
[2026-01-17 23:39:48] [INFO] warnings.warn('resource_tracker: process died unexpectedly, '
[2026-01-17 23:39:48] [ERROR] Traceback (most recent call last):
[2026-01-17 23:39:48] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-17 23:39:48] [ERROR] Traceback (most recent call last):
[2026-01-17 23:39:48] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-17 23:39:48] [INFO] cache[rtype].remove(name)
[2026-01-17 23:39:48] [INFO] KeyError: '/psm_73a34520'
[2026-01-17 23:39:48] [INFO] cache[rtype].remove(name)
[2026-01-17 23:39:48] [INFO] KeyError: '/psm_fcbcb013'
[2026-01-17 23:39:48] [INFO] 服务已停止
[2026-01-17 23:39:48] [ERROR] Traceback (most recent call last):
[2026-01-17 23:39:48] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-17 23:39:48] [INFO] cache[rtype].remove(name)
[2026-01-17 23:39:48] [INFO] KeyError: '/psm_9841b967'
[2026-01-17 23:39:48] [INFO] nvitop监控已启动
[2026-01-17 23:39:48] [ERROR] Traceback (most recent call last):
[2026-01-17 23:39:48] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-17 23:39:48] [INFO] cache[rtype].remove(name)
[2026-01-17 23:39:48] [INFO] KeyError: '/mp-dur9wq8e'
[2026-01-17 23:39:48] [ERROR] Traceback (most recent call last):
[2026-01-17 23:39:48] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-17 23:39:48] [INFO] cache[rtype].remove(name)
[2026-01-17 23:39:48] [INFO] KeyError: '/mp-fxludf96'
[2026-01-17 23:39:49] [INFO] nvitop监控已启动
[2026-01-17 23:39:52] [INFO] nvitop监控已启动
[2026-01-17 23:40:09] [INFO] VLLM GUI 服务器启动，端口: 5003
[2026-01-17 23:40:09] [INFO] nvitop监控已启动
[2026-01-17 23:40:10] [INFO] nvitop监控已启动
[2026-01-17 23:40:12] [INFO] 从conda info --base自动检测到conda路径: /mnt/AI-Acer4T/miniconda3
[2026-01-17 23:42:20] [SUCCESS] 方案已保存: Devstral-2-123B-Instruct-2512-TP2
[2026-01-17 23:42:22] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-17 23:42:22] [INFO] nvitop监控已启动
[2026-01-17 23:42:22] [INFO] nvitop监控已停止
[2026-01-17 23:42:49] [ERROR] Traceback (most recent call last):
[2026-01-17 23:42:49] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/bin/vllm", line 7, in <module>
[2026-01-17 23:42:49] [INFO] sys.exit(main())
[2026-01-17 23:42:49] [INFO] ^^^^^^
[2026-01-17 23:42:49] [INFO] File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/main.py", line 70, in main
[2026-01-17 23:42:49] [INFO] cmds[args.subparser].validate(args)
[2026-01-17 23:42:49] [INFO] File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/serve.py", line 63, in validate
[2026-01-17 23:42:49] [INFO] validate_parsed_serve_args(args)
[2026-01-17 23:42:49] [INFO] File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/cli_args.py", line 307, in validate_parsed_serve_args
[2026-01-17 23:42:49] [INFO] validate_chat_template(args.chat_template)
[2026-01-17 23:42:49] [INFO] File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/chat_utils.py", line 1199, in validate_chat_template
[2026-01-17 23:42:49] [INFO] raise ValueError(
[2026-01-17 23:42:49] [INFO] ValueError: The supplied chat template string (/mnt/AI-Acer4T/AI-Chat/models/minimax/minimax/MiniMax-M2.1-AWQ/chat_template.jinja) appears path-like, but doesn't exist! Tried: /mnt/AI-Acer4T/AI-Chat/models/minimax/minimax/MiniMax-M2.1-AWQ/chat_template.jinja and /mnt/AI-Acer4T/AI-Chat/models/minimax/minimax/MiniMax-M2.1-AWQ/chat_template.jinja
[2026-01-17 23:42:51] [INFO] nvitop监控已启动
[2026-01-17 23:43:30] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-17 23:43:30] [INFO] nvitop监控已启动
[2026-01-17 23:43:30] [INFO] nvitop监控已停止
[2026-01-17 23:43:55] [INFO] [0;36m(APIServer pid=272181)[0;0m INFO 01-17 23:43:55 [api_server.py:1278] vLLM API server version 0.14.0rc1.dev492+g19504ac07
[2026-01-17 23:43:55] [INFO] [0;36m(APIServer pid=272181)[0;0m INFO 01-17 23:43:55 [utils.py:254] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/Devstral-2/Devstral-2-123B-Instruct-2512', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/AI-Acer4T/AI-Chat/models/Devstral-2/Devstral-2-123B-Instruct-2512/chat_template.jinja', 'enable_auto_tool_choice': True, 'tool_call_parser': 'mistral', 'model': '/mnt/AI-Acer4T/AI-Chat/models/Devstral-2/Devstral-2-123B-Instruct-2512', 'trust_remote_code': True, 'max_model_len': 128000, 'served_model_name': ['VLLM-MODEL'], 'reasoning_parser': 'mistral', 'tensor_parallel_size': 2, 'kv_cache_dtype': 'fp8', 'max_num_seqs': 256, 'async_scheduling': True}
[2026-01-17 23:43:55] [INFO] [0;36m(APIServer pid=272181)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-17 23:44:17] [INFO] [0;36m(APIServer pid=272181)[0;0m INFO 01-17 23:44:17 [model.py:528] Resolved architecture: MistralForCausalLM
[2026-01-17 23:44:17] [INFO] [0;36m(APIServer pid=272181)[0;0m INFO 01-17 23:44:17 [model.py:1543] Using max model len 128000
[2026-01-17 23:44:17] [INFO] [0;36m(APIServer pid=272181)[0;0m INFO 01-17 23:44:17 [cache.py:206] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor.
[2026-01-17 23:44:18] [INFO] [0;36m(APIServer pid=272181)[0;0m INFO 01-17 23:44:18 [scheduler.py:231] Chunked prefill is enabled with max_num_batched_tokens=8192.
[2026-01-17 23:44:18] [INFO] [0;36m(APIServer pid=272181)[0;0m INFO 01-17 23:44:18 [vllm.py:640] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-17 23:44:18] [INFO] [0;36m(APIServer pid=272181)[0;0m INFO 01-17 23:44:18 [vllm.py:645] Asynchronous scheduling is enabled.
[2026-01-17 23:44:18] [INFO] [0;36m(APIServer pid=272181)[0;0m [2026-01-17 23:44:18] INFO tekken.py:195: Non special vocabulary size is 130072 with 1000 special tokens.
[2026-01-17 23:44:43] [INFO] [0;36m(EngineCore_DP0 pid=272647)[0;0m INFO 01-17 23:44:43 [core.py:97] Initializing a V1 LLM engine (v0.14.0rc1.dev492+g19504ac07) with config: model='/mnt/AI-Acer4T/AI-Chat/models/Devstral-2/Devstral-2-123B-Instruct-2512', speculative_config=None, tokenizer='/mnt/AI-Acer4T/AI-Chat/models/Devstral-2/Devstral-2-123B-Instruct-2512', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=fp8, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='mistral', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-17 23:44:43] [WARNING] [0;36m(EngineCore_DP0 pid=272647)[0;0m WARNING 01-17 23:44:43 [multiproc_executor.py:880] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-17 23:45:07] [INFO] INFO 01-17 23:45:07 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:35975 backend=nccl
[2026-01-17 23:45:07] [INFO] INFO 01-17 23:45:07 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:35975 backend=nccl
[2026-01-17 23:45:07] [INFO] INFO 01-17 23:45:07 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-17 23:45:08] [WARNING] WARNING 01-17 23:45:08 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-17 23:45:08] [WARNING] WARNING 01-17 23:45:08 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-17 23:45:08] [INFO] INFO 01-17 23:45:08 [parallel_state.py:1425] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
[2026-01-17 23:45:08] [INFO] INFO 01-17 23:45:08 [parallel_state.py:1425] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[2026-01-17 23:45:09] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m INFO 01-17 23:45:09 [gpu_model_runner.py:3789] Starting to load model /mnt/AI-Acer4T/AI-Chat/models/Devstral-2/Devstral-2-123B-Instruct-2512...
[2026-01-17 23:45:13] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m INFO 01-17 23:45:13 [cuda.py:351] Using FLASHINFER attention backend out of potential backends: ('FLASHINFER', 'TRITON_ATTN')
[2026-01-17 23:45:14] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:45:14] [INFO] Loading safetensors checkpoint shards:   0% Completed | 0/27 [00:00<?, ?it/s]
[2026-01-17 23:45:19] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:45:19] [INFO] Loading safetensors checkpoint shards:   4% Completed | 1/27 [00:04<02:09,  4.97s/it]
[2026-01-17 23:45:24] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:45:24] [INFO] Loading safetensors checkpoint shards:   7% Completed | 2/27 [00:09<02:03,  4.95s/it]
[2026-01-17 23:45:28] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:45:28] [INFO] Loading safetensors checkpoint shards:  11% Completed | 3/27 [00:14<01:57,  4.91s/it]
[2026-01-17 23:45:33] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:45:33] [INFO] Loading safetensors checkpoint shards:  15% Completed | 4/27 [00:19<01:49,  4.78s/it]
[2026-01-17 23:45:38] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:45:38] [INFO] Loading safetensors checkpoint shards:  19% Completed | 5/27 [00:24<01:49,  4.97s/it]
[2026-01-17 23:45:43] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:45:43] [INFO] Loading safetensors checkpoint shards:  22% Completed | 6/27 [00:29<01:45,  5.02s/it]
[2026-01-17 23:45:49] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:45:49] [INFO] Loading safetensors checkpoint shards:  26% Completed | 7/27 [00:34<01:41,  5.09s/it]
[2026-01-17 23:45:54] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:45:54] [INFO] Loading safetensors checkpoint shards:  30% Completed | 8/27 [00:39<01:35,  5.02s/it]
[2026-01-17 23:45:59] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:45:59] [INFO] Loading safetensors checkpoint shards:  33% Completed | 9/27 [00:45<01:32,  5.14s/it]
[2026-01-17 23:46:04] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:46:04] [INFO] Loading safetensors checkpoint shards:  37% Completed | 10/27 [00:50<01:27,  5.13s/it]
[2026-01-17 23:46:11] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:46:11] [INFO] Loading safetensors checkpoint shards:  41% Completed | 11/27 [00:56<01:28,  5.54s/it]
[2026-01-17 23:46:14] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:46:14] [INFO] Loading safetensors checkpoint shards:  44% Completed | 12/27 [01:00<01:13,  4.88s/it]
[2026-01-17 23:46:17] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:46:17] [INFO] Loading safetensors checkpoint shards:  48% Completed | 13/27 [01:03<00:59,  4.26s/it]
[2026-01-17 23:46:21] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:46:21] [INFO] Loading safetensors checkpoint shards:  52% Completed | 14/27 [01:07<00:57,  4.39s/it]
[2026-01-17 23:46:26] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:46:26] [INFO] Loading safetensors checkpoint shards:  56% Completed | 15/27 [01:12<00:53,  4.47s/it]
[2026-01-17 23:46:31] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:46:31] [INFO] Loading safetensors checkpoint shards:  59% Completed | 16/27 [01:17<00:50,  4.59s/it]
[2026-01-17 23:46:36] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:46:36] [INFO] Loading safetensors checkpoint shards:  63% Completed | 17/27 [01:22<00:47,  4.71s/it]
[2026-01-17 23:46:41] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:46:41] [INFO] Loading safetensors checkpoint shards:  67% Completed | 18/27 [01:26<00:42,  4.72s/it]
[2026-01-17 23:46:46] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:46:46] [INFO] Loading safetensors checkpoint shards:  70% Completed | 19/27 [01:31<00:37,  4.75s/it]
[2026-01-17 23:46:50] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:46:50] [INFO] Loading safetensors checkpoint shards:  74% Completed | 20/27 [01:36<00:32,  4.71s/it]
[2026-01-17 23:46:55] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:46:55] [INFO] Loading safetensors checkpoint shards:  78% Completed | 21/27 [01:41<00:29,  4.85s/it]
[2026-01-17 23:47:00] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:47:00] [INFO] Loading safetensors checkpoint shards:  81% Completed | 22/27 [01:46<00:23,  4.73s/it]
[2026-01-17 23:47:04] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:47:04] [INFO] Loading safetensors checkpoint shards:  85% Completed | 23/27 [01:50<00:18,  4.62s/it]
[2026-01-17 23:47:09] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:47:09] [INFO] Loading safetensors checkpoint shards:  89% Completed | 24/27 [01:54<00:13,  4.59s/it]
[2026-01-17 23:47:14] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:47:14] [INFO] Loading safetensors checkpoint shards:  93% Completed | 25/27 [01:59<00:09,  4.67s/it]
[2026-01-17 23:47:18] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:47:18] [INFO] Loading safetensors checkpoint shards:  96% Completed | 26/27 [02:04<00:04,  4.66s/it]
[2026-01-17 23:47:23] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:47:23] [INFO] Loading safetensors checkpoint shards: 100% Completed | 27/27 [02:09<00:00,  4.64s/it]
[2026-01-17 23:47:23] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:47:23] [INFO] Loading safetensors checkpoint shards: 100% Completed | 27/27 [02:09<00:00,  4.78s/it]
[2026-01-17 23:47:23] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:47:23] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m INFO 01-17 23:47:23 [default_loader.py:291] Loading weights took 129.12 seconds
[2026-01-17 23:47:23] [WARNING] [0;36m(Worker_TP0 pid=272861)[0;0m WARNING 01-17 23:47:23 [kv_cache.py:90] Checkpoint does not provide a q scaling factor. Setting it to k_scale. This only matters for FP8 Attention backends (flash-attn or flashinfer).
[2026-01-17 23:47:23] [WARNING] [0;36m(Worker_TP0 pid=272861)[0;0m WARNING 01-17 23:47:23 [kv_cache.py:104] Using KV cache scaling factor 1.0 for fp8_e4m3. If this is unintended, verify that k/v_scale scaling factors are properly set in the checkpoint.
[2026-01-17 23:47:23] [WARNING] [0;36m(Worker_TP0 pid=272861)[0;0m WARNING 01-17 23:47:23 [kv_cache.py:143] Using uncalibrated q_scale 1.0 and/or prob_scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure q/prob scaling factors are available in the fp8 checkpoint.
[2026-01-17 23:47:23] [WARNING] [0;36m(Worker_TP1 pid=272862)[0;0m WARNING 01-17 23:47:23 [kv_cache.py:90] Checkpoint does not provide a q scaling factor. Setting it to k_scale. This only matters for FP8 Attention backends (flash-attn or flashinfer).
[2026-01-17 23:47:23] [WARNING] [0;36m(Worker_TP1 pid=272862)[0;0m WARNING 01-17 23:47:23 [kv_cache.py:104] Using KV cache scaling factor 1.0 for fp8_e4m3. If this is unintended, verify that k/v_scale scaling factors are properly set in the checkpoint.
[2026-01-17 23:47:23] [WARNING] [0;36m(Worker_TP1 pid=272862)[0;0m WARNING 01-17 23:47:23 [kv_cache.py:143] Using uncalibrated q_scale 1.0 and/or prob_scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure q/prob scaling factors are available in the fp8 checkpoint.
[2026-01-17 23:47:24] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m INFO 01-17 23:47:24 [gpu_model_runner.py:3886] Model loading took 59.79 GiB memory and 134.241629 seconds
[2026-01-17 23:47:43] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m INFO 01-17 23:47:43 [backends.py:644] Using cache directory: /home/wuwen/.cache/vllm/torch_compile_cache/5dfa8b2b6f/rank_0_0/backbone for vLLM's torch.compile
[2026-01-17 23:47:43] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m INFO 01-17 23:47:43 [backends.py:704] Dynamo bytecode transform time: 19.02 s
[2026-01-17 23:48:01] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m INFO 01-17 23:48:01 [backends.py:261] Cache the graph of compile range (1, 8192) for later use
[2026-01-17 23:48:01] [INFO] [0;36m(Worker_TP1 pid=272862)[0;0m INFO 01-17 23:48:01 [backends.py:261] Cache the graph of compile range (1, 8192) for later use
[2026-01-17 23:48:24] [INFO] [0;36m(EngineCore_DP0 pid=272647)[0;0m INFO 01-17 23:48:24 [shm_broadcast.py:542] No available shared memory broadcast block found in 60 seconds. This typically happens when some processes are hanging or doing some time-consuming work (e.g. compilation, weight/kv cache quantization).
[2026-01-17 23:49:24] [INFO] [0;36m(EngineCore_DP0 pid=272647)[0;0m INFO 01-17 23:49:24 [shm_broadcast.py:542] No available shared memory broadcast block found in 60 seconds. This typically happens when some processes are hanging or doing some time-consuming work (e.g. compilation, weight/kv cache quantization).
