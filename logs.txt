[2026-01-17 17:02:15] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:56244 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:02:19] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:02:19 [loggers.py:257] Engine 000: Avg prompt throughput: 10486.5 tokens/s, Avg generation throughput: 50.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.8%
[2026-01-17 17:02:19] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:56244 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:02:22] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:56244 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:02:27] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:56244 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:02:29] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:02:29 [loggers.py:257] Engine 000: Avg prompt throughput: 15968.9 tokens/s, Avg generation throughput: 43.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 14.4%, Prefix cache hit rate: 95.9%
[2026-01-17 17:02:32] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:56244 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:02:39] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:02:39 [loggers.py:257] Engine 000: Avg prompt throughput: 5378.7 tokens/s, Avg generation throughput: 69.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 14.6%, Prefix cache hit rate: 95.9%
[2026-01-17 17:02:49] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:02:49 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 81.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 14.8%, Prefix cache hit rate: 95.9%
[2026-01-17 17:02:59] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:02:59 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 81.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 15.1%, Prefix cache hit rate: 95.9%
[2026-01-17 17:03:03] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:56244 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:03:06] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:56244 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:03:09] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:03:09 [loggers.py:257] Engine 000: Avg prompt throughput: 11294.9 tokens/s, Avg generation throughput: 50.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 15.3%, Prefix cache hit rate: 95.9%
[2026-01-17 17:03:11] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:56244 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:03:15] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:56244 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:03:18] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:56244 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:03:19] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:03:19 [loggers.py:257] Engine 000: Avg prompt throughput: 17091.0 tokens/s, Avg generation throughput: 44.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 15.4%, Prefix cache hit rate: 96.0%
[2026-01-17 17:03:29] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:03:29 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 20.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.0%
[2026-01-17 17:03:39] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:03:39 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.0%
[2026-01-17 17:04:32] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:33290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:04:36] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:33290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:04:39] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:04:39 [loggers.py:257] Engine 000: Avg prompt throughput: 11519.4 tokens/s, Avg generation throughput: 23.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.1%
[2026-01-17 17:04:42] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:33290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:04:45] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:33290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:04:49] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:33290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:04:49] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:04:49 [loggers.py:257] Engine 000: Avg prompt throughput: 11566.7 tokens/s, Avg generation throughput: 38.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.1%
[2026-01-17 17:04:51] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:33290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:04:53] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:33290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:04:59] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:04:59 [loggers.py:257] Engine 000: Avg prompt throughput: 17432.5 tokens/s, Avg generation throughput: 49.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.2%
[2026-01-17 17:04:59] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:33290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:05:03] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:33290 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:05:09] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:05:09 [loggers.py:257] Engine 000: Avg prompt throughput: 11671.6 tokens/s, Avg generation throughput: 45.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.2%
[2026-01-17 17:05:19] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:05:19 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.2%
[2026-01-17 17:05:31] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:51570 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:05:34] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:51570 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:05:37] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:51570 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:05:39] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:05:39 [loggers.py:257] Engine 000: Avg prompt throughput: 17657.1 tokens/s, Avg generation throughput: 35.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.3%
[2026-01-17 17:05:41] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:51570 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:05:46] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:51570 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:05:49] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:05:49 [loggers.py:257] Engine 000: Avg prompt throughput: 11827.6 tokens/s, Avg generation throughput: 22.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.4%
[2026-01-17 17:05:51] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:51570 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:05:56] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:51570 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:05:59] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:05:59 [loggers.py:257] Engine 000: Avg prompt throughput: 11865.0 tokens/s, Avg generation throughput: 36.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 16.0%, Prefix cache hit rate: 96.4%
[2026-01-17 17:06:01] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:51570 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:06:07] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:51570 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:06:09] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:06:09 [loggers.py:257] Engine 000: Avg prompt throughput: 11950.1 tokens/s, Avg generation throughput: 61.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 16.1%, Prefix cache hit rate: 96.4%
[2026-01-17 17:06:13] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:51570 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:06:19] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:06:19 [loggers.py:257] Engine 000: Avg prompt throughput: 5997.2 tokens/s, Avg generation throughput: 59.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.5%
[2026-01-17 17:06:29] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:06:29 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.5%
[2026-01-17 17:10:09] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:42998 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:10:19] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:10:19 [loggers.py:257] Engine 000: Avg prompt throughput: 6030.2 tokens/s, Avg generation throughput: 10.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.5%
[2026-01-17 17:10:29] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:10:29 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.5%
[2026-01-17 17:11:42] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:60218 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:11:48] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:60218 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:11:49] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:11:49 [loggers.py:257] Engine 000: Avg prompt throughput: 12116.1 tokens/s, Avg generation throughput: 25.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 16.4%, Prefix cache hit rate: 96.5%
[2026-01-17 17:11:53] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:60218 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:11:59] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:11:59 [loggers.py:257] Engine 000: Avg prompt throughput: 6097.0 tokens/s, Avg generation throughput: 25.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.5%
[2026-01-17 17:12:09] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:12:09 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.5%
[2026-01-17 17:13:57] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:32768 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:13:59] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:32768 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:13:59] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:13:59 [loggers.py:257] Engine 000: Avg prompt throughput: 12216.4 tokens/s, Avg generation throughput: 8.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 16.4%, Prefix cache hit rate: 96.6%
[2026-01-17 17:14:09] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:14:09 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 13.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.6%
[2026-01-17 17:14:19] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:14:19 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.6%
[2026-01-17 17:16:03] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:54776 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:16:09] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:16:09 [loggers.py:257] Engine 000: Avg prompt throughput: 6125.6 tokens/s, Avg generation throughput: 30.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.6%
[2026-01-17 17:16:19] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:16:19 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.6%
[2026-01-17 17:17:39] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:43186 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:17:45] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:43186 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:17:48] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:43186 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:17:49] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:17:49 [loggers.py:257] Engine 000: Avg prompt throughput: 18507.9 tokens/s, Avg generation throughput: 43.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 16.7%, Prefix cache hit rate: 96.7%
[2026-01-17 17:17:59] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:17:59 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.7%
[2026-01-17 17:18:09] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:18:09 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.7%
[2026-01-17 17:19:22] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:59024 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:19:28] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:59024 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:19:29] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:19:29 [loggers.py:257] Engine 000: Avg prompt throughput: 12429.4 tokens/s, Avg generation throughput: 45.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 16.8%, Prefix cache hit rate: 96.7%
[2026-01-17 17:19:33] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:59024 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:19:39] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:19:39 [loggers.py:257] Engine 000: Avg prompt throughput: 6251.4 tokens/s, Avg generation throughput: 46.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.7%
[2026-01-17 17:19:43] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:36880 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:19:49] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:19:49 [loggers.py:257] Engine 000: Avg prompt throughput: 6313.7 tokens/s, Avg generation throughput: 46.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 17.1%, Prefix cache hit rate: 96.7%
[2026-01-17 17:19:57] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:53516 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:19:59] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:19:59 [loggers.py:257] Engine 000: Avg prompt throughput: 6365.1 tokens/s, Avg generation throughput: 26.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 17.2%, Prefix cache hit rate: 96.8%
[2026-01-17 17:20:09] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:20:09 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 6.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.8%
[2026-01-17 17:20:11] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:46358 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:20:15] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:46358 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:20:17] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:46358 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:20:19] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:20:19 [loggers.py:257] Engine 000: Avg prompt throughput: 19374.3 tokens/s, Avg generation throughput: 38.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.8%
[2026-01-17 17:20:29] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:20:29 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.8%
[2026-01-17 17:22:33] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:43774 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:22:39] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:22:39 [loggers.py:257] Engine 000: Avg prompt throughput: 6489.3 tokens/s, Avg generation throughput: 14.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.8%
[2026-01-17 17:22:41] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:40408 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:22:49] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:22:49 [loggers.py:257] Engine 000: Avg prompt throughput: 6535.3 tokens/s, Avg generation throughput: 31.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.8%
[2026-01-17 17:22:59] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:22:59 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.8%
[2026-01-17 17:40:34] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:44204 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:40:36] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:44204 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:40:39] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:40:39 [loggers.py:257] Engine 000: Avg prompt throughput: 13466.5 tokens/s, Avg generation throughput: 25.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.6%, Prefix cache hit rate: 96.8%
[2026-01-17 17:40:49] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:40:49 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 76.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.8%, Prefix cache hit rate: 96.8%
[2026-01-17 17:40:59] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:40:59 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 77.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 19.0%, Prefix cache hit rate: 96.8%
[2026-01-17 17:41:09] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:41:09 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 76.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 19.2%, Prefix cache hit rate: 96.8%
[2026-01-17 17:41:19] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:41:19 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 76.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 19.4%, Prefix cache hit rate: 96.8%
[2026-01-17 17:41:25] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:44204 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:41:29] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:41:29 [loggers.py:257] Engine 000: Avg prompt throughput: 7279.8 tokens/s, Avg generation throughput: 50.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.8%
[2026-01-17 17:41:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:44204 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 17:41:39] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:41:39 [loggers.py:257] Engine 000: Avg prompt throughput: 7295.4 tokens/s, Avg generation throughput: 15.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.8%
[2026-01-17 17:41:49] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 17:41:49 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.8%
[2026-01-17 20:52:39] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:47834 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 20:52:46] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:47834 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 20:52:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 20:52:50 [loggers.py:257] Engine 000: Avg prompt throughput: 5568.3 tokens/s, Avg generation throughput: 32.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.5%
[2026-01-17 20:52:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:47834 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 20:52:55] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:47834 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 20:53:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 20:53:00 [loggers.py:257] Engine 000: Avg prompt throughput: 5940.6 tokens/s, Avg generation throughput: 46.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.6%
[2026-01-17 20:53:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 20:53:10 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.6%
[2026-01-17 20:54:59] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:46410 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 20:55:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 20:55:00 [loggers.py:257] Engine 000: Avg prompt throughput: 2997.3 tokens/s, Avg generation throughput: 7.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.1%, Prefix cache hit rate: 96.6%
[2026-01-17 20:55:01] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:46410 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 20:55:05] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:46410 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 20:55:07] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:46410 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 20:55:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 20:55:10 [loggers.py:257] Engine 000: Avg prompt throughput: 9068.7 tokens/s, Avg generation throughput: 29.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.6%
[2026-01-17 20:55:12] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:46410 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 20:55:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 20:55:20 [loggers.py:257] Engine 000: Avg prompt throughput: 3041.8 tokens/s, Avg generation throughput: 10.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.6%
[2026-01-17 20:55:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 20:55:30 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.6%
[2026-01-17 20:57:08] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45946 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 20:57:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 20:57:10 [loggers.py:257] Engine 000: Avg prompt throughput: 3526.7 tokens/s, Avg generation throughput: 7.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.6%
[2026-01-17 20:57:11] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45946 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 20:57:14] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45946 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 20:57:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 20:57:20 [loggers.py:257] Engine 000: Avg prompt throughput: 7091.3 tokens/s, Avg generation throughput: 41.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.6%
[2026-01-17 20:57:21] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45946 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 20:57:25] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45946 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 20:57:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 20:57:30 [loggers.py:257] Engine 000: Avg prompt throughput: 7181.4 tokens/s, Avg generation throughput: 28.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.6%
[2026-01-17 20:57:37] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:58068 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 20:57:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 20:57:40 [loggers.py:257] Engine 000: Avg prompt throughput: 3654.4 tokens/s, Avg generation throughput: 18.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.6%
[2026-01-17 20:57:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:58068 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 20:57:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 20:57:50 [loggers.py:257] Engine 000: Avg prompt throughput: 3685.7 tokens/s, Avg generation throughput: 20.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.6%
[2026-01-17 20:58:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 20:58:00 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.6%
[2026-01-17 21:45:29] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:39612 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:45:36] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:39612 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:45:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:45:40 [loggers.py:257] Engine 000: Avg prompt throughput: 4060.4 tokens/s, Avg generation throughput: 34.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.4%
[2026-01-17 21:45:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:45:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.4%
[2026-01-17 21:46:49] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:36694 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:46:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:46:50 [loggers.py:257] Engine 000: Avg prompt throughput: 2058.0 tokens/s, Avg generation throughput: 5.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.6%, Prefix cache hit rate: 96.5%
[2026-01-17 21:46:55] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:36694 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:47:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:47:00 [loggers.py:257] Engine 000: Avg prompt throughput: 2130.7 tokens/s, Avg generation throughput: 31.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.5%
[2026-01-17 21:47:02] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:44270 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:47:09] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:44270 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:47:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:47:10 [loggers.py:257] Engine 000: Avg prompt throughput: 4495.5 tokens/s, Avg generation throughput: 20.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.5%
[2026-01-17 21:47:13] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:44270 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:47:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:47:20 [loggers.py:257] Engine 000: Avg prompt throughput: 2267.2 tokens/s, Avg generation throughput: 23.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.5%
[2026-01-17 21:47:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:47:30 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.5%
[2026-01-17 21:47:55] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:52584 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:48:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:48:00 [loggers.py:257] Engine 000: Avg prompt throughput: 2016.9 tokens/s, Avg generation throughput: 16.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.5%
[2026-01-17 21:48:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:48:10 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.5%
[2026-01-17 21:48:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:44388 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:48:35] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:44388 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:48:39] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:44388 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:48:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:48:40 [loggers.py:257] Engine 000: Avg prompt throughput: 6108.6 tokens/s, Avg generation throughput: 20.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.5%, Prefix cache hit rate: 96.5%
[2026-01-17 21:48:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:48:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.5%
[2026-01-17 21:49:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:49:00 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.5%
[2026-01-17 21:49:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:56034 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:49:28] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:56034 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:49:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:49:30 [loggers.py:257] Engine 000: Avg prompt throughput: 4696.5 tokens/s, Avg generation throughput: 20.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.3%
[2026-01-17 21:49:33] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:56034 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:49:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:49:40 [loggers.py:257] Engine 000: Avg prompt throughput: 2520.4 tokens/s, Avg generation throughput: 35.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.3%
[2026-01-17 21:49:41] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:56034 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:49:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:49:50 [loggers.py:257] Engine 000: Avg prompt throughput: 2555.3 tokens/s, Avg generation throughput: 25.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.3%
[2026-01-17 21:49:51] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:47050 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:50:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:50:00 [loggers.py:257] Engine 000: Avg prompt throughput: 2608.8 tokens/s, Avg generation throughput: 53.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.3%
[2026-01-17 21:50:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:50:10 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.3%
[2026-01-17 21:53:44] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:38958 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:53:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:53:50 [loggers.py:257] Engine 000: Avg prompt throughput: 2015.6 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.3%
[2026-01-17 21:54:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:54:00 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.3%
[2026-01-17 21:54:28] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:51468 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:54:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:54:30 [loggers.py:257] Engine 000: Avg prompt throughput: 2018.8 tokens/s, Avg generation throughput: 17.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.5%, Prefix cache hit rate: 96.3%
[2026-01-17 21:54:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:54:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 2.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.3%
[2026-01-17 21:54:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:54:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.3%
[2026-01-17 21:55:04] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:39182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:55:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:55:10 [loggers.py:257] Engine 000: Avg prompt throughput: 2027.2 tokens/s, Avg generation throughput: 11.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.3%
[2026-01-17 21:55:11] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:39194 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:55:16] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:39194 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:55:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:55:20 [loggers.py:257] Engine 000: Avg prompt throughput: 4199.3 tokens/s, Avg generation throughput: 32.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.3%
[2026-01-17 21:55:23] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:39194 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:55:29] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:39194 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:55:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:55:30 [loggers.py:257] Engine 000: Avg prompt throughput: 4321.3 tokens/s, Avg generation throughput: 20.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.9%, Prefix cache hit rate: 96.3%
[2026-01-17 21:55:34] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:39194 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:55:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:55:40 [loggers.py:257] Engine 000: Avg prompt throughput: 2411.6 tokens/s, Avg generation throughput: 23.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.3%
[2026-01-17 21:55:41] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:39194 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:55:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:55:50 [loggers.py:257] Engine 000: Avg prompt throughput: 2434.1 tokens/s, Avg generation throughput: 21.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.3%
[2026-01-17 21:55:52] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45676 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:56:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:56:00 [loggers.py:257] Engine 000: Avg prompt throughput: 2445.0 tokens/s, Avg generation throughput: 13.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.3%
[2026-01-17 21:56:03] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:39262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:56:09] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:39262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:56:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:56:10 [loggers.py:257] Engine 000: Avg prompt throughput: 2652.8 tokens/s, Avg generation throughput: 12.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.3%
[2026-01-17 21:56:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:56:20 [loggers.py:257] Engine 000: Avg prompt throughput: 3709.9 tokens/s, Avg generation throughput: 79.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.2%, Prefix cache hit rate: 96.2%
[2026-01-17 21:56:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:56:30 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 86.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.4%, Prefix cache hit rate: 96.2%
[2026-01-17 21:56:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:56:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 85.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.7%, Prefix cache hit rate: 96.2%
[2026-01-17 21:56:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:56:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 85.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.9%, Prefix cache hit rate: 96.2%
[2026-01-17 21:57:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:57:00 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 85.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 11.1%, Prefix cache hit rate: 96.2%
[2026-01-17 21:57:08] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:39262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:57:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:57:10 [loggers.py:257] Engine 000: Avg prompt throughput: 4162.9 tokens/s, Avg generation throughput: 55.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 11.2%, Prefix cache hit rate: 96.2%
[2026-01-17 21:57:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:57:20 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 34.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.2%
[2026-01-17 21:57:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:57:30 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.2%
[2026-01-17 21:58:42] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:58422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:58:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:58:50 [loggers.py:257] Engine 000: Avg prompt throughput: 4187.9 tokens/s, Avg generation throughput: 45.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.2%
[2026-01-17 21:58:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:58422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 21:59:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:59:00 [loggers.py:257] Engine 000: Avg prompt throughput: 4245.4 tokens/s, Avg generation throughput: 79.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 11.6%, Prefix cache hit rate: 96.2%
[2026-01-17 21:59:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:59:10 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 84.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 11.9%, Prefix cache hit rate: 96.2%
[2026-01-17 21:59:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:59:20 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 84.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.1%, Prefix cache hit rate: 96.2%
[2026-01-17 21:59:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:59:30 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 84.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.3%, Prefix cache hit rate: 96.2%
[2026-01-17 21:59:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:59:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 83.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.5%, Prefix cache hit rate: 96.2%
[2026-01-17 21:59:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 21:59:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 83.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.8%, Prefix cache hit rate: 96.2%
[2026-01-17 22:00:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:00:00 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 83.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 13.0%, Prefix cache hit rate: 96.2%
[2026-01-17 22:00:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:00:10 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 82.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 13.2%, Prefix cache hit rate: 96.2%
[2026-01-17 22:00:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:00:20 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 82.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 13.4%, Prefix cache hit rate: 96.2%
[2026-01-17 22:00:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:00:30 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 82.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 13.7%, Prefix cache hit rate: 96.2%
[2026-01-17 22:00:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:00:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 81.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 13.9%, Prefix cache hit rate: 96.2%
[2026-01-17 22:00:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:00:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 81.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 14.1%, Prefix cache hit rate: 96.2%
[2026-01-17 22:01:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:01:00 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 76.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.2%
[2026-01-17 22:01:02] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:58422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:01:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:01:10 [loggers.py:257] Engine 000: Avg prompt throughput: 5375.7 tokens/s, Avg generation throughput: 40.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.1%
[2026-01-17 22:01:12] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:58422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:01:17] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:58422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:01:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:01:20 [loggers.py:257] Engine 000: Avg prompt throughput: 10821.3 tokens/s, Avg generation throughput: 22.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.1%
[2026-01-17 22:01:21] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:58422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:01:26] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:58422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:01:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:01:30 [loggers.py:257] Engine 000: Avg prompt throughput: 10888.0 tokens/s, Avg generation throughput: 42.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 14.7%, Prefix cache hit rate: 96.2%
[2026-01-17 22:01:36] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:49988 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:01:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:01:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 11.7%, Prefix cache hit rate: 95.9%
[2026-01-17 22:01:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:01:50 [loggers.py:257] Engine 000: Avg prompt throughput: 5428.9 tokens/s, Avg generation throughput: 60.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 14.8%, Prefix cache hit rate: 95.9%
[2026-01-17 22:01:56] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:49988 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:02:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:02:00 [loggers.py:257] Engine 000: Avg prompt throughput: 5533.3 tokens/s, Avg generation throughput: 68.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 14.9%, Prefix cache hit rate: 95.9%
[2026-01-17 22:02:01] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:49988 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:02:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:02:10 [loggers.py:257] Engine 000: Avg prompt throughput: 5591.2 tokens/s, Avg generation throughput: 43.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.9%
[2026-01-17 22:02:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:02:20 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.9%
[2026-01-17 22:07:28] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:37262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:07:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:07:30 [loggers.py:257] Engine 000: Avg prompt throughput: 5614.3 tokens/s, Avg generation throughput: 14.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 15.1%, Prefix cache hit rate: 95.9%
[2026-01-17 22:07:33] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:37262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:07:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:07:40 [loggers.py:257] Engine 000: Avg prompt throughput: 5658.0 tokens/s, Avg generation throughput: 69.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 15.4%, Prefix cache hit rate: 96.0%
[2026-01-17 22:07:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:07:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 80.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 15.6%, Prefix cache hit rate: 96.0%
[2026-01-17 22:08:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:08:00 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 80.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 15.8%, Prefix cache hit rate: 96.0%
[2026-01-17 22:08:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:08:10 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 80.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 16.0%, Prefix cache hit rate: 96.0%
[2026-01-17 22:08:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:08:20 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 80.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 16.2%, Prefix cache hit rate: 96.0%
[2026-01-17 22:08:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:08:30 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 79.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 16.4%, Prefix cache hit rate: 96.0%
[2026-01-17 22:08:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:08:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 16.6%, Prefix cache hit rate: 96.0%
[2026-01-17 22:08:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:08:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 16.9%, Prefix cache hit rate: 96.0%
[2026-01-17 22:09:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:09:00 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 17.1%, Prefix cache hit rate: 96.0%
[2026-01-17 22:09:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:09:10 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 17.3%, Prefix cache hit rate: 96.0%
[2026-01-17 22:09:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:09:20 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 17.5%, Prefix cache hit rate: 96.0%
[2026-01-17 22:09:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:09:30 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 78.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 17.7%, Prefix cache hit rate: 96.0%
[2026-01-17 22:09:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:09:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 77.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 17.9%, Prefix cache hit rate: 96.0%
[2026-01-17 22:09:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:09:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 77.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.1%, Prefix cache hit rate: 96.0%
[2026-01-17 22:10:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:10:00 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 77.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.3%, Prefix cache hit rate: 96.0%
[2026-01-17 22:10:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:10:10 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 77.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.5%, Prefix cache hit rate: 96.0%
[2026-01-17 22:10:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:10:20 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 77.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.7%, Prefix cache hit rate: 96.0%
[2026-01-17 22:10:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:10:30 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 76.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.9%, Prefix cache hit rate: 96.0%
[2026-01-17 22:10:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:10:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 76.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 19.2%, Prefix cache hit rate: 96.0%
[2026-01-17 22:10:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:10:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 76.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 19.4%, Prefix cache hit rate: 96.0%
[2026-01-17 22:11:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:11:00 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 75.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 19.6%, Prefix cache hit rate: 96.0%
[2026-01-17 22:11:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:37262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:11:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:11:10 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 59.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.0%
[2026-01-17 22:11:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:11:20 [loggers.py:257] Engine 000: Avg prompt throughput: 7435.2 tokens/s, Avg generation throughput: 29.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.8%
[2026-01-17 22:11:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:37262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:11:25] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:37262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:11:28] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:37262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:11:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:11:30 [loggers.py:257] Engine 000: Avg prompt throughput: 22857.6 tokens/s, Avg generation throughput: 47.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 20.6%, Prefix cache hit rate: 95.9%
[2026-01-17 22:11:33] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:37262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:11:36] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:37262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:11:39] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:37262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:11:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:11:40 [loggers.py:257] Engine 000: Avg prompt throughput: 23229.1 tokens/s, Avg generation throughput: 39.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 21.0%, Prefix cache hit rate: 96.0%
[2026-01-17 22:11:42] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:37262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:11:45] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:37262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:11:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:11:50 [loggers.py:257] Engine 000: Avg prompt throughput: 15855.8 tokens/s, Avg generation throughput: 48.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 21.6%, Prefix cache hit rate: 96.0%
[2026-01-17 22:12:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:12:00 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 73.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 21.8%, Prefix cache hit rate: 96.0%
[2026-01-17 22:12:09] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:37262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:12:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:12:10 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 49.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.0%
[2026-01-17 22:12:16] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:37262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:12:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:12:20 [loggers.py:257] Engine 000: Avg prompt throughput: 16349.5 tokens/s, Avg generation throughput: 60.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 22.1%, Prefix cache hit rate: 96.0%
[2026-01-17 22:12:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:12:30 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 30.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.0%
[2026-01-17 22:12:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:12:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.0%
[2026-01-17 22:29:23] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:36928 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:29:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:29:30 [loggers.py:257] Engine 000: Avg prompt throughput: 8233.4 tokens/s, Avg generation throughput: 45.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 22.3%, Prefix cache hit rate: 96.1%
[2026-01-17 22:29:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:29:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 73.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 22.5%, Prefix cache hit rate: 96.1%
[2026-01-17 22:29:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:29:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 73.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 22.7%, Prefix cache hit rate: 96.1%
[2026-01-17 22:30:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:30:00 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 72.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 22.9%, Prefix cache hit rate: 96.1%
[2026-01-17 22:30:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:30:10 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 72.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 23.0%, Prefix cache hit rate: 96.1%
[2026-01-17 22:30:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:30:20 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 72.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 23.2%, Prefix cache hit rate: 96.1%
[2026-01-17 22:30:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:30:30 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 72.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 23.4%, Prefix cache hit rate: 96.1%
[2026-01-17 22:30:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:30:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 71.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 23.6%, Prefix cache hit rate: 96.1%
[2026-01-17 22:30:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:30:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 71.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 23.8%, Prefix cache hit rate: 96.1%
[2026-01-17 22:31:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:31:00 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 71.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 24.0%, Prefix cache hit rate: 96.1%
[2026-01-17 22:31:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:31:10 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 71.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 24.2%, Prefix cache hit rate: 96.1%
[2026-01-17 22:31:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:31:20 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 71.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 24.4%, Prefix cache hit rate: 96.1%
[2026-01-17 22:31:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:31:30 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 70.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 24.6%, Prefix cache hit rate: 96.1%
[2026-01-17 22:31:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:31:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 70.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 24.8%, Prefix cache hit rate: 96.1%
[2026-01-17 22:31:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:31:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 70.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 25.0%, Prefix cache hit rate: 96.1%
[2026-01-17 22:32:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:32:00 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 70.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 25.2%, Prefix cache hit rate: 96.1%
[2026-01-17 22:32:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:32:10 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 70.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 25.3%, Prefix cache hit rate: 96.1%
[2026-01-17 22:32:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:32:20 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 70.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 25.5%, Prefix cache hit rate: 96.1%
[2026-01-17 22:32:23] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:36928 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:32:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:32:30 [loggers.py:257] Engine 000: Avg prompt throughput: 9552.3 tokens/s, Avg generation throughput: 18.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.0%
[2026-01-17 22:32:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:36928 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:32:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:32:40 [loggers.py:257] Engine 000: Avg prompt throughput: 9573.5 tokens/s, Avg generation throughput: 63.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.0%
[2026-01-17 22:32:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:32:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.0%
[2026-01-17 22:34:27] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:34:27 [serving_chat.py:331] Error in preprocessing prompt inputs
[2026-01-17 22:34:27] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:34:27 [serving_chat.py:331] Traceback (most recent call last):
[2026-01-17 22:34:27] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:34:27 [serving_chat.py:331]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/serving_chat.py", line 309, in create_chat_completion
[2026-01-17 22:34:27] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:34:27 [serving_chat.py:331]     conversation, engine_prompts = await self._preprocess_chat(
[2026-01-17 22:34:27] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:34:27 [serving_chat.py:331]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-17 22:34:27] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:34:27 [serving_chat.py:331]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/serving_engine.py", line 1254, in _preprocess_chat
[2026-01-17 22:34:27] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:34:27 [serving_chat.py:331]     prompt_inputs = await self._tokenize_prompt_input_async(
[2026-01-17 22:34:27] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:34:27 [serving_chat.py:331]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-17 22:34:27] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:34:27 [serving_chat.py:331]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/serving_engine.py", line 1095, in _tokenize_prompt_input_async
[2026-01-17 22:34:27] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:34:27 [serving_chat.py:331]     async for result in self._tokenize_prompt_inputs_async(
[2026-01-17 22:34:27] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:34:27 [serving_chat.py:331]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/serving_engine.py", line 1116, in _tokenize_prompt_inputs_async
[2026-01-17 22:34:27] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:34:27 [serving_chat.py:331]     yield await self._normalize_prompt_text_to_input(
[2026-01-17 22:34:27] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:34:27 [serving_chat.py:331]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-17 22:34:27] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:34:27 [serving_chat.py:331]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/serving_engine.py", line 980, in _normalize_prompt_text_to_input
[2026-01-17 22:34:27] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:34:27 [serving_chat.py:331]     return self._validate_input(request, input_ids, input_text)
[2026-01-17 22:34:27] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:34:27 [serving_chat.py:331]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-17 22:34:27] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:34:27 [serving_chat.py:331]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/serving_engine.py", line 1073, in _validate_input
[2026-01-17 22:34:27] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:34:27 [serving_chat.py:331]     raise VLLMValidationError(
[2026-01-17 22:34:27] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:34:27 [serving_chat.py:331] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 32000. This model's maximum context length is 128000 tokens and your request has 96360 input tokens (32000 > 128000 - 96360). (parameter=max_tokens, value=32000)
[2026-01-17 22:34:27] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:38672 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
[2026-01-17 22:34:28] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:38672 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:34:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:34:50 [loggers.py:257] Engine 000: Avg prompt throughput: 7745.9 tokens/s, Avg generation throughput: 61.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 21.0%, Prefix cache hit rate: 95.5%
[2026-01-17 22:34:54] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:38672 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:35:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:35:00 [loggers.py:257] Engine 000: Avg prompt throughput: 1971.0 tokens/s, Avg generation throughput: 75.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.4%, Prefix cache hit rate: 95.5%
[2026-01-17 22:35:02] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:38672 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:35:05] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:38672 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:35:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:35:10 [loggers.py:257] Engine 000: Avg prompt throughput: 4219.4 tokens/s, Avg generation throughput: 68.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.0%, Prefix cache hit rate: 95.5%
[2026-01-17 22:35:11] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:38672 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:35:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:35:20 [loggers.py:257] Engine 000: Avg prompt throughput: 336.6 tokens/s, Avg generation throughput: 92.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.4%
[2026-01-17 22:35:21] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:38672 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:35:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:35:30 [loggers.py:257] Engine 000: Avg prompt throughput: 1982.4 tokens/s, Avg generation throughput: 18.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.4%
[2026-01-17 22:35:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:35:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.4%
[2026-01-17 22:36:08] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:36:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:36:10 [loggers.py:257] Engine 000: Avg prompt throughput: 1994.4 tokens/s, Avg generation throughput: 11.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.4%, Prefix cache hit rate: 95.4%
[2026-01-17 22:36:12] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:36:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:36:20 [loggers.py:257] Engine 000: Avg prompt throughput: 4251.3 tokens/s, Avg generation throughput: 44.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 11.5%, Prefix cache hit rate: 95.3%
[2026-01-17 22:36:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:36:30 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 85.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 11.8%, Prefix cache hit rate: 95.3%
[2026-01-17 22:36:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:36:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 84.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.0%, Prefix cache hit rate: 95.3%
[2026-01-17 22:36:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:36:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 84.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.2%, Prefix cache hit rate: 95.3%
[2026-01-17 22:37:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:37:00 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 84.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.4%, Prefix cache hit rate: 95.3%
[2026-01-17 22:37:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:37:10 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 83.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.7%, Prefix cache hit rate: 95.3%
[2026-01-17 22:37:16] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:37:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:37:20 [loggers.py:257] Engine 000: Avg prompt throughput: 4724.9 tokens/s, Avg generation throughput: 62.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.8%, Prefix cache hit rate: 95.3%
[2026-01-17 22:37:22] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:37:24] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:37:27] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:37:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:37:30 [loggers.py:257] Engine 000: Avg prompt throughput: 14812.9 tokens/s, Avg generation throughput: 32.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.3%
[2026-01-17 22:37:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:37:33] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:37:36] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:37:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:37:40 [loggers.py:257] Engine 000: Avg prompt throughput: 15870.5 tokens/s, Avg generation throughput: 55.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 14.6%, Prefix cache hit rate: 95.3%
[2026-01-17 22:37:45] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:37:49] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:37:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:37:50 [loggers.py:257] Engine 000: Avg prompt throughput: 10980.7 tokens/s, Avg generation throughput: 41.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 14.9%, Prefix cache hit rate: 95.4%
[2026-01-17 22:37:52] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:37:54] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:38:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:38:00 [loggers.py:257] Engine 000: Avg prompt throughput: 11238.4 tokens/s, Avg generation throughput: 57.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 15.3%, Prefix cache hit rate: 95.4%
[2026-01-17 22:38:08] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:38:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:38:10 [loggers.py:257] Engine 000: Avg prompt throughput: 5737.2 tokens/s, Avg generation throughput: 58.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.4%
[2026-01-17 22:38:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:38:13] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:38:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:38:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:38:20 [loggers.py:257] Engine 000: Avg prompt throughput: 11585.6 tokens/s, Avg generation throughput: 53.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.4%
[2026-01-17 22:38:23] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:38:26] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:38:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:38:30 [loggers.py:257] Engine 000: Avg prompt throughput: 17876.2 tokens/s, Avg generation throughput: 52.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 16.4%, Prefix cache hit rate: 95.5%
[2026-01-17 22:38:39] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:38:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:38:40 [loggers.py:257] Engine 000: Avg prompt throughput: 6160.6 tokens/s, Avg generation throughput: 64.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 16.6%, Prefix cache hit rate: 95.5%
[2026-01-17 22:38:42] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:38:45] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:38:47] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:38:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:38:50 [loggers.py:257] Engine 000: Avg prompt throughput: 18688.5 tokens/s, Avg generation throughput: 41.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 17.0%, Prefix cache hit rate: 95.5%
[2026-01-17 22:38:53] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:38:55] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:38:58] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:39:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:39:00 [loggers.py:257] Engine 000: Avg prompt throughput: 19263.1 tokens/s, Avg generation throughput: 37.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 17.7%, Prefix cache hit rate: 95.6%
[2026-01-17 22:39:04] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:39:07] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:39:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:39:10 [loggers.py:257] Engine 000: Avg prompt throughput: 13255.6 tokens/s, Avg generation throughput: 47.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.6%
[2026-01-17 22:39:11] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:39:14] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:39:17] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:39:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:39:20 [loggers.py:257] Engine 000: Avg prompt throughput: 20057.0 tokens/s, Avg generation throughput: 43.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.2%, Prefix cache hit rate: 95.6%
[2026-01-17 22:39:23] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:39:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:39:30 [loggers.py:257] Engine 000: Avg prompt throughput: 6759.1 tokens/s, Avg generation throughput: 66.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.3%, Prefix cache hit rate: 95.7%
[2026-01-17 22:39:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:39:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 77.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.5%, Prefix cache hit rate: 95.7%
[2026-01-17 22:39:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:39:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 77.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.7%, Prefix cache hit rate: 95.7%
[2026-01-17 22:39:52] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:40:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:40:00 [loggers.py:257] Engine 000: Avg prompt throughput: 6968.2 tokens/s, Avg generation throughput: 59.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.9%, Prefix cache hit rate: 95.7%
[2026-01-17 22:40:02] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:40:05] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:40:08] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:40:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:40:10 [loggers.py:257] Engine 000: Avg prompt throughput: 21102.2 tokens/s, Avg generation throughput: 34.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.7%
[2026-01-17 22:40:11] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:40:15] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:40:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:40:20 [loggers.py:257] Engine 000: Avg prompt throughput: 14347.6 tokens/s, Avg generation throughput: 52.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 19.4%, Prefix cache hit rate: 95.8%
[2026-01-17 22:40:22] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:40:28] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:45334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:40:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:40:30 [loggers.py:257] Engine 000: Avg prompt throughput: 14454.7 tokens/s, Avg generation throughput: 49.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 19.5%, Prefix cache hit rate: 95.8%
[2026-01-17 22:40:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:40:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.8%
[2026-01-17 22:40:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:40:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.8%
[2026-01-17 22:43:29] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:43:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:43:30 [loggers.py:257] Engine 000: Avg prompt throughput: 7258.3 tokens/s, Avg generation throughput: 2.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 19.5%, Prefix cache hit rate: 95.8%
[2026-01-17 22:43:32] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:43:36] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:43:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:43:40 [loggers.py:257] Engine 000: Avg prompt throughput: 14900.4 tokens/s, Avg generation throughput: 48.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 20.2%, Prefix cache hit rate: 95.8%
[2026-01-17 22:43:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:43:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 74.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 20.4%, Prefix cache hit rate: 95.8%
[2026-01-17 22:44:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:44:00 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 74.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 20.6%, Prefix cache hit rate: 95.8%
[2026-01-17 22:44:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:44:10 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 74.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 20.9%, Prefix cache hit rate: 95.8%
[2026-01-17 22:44:15] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:44:19] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:44:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:44:20 [loggers.py:257] Engine 000: Avg prompt throughput: 15669.2 tokens/s, Avg generation throughput: 41.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 21.2%, Prefix cache hit rate: 95.8%
[2026-01-17 22:44:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:44:30 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 74.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 21.4%, Prefix cache hit rate: 95.8%
[2026-01-17 22:44:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:44:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 74.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 21.6%, Prefix cache hit rate: 95.8%
[2026-01-17 22:44:44] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:44:48] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:44:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:44:50 [loggers.py:257] Engine 000: Avg prompt throughput: 16196.9 tokens/s, Avg generation throughput: 43.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.9%
[2026-01-17 22:44:51] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:45:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:45:00 [loggers.py:257] Engine 000: Avg prompt throughput: 8204.7 tokens/s, Avg generation throughput: 61.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 22.2%, Prefix cache hit rate: 95.9%
[2026-01-17 22:45:09] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:45:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:45:10 [loggers.py:257] Engine 000: Avg prompt throughput: 8320.9 tokens/s, Avg generation throughput: 56.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 22.4%, Prefix cache hit rate: 95.9%
[2026-01-17 22:45:16] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:45:19] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:45:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:45:20 [loggers.py:257] Engine 000: Avg prompt throughput: 16763.9 tokens/s, Avg generation throughput: 46.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 22.6%, Prefix cache hit rate: 95.9%
[2026-01-17 22:45:22] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:45:25] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:45:28] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:45:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:45:30 [loggers.py:257] Engine 000: Avg prompt throughput: 25492.4 tokens/s, Avg generation throughput: 34.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 23.0%, Prefix cache hit rate: 96.0%
[2026-01-17 22:45:35] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:45:38] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:45:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:45:40 [loggers.py:257] Engine 000: Avg prompt throughput: 17198.3 tokens/s, Avg generation throughput: 46.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 23.2%, Prefix cache hit rate: 96.0%
[2026-01-17 22:45:45] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:45:48] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:45:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:45:50 [loggers.py:257] Engine 000: Avg prompt throughput: 17357.9 tokens/s, Avg generation throughput: 45.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 23.4%, Prefix cache hit rate: 96.1%
[2026-01-17 22:45:51] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:45:56] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:46:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:46:00 [loggers.py:257] Engine 000: Avg prompt throughput: 17500.7 tokens/s, Avg generation throughput: 47.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 23.7%, Prefix cache hit rate: 96.1%
[2026-01-17 22:46:02] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:46:08] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:46:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:46:10 [loggers.py:257] Engine 000: Avg prompt throughput: 17618.6 tokens/s, Avg generation throughput: 44.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 23.8%, Prefix cache hit rate: 96.1%
[2026-01-17 22:46:11] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:46:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:46:20 [loggers.py:257] Engine 000: Avg prompt throughput: 8884.3 tokens/s, Avg generation throughput: 58.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 24.1%, Prefix cache hit rate: 96.1%
[2026-01-17 22:46:24] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:46:27] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:41182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:46:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:46:30 [loggers.py:257] Engine 000: Avg prompt throughput: 17922.7 tokens/s, Avg generation throughput: 43.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.2%
[2026-01-17 22:46:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:46:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.2%
[2026-01-17 22:48:55] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:48:59] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:49:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:49:00 [loggers.py:257] Engine 000: Avg prompt throughput: 18114.1 tokens/s, Avg generation throughput: 18.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 24.6%, Prefix cache hit rate: 96.2%
[2026-01-17 22:49:04] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:49:09] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:49:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:49:10 [loggers.py:257] Engine 000: Avg prompt throughput: 18803.6 tokens/s, Avg generation throughput: 39.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 25.5%, Prefix cache hit rate: 96.2%
[2026-01-17 22:49:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:49:20 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 70.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 25.7%, Prefix cache hit rate: 96.2%
[2026-01-17 22:49:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:49:30 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 69.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 25.9%, Prefix cache hit rate: 96.2%
[2026-01-17 22:49:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:49:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 69.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 26.1%, Prefix cache hit rate: 96.2%
[2026-01-17 22:49:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:49:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 69.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 26.3%, Prefix cache hit rate: 96.2%
[2026-01-17 22:49:52] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:49:52 [serving_chat.py:331] Error in preprocessing prompt inputs
[2026-01-17 22:49:52] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:49:52 [serving_chat.py:331] Traceback (most recent call last):
[2026-01-17 22:49:52] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:49:52 [serving_chat.py:331]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/serving_chat.py", line 309, in create_chat_completion
[2026-01-17 22:49:52] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:49:52 [serving_chat.py:331]     conversation, engine_prompts = await self._preprocess_chat(
[2026-01-17 22:49:52] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:49:52 [serving_chat.py:331]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-17 22:49:52] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:49:52 [serving_chat.py:331]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/serving_engine.py", line 1254, in _preprocess_chat
[2026-01-17 22:49:52] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:49:52 [serving_chat.py:331]     prompt_inputs = await self._tokenize_prompt_input_async(
[2026-01-17 22:49:52] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:49:52 [serving_chat.py:331]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-17 22:49:52] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:49:52 [serving_chat.py:331]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/serving_engine.py", line 1095, in _tokenize_prompt_input_async
[2026-01-17 22:49:52] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:49:52 [serving_chat.py:331]     async for result in self._tokenize_prompt_inputs_async(
[2026-01-17 22:49:52] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:49:52 [serving_chat.py:331]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/serving_engine.py", line 1116, in _tokenize_prompt_inputs_async
[2026-01-17 22:49:52] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:49:52 [serving_chat.py:331]     yield await self._normalize_prompt_text_to_input(
[2026-01-17 22:49:52] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:49:52 [serving_chat.py:331]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-17 22:49:52] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:49:52 [serving_chat.py:331]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/serving_engine.py", line 980, in _normalize_prompt_text_to_input
[2026-01-17 22:49:52] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:49:52 [serving_chat.py:331]     return self._validate_input(request, input_ids, input_text)
[2026-01-17 22:49:52] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:49:52 [serving_chat.py:331]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-17 22:49:52] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:49:52 [serving_chat.py:331]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/serving_engine.py", line 1073, in _validate_input
[2026-01-17 22:49:52] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:49:52 [serving_chat.py:331]     raise VLLMValidationError(
[2026-01-17 22:49:52] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 22:49:52 [serving_chat.py:331] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 32000. This model's maximum context length is 128000 tokens and your request has 97587 input tokens (32000 > 128000 - 97587). (parameter=max_tokens, value=32000)
[2026-01-17 22:49:52] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
[2026-01-17 22:49:53] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:50:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:50:00 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 5.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 15.5%, Prefix cache hit rate: 95.8%
[2026-01-17 22:50:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:50:10 [loggers.py:257] Engine 000: Avg prompt throughput: 7869.3 tokens/s, Avg generation throughput: 17.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 21.2%, Prefix cache hit rate: 95.8%
[2026-01-17 22:50:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:50:20 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 73.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 21.4%, Prefix cache hit rate: 95.8%
[2026-01-17 22:50:24] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:50:29] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:50:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:50:30 [loggers.py:257] Engine 000: Avg prompt throughput: 4084.6 tokens/s, Avg generation throughput: 60.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.7%, Prefix cache hit rate: 95.8%
[2026-01-17 22:50:32] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:50:36] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:50:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:50:40 [loggers.py:257] Engine 000: Avg prompt throughput: 4376.0 tokens/s, Avg generation throughput: 52.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.1%, Prefix cache hit rate: 95.8%
[2026-01-17 22:50:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:50:50 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 91.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.4%, Prefix cache hit rate: 95.8%
[2026-01-17 22:50:53] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:50:56] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:50:58] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:51:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:51:00 [loggers.py:257] Engine 000: Avg prompt throughput: 7192.9 tokens/s, Avg generation throughput: 48.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.6%, Prefix cache hit rate: 95.8%
[2026-01-17 22:51:02] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:51:05] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:51:08] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:51:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:51:10 [loggers.py:257] Engine 000: Avg prompt throughput: 7843.4 tokens/s, Avg generation throughput: 44.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.2%, Prefix cache hit rate: 95.8%
[2026-01-17 22:51:19] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:51:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:51:20 [loggers.py:257] Engine 000: Avg prompt throughput: 2738.7 tokens/s, Avg generation throughput: 74.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.4%, Prefix cache hit rate: 95.8%
[2026-01-17 22:51:22] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:51:25] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:51:28] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:51:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:51:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:51:30 [loggers.py:257] Engine 000: Avg prompt throughput: 8501.3 tokens/s, Avg generation throughput: 37.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.8%
[2026-01-17 22:51:33] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:51:35] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:51:39] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:51:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:51:40 [loggers.py:257] Engine 000: Avg prompt throughput: 11819.0 tokens/s, Avg generation throughput: 45.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.2%, Prefix cache hit rate: 95.9%
[2026-01-17 22:51:42] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:51:45] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:51:49] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:51:50] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:51:50 [loggers.py:257] Engine 000: Avg prompt throughput: 9254.0 tokens/s, Avg generation throughput: 51.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.3%, Prefix cache hit rate: 95.9%
[2026-01-17 22:51:54] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:52:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:52:00 [loggers.py:257] Engine 000: Avg prompt throughput: 1232.7 tokens/s, Avg generation throughput: 77.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.4%, Prefix cache hit rate: 95.8%
[2026-01-17 22:52:05] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:35096 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:52:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:52:10 [loggers.py:257] Engine 000: Avg prompt throughput: 1992.1 tokens/s, Avg generation throughput: 64.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.8%
[2026-01-17 22:52:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:52:20 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.8%
[2026-01-17 22:54:09] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:34668 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:54:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:54:10 [loggers.py:257] Engine 000: Avg prompt throughput: 2003.4 tokens/s, Avg generation throughput: 4.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.4%, Prefix cache hit rate: 95.8%
[2026-01-17 22:54:13] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:34668 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:54:19] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:34668 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:54:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:54:20 [loggers.py:257] Engine 000: Avg prompt throughput: 4119.4 tokens/s, Avg generation throughput: 46.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.7%, Prefix cache hit rate: 95.8%
[2026-01-17 22:54:22] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:34668 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:54:25] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:34668 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:54:28] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:34668 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:54:30] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:54:30 [loggers.py:257] Engine 000: Avg prompt throughput: 6494.5 tokens/s, Avg generation throughput: 35.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.8%
[2026-01-17 22:54:40] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:54:40 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.8%
[2026-01-17 22:54:57] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:39328 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:55:00] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:55:00 [loggers.py:257] Engine 000: Avg prompt throughput: 2221.1 tokens/s, Avg generation throughput: 8.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.8%
[2026-01-17 22:55:03] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:39328 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:55:07] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO:     127.0.0.1:39328 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 22:55:10] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:55:10 [loggers.py:257] Engine 000: Avg prompt throughput: 4485.3 tokens/s, Avg generation throughput: 45.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.8%
[2026-01-17 22:55:20] [INFO] [0;36m(APIServer pid=53343)[0;0m INFO 01-17 22:55:20 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 95.8%
[2026-01-17 23:01:35] [INFO] [0;36m(Worker_TP1 pid=53925)[0;0m INFO 01-17 23:01:35 [multiproc_executor.py:707] Parent process exited, terminating worker
[2026-01-17 23:01:35] [ERROR] [0;36m(APIServer pid=53343)[0;0m ERROR 01-17 23:01:35 [core_client.py:610] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.
[2026-01-17 23:01:35] [INFO] [0;36m(Worker_TP0 pid=53924)[0;0m INFO 01-17 23:01:35 [multiproc_executor.py:707] Parent process exited, terminating worker
[2026-01-17 23:01:35] [INFO] [0;36m(Worker_TP1 pid=53925)[0;0m INFO 01-17 23:01:35 [multiproc_executor.py:751] WorkerProc shutting down.
[2026-01-17 23:01:35] [INFO] [0;36m(Worker_TP0 pid=53924)[0;0m INFO 01-17 23:01:35 [multiproc_executor.py:751] WorkerProc shutting down.
[2026-01-17 23:01:35] [INFO] [0;36m(Worker_TP1 pid=53925)[0;0m /mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py:147: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.
[2026-01-17 23:01:35] [INFO] [0;36m(Worker_TP1 pid=53925)[0;0m   warnings.warn('resource_tracker: process died unexpectedly, '
[2026-01-17 23:01:35] [INFO] [0;36m(Worker_TP0 pid=53924)[0;0m /mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py:147: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.
[2026-01-17 23:01:35] [INFO] [0;36m(Worker_TP0 pid=53924)[0;0m   warnings.warn('resource_tracker: process died unexpectedly, '
[2026-01-17 23:01:35] [ERROR] Traceback (most recent call last):
[2026-01-17 23:01:35] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-17 23:01:35] [INFO] cache[rtype].remove(name)
[2026-01-17 23:01:35] [INFO] KeyError: '/psm_24685522'
[2026-01-17 23:01:35] [ERROR] Traceback (most recent call last):
[2026-01-17 23:01:35] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-17 23:01:35] [INFO] cache[rtype].remove(name)
[2026-01-17 23:01:35] [INFO] KeyError: '/psm_e3bc7a23'
[2026-01-17 23:01:35] [ERROR] Traceback (most recent call last):
[2026-01-17 23:01:35] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-17 23:01:35] [INFO] cache[rtype].remove(name)
[2026-01-17 23:01:35] [INFO] KeyError: '/mp-blwhkhtg'
[2026-01-17 23:01:35] [INFO] 服务已停止
[2026-01-17 23:01:36] [ERROR] Traceback (most recent call last):
[2026-01-17 23:01:36] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-17 23:01:36] [INFO] cache[rtype].remove(name)
[2026-01-17 23:01:36] [INFO] KeyError: '/psm_0582c16f'
[2026-01-17 23:01:36] [ERROR] Traceback (most recent call last):
[2026-01-17 23:01:36] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-17 23:01:36] [INFO] cache[rtype].remove(name)
[2026-01-17 23:01:36] [INFO] KeyError: '/mp-fu6kqlw8'
[2026-01-17 23:01:36] [INFO] nvitop监控已启动
[2026-01-17 23:01:36] [INFO] nvitop监控已启动
[2026-01-17 23:01:41] [INFO] nvitop监控已启动
[2026-01-17 23:02:47] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-17 23:02:47] [INFO] nvitop监控已启动
[2026-01-17 23:02:47] [INFO] nvitop监控已停止
[2026-01-17 23:03:03] [INFO] nvitop监控已启动
[2026-01-17 23:03:03] [INFO] 服务已停止
[2026-01-17 23:03:03] [INFO] nvitop监控已启动
[2026-01-17 23:03:04] [INFO] nvitop监控已启动
[2026-01-17 23:03:06] [SUCCESS] 正在关闭服务器...
[2026-01-17 23:03:37] [INFO] VLLM GUI 服务器启动，端口: 5000
[2026-01-17 23:03:38] [INFO] nvitop监控已启动
[2026-01-17 23:03:38] [INFO] nvitop监控已启动
[2026-01-17 23:03:39] [INFO] 从conda info --base自动检测到conda路径: /mnt/AI-Acer4T/miniconda3
[2026-01-17 23:04:21] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-17 23:04:21] [INFO] nvitop监控已启动
[2026-01-17 23:04:21] [INFO] nvitop监控已停止
[2026-01-17 23:04:45] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:04:45 [api_server.py:1278] vLLM API server version 0.14.0rc1.dev492+g19504ac07
[2026-01-17 23:04:45] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:04:45 [utils.py:254] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/minimax/MiniMax-M2.1-NVFP4-TYW', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/AI-Acer4T/AI-Chat/models/minimax/MiniMax-M2.1-NVFP4-TYW/chat_template.jinja', 'enable_auto_tool_choice': True, 'tool_call_parser': 'minimax_m2', 'model': '/mnt/AI-Acer4T/AI-Chat/models/minimax/MiniMax-M2.1-NVFP4-TYW', 'trust_remote_code': True, 'max_model_len': 128000, 'quantization': 'nvfp4', 'served_model_name': ['VLLM-MODEL'], 'reasoning_parser': 'minimax_m2_append_think', 'tensor_parallel_size': 2, 'all2all_backend': 'pplx', 'gpu_memory_utilization': 0.95, 'kv_cache_dtype': 'fp8', 'max_num_seqs': 256, 'enable_chunked_prefill': True, 'async_scheduling': True}
[2026-01-17 23:04:45] [INFO] [0;36m(APIServer pid=253022)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-17 23:04:45] [INFO] [0;36m(APIServer pid=253022)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-17 23:04:46] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:04:46 [model.py:528] Resolved architecture: MiniMaxM2ForCausalLM
[2026-01-17 23:04:46] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:04:46 [model.py:1543] Using max model len 128000
[2026-01-17 23:04:46] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:04:46 [cache.py:206] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor.
[2026-01-17 23:04:46] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:04:46 [scheduler.py:231] Chunked prefill is enabled with max_num_batched_tokens=8192.
[2026-01-17 23:04:46] [WARNING] [0;36m(APIServer pid=253022)[0;0m WARNING 01-17 23:04:46 [modelopt.py:1018] Detected ModelOpt NVFP4 checkpoint. Please note that the format is experimental and could change in future.
[2026-01-17 23:04:46] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:04:46 [vllm.py:640] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-17 23:04:46] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:04:46 [vllm.py:645] Asynchronous scheduling is enabled.
[2026-01-17 23:04:47] [INFO] [0;36m(APIServer pid=253022)[0;0m The tokenizer you are loading from '/mnt/AI-Acer4T/AI-Chat/models/minimax/MiniMax-M2.1-NVFP4-TYW' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[2026-01-17 23:05:09] [INFO] [0;36m(EngineCore_DP0 pid=253248)[0;0m INFO 01-17 23:05:09 [core.py:97] Initializing a V1 LLM engine (v0.14.0rc1.dev492+g19504ac07) with config: model='/mnt/AI-Acer4T/AI-Chat/models/minimax/MiniMax-M2.1-NVFP4-TYW', speculative_config=None, tokenizer='/mnt/AI-Acer4T/AI-Chat/models/minimax/MiniMax-M2.1-NVFP4-TYW', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=modelopt_fp4, enforce_eager=False, kv_cache_dtype=fp8, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='minimax_m2_append_think', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-17 23:05:09] [WARNING] [0;36m(EngineCore_DP0 pid=253248)[0;0m WARNING 01-17 23:05:09 [multiproc_executor.py:880] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-17 23:05:32] [INFO] INFO 01-17 23:05:32 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:34341 backend=nccl
[2026-01-17 23:05:32] [INFO] INFO 01-17 23:05:32 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:34341 backend=nccl
[2026-01-17 23:05:33] [INFO] INFO 01-17 23:05:33 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-17 23:05:33] [WARNING] WARNING 01-17 23:05:33 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-17 23:05:33] [WARNING] WARNING 01-17 23:05:33 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-17 23:05:33] [INFO] INFO 01-17 23:05:33 [parallel_state.py:1425] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[2026-01-17 23:05:33] [INFO] INFO 01-17 23:05:33 [parallel_state.py:1425] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
[2026-01-17 23:05:35] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m INFO 01-17 23:05:35 [gpu_model_runner.py:3789] Starting to load model /mnt/AI-Acer4T/AI-Chat/models/minimax/MiniMax-M2.1-NVFP4-TYW...
[2026-01-17 23:05:35] [INFO] [0;36m(Worker_TP1 pid=253471)[0;0m INFO 01-17 23:05:35 [modelopt.py:1142] Using flashinfer-cutlass for NVFP4 GEMM
[2026-01-17 23:05:35] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m INFO 01-17 23:05:35 [modelopt.py:1142] Using flashinfer-cutlass for NVFP4 GEMM
[2026-01-17 23:05:39] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m INFO 01-17 23:05:39 [cuda.py:351] Using FLASHINFER attention backend out of potential backends: ('FLASHINFER', 'TRITON_ATTN')
[2026-01-17 23:05:39] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m INFO 01-17 23:05:39 [nvfp4.py:110] Using vLLM CUTASS backend for NvFp4 MoE
[2026-01-17 23:05:39] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:05:39] [INFO] Loading safetensors checkpoint shards:   0% Completed | 0/27 [00:00<?, ?it/s]
[2026-01-17 23:05:40] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:05:40] [INFO] Loading safetensors checkpoint shards:   4% Completed | 1/27 [00:00<00:24,  1.07it/s]
[2026-01-17 23:05:41] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:05:41] [INFO] Loading safetensors checkpoint shards:   7% Completed | 2/27 [00:01<00:24,  1.02it/s]
[2026-01-17 23:05:42] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:05:42] [INFO] Loading safetensors checkpoint shards:  11% Completed | 3/27 [00:02<00:22,  1.09it/s]
[2026-01-17 23:05:43] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:05:43] [INFO] Loading safetensors checkpoint shards:  15% Completed | 4/27 [00:03<00:22,  1.04it/s]
[2026-01-17 23:05:44] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:05:44] [INFO] Loading safetensors checkpoint shards:  19% Completed | 5/27 [00:04<00:21,  1.02it/s]
[2026-01-17 23:05:45] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:05:45] [INFO] Loading safetensors checkpoint shards:  22% Completed | 6/27 [00:05<00:21,  1.00s/it]
[2026-01-17 23:05:46] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:05:46] [INFO] Loading safetensors checkpoint shards:  26% Completed | 7/27 [00:06<00:20,  1.01s/it]
[2026-01-17 23:05:48] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:05:48] [INFO] Loading safetensors checkpoint shards:  30% Completed | 8/27 [00:08<00:24,  1.27s/it]
[2026-01-17 23:05:49] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:05:49] [INFO] Loading safetensors checkpoint shards:  33% Completed | 9/27 [00:09<00:21,  1.20s/it]
[2026-01-17 23:05:50] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:05:50] [INFO] Loading safetensors checkpoint shards:  37% Completed | 10/27 [00:10<00:19,  1.15s/it]
[2026-01-17 23:05:51] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:05:51] [INFO] Loading safetensors checkpoint shards:  41% Completed | 11/27 [00:11<00:17,  1.11s/it]
[2026-01-17 23:05:52] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:05:52] [INFO] Loading safetensors checkpoint shards:  44% Completed | 12/27 [00:12<00:16,  1.09s/it]
[2026-01-17 23:05:53] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:05:53] [INFO] Loading safetensors checkpoint shards:  48% Completed | 13/27 [00:13<00:15,  1.07s/it]
[2026-01-17 23:05:54] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:05:54] [INFO] Loading safetensors checkpoint shards:  52% Completed | 14/27 [00:14<00:13,  1.07s/it]
[2026-01-17 23:05:55] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:05:55] [INFO] Loading safetensors checkpoint shards:  56% Completed | 15/27 [00:16<00:12,  1.06s/it]
[2026-01-17 23:05:56] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:05:56] [INFO] Loading safetensors checkpoint shards:  59% Completed | 16/27 [00:17<00:11,  1.05s/it]
[2026-01-17 23:05:57] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:05:57] [INFO] Loading safetensors checkpoint shards:  63% Completed | 17/27 [00:18<00:10,  1.05s/it]
[2026-01-17 23:05:58] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:05:58] [INFO] Loading safetensors checkpoint shards:  67% Completed | 18/27 [00:19<00:09,  1.04s/it]
[2026-01-17 23:05:59] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:05:59] [INFO] Loading safetensors checkpoint shards:  70% Completed | 19/27 [00:20<00:08,  1.05s/it]
[2026-01-17 23:06:01] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:06:01] [INFO] Loading safetensors checkpoint shards:  74% Completed | 20/27 [00:21<00:07,  1.04s/it]
[2026-01-17 23:06:02] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:06:02] [INFO] Loading safetensors checkpoint shards:  78% Completed | 21/27 [00:22<00:06,  1.04s/it]
[2026-01-17 23:06:03] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:06:03] [INFO] Loading safetensors checkpoint shards:  81% Completed | 22/27 [00:23<00:05,  1.04s/it]
[2026-01-17 23:06:04] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:06:04] [INFO] Loading safetensors checkpoint shards:  85% Completed | 23/27 [00:24<00:04,  1.04s/it]
[2026-01-17 23:06:05] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:06:05] [INFO] Loading safetensors checkpoint shards:  89% Completed | 24/27 [00:25<00:03,  1.04s/it]
[2026-01-17 23:06:06] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:06:06] [INFO] Loading safetensors checkpoint shards:  93% Completed | 25/27 [00:26<00:02,  1.04s/it]
[2026-01-17 23:06:06] [WARNING] [0;36m(Worker_TP1 pid=253471)[0;0m WARNING 01-17 23:06:06 [kv_cache.py:90] Checkpoint does not provide a q scaling factor. Setting it to k_scale. This only matters for FP8 Attention backends (flash-attn or flashinfer).
[2026-01-17 23:06:06] [WARNING] [0;36m(Worker_TP1 pid=253471)[0;0m WARNING 01-17 23:06:06 [kv_cache.py:104] Using KV cache scaling factor 1.0 for fp8_e4m3. If this is unintended, verify that k/v_scale scaling factors are properly set in the checkpoint.
[2026-01-17 23:06:06] [WARNING] [0;36m(Worker_TP1 pid=253471)[0;0m WARNING 01-17 23:06:06 [kv_cache.py:143] Using uncalibrated q_scale 1.0 and/or prob_scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure q/prob scaling factors are available in the fp8 checkpoint.
[2026-01-17 23:06:07] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:06:07] [INFO] Loading safetensors checkpoint shards:  96% Completed | 26/27 [00:27<00:01,  1.00s/it]
[2026-01-17 23:06:07] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:06:07] [INFO] Loading safetensors checkpoint shards: 100% Completed | 27/27 [00:27<00:00,  1.34it/s]
[2026-01-17 23:06:07] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:06:07] [INFO] Loading safetensors checkpoint shards: 100% Completed | 27/27 [00:27<00:00,  1.02s/it]
[2026-01-17 23:06:07] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:06:07] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m INFO 01-17 23:06:07 [default_loader.py:291] Loading weights took 27.51 seconds
[2026-01-17 23:06:07] [WARNING] [0;36m(Worker_TP0 pid=253470)[0;0m WARNING 01-17 23:06:07 [kv_cache.py:90] Checkpoint does not provide a q scaling factor. Setting it to k_scale. This only matters for FP8 Attention backends (flash-attn or flashinfer).
[2026-01-17 23:06:07] [WARNING] [0;36m(Worker_TP0 pid=253470)[0;0m WARNING 01-17 23:06:07 [kv_cache.py:104] Using KV cache scaling factor 1.0 for fp8_e4m3. If this is unintended, verify that k/v_scale scaling factors are properly set in the checkpoint.
[2026-01-17 23:06:07] [WARNING] [0;36m(Worker_TP0 pid=253470)[0;0m WARNING 01-17 23:06:07 [kv_cache.py:143] Using uncalibrated q_scale 1.0 and/or prob_scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure q/prob scaling factors are available in the fp8 checkpoint.
[2026-01-17 23:06:07] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m INFO 01-17 23:06:07 [gpu_model_runner.py:3886] Model loading took 60.93 GiB memory and 32.096454 seconds
[2026-01-17 23:06:23] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m INFO 01-17 23:06:23 [backends.py:644] Using cache directory: /home/wuwen/.cache/vllm/torch_compile_cache/468f724d39/rank_0_0/backbone for vLLM's torch.compile
[2026-01-17 23:06:23] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m INFO 01-17 23:06:23 [backends.py:704] Dynamo bytecode transform time: 15.28 s
[2026-01-17 23:06:39] [INFO] [0;36m(Worker_TP1 pid=253471)[0;0m INFO 01-17 23:06:39 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 5.330 s
[2026-01-17 23:06:39] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m INFO 01-17 23:06:39 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 4.113 s
[2026-01-17 23:06:39] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m INFO 01-17 23:06:39 [monitor.py:34] torch.compile takes 19.39 s in total
[2026-01-17 23:06:40] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m INFO 01-17 23:06:40 [gpu_worker.py:358] Available KV cache memory: 26.74 GiB
[2026-01-17 23:06:41] [INFO] [0;36m(EngineCore_DP0 pid=253248)[0;0m INFO 01-17 23:06:41 [kv_cache_utils.py:1305] GPU KV cache size: 452,176 tokens
[2026-01-17 23:06:41] [INFO] [0;36m(EngineCore_DP0 pid=253248)[0;0m INFO 01-17 23:06:41 [kv_cache_utils.py:1310] Maximum concurrency for 128,000 tokens per request: 3.53x
[2026-01-17 23:06:41] [INFO] [0;36m(Worker_TP1 pid=253471)[0;0m 2026-01-17 23:06:41,137 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[2026-01-17 23:06:41] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m 2026-01-17 23:06:41,137 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[2026-01-17 23:06:41] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m 2026-01-17 23:06:41,695 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[2026-01-17 23:06:41] [INFO] [0;36m(Worker_TP1 pid=253471)[0;0m 2026-01-17 23:06:41,695 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[2026-01-17 23:06:41] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m INFO 01-17 23:06:41 [kernel_warmup.py:64] Warming up FlashInfer attention.
[2026-01-17 23:06:41] [INFO] [0;36m(Worker_TP1 pid=253471)[0;0m INFO 01-17 23:06:41 [kernel_warmup.py:64] Warming up FlashInfer attention.
[2026-01-17 23:06:42] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:06:42] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
[2026-01-17 23:06:42] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:08,  5.81it/s]
[2026-01-17 23:06:42] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:08,  5.96it/s]
[2026-01-17 23:06:43] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:07,  6.04it/s]
[2026-01-17 23:06:43] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:00<00:07,  6.06it/s]
[2026-01-17 23:06:43] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:00<00:07,  6.08it/s]
[2026-01-17 23:06:43] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:07,  6.06it/s]
[2026-01-17 23:06:43] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▎        | 7/51 [00:01<00:07,  5.88it/s]
[2026-01-17 23:06:43] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:01<00:07,  5.89it/s]
[2026-01-17 23:06:44] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:01<00:07,  5.94it/s]
[2026-01-17 23:06:44] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|█▉        | 10/51 [00:01<00:06,  5.97it/s]
[2026-01-17 23:06:44] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:01<00:06,  6.00it/s]
[2026-01-17 23:06:44] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:02<00:06,  6.01it/s]
[2026-01-17 23:06:44] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:02<00:06,  6.03it/s]
[2026-01-17 23:06:44] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:02<00:06,  6.04it/s]
[2026-01-17 23:06:45] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:02<00:05,  6.04it/s]
[2026-01-17 23:06:45] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 16/51 [00:02<00:05,  6.05it/s]
[2026-01-17 23:06:45] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:02<00:05,  6.02it/s]
[2026-01-17 23:06:45] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:02<00:05,  6.01it/s]
[2026-01-17 23:06:45] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 19/51 [00:03<00:05,  5.98it/s]
[2026-01-17 23:06:45] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:03<00:05,  5.96it/s]
[2026-01-17 23:06:46] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:03<00:05,  5.93it/s]
[2026-01-17 23:06:46] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 22/51 [00:03<00:04,  5.95it/s]
[2026-01-17 23:06:46] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:03<00:04,  5.98it/s]
[2026-01-17 23:06:46] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:04<00:04,  5.99it/s]
[2026-01-17 23:06:46] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 25/51 [00:04<00:04,  5.99it/s]
[2026-01-17 23:06:46] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:04<00:04,  6.01it/s]
[2026-01-17 23:06:47] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:04<00:04,  6.00it/s]
[2026-01-17 23:06:47] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:04<00:03,  5.98it/s]
[2026-01-17 23:06:47] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:04<00:03,  5.96it/s]
[2026-01-17 23:06:47] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:05<00:03,  5.96it/s]
[2026-01-17 23:06:47] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 31/51 [00:05<00:03,  5.92it/s]
[2026-01-17 23:06:47] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:05<00:03,  5.93it/s]
[2026-01-17 23:06:48] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:05<00:03,  5.92it/s]
[2026-01-17 23:06:48] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:05<00:02,  5.92it/s]
[2026-01-17 23:06:48] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:05<00:02,  5.92it/s]
[2026-01-17 23:06:48] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:06<00:02,  5.92it/s]
[2026-01-17 23:06:48] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 37/51 [00:06<00:02,  5.93it/s]
[2026-01-17 23:06:49] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:06<00:02,  5.92it/s]
[2026-01-17 23:06:49] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▋  | 39/51 [00:06<00:02,  5.94it/s]
[2026-01-17 23:06:49] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:06<00:01,  5.92it/s]
[2026-01-17 23:06:49] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:06<00:01,  5.93it/s]
[2026-01-17 23:06:49] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:07<00:01,  5.91it/s]
[2026-01-17 23:06:49] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 43/51 [00:07<00:01,  5.87it/s]
[2026-01-17 23:06:50] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:07<00:01,  5.11it/s]
[2026-01-17 23:06:50] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 45/51 [00:07<00:01,  5.32it/s]
[2026-01-17 23:06:50] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|█████████ | 46/51 [00:07<00:00,  5.51it/s]
[2026-01-17 23:06:50] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:07<00:00,  5.62it/s]
[2026-01-17 23:06:50] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 48/51 [00:08<00:00,  5.73it/s]
[2026-01-17 23:06:50] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▌| 49/51 [00:08<00:00,  5.80it/s]
[2026-01-17 23:06:51] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:08<00:00,  5.86it/s]
[2026-01-17 23:06:51] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:09<00:00,  3.34it/s]
[2026-01-17 23:06:51] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:09<00:00,  5.62it/s]
[2026-01-17 23:06:51] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m
[2026-01-17 23:06:51] [INFO] Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
[2026-01-17 23:06:51] [INFO] Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:06,  5.50it/s]
[2026-01-17 23:06:52] [INFO] Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:00<00:05,  5.62it/s]
[2026-01-17 23:06:52] [INFO] Capturing CUDA graphs (decode, FULL):   9%|▊         | 3/35 [00:00<00:05,  5.65it/s]
[2026-01-17 23:06:52] [INFO] Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:07,  4.38it/s]
[2026-01-17 23:06:52] [INFO] Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:01<00:06,  4.78it/s]
[2026-01-17 23:06:52] [INFO] Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:01<00:05,  5.02it/s]
[2026-01-17 23:06:53] [INFO] Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:01<00:05,  5.24it/s]
[2026-01-17 23:06:53] [INFO] Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:01<00:05,  5.34it/s]
[2026-01-17 23:06:53] [INFO] Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:01<00:04,  5.44it/s]
[2026-01-17 23:06:53] [INFO] Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:01<00:04,  5.52it/s]
[2026-01-17 23:06:53] [INFO] Capturing CUDA graphs (decode, FULL):  31%|███▏      | 11/35 [00:02<00:04,  5.55it/s]
[2026-01-17 23:06:53] [INFO] Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:02<00:04,  5.50it/s]
[2026-01-17 23:06:54] [INFO] Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:02<00:03,  5.55it/s]
[2026-01-17 23:06:54] [INFO] Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:02<00:03,  5.55it/s]
[2026-01-17 23:06:54] [INFO] Capturing CUDA graphs (decode, FULL):  43%|████▎     | 15/35 [00:02<00:03,  5.56it/s]
[2026-01-17 23:06:54] [INFO] Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:02<00:03,  5.59it/s]
[2026-01-17 23:06:54] [INFO] Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:03<00:03,  5.59it/s]
[2026-01-17 23:06:55] [INFO] Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:03<00:03,  5.60it/s]
[2026-01-17 23:06:55] [INFO] Capturing CUDA graphs (decode, FULL):  54%|█████▍    | 19/35 [00:03<00:03,  4.44it/s]
[2026-01-17 23:06:55] [INFO] Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:03<00:03,  4.73it/s]
[2026-01-17 23:06:55] [INFO] Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:04<00:02,  4.98it/s]
[2026-01-17 23:06:55] [INFO] Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:04<00:02,  5.14it/s]
[2026-01-17 23:06:56] [INFO] Capturing CUDA graphs (decode, FULL):  66%|██████▌   | 23/35 [00:04<00:02,  5.28it/s]
[2026-01-17 23:06:56] [INFO] Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:04<00:02,  5.39it/s]
[2026-01-17 23:06:56] [INFO] Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:04<00:01,  5.46it/s]
[2026-01-17 23:06:56] [INFO] Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:04<00:01,  5.49it/s]
[2026-01-17 23:06:56] [INFO] Capturing CUDA graphs (decode, FULL):  77%|███████▋  | 27/35 [00:05<00:01,  5.53it/s]
[2026-01-17 23:06:57] [INFO] Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:05<00:01,  5.54it/s]
[2026-01-17 23:06:57] [INFO] Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:05<00:01,  5.54it/s]
[2026-01-17 23:06:57] [INFO] Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:05<00:00,  5.54it/s]
[2026-01-17 23:06:57] [INFO] Capturing CUDA graphs (decode, FULL):  89%|████████▊ | 31/35 [00:05<00:00,  5.56it/s]
[2026-01-17 23:06:57] [INFO] Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:05<00:00,  5.60it/s]
[2026-01-17 23:06:57] [INFO] Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:06<00:00,  5.62it/s]
[2026-01-17 23:06:58] [INFO] Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:06<00:00,  5.65it/s]
[2026-01-17 23:06:58] [INFO] Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:06<00:00,  5.61it/s]
[2026-01-17 23:06:58] [INFO] Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:06<00:00,  5.37it/s]
[2026-01-17 23:06:58] [INFO] [0;36m(Worker_TP1 pid=253471)[0;0m INFO 01-17 23:06:58 [custom_all_reduce.py:216] Registering 15958 cuda graph addresses
[2026-01-17 23:06:58] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m INFO 01-17 23:06:58 [custom_all_reduce.py:216] Registering 15958 cuda graph addresses
[2026-01-17 23:06:59] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m INFO 01-17 23:06:59 [gpu_model_runner.py:4837] Graph capturing finished in 17 secs, took -0.07 GiB
[2026-01-17 23:06:59] [INFO] [0;36m(EngineCore_DP0 pid=253248)[0;0m INFO 01-17 23:06:59 [core.py:273] init engine (profile, create kv cache, warmup model) took 51.09 seconds
[2026-01-17 23:06:59] [INFO] [0;36m(EngineCore_DP0 pid=253248)[0;0m The tokenizer you are loading from '/mnt/AI-Acer4T/AI-Chat/models/minimax/MiniMax-M2.1-NVFP4-TYW' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[2026-01-17 23:06:59] [INFO] [0;36m(EngineCore_DP0 pid=253248)[0;0m INFO 01-17 23:06:59 [core.py:186] Batch queue is enabled with size 2
[2026-01-17 23:06:59] [INFO] [0;36m(EngineCore_DP0 pid=253248)[0;0m INFO 01-17 23:06:59 [vllm.py:645] Asynchronous scheduling is enabled.
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [api_server.py:1020] Supported tasks: ['generate']
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m The tokenizer you are loading from '/mnt/AI-Acer4T/AI-Chat/models/minimax/MiniMax-M2.1-NVFP4-TYW' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[2026-01-17 23:07:00] [WARNING] [0;36m(APIServer pid=253022)[0;0m WARNING 01-17 23:07:00 [model.py:1356] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [serving_responses.py:224] Using default chat sampling params from model: {'top_k': 40, 'top_p': 0.95}
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [serving_engine.py:271] "auto" tool choice has been enabled.
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [serving_engine.py:271] "auto" tool choice has been enabled.
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [serving_chat.py:146] Using default chat sampling params from model: {'top_k': 40, 'top_p': 0.95}
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [serving_chat.py:182] Warming up chat template processing...
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [chat_utils.py:599] Detected the chat template content format to be 'openai'. You can set `--chat-template-content-format` to override this.
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [serving_chat.py:218] Chat template warmup completed in 43.2ms
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [serving_completion.py:78] Using default completion sampling params from model: {'top_k': 40, 'top_p': 0.95}
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [serving_engine.py:271] "auto" tool choice has been enabled.
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [serving_chat.py:146] Using default chat sampling params from model: {'top_k': 40, 'top_p': 0.95}
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [api_server.py:1352] Starting vLLM API server 0 on http://0.0.0.0:8005
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:38] Available routes are:
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /docs, Methods: HEAD, GET
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /tokenize, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /detokenize, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /pause, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /resume, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /is_paused, Methods: GET
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /metrics, Methods: GET
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /health, Methods: GET
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /load, Methods: GET
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /v1/models, Methods: GET
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /version, Methods: GET
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /v1/responses, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /v1/messages, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /v1/completions, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /ping, Methods: GET
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /ping, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /invocations, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /classify, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /score, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /v1/score, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /rerank, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /v1/rerank, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /v2/rerank, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:07:00 [launcher.py:46] Route: /pooling, Methods: POST
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     Started server process [253022]
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     Waiting for application startup.
[2026-01-17 23:07:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     Application startup complete.
[2026-01-17 23:08:21] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:08:25] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:08:28] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:08:30] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:08:30 [loggers.py:257] Engine 000: Avg prompt throughput: 4644.2 tokens/s, Avg generation throughput: 22.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 10.4%, Prefix cache hit rate: 49.8%
[2026-01-17 23:08:35] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:08:39] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:08:40] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:08:40 [loggers.py:257] Engine 000: Avg prompt throughput: 9386.6 tokens/s, Avg generation throughput: 43.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 66.4%
[2026-01-17 23:08:47] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:08:50] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:08:50] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:08:50 [loggers.py:257] Engine 000: Avg prompt throughput: 13396.1 tokens/s, Avg generation throughput: 23.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 75.5%
[2026-01-17 23:08:54] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:09:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:09:00 [loggers.py:257] Engine 000: Avg prompt throughput: 14229.8 tokens/s, Avg generation throughput: 47.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 82.6%
[2026-01-17 23:09:02] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:09:07] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:09:10] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:09:10 [loggers.py:257] Engine 000: Avg prompt throughput: 14602.7 tokens/s, Avg generation throughput: 47.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 87.0%
[2026-01-17 23:09:12] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:09:15] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:09:19] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:09:20] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:09:20 [loggers.py:257] Engine 000: Avg prompt throughput: 22113.2 tokens/s, Avg generation throughput: 31.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 16.4%, Prefix cache hit rate: 90.5%
[2026-01-17 23:09:30] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:09:30 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 75.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 16.6%, Prefix cache hit rate: 90.5%
[2026-01-17 23:09:35] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:09:40] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:09:40 [loggers.py:257] Engine 000: Avg prompt throughput: 7563.0 tokens/s, Avg generation throughput: 34.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 91.2%
[2026-01-17 23:09:44] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:09:50] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:09:50 [loggers.py:257] Engine 000: Avg prompt throughput: 7594.4 tokens/s, Avg generation throughput: 50.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 16.9%, Prefix cache hit rate: 91.9%
[2026-01-17 23:09:56] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:09:59] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:10:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:10:00 [loggers.py:257] Engine 000: Avg prompt throughput: 15301.5 tokens/s, Avg generation throughput: 37.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 17.0%, Prefix cache hit rate: 92.9%
[2026-01-17 23:10:05] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:10:07] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:10:10] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:10:10 [loggers.py:257] Engine 000: Avg prompt throughput: 15421.3 tokens/s, Avg generation throughput: 43.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 17.1%, Prefix cache hit rate: 93.8%
[2026-01-17 23:10:15] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:10:20] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:10:20 [loggers.py:257] Engine 000: Avg prompt throughput: 7754.2 tokens/s, Avg generation throughput: 27.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 94.1%
[2026-01-17 23:10:21] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:10:24] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:10:26] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:10:29] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:10:30] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:10:30 [loggers.py:257] Engine 000: Avg prompt throughput: 31471.2 tokens/s, Avg generation throughput: 34.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 17.6%, Prefix cache hit rate: 95.1%
[2026-01-17 23:10:32] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:10:35] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:10:40] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:10:40] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:10:40 [loggers.py:257] Engine 000: Avg prompt throughput: 24183.2 tokens/s, Avg generation throughput: 24.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.0%, Prefix cache hit rate: 95.7%
[2026-01-17 23:10:43] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:10:50] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:10:50 [loggers.py:257] Engine 000: Avg prompt throughput: 8251.8 tokens/s, Avg generation throughput: 58.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.4%, Prefix cache hit rate: 95.8%
[2026-01-17 23:11:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:11:00 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 73.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.5%, Prefix cache hit rate: 95.8%
[2026-01-17 23:11:02] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:11:06] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:11:10] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:11:10 [loggers.py:257] Engine 000: Avg prompt throughput: 16835.3 tokens/s, Avg generation throughput: 43.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.8%, Prefix cache hit rate: 96.0%
[2026-01-17 23:11:15] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:11:20] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:11:20 [loggers.py:257] Engine 000: Avg prompt throughput: 8506.5 tokens/s, Avg generation throughput: 51.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.9%, Prefix cache hit rate: 96.1%
[2026-01-17 23:11:27] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:11:30] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:11:30 [loggers.py:257] Engine 000: Avg prompt throughput: 8568.4 tokens/s, Avg generation throughput: 48.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.3%
[2026-01-17 23:11:31] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:11:36] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:11:39] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:11:40] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:11:40 [loggers.py:257] Engine 000: Avg prompt throughput: 25852.0 tokens/s, Avg generation throughput: 31.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 19.1%, Prefix cache hit rate: 96.6%
[2026-01-17 23:11:42] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:11:46] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:11:50] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:11:50 [loggers.py:257] Engine 000: Avg prompt throughput: 17338.6 tokens/s, Avg generation throughput: 34.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.8%
[2026-01-17 23:11:51] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO:     127.0.0.1:44726 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:12:00] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:12:00 [loggers.py:257] Engine 000: Avg prompt throughput: 8707.4 tokens/s, Avg generation throughput: 50.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.9%
[2026-01-17 23:12:10] [INFO] [0;36m(APIServer pid=253022)[0;0m INFO 01-17 23:12:10 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 96.9%
[2026-01-17 23:13:05] [INFO] VLLM GUI 服务器启动，端口: 5002
[2026-01-17 23:13:06] [INFO] nvitop监控已启动
[2026-01-17 23:13:07] [INFO] nvitop监控已启动
[2026-01-17 23:13:08] [INFO] 从conda info --base自动检测到conda路径: /mnt/AI-Acer4T/miniconda3
[2026-01-17 23:13:20] [SUCCESS] 方案已保存: vllm_配置_2026/1/17
[2026-01-17 23:13:40] [SUCCESS] 方案已删除: vllm_配置_2026/1/17
[2026-01-17 23:16:32] [SUCCESS] 方案已保存: MiniMax-M2.1-NVFP4-TYW-TP2
[2026-01-17 23:17:34] [SUCCESS] 方案已保存: MiniMax-M2.1-FP8-INT4-AWQ-M-TP2
[2026-01-17 23:18:11] [SUCCESS] 方案已保存: MiniMax-M2.1-NVFP4-TP2
[2026-01-17 23:18:54] [SUCCESS] 方案已保存: MiniMax-M2.1-AWQ-TP2
[2026-01-17 23:19:03] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-17 23:19:03] [INFO] nvitop监控已启动
[2026-01-17 23:19:03] [INFO] nvitop监控已停止
[2026-01-17 23:19:28] [ERROR] Traceback (most recent call last):
[2026-01-17 23:19:28] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/bin/vllm", line 7, in <module>
[2026-01-17 23:19:28] [INFO] sys.exit(main())
[2026-01-17 23:19:28] [INFO] ^^^^^^
[2026-01-17 23:19:28] [INFO] File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/main.py", line 70, in main
[2026-01-17 23:19:28] [INFO] cmds[args.subparser].validate(args)
[2026-01-17 23:19:28] [INFO] File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/serve.py", line 63, in validate
[2026-01-17 23:19:28] [INFO] validate_parsed_serve_args(args)
[2026-01-17 23:19:28] [INFO] File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/cli_args.py", line 307, in validate_parsed_serve_args
[2026-01-17 23:19:28] [INFO] validate_chat_template(args.chat_template)
[2026-01-17 23:19:28] [INFO] File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/chat_utils.py", line 1199, in validate_chat_template
[2026-01-17 23:19:28] [INFO] raise ValueError(
[2026-01-17 23:19:28] [INFO] ValueError: The supplied chat template string (/mnt/AI-Acer4T/AI-Chat/models/minimax/minimax/MiniMax-M2.1-AWQ/chat_template.jinja) appears path-like, but doesn't exist! Tried: /mnt/AI-Acer4T/AI-Chat/models/minimax/minimax/MiniMax-M2.1-AWQ/chat_template.jinja and /mnt/AI-Acer4T/AI-Chat/models/minimax/minimax/MiniMax-M2.1-AWQ/chat_template.jinja
[2026-01-17 23:19:29] [INFO] nvitop监控已启动
[2026-01-17 23:21:06] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-17 23:21:06] [INFO] nvitop监控已启动
[2026-01-17 23:21:06] [INFO] nvitop监控已停止
[2026-01-17 23:21:18] [INFO] nvitop监控已启动
[2026-01-17 23:21:18] [INFO] 服务已停止
[2026-01-17 23:21:19] [INFO] nvitop监控已启动
[2026-01-17 23:21:35] [INFO] [0;36m(APIServer pid=253022)[0;0m [0;36m(Worker_TP1 pid=253471)[0;0m INFO 01-17 23:21:35 [multiproc_executor.py:707] Parent process exited, terminating worker
[2026-01-17 23:21:35] [ERROR] ERROR 01-17 23:21:35 [core_client.py:610] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.
[2026-01-17 23:21:35] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m INFO 01-17 23:21:35 [multiproc_executor.py:707] Parent process exited, terminating worker
[2026-01-17 23:21:35] [INFO] [0;36m(Worker_TP1 pid=253471)[0;0m INFO 01-17 23:21:35 [multiproc_executor.py:751] WorkerProc shutting down.
[2026-01-17 23:21:35] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m INFO 01-17 23:21:35 [multiproc_executor.py:751] WorkerProc shutting down.
[2026-01-17 23:21:35] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m /mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py:147: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.
[2026-01-17 23:21:35] [INFO] [0;36m(Worker_TP0 pid=253470)[0;0m   warnings.warn('resource_tracker: process died unexpectedly, '
[2026-01-17 23:21:35] [INFO] [0;36m(Worker_TP1 pid=253471)[0;0m /mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py:147: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.
[2026-01-17 23:21:35] [INFO] [0;36m(Worker_TP1 pid=253471)[0;0m   warnings.warn('resource_tracker: process died unexpectedly, '
[2026-01-17 23:21:35] [ERROR] Traceback (most recent call last):
[2026-01-17 23:21:35] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-17 23:21:35] [INFO] cache[rtype].remove(name)
[2026-01-17 23:21:35] [INFO] KeyError: '/psm_9655ef00'
[2026-01-17 23:21:35] [ERROR] Traceback (most recent call last):
[2026-01-17 23:21:35] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-17 23:21:35] [INFO] cache[rtype].remove(name)
[2026-01-17 23:21:35] [INFO] KeyError: '/psm_04df7afb'
[2026-01-17 23:21:35] [INFO] 服务已停止
[2026-01-17 23:21:35] [INFO] nvitop监控已启动
[2026-01-17 23:21:35] [ERROR] Traceback (most recent call last):
[2026-01-17 23:21:35] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-17 23:21:35] [INFO] cache[rtype].remove(name)
[2026-01-17 23:21:35] [INFO] KeyError: '/psm_1d9c31ae'
[2026-01-17 23:21:36] [ERROR] Traceback (most recent call last):
[2026-01-17 23:21:36] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-17 23:21:36] [ERROR] Traceback (most recent call last):
[2026-01-17 23:21:36] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-17 23:21:36] [INFO] cache[rtype].remove(name)
[2026-01-17 23:21:36] [INFO] KeyError: '/mp-gpyjq2ya'
[2026-01-17 23:21:36] [INFO] cache[rtype].remove(name)
[2026-01-17 23:21:36] [INFO] KeyError: '/mp-37a58tmd'
[2026-01-17 23:21:36] [INFO] nvitop监控已启动
[2026-01-17 23:21:37] [SUCCESS] 正在关闭服务器...
[2026-01-17 23:21:47] [INFO] VLLM GUI 服务器启动，端口: 5002
[2026-01-17 23:21:47] [INFO] nvitop监控已启动
[2026-01-17 23:21:48] [INFO] nvitop监控已启动
[2026-01-17 23:21:49] [INFO] 从conda info --base自动检测到conda路径: /mnt/AI-Acer4T/miniconda3
[2026-01-17 23:21:54] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-17 23:21:54] [INFO] nvitop监控已启动
[2026-01-17 23:21:54] [INFO] nvitop监控已停止
[2026-01-17 23:22:19] [ERROR] Traceback (most recent call last):
[2026-01-17 23:22:19] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/bin/vllm", line 7, in <module>
[2026-01-17 23:22:19] [INFO] sys.exit(main())
[2026-01-17 23:22:19] [INFO] ^^^^^^
[2026-01-17 23:22:19] [INFO] File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/main.py", line 70, in main
[2026-01-17 23:22:19] [INFO] cmds[args.subparser].validate(args)
[2026-01-17 23:22:19] [INFO] File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/serve.py", line 63, in validate
[2026-01-17 23:22:19] [INFO] validate_parsed_serve_args(args)
[2026-01-17 23:22:19] [INFO] File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/cli_args.py", line 307, in validate_parsed_serve_args
[2026-01-17 23:22:19] [INFO] validate_chat_template(args.chat_template)
[2026-01-17 23:22:19] [INFO] File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/chat_utils.py", line 1199, in validate_chat_template
[2026-01-17 23:22:19] [INFO] raise ValueError(
[2026-01-17 23:22:19] [INFO] ValueError: The supplied chat template string (/mnt/AI-Acer4T/AI-Chat/models/minimax/minimax/MiniMax-M2.1-AWQ/chat_template.jinja) appears path-like, but doesn't exist! Tried: /mnt/AI-Acer4T/AI-Chat/models/minimax/minimax/MiniMax-M2.1-AWQ/chat_template.jinja and /mnt/AI-Acer4T/AI-Chat/models/minimax/minimax/MiniMax-M2.1-AWQ/chat_template.jinja
[2026-01-17 23:22:21] [INFO] nvitop监控已启动
[2026-01-17 23:22:55] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-17 23:22:55] [INFO] nvitop监控已启动
[2026-01-17 23:22:55] [INFO] nvitop监控已停止
[2026-01-17 23:23:21] [ERROR] Traceback (most recent call last):
[2026-01-17 23:23:21] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/bin/vllm", line 7, in <module>
[2026-01-17 23:23:21] [INFO] sys.exit(main())
[2026-01-17 23:23:21] [INFO] ^^^^^^
[2026-01-17 23:23:21] [INFO] File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/main.py", line 70, in main
[2026-01-17 23:23:21] [INFO] cmds[args.subparser].validate(args)
[2026-01-17 23:23:21] [INFO] File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/serve.py", line 63, in validate
[2026-01-17 23:23:21] [INFO] validate_parsed_serve_args(args)
[2026-01-17 23:23:21] [INFO] File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/cli_args.py", line 307, in validate_parsed_serve_args
[2026-01-17 23:23:21] [INFO] validate_chat_template(args.chat_template)
[2026-01-17 23:23:21] [INFO] File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/chat_utils.py", line 1199, in validate_chat_template
[2026-01-17 23:23:21] [INFO] raise ValueError(
[2026-01-17 23:23:21] [INFO] ValueError: The supplied chat template string (/mnt/AI-Acer4T/AI-Chat/models/minimax/minimax/MiniMax-M2.1-AWQ/chat_template.jinja) appears path-like, but doesn't exist! Tried: /mnt/AI-Acer4T/AI-Chat/models/minimax/minimax/MiniMax-M2.1-AWQ/chat_template.jinja and /mnt/AI-Acer4T/AI-Chat/models/minimax/minimax/MiniMax-M2.1-AWQ/chat_template.jinja
[2026-01-17 23:23:22] [INFO] nvitop监控已启动
[2026-01-17 23:23:41] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-17 23:23:41] [INFO] nvitop监控已启动
[2026-01-17 23:23:41] [INFO] nvitop监控已停止
[2026-01-17 23:24:03] [INFO] [0;36m(APIServer pid=262303)[0;0m INFO 01-17 23:24:03 [api_server.py:1278] vLLM API server version 0.14.0rc1.dev492+g19504ac07
[2026-01-17 23:24:03] [INFO] [0;36m(APIServer pid=262303)[0;0m INFO 01-17 23:24:03 [utils.py:254] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/minimax/MiniMax-M2.1-FP8-INT4-AWQ-M', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/AI-Acer4T/AI-Chat/models/minimax/MiniMax-M2.1-FP8-INT4-AWQ-M/chat_template.jinja', 'enable_auto_tool_choice': True, 'tool_call_parser': 'minimax_m2', 'model': '/mnt/AI-Acer4T/AI-Chat/models/minimax/MiniMax-M2.1-FP8-INT4-AWQ-M', 'trust_remote_code': True, 'max_model_len': 128000, 'quantization': 'awq', 'served_model_name': ['VLLM-MODEL'], 'reasoning_parser': 'minimax_m2_append_think', 'tensor_parallel_size': 2, 'all2all_backend': 'pplx', 'kv_cache_dtype': 'fp8', 'max_num_seqs': 256, 'enable_chunked_prefill': True, 'async_scheduling': True}
[2026-01-17 23:24:03] [INFO] [0;36m(APIServer pid=262303)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-17 23:24:03] [INFO] [0;36m(APIServer pid=262303)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-17 23:24:03] [INFO] [0;36m(APIServer pid=262303)[0;0m Config sanitized in configuration_minimax_m2.py
[2026-01-17 23:24:03] [INFO] [0;36m(APIServer pid=262303)[0;0m INFO 01-17 23:24:03 [model.py:528] Resolved architecture: MiniMaxM2ForCausalLM
[2026-01-17 23:24:03] [ERROR] [0;36m(APIServer pid=262303)[0;0m ERROR 01-17 23:24:03 [repo_utils.py:65] Error retrieving safetensors: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/mnt/AI-Acer4T/AI-Chat/models/minimax/MiniMax-M2.1-FP8-INT4-AWQ-M'. Use `repo_type` argument if needed., retrying 1 of 2
[2026-01-17 23:24:05] [ERROR] [0;36m(APIServer pid=262303)[0;0m ERROR 01-17 23:24:05 [repo_utils.py:63] Error retrieving safetensors: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/mnt/AI-Acer4T/AI-Chat/models/minimax/MiniMax-M2.1-FP8-INT4-AWQ-M'. Use `repo_type` argument if needed.
[2026-01-17 23:24:05] [INFO] [0;36m(APIServer pid=262303)[0;0m INFO 01-17 23:24:05 [model.py:1864] Downcasting torch.float32 to torch.bfloat16.
[2026-01-17 23:24:05] [INFO] [0;36m(APIServer pid=262303)[0;0m INFO 01-17 23:24:05 [model.py:1543] Using max model len 128000
[2026-01-17 23:24:06] [ERROR] [0;36m(APIServer pid=262303)[0;0m Traceback (most recent call last):
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/bin/vllm", line 7, in <module>
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m     sys.exit(main())
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m              ^^^^^^
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m     args.dispatch_function(args)
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m     uvloop.run(run_server(args))
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m     return __asyncio.run(
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m            ^^^^^^^^^^^^^^
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m     return runner.run(main)
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m     return self._loop.run_until_complete(task)
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m     return await main
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m            ^^^^^^^^^^
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 1325, in run_server
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 1344, in run_server_worker
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m     async with build_async_engine_client(
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m     return await anext(self.gen)
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 173, in build_async_engine_client
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m     return await anext(self.gen)
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 199, in build_async_engine_client_from_engine_args
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/engine/arg_utils.py", line 1365, in create_engine_config
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m     model_config = self.create_model_config()
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/engine/arg_utils.py", line 1220, in create_model_config
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m     return ModelConfig(
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m            ^^^^^^^^^^^^
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/pydantic/_internal/_dataclasses.py", line 121, in __init__
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m   Value error, Quantization method specified in the model config (compressed-tensors) does not match the quantization method specified in the `quantization` argument (awq). [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]
[2026-01-17 23:24:06] [INFO] [0;36m(APIServer pid=262303)[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error
[2026-01-17 23:24:07] [INFO] nvitop监控已启动
[2026-01-17 23:25:37] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-17 23:25:37] [INFO] nvitop监控已启动
[2026-01-17 23:25:37] [INFO] nvitop监控已停止
[2026-01-17 23:26:02] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:26:02 [api_server.py:1278] vLLM API server version 0.14.0rc1.dev492+g19504ac07
[2026-01-17 23:26:02] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:26:02 [utils.py:254] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/minimax/MiniMax-M2.1-FP8-INT4-AWQ-M', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/AI-Acer4T/AI-Chat/models/minimax/MiniMax-M2.1-FP8-INT4-AWQ-M/chat_template.jinja', 'enable_auto_tool_choice': True, 'tool_call_parser': 'minimax_m2', 'model': '/mnt/AI-Acer4T/AI-Chat/models/minimax/MiniMax-M2.1-FP8-INT4-AWQ-M', 'trust_remote_code': True, 'max_model_len': 128000, 'served_model_name': ['VLLM-MODEL'], 'reasoning_parser': 'minimax_m2_append_think', 'tensor_parallel_size': 2, 'all2all_backend': 'pplx', 'kv_cache_dtype': 'fp8', 'max_num_seqs': 256, 'enable_chunked_prefill': True, 'async_scheduling': True}
[2026-01-17 23:26:02] [INFO] [0;36m(APIServer pid=262986)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-17 23:26:02] [INFO] [0;36m(APIServer pid=262986)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-17 23:26:02] [INFO] [0;36m(APIServer pid=262986)[0;0m Config sanitized in configuration_minimax_m2.py
[2026-01-17 23:26:03] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:26:03 [model.py:528] Resolved architecture: MiniMaxM2ForCausalLM
[2026-01-17 23:26:03] [ERROR] [0;36m(APIServer pid=262986)[0;0m ERROR 01-17 23:26:03 [repo_utils.py:65] Error retrieving safetensors: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/mnt/AI-Acer4T/AI-Chat/models/minimax/MiniMax-M2.1-FP8-INT4-AWQ-M'. Use `repo_type` argument if needed., retrying 1 of 2
[2026-01-17 23:26:05] [ERROR] [0;36m(APIServer pid=262986)[0;0m ERROR 01-17 23:26:05 [repo_utils.py:63] Error retrieving safetensors: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/mnt/AI-Acer4T/AI-Chat/models/minimax/MiniMax-M2.1-FP8-INT4-AWQ-M'. Use `repo_type` argument if needed.
[2026-01-17 23:26:05] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:26:05 [model.py:1864] Downcasting torch.float32 to torch.bfloat16.
[2026-01-17 23:26:05] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:26:05 [model.py:1543] Using max model len 128000
[2026-01-17 23:26:05] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:26:05 [cache.py:206] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor.
[2026-01-17 23:26:05] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:26:05 [scheduler.py:231] Chunked prefill is enabled with max_num_batched_tokens=8192.
[2026-01-17 23:26:05] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:26:05 [vllm.py:640] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-17 23:26:05] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:26:05 [vllm.py:645] Asynchronous scheduling is enabled.
[2026-01-17 23:26:30] [INFO] [0;36m(EngineCore_DP0 pid=263286)[0;0m INFO 01-17 23:26:30 [core.py:97] Initializing a V1 LLM engine (v0.14.0rc1.dev492+g19504ac07) with config: model='/mnt/AI-Acer4T/AI-Chat/models/minimax/MiniMax-M2.1-FP8-INT4-AWQ-M', speculative_config=None, tokenizer='/mnt/AI-Acer4T/AI-Chat/models/minimax/MiniMax-M2.1-FP8-INT4-AWQ-M', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=fp8, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='minimax_m2_append_think', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['+quant_fp8', 'none', '+quant_fp8'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': True, 'fuse_act_quant': True, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-17 23:26:30] [WARNING] [0;36m(EngineCore_DP0 pid=263286)[0;0m WARNING 01-17 23:26:30 [multiproc_executor.py:880] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-17 23:26:55] [INFO] INFO 01-17 23:26:55 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:57891 backend=nccl
[2026-01-17 23:26:55] [INFO] INFO 01-17 23:26:55 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:57891 backend=nccl
[2026-01-17 23:26:55] [INFO] INFO 01-17 23:26:55 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-17 23:26:56] [WARNING] WARNING 01-17 23:26:56 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-17 23:26:56] [WARNING] WARNING 01-17 23:26:56 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-17 23:26:56] [INFO] INFO 01-17 23:26:56 [parallel_state.py:1425] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[2026-01-17 23:26:56] [INFO] INFO 01-17 23:26:56 [parallel_state.py:1425] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
[2026-01-17 23:26:57] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m INFO 01-17 23:26:57 [gpu_model_runner.py:3789] Starting to load model /mnt/AI-Acer4T/AI-Chat/models/minimax/MiniMax-M2.1-FP8-INT4-AWQ-M...
[2026-01-17 23:27:01] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m INFO 01-17 23:27:01 [cuda.py:351] Using FLASHINFER attention backend out of potential backends: ('FLASHINFER', 'TRITON_ATTN')
[2026-01-17 23:27:01] [INFO] [0;36m(Worker_TP1 pid=263552)[0;0m INFO 01-17 23:27:01 [compressed_tensors_moe.py:184] Using CompressedTensorsWNA16MarlinMoEMethod
[2026-01-17 23:27:01] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m INFO 01-17 23:27:01 [compressed_tensors_moe.py:184] Using CompressedTensorsWNA16MarlinMoEMethod
[2026-01-17 23:27:02] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:02] [INFO] Loading safetensors checkpoint shards:   0% Completed | 0/125 [00:00<?, ?it/s]
[2026-01-17 23:27:04] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:04] [INFO] Loading safetensors checkpoint shards:   1% Completed | 1/125 [00:02<05:57,  2.88s/it]
[2026-01-17 23:27:05] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:05] [INFO] Loading safetensors checkpoint shards:   2% Completed | 2/125 [00:03<03:31,  1.72s/it]
[2026-01-17 23:27:07] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:07] [INFO] Loading safetensors checkpoint shards:   2% Completed | 3/125 [00:05<03:35,  1.77s/it]
[2026-01-17 23:27:08] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:08] [INFO] Loading safetensors checkpoint shards:   3% Completed | 4/125 [00:06<02:51,  1.42s/it]
[2026-01-17 23:27:10] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:10] [INFO] Loading safetensors checkpoint shards:   4% Completed | 5/125 [00:08<03:05,  1.55s/it]
[2026-01-17 23:27:11] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:11] [INFO] Loading safetensors checkpoint shards:   5% Completed | 6/125 [00:09<02:35,  1.31s/it]
[2026-01-17 23:27:12] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:12] [INFO] Loading safetensors checkpoint shards:   6% Completed | 7/125 [00:09<02:17,  1.16s/it]
[2026-01-17 23:27:13] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:13] [INFO] Loading safetensors checkpoint shards:   6% Completed | 8/125 [00:11<02:43,  1.40s/it]
[2026-01-17 23:27:14] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:14] [INFO] Loading safetensors checkpoint shards:   7% Completed | 9/125 [00:12<02:28,  1.28s/it]
[2026-01-17 23:27:16] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:16] [INFO] Loading safetensors checkpoint shards:   8% Completed | 10/125 [00:14<02:52,  1.50s/it]
[2026-01-17 23:27:17] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:17] [INFO] Loading safetensors checkpoint shards:   9% Completed | 11/125 [00:15<02:27,  1.29s/it]
[2026-01-17 23:27:19] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:19] [INFO] Loading safetensors checkpoint shards:  10% Completed | 12/125 [00:17<02:45,  1.46s/it]
[2026-01-17 23:27:20] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:20] [INFO] Loading safetensors checkpoint shards:  10% Completed | 13/125 [00:18<02:26,  1.31s/it]
[2026-01-17 23:27:22] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:22] [INFO] Loading safetensors checkpoint shards:  11% Completed | 14/125 [00:20<02:50,  1.54s/it]
[2026-01-17 23:27:23] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:23] [INFO] Loading safetensors checkpoint shards:  12% Completed | 15/125 [00:21<02:32,  1.39s/it]
[2026-01-17 23:27:25] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:25] [INFO] Loading safetensors checkpoint shards:  13% Completed | 16/125 [00:23<02:54,  1.60s/it]
[2026-01-17 23:27:26] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:26] [INFO] Loading safetensors checkpoint shards:  14% Completed | 17/125 [00:24<02:28,  1.37s/it]
[2026-01-17 23:27:28] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:28] [INFO] Loading safetensors checkpoint shards:  14% Completed | 18/125 [00:26<02:38,  1.49s/it]
[2026-01-17 23:27:30] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:30] [INFO] Loading safetensors checkpoint shards:  15% Completed | 19/125 [00:28<02:50,  1.60s/it]
[2026-01-17 23:27:31] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:31] [INFO] Loading safetensors checkpoint shards:  16% Completed | 20/125 [00:29<02:29,  1.42s/it]
[2026-01-17 23:27:33] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:33] [INFO] Loading safetensors checkpoint shards:  17% Completed | 21/125 [00:31<02:49,  1.63s/it]
[2026-01-17 23:27:34] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:34] [INFO] Loading safetensors checkpoint shards:  18% Completed | 22/125 [00:32<02:27,  1.44s/it]
[2026-01-17 23:27:36] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:36] [INFO] Loading safetensors checkpoint shards:  18% Completed | 23/125 [00:34<02:43,  1.60s/it]
[2026-01-17 23:27:37] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:37] [INFO] Loading safetensors checkpoint shards:  19% Completed | 24/125 [00:35<02:19,  1.38s/it]
[2026-01-17 23:27:39] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:39] [INFO] Loading safetensors checkpoint shards:  20% Completed | 25/125 [00:37<02:36,  1.57s/it]
[2026-01-17 23:27:40] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:40] [INFO] Loading safetensors checkpoint shards:  21% Completed | 26/125 [00:38<02:19,  1.41s/it]
[2026-01-17 23:27:42] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:42] [INFO] Loading safetensors checkpoint shards:  22% Completed | 27/125 [00:40<02:38,  1.61s/it]
[2026-01-17 23:27:43] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:43] [INFO] Loading safetensors checkpoint shards:  22% Completed | 28/125 [00:41<02:18,  1.42s/it]
[2026-01-17 23:27:45] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:45] [INFO] Loading safetensors checkpoint shards:  23% Completed | 29/125 [00:43<02:36,  1.63s/it]
[2026-01-17 23:27:46] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:46] [INFO] Loading safetensors checkpoint shards:  24% Completed | 30/125 [00:44<02:14,  1.42s/it]
[2026-01-17 23:27:48] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:48] [INFO] Loading safetensors checkpoint shards:  25% Completed | 31/125 [00:46<02:24,  1.54s/it]
[2026-01-17 23:27:50] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:50] [INFO] Loading safetensors checkpoint shards:  26% Completed | 32/125 [00:48<02:39,  1.71s/it]
[2026-01-17 23:27:51] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:51] [INFO] Loading safetensors checkpoint shards:  26% Completed | 33/125 [00:49<02:15,  1.47s/it]
[2026-01-17 23:27:53] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:53] [INFO] Loading safetensors checkpoint shards:  27% Completed | 34/125 [00:51<02:31,  1.66s/it]
[2026-01-17 23:27:54] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:54] [INFO] Loading safetensors checkpoint shards:  28% Completed | 35/125 [00:52<02:15,  1.51s/it]
[2026-01-17 23:27:56] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:56] [INFO] Loading safetensors checkpoint shards:  29% Completed | 36/125 [00:54<02:34,  1.74s/it]
[2026-01-17 23:27:57] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:57] [INFO] Loading safetensors checkpoint shards:  30% Completed | 37/125 [00:55<02:13,  1.52s/it]
[2026-01-17 23:27:59] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:27:59] [INFO] Loading safetensors checkpoint shards:  30% Completed | 38/125 [00:57<02:27,  1.70s/it]
[2026-01-17 23:28:00] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:00] [INFO] Loading safetensors checkpoint shards:  31% Completed | 39/125 [00:58<02:05,  1.46s/it]
[2026-01-17 23:28:02] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:02] [INFO] Loading safetensors checkpoint shards:  32% Completed | 40/125 [01:00<02:11,  1.55s/it]
[2026-01-17 23:28:03] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:03] [INFO] Loading safetensors checkpoint shards:  33% Completed | 41/125 [01:01<01:54,  1.36s/it]
[2026-01-17 23:28:05] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:05] [INFO] Loading safetensors checkpoint shards:  34% Completed | 42/125 [01:03<02:06,  1.53s/it]
[2026-01-17 23:28:06] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:06] [INFO] Loading safetensors checkpoint shards:  34% Completed | 43/125 [01:04<01:50,  1.34s/it]
[2026-01-17 23:28:07] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:07] [INFO] Loading safetensors checkpoint shards:  35% Completed | 44/125 [01:05<01:37,  1.20s/it]
[2026-01-17 23:28:09] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:09] [INFO] Loading safetensors checkpoint shards:  36% Completed | 45/125 [01:07<01:53,  1.42s/it]
[2026-01-17 23:28:09] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:09] [INFO] Loading safetensors checkpoint shards:  37% Completed | 46/125 [01:07<01:39,  1.26s/it]
[2026-01-17 23:28:11] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:11] [INFO] Loading safetensors checkpoint shards:  38% Completed | 47/125 [01:09<01:53,  1.46s/it]
[2026-01-17 23:28:12] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:12] [INFO] Loading safetensors checkpoint shards:  38% Completed | 48/125 [01:10<01:39,  1.29s/it]
[2026-01-17 23:28:14] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:14] [INFO] Loading safetensors checkpoint shards:  39% Completed | 49/125 [01:12<01:52,  1.48s/it]
[2026-01-17 23:28:15] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:15] [INFO] Loading safetensors checkpoint shards:  40% Completed | 50/125 [01:13<01:37,  1.30s/it]
[2026-01-17 23:28:17] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:17] [INFO] Loading safetensors checkpoint shards:  41% Completed | 51/125 [01:15<01:49,  1.49s/it]
[2026-01-17 23:28:18] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:18] [INFO] Loading safetensors checkpoint shards:  42% Completed | 52/125 [01:16<01:35,  1.31s/it]
[2026-01-17 23:28:20] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:20] [INFO] Loading safetensors checkpoint shards:  42% Completed | 53/125 [01:18<01:47,  1.49s/it]
[2026-01-17 23:28:21] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:21] [INFO] Loading safetensors checkpoint shards:  43% Completed | 54/125 [01:19<01:31,  1.29s/it]
[2026-01-17 23:28:22] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:22] [INFO] Loading safetensors checkpoint shards:  44% Completed | 55/125 [01:20<01:41,  1.45s/it]
[2026-01-17 23:28:23] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:23] [INFO] Loading safetensors checkpoint shards:  45% Completed | 56/125 [01:21<01:29,  1.30s/it]
[2026-01-17 23:28:25] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:25] [INFO] Loading safetensors checkpoint shards:  46% Completed | 57/125 [01:23<01:40,  1.48s/it]
[2026-01-17 23:28:26] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:26] [INFO] Loading safetensors checkpoint shards:  46% Completed | 58/125 [01:24<01:26,  1.30s/it]
[2026-01-17 23:28:27] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:27] [INFO] Loading safetensors checkpoint shards:  47% Completed | 59/125 [01:25<01:16,  1.16s/it]
[2026-01-17 23:28:29] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:29] [INFO] Loading safetensors checkpoint shards:  48% Completed | 60/125 [01:27<01:30,  1.40s/it]
[2026-01-17 23:28:31] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:31] [INFO] Loading safetensors checkpoint shards:  49% Completed | 61/125 [01:29<01:40,  1.57s/it]
[2026-01-17 23:28:32] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:32] [INFO] Loading safetensors checkpoint shards:  50% Completed | 62/125 [01:30<01:27,  1.38s/it]
[2026-01-17 23:28:33] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:33] [INFO] Loading safetensors checkpoint shards:  50% Completed | 63/125 [01:31<01:17,  1.25s/it]
[2026-01-17 23:28:35] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:35] [INFO] Loading safetensors checkpoint shards:  51% Completed | 64/125 [01:33<01:29,  1.46s/it]
[2026-01-17 23:28:36] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:36] [INFO] Loading safetensors checkpoint shards:  52% Completed | 65/125 [01:34<01:18,  1.31s/it]
[2026-01-17 23:28:37] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:37] [INFO] Loading safetensors checkpoint shards:  53% Completed | 66/125 [01:35<01:10,  1.19s/it]
[2026-01-17 23:28:39] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:39] [INFO] Loading safetensors checkpoint shards:  54% Completed | 67/125 [01:36<01:21,  1.40s/it]
[2026-01-17 23:28:39] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:39] [INFO] Loading safetensors checkpoint shards:  54% Completed | 68/125 [01:37<01:10,  1.23s/it]
[2026-01-17 23:28:41] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:41] [INFO] Loading safetensors checkpoint shards:  55% Completed | 69/125 [01:39<01:18,  1.40s/it]
[2026-01-17 23:28:42] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:42] [INFO] Loading safetensors checkpoint shards:  56% Completed | 70/125 [01:40<01:07,  1.23s/it]
[2026-01-17 23:28:44] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:44] [INFO] Loading safetensors checkpoint shards:  57% Completed | 71/125 [01:42<01:14,  1.39s/it]
[2026-01-17 23:28:45] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:45] [INFO] Loading safetensors checkpoint shards:  58% Completed | 72/125 [01:43<01:04,  1.21s/it]
[2026-01-17 23:28:46] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:46] [INFO] Loading safetensors checkpoint shards:  58% Completed | 73/125 [01:44<01:11,  1.38s/it]
[2026-01-17 23:28:47] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:47] [INFO] Loading safetensors checkpoint shards:  59% Completed | 74/125 [01:45<01:01,  1.21s/it]
[2026-01-17 23:28:49] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:49] [INFO] Loading safetensors checkpoint shards:  60% Completed | 75/125 [01:47<01:11,  1.43s/it]
[2026-01-17 23:28:50] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:50] [INFO] Loading safetensors checkpoint shards:  61% Completed | 76/125 [01:48<01:02,  1.27s/it]
[2026-01-17 23:28:52] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:52] [INFO] Loading safetensors checkpoint shards:  62% Completed | 77/125 [01:50<01:08,  1.43s/it]
[2026-01-17 23:28:54] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:54] [INFO] Loading safetensors checkpoint shards:  62% Completed | 78/125 [01:52<01:13,  1.57s/it]
[2026-01-17 23:28:55] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:55] [INFO] Loading safetensors checkpoint shards:  63% Completed | 79/125 [01:53<01:02,  1.36s/it]
[2026-01-17 23:28:58] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:58] [INFO] Loading safetensors checkpoint shards:  64% Completed | 80/125 [01:56<01:24,  1.88s/it]
[2026-01-17 23:28:59] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:28:59] [INFO] Loading safetensors checkpoint shards:  65% Completed | 81/125 [01:57<01:09,  1.59s/it]
[2026-01-17 23:29:01] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:01] [INFO] Loading safetensors checkpoint shards:  66% Completed | 82/125 [01:58<01:12,  1.69s/it]
[2026-01-17 23:29:01] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:01] [INFO] Loading safetensors checkpoint shards:  66% Completed | 83/125 [01:59<01:01,  1.47s/it]
[2026-01-17 23:29:03] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:03] [INFO] Loading safetensors checkpoint shards:  67% Completed | 84/125 [02:01<01:06,  1.63s/it]
[2026-01-17 23:29:04] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:04] [INFO] Loading safetensors checkpoint shards:  68% Completed | 85/125 [02:02<00:56,  1.42s/it]
[2026-01-17 23:29:06] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:06] [INFO] Loading safetensors checkpoint shards:  69% Completed | 86/125 [02:04<01:01,  1.58s/it]
[2026-01-17 23:29:07] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:07] [INFO] Loading safetensors checkpoint shards:  70% Completed | 87/125 [02:05<00:52,  1.39s/it]
[2026-01-17 23:29:09] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:09] [INFO] Loading safetensors checkpoint shards:  70% Completed | 88/125 [02:07<00:57,  1.56s/it]
[2026-01-17 23:29:10] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:10] [INFO] Loading safetensors checkpoint shards:  71% Completed | 89/125 [02:08<00:49,  1.38s/it]
[2026-01-17 23:29:12] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:12] [INFO] Loading safetensors checkpoint shards:  72% Completed | 90/125 [02:10<00:54,  1.56s/it]
[2026-01-17 23:29:14] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:14] [INFO] Loading safetensors checkpoint shards:  73% Completed | 91/125 [02:12<00:57,  1.69s/it]
[2026-01-17 23:29:15] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:15] [INFO] Loading safetensors checkpoint shards:  74% Completed | 92/125 [02:13<00:48,  1.46s/it]
[2026-01-17 23:29:17] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:17] [INFO] Loading safetensors checkpoint shards:  74% Completed | 93/125 [02:15<00:51,  1.62s/it]
[2026-01-17 23:29:18] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:18] [INFO] Loading safetensors checkpoint shards:  75% Completed | 94/125 [02:16<00:43,  1.42s/it]
[2026-01-17 23:29:20] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:20] [INFO] Loading safetensors checkpoint shards:  76% Completed | 95/125 [02:18<00:45,  1.53s/it]
[2026-01-17 23:29:21] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:21] [INFO] Loading safetensors checkpoint shards:  77% Completed | 96/125 [02:19<00:38,  1.32s/it]
[2026-01-17 23:29:22] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:22] [INFO] Loading safetensors checkpoint shards:  78% Completed | 97/125 [02:20<00:40,  1.45s/it]
[2026-01-17 23:29:23] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:23] [INFO] Loading safetensors checkpoint shards:  78% Completed | 98/125 [02:21<00:34,  1.26s/it]
[2026-01-17 23:29:25] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:25] [INFO] Loading safetensors checkpoint shards:  79% Completed | 99/125 [02:23<00:36,  1.41s/it]
[2026-01-17 23:29:26] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:26] [INFO] Loading safetensors checkpoint shards:  80% Completed | 100/125 [02:24<00:30,  1.24s/it]
[2026-01-17 23:29:28] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:28] [INFO] Loading safetensors checkpoint shards:  81% Completed | 101/125 [02:26<00:33,  1.39s/it]
[2026-01-17 23:29:29] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:29] [INFO] Loading safetensors checkpoint shards:  82% Completed | 102/125 [02:26<00:28,  1.25s/it]
[2026-01-17 23:29:29] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:29] [INFO] Loading safetensors checkpoint shards:  82% Completed | 103/125 [02:27<00:25,  1.16s/it]
[2026-01-17 23:29:31] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:31] [INFO] Loading safetensors checkpoint shards:  83% Completed | 104/125 [02:29<00:29,  1.40s/it]
[2026-01-17 23:29:32] [INFO] Executed command in new terminal (gnome-terminal): nvitop
[2026-01-17 23:29:32] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:32] [INFO] Loading safetensors checkpoint shards:  84% Completed | 105/125 [02:30<00:25,  1.27s/it]
[2026-01-17 23:29:34] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:34] [INFO] Loading safetensors checkpoint shards:  85% Completed | 106/125 [02:32<00:28,  1.49s/it]
[2026-01-17 23:29:35] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:35] [INFO] Loading safetensors checkpoint shards:  86% Completed | 107/125 [02:33<00:23,  1.33s/it]
[2026-01-17 23:29:37] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:37] [INFO] Loading safetensors checkpoint shards:  86% Completed | 108/125 [02:35<00:26,  1.53s/it]
[2026-01-17 23:29:38] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:38] [INFO] Loading safetensors checkpoint shards:  87% Completed | 109/125 [02:36<00:21,  1.36s/it]
[2026-01-17 23:29:40] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:40] [INFO] Loading safetensors checkpoint shards:  88% Completed | 110/125 [02:38<00:23,  1.53s/it]
[2026-01-17 23:29:41] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:41] [INFO] Loading safetensors checkpoint shards:  89% Completed | 111/125 [02:39<00:19,  1.36s/it]
[2026-01-17 23:29:43] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:43] [INFO] Loading safetensors checkpoint shards:  90% Completed | 112/125 [02:41<00:20,  1.55s/it]
[2026-01-17 23:29:44] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:44] [INFO] Loading safetensors checkpoint shards:  90% Completed | 113/125 [02:42<00:16,  1.34s/it]
[2026-01-17 23:29:46] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:46] [INFO] Loading safetensors checkpoint shards:  91% Completed | 114/125 [02:44<00:16,  1.46s/it]
[2026-01-17 23:29:48] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:48] [INFO] Loading safetensors checkpoint shards:  92% Completed | 115/125 [02:46<00:15,  1.55s/it]
[2026-01-17 23:29:48] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:48] [INFO] Loading safetensors checkpoint shards:  93% Completed | 116/125 [02:46<00:12,  1.34s/it]
[2026-01-17 23:29:50] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:50] [INFO] Loading safetensors checkpoint shards:  94% Completed | 117/125 [02:48<00:11,  1.48s/it]
[2026-01-17 23:29:51] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:51] [INFO] Loading safetensors checkpoint shards:  94% Completed | 118/125 [02:49<00:09,  1.29s/it]
[2026-01-17 23:29:53] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:53] [INFO] Loading safetensors checkpoint shards:  95% Completed | 119/125 [02:51<00:08,  1.43s/it]
[2026-01-17 23:29:54] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:54] [INFO] Loading safetensors checkpoint shards:  96% Completed | 120/125 [02:52<00:06,  1.25s/it]
[2026-01-17 23:29:55] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:55] [INFO] Loading safetensors checkpoint shards:  97% Completed | 121/125 [02:53<00:05,  1.41s/it]
[2026-01-17 23:29:56] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:56] [INFO] Loading safetensors checkpoint shards:  98% Completed | 122/125 [02:54<00:03,  1.24s/it]
[2026-01-17 23:29:58] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:58] [INFO] Loading safetensors checkpoint shards:  98% Completed | 123/125 [02:56<00:02,  1.40s/it]
[2026-01-17 23:29:59] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:29:59] [INFO] Loading safetensors checkpoint shards:  99% Completed | 124/125 [02:57<00:01,  1.24s/it]
[2026-01-17 23:30:00] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:30:00] [INFO] Loading safetensors checkpoint shards: 100% Completed | 125/125 [02:58<00:00,  1.20s/it]
[2026-01-17 23:30:00] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:30:00] [INFO] Loading safetensors checkpoint shards: 100% Completed | 125/125 [02:58<00:00,  1.43s/it]
[2026-01-17 23:30:00] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:30:00] [WARNING] [0;36m(Worker_TP1 pid=263552)[0;0m WARNING 01-17 23:30:00 [kv_cache.py:90] Checkpoint does not provide a q scaling factor. Setting it to k_scale. This only matters for FP8 Attention backends (flash-attn or flashinfer).
[2026-01-17 23:30:00] [WARNING] [0;36m(Worker_TP1 pid=263552)[0;0m WARNING 01-17 23:30:00 [kv_cache.py:104] Using KV cache scaling factor 1.0 for fp8_e4m3. If this is unintended, verify that k/v_scale scaling factors are properly set in the checkpoint.
[2026-01-17 23:30:00] [WARNING] [0;36m(Worker_TP1 pid=263552)[0;0m WARNING 01-17 23:30:00 [kv_cache.py:143] Using uncalibrated q_scale 1.0 and/or prob_scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure q/prob scaling factors are available in the fp8 checkpoint.
[2026-01-17 23:30:00] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m INFO 01-17 23:30:00 [default_loader.py:291] Loading weights took 178.48 seconds
[2026-01-17 23:30:00] [WARNING] [0;36m(Worker_TP0 pid=263551)[0;0m WARNING 01-17 23:30:00 [kv_cache.py:90] Checkpoint does not provide a q scaling factor. Setting it to k_scale. This only matters for FP8 Attention backends (flash-attn or flashinfer).
[2026-01-17 23:30:00] [WARNING] [0;36m(Worker_TP0 pid=263551)[0;0m WARNING 01-17 23:30:00 [kv_cache.py:104] Using KV cache scaling factor 1.0 for fp8_e4m3. If this is unintended, verify that k/v_scale scaling factors are properly set in the checkpoint.
[2026-01-17 23:30:00] [WARNING] [0;36m(Worker_TP0 pid=263551)[0;0m WARNING 01-17 23:30:00 [kv_cache.py:143] Using uncalibrated q_scale 1.0 and/or prob_scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure q/prob scaling factors are available in the fp8 checkpoint.
[2026-01-17 23:30:02] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m INFO 01-17 23:30:02 [gpu_model_runner.py:3886] Model loading took 61.47 GiB memory and 184.997695 seconds
[2026-01-17 23:30:20] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m INFO 01-17 23:30:20 [backends.py:644] Using cache directory: /home/wuwen/.cache/vllm/torch_compile_cache/5655bae0ef/rank_0_0/backbone for vLLM's torch.compile
[2026-01-17 23:30:20] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m INFO 01-17 23:30:20 [backends.py:704] Dynamo bytecode transform time: 16.95 s
[2026-01-17 23:30:37] [INFO] [0;36m(Worker_TP1 pid=263552)[0;0m INFO 01-17 23:30:37 [backends.py:261] Cache the graph of compile range (1, 8192) for later use
[2026-01-17 23:30:37] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m INFO 01-17 23:30:37 [backends.py:261] Cache the graph of compile range (1, 8192) for later use
[2026-01-17 23:30:38] [INFO] [0;36m(Worker_TP1 pid=263552)[0;0m /mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:312: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
[2026-01-17 23:30:38] [INFO] [0;36m(Worker_TP1 pid=263552)[0;0m   warnings.warn(
[2026-01-17 23:30:38] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m /mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:312: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
[2026-01-17 23:30:38] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m   warnings.warn(
[2026-01-17 23:31:02] [INFO] [0;36m(EngineCore_DP0 pid=263286)[0;0m INFO 01-17 23:31:02 [shm_broadcast.py:542] No available shared memory broadcast block found in 60 seconds. This typically happens when some processes are hanging or doing some time-consuming work (e.g. compilation, weight/kv cache quantization).
[2026-01-17 23:32:02] [INFO] [0;36m(EngineCore_DP0 pid=263286)[0;0m INFO 01-17 23:32:02 [shm_broadcast.py:542] No available shared memory broadcast block found in 60 seconds. This typically happens when some processes are hanging or doing some time-consuming work (e.g. compilation, weight/kv cache quantization).
[2026-01-17 23:32:30] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m INFO 01-17 23:32:30 [backends.py:278] Compiling a graph for compile range (1, 8192) takes 115.33 s
[2026-01-17 23:32:30] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m INFO 01-17 23:32:30 [monitor.py:34] torch.compile takes 132.28 s in total
[2026-01-17 23:32:31] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m INFO 01-17 23:32:31 [gpu_worker.py:358] Available KV cache memory: 22.04 GiB
[2026-01-17 23:32:32] [INFO] [0;36m(EngineCore_DP0 pid=263286)[0;0m INFO 01-17 23:32:32 [kv_cache_utils.py:1305] GPU KV cache size: 372,768 tokens
[2026-01-17 23:32:32] [INFO] [0;36m(EngineCore_DP0 pid=263286)[0;0m INFO 01-17 23:32:32 [kv_cache_utils.py:1310] Maximum concurrency for 128,000 tokens per request: 2.91x
[2026-01-17 23:32:32] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m 2026-01-17 23:32:32,361 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[2026-01-17 23:32:32] [INFO] [0;36m(Worker_TP1 pid=263552)[0;0m 2026-01-17 23:32:32,361 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[2026-01-17 23:32:32] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m 2026-01-17 23:32:32,517 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[2026-01-17 23:32:32] [INFO] [0;36m(Worker_TP1 pid=263552)[0;0m 2026-01-17 23:32:32,517 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[2026-01-17 23:32:32] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m INFO 01-17 23:32:32 [kernel_warmup.py:64] Warming up FlashInfer attention.
[2026-01-17 23:32:32] [INFO] [0;36m(Worker_TP1 pid=263552)[0;0m INFO 01-17 23:32:32 [kernel_warmup.py:64] Warming up FlashInfer attention.
[2026-01-17 23:32:33] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:32:34] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
[2026-01-17 23:32:34] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:05,  9.12it/s]
[2026-01-17 23:32:34] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:05,  9.27it/s]
[2026-01-17 23:32:34] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:05,  9.32it/s]
[2026-01-17 23:32:34] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:00<00:04,  9.48it/s]
[2026-01-17 23:32:34] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:00<00:04,  9.52it/s]
[2026-01-17 23:32:34] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:04,  9.33it/s]
[2026-01-17 23:32:34] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▎        | 7/51 [00:00<00:04,  8.92it/s]
[2026-01-17 23:32:34] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:00<00:04,  9.10it/s]
[2026-01-17 23:32:35] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:00<00:04,  9.19it/s]
[2026-01-17 23:32:35] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|█▉        | 10/51 [00:01<00:04,  9.21it/s]
[2026-01-17 23:32:35] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:01<00:04,  9.23it/s]
[2026-01-17 23:32:35] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:01<00:04,  9.10it/s]
[2026-01-17 23:32:35] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:01<00:04,  9.13it/s]
[2026-01-17 23:32:35] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:01<00:04,  9.21it/s]
[2026-01-17 23:32:35] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:01<00:03,  9.34it/s]
[2026-01-17 23:32:35] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 16/51 [00:01<00:03,  9.44it/s]
[2026-01-17 23:32:35] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:01<00:03,  9.45it/s]
[2026-01-17 23:32:36] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:01<00:03,  9.47it/s]
[2026-01-17 23:32:36] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 19/51 [00:02<00:03,  9.43it/s]
[2026-01-17 23:32:36] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:02<00:03,  9.39it/s]
[2026-01-17 23:32:36] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:02<00:03,  9.36it/s]
[2026-01-17 23:32:36] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 22/51 [00:02<00:03,  9.38it/s]
[2026-01-17 23:32:36] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:02<00:02,  9.38it/s]
[2026-01-17 23:32:36] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:02<00:02,  9.43it/s]
[2026-01-17 23:32:36] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 25/51 [00:02<00:02,  9.35it/s]
[2026-01-17 23:32:36] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:02<00:02,  9.37it/s]
[2026-01-17 23:32:36] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:02<00:02,  9.34it/s]
[2026-01-17 23:32:37] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:03<00:02,  9.31it/s]
[2026-01-17 23:32:37] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:03<00:02,  9.30it/s]
[2026-01-17 23:32:37] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:03<00:02,  9.28it/s]
[2026-01-17 23:32:37] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 31/51 [00:03<00:02,  9.19it/s]
[2026-01-17 23:32:37] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:03<00:02,  9.23it/s]
[2026-01-17 23:32:37] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:03<00:01,  9.23it/s]
[2026-01-17 23:32:37] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:03<00:01,  9.25it/s]
[2026-01-17 23:32:37] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:03<00:01,  9.26it/s]
[2026-01-17 23:32:37] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:03<00:01,  9.25it/s]
[2026-01-17 23:32:38] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 37/51 [00:03<00:01,  9.29it/s]
[2026-01-17 23:32:38] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:04<00:01,  9.29it/s]
[2026-01-17 23:32:38] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▋  | 39/51 [00:04<00:01,  9.28it/s]
[2026-01-17 23:32:38] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:04<00:01,  9.29it/s]
[2026-01-17 23:32:38] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:04<00:01,  9.28it/s]
[2026-01-17 23:32:38] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:04<00:00,  9.24it/s]
[2026-01-17 23:32:38] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 43/51 [00:04<00:00,  9.18it/s]
[2026-01-17 23:32:38] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:04<00:00,  9.15it/s]
[2026-01-17 23:32:38] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 45/51 [00:04<00:00,  9.23it/s]
[2026-01-17 23:32:39] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|█████████ | 46/51 [00:04<00:00,  9.26it/s]
[2026-01-17 23:32:39] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:05<00:00,  9.12it/s]
[2026-01-17 23:32:39] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 48/51 [00:05<00:00,  9.10it/s]
[2026-01-17 23:32:39] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▌| 49/51 [00:05<00:00,  9.18it/s]
[2026-01-17 23:32:39] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:05<00:00,  9.13it/s]
[2026-01-17 23:32:39] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:05<00:00,  4.42it/s]
[2026-01-17 23:32:39] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:05<00:00,  8.65it/s]
[2026-01-17 23:32:39] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m
[2026-01-17 23:32:39] [INFO] Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
[2026-01-17 23:32:40] [INFO] Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:03,  8.87it/s]
[2026-01-17 23:32:40] [INFO] Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:00<00:03,  8.94it/s]
[2026-01-17 23:32:40] [INFO] Capturing CUDA graphs (decode, FULL):   9%|▊         | 3/35 [00:00<00:03,  9.02it/s]
[2026-01-17 23:32:40] [INFO] Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:03,  9.03it/s]
[2026-01-17 23:32:40] [INFO] Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:00<00:03,  9.12it/s]
[2026-01-17 23:32:40] [INFO] Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:00<00:03,  8.98it/s]
[2026-01-17 23:32:40] [INFO] Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:00<00:03,  8.91it/s]
[2026-01-17 23:32:40] [INFO] Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:00<00:03,  8.92it/s]
[2026-01-17 23:32:40] [INFO] Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:01<00:02,  9.00it/s]
[2026-01-17 23:32:41] [INFO] Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:01<00:02,  9.05it/s]
[2026-01-17 23:32:41] [INFO] Capturing CUDA graphs (decode, FULL):  31%|███▏      | 11/35 [00:01<00:02,  9.09it/s]
[2026-01-17 23:32:41] [INFO] Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:01<00:02,  9.09it/s]
[2026-01-17 23:32:41] [INFO] Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:01<00:02,  9.11it/s]
[2026-01-17 23:32:41] [INFO] Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:01<00:02,  9.09it/s]
[2026-01-17 23:32:41] [INFO] Capturing CUDA graphs (decode, FULL):  43%|████▎     | 15/35 [00:01<00:02,  9.12it/s]
[2026-01-17 23:32:41] [INFO] Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:01<00:02,  9.11it/s]
[2026-01-17 23:32:41] [INFO] Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:01<00:01,  9.07it/s]
[2026-01-17 23:32:41] [INFO] Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:01<00:01,  9.03it/s]
[2026-01-17 23:32:42] [INFO] Capturing CUDA graphs (decode, FULL):  54%|█████▍    | 19/35 [00:02<00:01,  9.08it/s]
[2026-01-17 23:32:42] [INFO] Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:02<00:01,  9.10it/s]
[2026-01-17 23:32:42] [INFO] Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:02<00:01,  9.09it/s]
[2026-01-17 23:32:42] [INFO] Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:02<00:01,  9.12it/s]
[2026-01-17 23:32:42] [INFO] Capturing CUDA graphs (decode, FULL):  66%|██████▌   | 23/35 [00:02<00:01,  9.08it/s]
[2026-01-17 23:32:42] [INFO] Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:02<00:01,  9.06it/s]
[2026-01-17 23:32:42] [INFO] Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:02<00:01,  9.04it/s]
[2026-01-17 23:32:42] [INFO] Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:02<00:00,  9.08it/s]
[2026-01-17 23:32:42] [INFO] Capturing CUDA graphs (decode, FULL):  77%|███████▋  | 27/35 [00:02<00:00,  9.14it/s]
[2026-01-17 23:32:43] [INFO] Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:03<00:00,  9.12it/s]
[2026-01-17 23:32:43] [INFO] Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:03<00:00,  9.16it/s]
[2026-01-17 23:32:43] [INFO] Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:03<00:00,  9.18it/s]
[2026-01-17 23:32:43] [INFO] Capturing CUDA graphs (decode, FULL):  89%|████████▊ | 31/35 [00:03<00:00,  9.21it/s]
[2026-01-17 23:32:43] [INFO] Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:03<00:00,  9.22it/s]
[2026-01-17 23:32:43] [INFO] Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:03<00:00,  9.20it/s]
[2026-01-17 23:32:43] [INFO] Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:03<00:00,  9.25it/s]
[2026-01-17 23:32:43] [INFO] Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:03<00:00,  9.03it/s]
[2026-01-17 23:32:43] [INFO] Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:03<00:00,  9.08it/s]
[2026-01-17 23:32:43] [INFO] [0;36m(Worker_TP1 pid=263552)[0;0m INFO 01-17 23:32:43 [custom_all_reduce.py:216] Registering 15958 cuda graph addresses
[2026-01-17 23:32:43] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m INFO 01-17 23:32:43 [custom_all_reduce.py:216] Registering 15958 cuda graph addresses
[2026-01-17 23:32:44] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m INFO 01-17 23:32:44 [gpu_model_runner.py:4837] Graph capturing finished in 11 secs, took 0.66 GiB
[2026-01-17 23:32:44] [INFO] [0;36m(EngineCore_DP0 pid=263286)[0;0m INFO 01-17 23:32:44 [core.py:273] init engine (profile, create kv cache, warmup model) took 161.61 seconds
[2026-01-17 23:32:45] [INFO] [0;36m(EngineCore_DP0 pid=263286)[0;0m INFO 01-17 23:32:45 [core.py:186] Batch queue is enabled with size 2
[2026-01-17 23:32:45] [INFO] [0;36m(EngineCore_DP0 pid=263286)[0;0m INFO 01-17 23:32:45 [vllm.py:645] Asynchronous scheduling is enabled.
[2026-01-17 23:32:45] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:45 [api_server.py:1020] Supported tasks: ['generate']
[2026-01-17 23:32:45] [INFO] [0;36m(APIServer pid=262986)[0;0m Config sanitized in configuration_minimax_m2.py
[2026-01-17 23:32:46] [WARNING] [0;36m(APIServer pid=262986)[0;0m WARNING 01-17 23:32:46 [model.py:1356] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [serving_responses.py:224] Using default chat sampling params from model: {'top_k': 40, 'top_p': 0.95}
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [serving_engine.py:271] "auto" tool choice has been enabled.
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [serving_engine.py:271] "auto" tool choice has been enabled.
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [serving_chat.py:146] Using default chat sampling params from model: {'top_k': 40, 'top_p': 0.95}
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [serving_chat.py:182] Warming up chat template processing...
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [chat_utils.py:599] Detected the chat template content format to be 'openai'. You can set `--chat-template-content-format` to override this.
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [serving_chat.py:218] Chat template warmup completed in 45.9ms
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [serving_completion.py:78] Using default completion sampling params from model: {'top_k': 40, 'top_p': 0.95}
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [serving_engine.py:271] "auto" tool choice has been enabled.
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [serving_chat.py:146] Using default chat sampling params from model: {'top_k': 40, 'top_p': 0.95}
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [api_server.py:1352] Starting vLLM API server 0 on http://0.0.0.0:8005
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:38] Available routes are:
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /docs, Methods: GET, HEAD
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /tokenize, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /detokenize, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /pause, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /resume, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /is_paused, Methods: GET
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /metrics, Methods: GET
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /health, Methods: GET
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /load, Methods: GET
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /v1/models, Methods: GET
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /version, Methods: GET
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /v1/responses, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /v1/messages, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /v1/completions, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /ping, Methods: GET
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /ping, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /invocations, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /classify, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /score, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /v1/score, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /rerank, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /v1/rerank, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /v2/rerank, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:32:46 [launcher.py:46] Route: /pooling, Methods: POST
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     Started server process [262986]
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     Waiting for application startup.
[2026-01-17 23:32:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     Application startup complete.
[2026-01-17 23:34:01] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:34:22] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:34:26] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:34:26 [loggers.py:257] Engine 000: Avg prompt throughput: 17525.8 tokens/s, Avg generation throughput: 34.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 49.8%
[2026-01-17 23:34:28] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:34:32] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:34:36] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:34:36 [loggers.py:257] Engine 000: Avg prompt throughput: 17688.1 tokens/s, Avg generation throughput: 42.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 23.9%, Prefix cache hit rate: 74.8%
[2026-01-17 23:34:43] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:34:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:34:46 [loggers.py:257] Engine 000: Avg prompt throughput: 8935.5 tokens/s, Avg generation throughput: 48.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 79.8%
[2026-01-17 23:34:47] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:34:50] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:34:53] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:34:56] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:34:56 [loggers.py:257] Engine 000: Avg prompt throughput: 27147.6 tokens/s, Avg generation throughput: 35.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 87.2%
[2026-01-17 23:34:58] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:35:02] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:35:06] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:35:06 [loggers.py:257] Engine 000: Avg prompt throughput: 18389.5 tokens/s, Avg generation throughput: 38.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 24.8%, Prefix cache hit rate: 89.7%
[2026-01-17 23:35:09] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:35:16] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:35:16] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:35:16 [loggers.py:257] Engine 000: Avg prompt throughput: 9263.4 tokens/s, Avg generation throughput: 40.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 90.6%
[2026-01-17 23:35:22] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:35:26] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:35:26 [loggers.py:257] Engine 000: Avg prompt throughput: 18597.7 tokens/s, Avg generation throughput: 35.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 92.1%
[2026-01-17 23:35:26] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:35:36] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:35:36 [loggers.py:257] Engine 000: Avg prompt throughput: 9336.0 tokens/s, Avg generation throughput: 67.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 92.6%
[2026-01-17 23:35:38] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:35:43] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:35:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:35:46 [loggers.py:257] Engine 000: Avg prompt throughput: 18824.7 tokens/s, Avg generation throughput: 36.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 93.5%
[2026-01-17 23:35:47] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:35:51] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:35:56] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:35:56] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:35:56 [loggers.py:257] Engine 000: Avg prompt throughput: 28410.5 tokens/s, Avg generation throughput: 27.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 25.5%, Prefix cache hit rate: 94.5%
[2026-01-17 23:35:59] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:36:03] [ERROR] [0;36m(APIServer pid=262986)[0;0m ERROR 01-17 23:36:03 [serving_chat.py:331] Error in preprocessing prompt inputs
[2026-01-17 23:36:03] [ERROR] [0;36m(APIServer pid=262986)[0;0m ERROR 01-17 23:36:03 [serving_chat.py:331] Traceback (most recent call last):
[2026-01-17 23:36:03] [ERROR] [0;36m(APIServer pid=262986)[0;0m ERROR 01-17 23:36:03 [serving_chat.py:331]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/serving_chat.py", line 309, in create_chat_completion
[2026-01-17 23:36:03] [ERROR] [0;36m(APIServer pid=262986)[0;0m ERROR 01-17 23:36:03 [serving_chat.py:331]     conversation, engine_prompts = await self._preprocess_chat(
[2026-01-17 23:36:03] [ERROR] [0;36m(APIServer pid=262986)[0;0m ERROR 01-17 23:36:03 [serving_chat.py:331]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-17 23:36:03] [ERROR] [0;36m(APIServer pid=262986)[0;0m ERROR 01-17 23:36:03 [serving_chat.py:331]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/serving_engine.py", line 1254, in _preprocess_chat
[2026-01-17 23:36:03] [ERROR] [0;36m(APIServer pid=262986)[0;0m ERROR 01-17 23:36:03 [serving_chat.py:331]     prompt_inputs = await self._tokenize_prompt_input_async(
[2026-01-17 23:36:03] [ERROR] [0;36m(APIServer pid=262986)[0;0m ERROR 01-17 23:36:03 [serving_chat.py:331]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-17 23:36:03] [ERROR] [0;36m(APIServer pid=262986)[0;0m ERROR 01-17 23:36:03 [serving_chat.py:331]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/serving_engine.py", line 1095, in _tokenize_prompt_input_async
[2026-01-17 23:36:03] [ERROR] [0;36m(APIServer pid=262986)[0;0m ERROR 01-17 23:36:03 [serving_chat.py:331]     async for result in self._tokenize_prompt_inputs_async(
[2026-01-17 23:36:03] [ERROR] [0;36m(APIServer pid=262986)[0;0m ERROR 01-17 23:36:03 [serving_chat.py:331]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/serving_engine.py", line 1116, in _tokenize_prompt_inputs_async
[2026-01-17 23:36:03] [ERROR] [0;36m(APIServer pid=262986)[0;0m ERROR 01-17 23:36:03 [serving_chat.py:331]     yield await self._normalize_prompt_text_to_input(
[2026-01-17 23:36:03] [ERROR] [0;36m(APIServer pid=262986)[0;0m ERROR 01-17 23:36:03 [serving_chat.py:331]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-17 23:36:03] [ERROR] [0;36m(APIServer pid=262986)[0;0m ERROR 01-17 23:36:03 [serving_chat.py:331]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/serving_engine.py", line 980, in _normalize_prompt_text_to_input
[2026-01-17 23:36:03] [ERROR] [0;36m(APIServer pid=262986)[0;0m ERROR 01-17 23:36:03 [serving_chat.py:331]     return self._validate_input(request, input_ids, input_text)
[2026-01-17 23:36:03] [ERROR] [0;36m(APIServer pid=262986)[0;0m ERROR 01-17 23:36:03 [serving_chat.py:331]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-17 23:36:03] [ERROR] [0;36m(APIServer pid=262986)[0;0m ERROR 01-17 23:36:03 [serving_chat.py:331]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/serving_engine.py", line 1073, in _validate_input
[2026-01-17 23:36:03] [ERROR] [0;36m(APIServer pid=262986)[0;0m ERROR 01-17 23:36:03 [serving_chat.py:331]     raise VLLMValidationError(
[2026-01-17 23:36:03] [ERROR] [0;36m(APIServer pid=262986)[0;0m ERROR 01-17 23:36:03 [serving_chat.py:331] vllm.exceptions.VLLMValidationError: 'max_tokens' or 'max_completion_tokens' is too large: 32000. This model's maximum context length is 128000 tokens and your request has 96104 input tokens (32000 > 128000 - 96104). (parameter=max_tokens, value=32000)
[2026-01-17 23:36:03] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
[2026-01-17 23:36:04] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:36:06] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:36:06 [loggers.py:257] Engine 000: Avg prompt throughput: 9580.3 tokens/s, Avg generation throughput: 20.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.6%, Prefix cache hit rate: 91.1%
[2026-01-17 23:36:16] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:36:16 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 19.6%, Prefix cache hit rate: 91.1%
[2026-01-17 23:36:26] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:36:26 [loggers.py:257] Engine 000: Avg prompt throughput: 7320.8 tokens/s, Avg generation throughput: 62.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 91.1%
[2026-01-17 23:36:27] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:36:30] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:36:33] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:36:36] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:36:36] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:36:36 [loggers.py:257] Engine 000: Avg prompt throughput: 7155.9 tokens/s, Avg generation throughput: 35.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 91.4%
[2026-01-17 23:36:40] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:36:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:36:46 [loggers.py:257] Engine 000: Avg prompt throughput: 5662.6 tokens/s, Avg generation throughput: 54.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 91.4%
[2026-01-17 23:36:47] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:36:51] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:36:56] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:36:56 [loggers.py:257] Engine 000: Avg prompt throughput: 3495.6 tokens/s, Avg generation throughput: 87.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 91.3%
[2026-01-17 23:37:03] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:37:06] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:37:06 [loggers.py:257] Engine 000: Avg prompt throughput: 2404.9 tokens/s, Avg generation throughput: 88.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.5%, Prefix cache hit rate: 91.3%
[2026-01-17 23:37:08] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:37:13] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:37:16] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:37:16 [loggers.py:257] Engine 000: Avg prompt throughput: 5858.1 tokens/s, Avg generation throughput: 43.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 91.3%
[2026-01-17 23:37:17] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:37:20] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:37:26] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:37:26] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:37:26 [loggers.py:257] Engine 000: Avg prompt throughput: 6119.9 tokens/s, Avg generation throughput: 59.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 91.5%
[2026-01-17 23:37:30] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:37:33] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:37:36] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:37:36 [loggers.py:257] Engine 000: Avg prompt throughput: 9421.5 tokens/s, Avg generation throughput: 57.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 91.8%
[2026-01-17 23:37:38] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:37:43] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:37:46] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:37:46 [loggers.py:257] Engine 000: Avg prompt throughput: 6582.8 tokens/s, Avg generation throughput: 39.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 91.9%
[2026-01-17 23:37:47] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO:     127.0.0.1:47562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-17 23:37:56] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:37:56 [loggers.py:257] Engine 000: Avg prompt throughput: 3803.9 tokens/s, Avg generation throughput: 44.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 91.9%
[2026-01-17 23:38:06] [INFO] [0;36m(APIServer pid=262986)[0;0m INFO 01-17 23:38:06 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 91.9%
[2026-01-17 23:39:22] [INFO] VLLM GUI 服务器启动，端口: 5003
[2026-01-17 23:39:23] [INFO] nvitop监控已启动
[2026-01-17 23:39:24] [INFO] nvitop监控已启动
[2026-01-17 23:39:25] [INFO] 从conda info --base自动检测到conda路径: /mnt/AI-Acer4T/miniconda3
[2026-01-17 23:39:27] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-17 23:39:27] [INFO] nvitop监控已启动
[2026-01-17 23:39:27] [INFO] nvitop监控已停止
[2026-01-17 23:39:38] [WARNING] WARNING 01-17 23:39:38 [cuda.py:569] Detected different devices in the system: NVIDIA RTX PRO 6000 Blackwell Workstation Edition, NVIDIA RTX PRO 6000 Blackwell Workstation Edition, NVIDIA GeForce RTX 5060 Ti. Please make sure to set `CUDA_DEVICE_ORDER=PCI_BUS_ID` to avoid unexpected behavior.
[2026-01-17 23:39:39] [INFO] nvitop监控已启动
[2026-01-17 23:39:39] [INFO] 服务已停止
[2026-01-17 23:39:40] [INFO] nvitop监控已启动
[2026-01-17 23:39:41] [SUCCESS] 正在关闭服务器...
[2026-01-17 23:39:48] [INFO] [0;36m(Worker_TP1 pid=263552)[0;0m [0;36m(Worker_TP0 pid=263551)[0;0m INFO 01-17 23:39:48 [multiproc_executor.py:707] Parent process exited, terminating worker
[2026-01-17 23:39:48] [ERROR] [0;36m(APIServer pid=262986)[0;0m ERROR 01-17 23:39:48 [core_client.py:610] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.
[2026-01-17 23:39:48] [INFO] INFO 01-17 23:39:48 [multiproc_executor.py:707] Parent process exited, terminating worker
[2026-01-17 23:39:48] [INFO] [0;36m(Worker_TP0 pid=263551)[0;0m INFO 01-17 23:39:48 [multiproc_executor.py:751] WorkerProc shutting down.
[2026-01-17 23:39:48] [INFO] [0;36m(Worker_TP1 pid=263552)[0;0m INFO 01-17 23:39:48 [multiproc_executor.py:751] WorkerProc shutting down.
[2026-01-17 23:39:48] [INFO] [0;36m(Worker_TP1 pid=263552)[0;0m [0;36m(Worker_TP0 pid=263551)[0;0m /mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py:147: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.
[2026-01-17 23:39:48] [INFO] /mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py:147: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.
[2026-01-17 23:39:48] [INFO] [0;36m(Worker_TP1 pid=263552)[0;0m [0;36m(Worker_TP0 pid=263551)[0;0m   warnings.warn('resource_tracker: process died unexpectedly, '
[2026-01-17 23:39:48] [INFO] warnings.warn('resource_tracker: process died unexpectedly, '
[2026-01-17 23:39:48] [ERROR] Traceback (most recent call last):
[2026-01-17 23:39:48] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-17 23:39:48] [ERROR] Traceback (most recent call last):
[2026-01-17 23:39:48] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-17 23:39:48] [INFO] cache[rtype].remove(name)
[2026-01-17 23:39:48] [INFO] KeyError: '/psm_73a34520'
[2026-01-17 23:39:48] [INFO] cache[rtype].remove(name)
[2026-01-17 23:39:48] [INFO] KeyError: '/psm_fcbcb013'
[2026-01-17 23:39:48] [INFO] 服务已停止
[2026-01-17 23:39:48] [ERROR] Traceback (most recent call last):
[2026-01-17 23:39:48] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-17 23:39:48] [INFO] cache[rtype].remove(name)
[2026-01-17 23:39:48] [INFO] KeyError: '/psm_9841b967'
[2026-01-17 23:39:48] [INFO] nvitop监控已启动
[2026-01-17 23:39:48] [ERROR] Traceback (most recent call last):
[2026-01-17 23:39:48] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-17 23:39:48] [INFO] cache[rtype].remove(name)
[2026-01-17 23:39:48] [INFO] KeyError: '/mp-dur9wq8e'
[2026-01-17 23:39:48] [ERROR] Traceback (most recent call last):
[2026-01-17 23:39:48] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-17 23:39:48] [INFO] cache[rtype].remove(name)
[2026-01-17 23:39:48] [INFO] KeyError: '/mp-fxludf96'
[2026-01-17 23:39:49] [INFO] nvitop监控已启动
[2026-01-17 23:39:52] [INFO] nvitop监控已启动
[2026-01-17 23:40:09] [INFO] VLLM GUI 服务器启动，端口: 5003
[2026-01-17 23:40:09] [INFO] nvitop监控已启动
[2026-01-17 23:40:10] [INFO] nvitop监控已启动
[2026-01-17 23:40:12] [INFO] 从conda info --base自动检测到conda路径: /mnt/AI-Acer4T/miniconda3
[2026-01-17 23:42:20] [SUCCESS] 方案已保存: Devstral-2-123B-Instruct-2512-TP2
[2026-01-17 23:42:22] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-17 23:42:22] [INFO] nvitop监控已启动
[2026-01-17 23:42:22] [INFO] nvitop监控已停止
[2026-01-17 23:42:49] [ERROR] Traceback (most recent call last):
[2026-01-17 23:42:49] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/bin/vllm", line 7, in <module>
[2026-01-17 23:42:49] [INFO] sys.exit(main())
[2026-01-17 23:42:49] [INFO] ^^^^^^
[2026-01-17 23:42:49] [INFO] File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/main.py", line 70, in main
[2026-01-17 23:42:49] [INFO] cmds[args.subparser].validate(args)
[2026-01-17 23:42:49] [INFO] File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/serve.py", line 63, in validate
[2026-01-17 23:42:49] [INFO] validate_parsed_serve_args(args)
[2026-01-17 23:42:49] [INFO] File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/cli_args.py", line 307, in validate_parsed_serve_args
[2026-01-17 23:42:49] [INFO] validate_chat_template(args.chat_template)
[2026-01-17 23:42:49] [INFO] File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/chat_utils.py", line 1199, in validate_chat_template
[2026-01-17 23:42:49] [INFO] raise ValueError(
[2026-01-17 23:42:49] [INFO] ValueError: The supplied chat template string (/mnt/AI-Acer4T/AI-Chat/models/minimax/minimax/MiniMax-M2.1-AWQ/chat_template.jinja) appears path-like, but doesn't exist! Tried: /mnt/AI-Acer4T/AI-Chat/models/minimax/minimax/MiniMax-M2.1-AWQ/chat_template.jinja and /mnt/AI-Acer4T/AI-Chat/models/minimax/minimax/MiniMax-M2.1-AWQ/chat_template.jinja
[2026-01-17 23:42:51] [INFO] nvitop监控已启动
[2026-01-17 23:43:30] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-17 23:43:30] [INFO] nvitop监控已启动
[2026-01-17 23:43:30] [INFO] nvitop监控已停止
[2026-01-17 23:43:55] [INFO] [0;36m(APIServer pid=272181)[0;0m INFO 01-17 23:43:55 [api_server.py:1278] vLLM API server version 0.14.0rc1.dev492+g19504ac07
[2026-01-17 23:43:55] [INFO] [0;36m(APIServer pid=272181)[0;0m INFO 01-17 23:43:55 [utils.py:254] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/Devstral-2/Devstral-2-123B-Instruct-2512', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/AI-Acer4T/AI-Chat/models/Devstral-2/Devstral-2-123B-Instruct-2512/chat_template.jinja', 'enable_auto_tool_choice': True, 'tool_call_parser': 'mistral', 'model': '/mnt/AI-Acer4T/AI-Chat/models/Devstral-2/Devstral-2-123B-Instruct-2512', 'trust_remote_code': True, 'max_model_len': 128000, 'served_model_name': ['VLLM-MODEL'], 'reasoning_parser': 'mistral', 'tensor_parallel_size': 2, 'kv_cache_dtype': 'fp8', 'max_num_seqs': 256, 'async_scheduling': True}
[2026-01-17 23:43:55] [INFO] [0;36m(APIServer pid=272181)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-17 23:44:17] [INFO] [0;36m(APIServer pid=272181)[0;0m INFO 01-17 23:44:17 [model.py:528] Resolved architecture: MistralForCausalLM
[2026-01-17 23:44:17] [INFO] [0;36m(APIServer pid=272181)[0;0m INFO 01-17 23:44:17 [model.py:1543] Using max model len 128000
[2026-01-17 23:44:17] [INFO] [0;36m(APIServer pid=272181)[0;0m INFO 01-17 23:44:17 [cache.py:206] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor.
[2026-01-17 23:44:18] [INFO] [0;36m(APIServer pid=272181)[0;0m INFO 01-17 23:44:18 [scheduler.py:231] Chunked prefill is enabled with max_num_batched_tokens=8192.
[2026-01-17 23:44:18] [INFO] [0;36m(APIServer pid=272181)[0;0m INFO 01-17 23:44:18 [vllm.py:640] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-17 23:44:18] [INFO] [0;36m(APIServer pid=272181)[0;0m INFO 01-17 23:44:18 [vllm.py:645] Asynchronous scheduling is enabled.
[2026-01-17 23:44:18] [INFO] [0;36m(APIServer pid=272181)[0;0m [2026-01-17 23:44:18] INFO tekken.py:195: Non special vocabulary size is 130072 with 1000 special tokens.
[2026-01-17 23:44:43] [INFO] [0;36m(EngineCore_DP0 pid=272647)[0;0m INFO 01-17 23:44:43 [core.py:97] Initializing a V1 LLM engine (v0.14.0rc1.dev492+g19504ac07) with config: model='/mnt/AI-Acer4T/AI-Chat/models/Devstral-2/Devstral-2-123B-Instruct-2512', speculative_config=None, tokenizer='/mnt/AI-Acer4T/AI-Chat/models/Devstral-2/Devstral-2-123B-Instruct-2512', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=fp8, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='mistral', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-17 23:44:43] [WARNING] [0;36m(EngineCore_DP0 pid=272647)[0;0m WARNING 01-17 23:44:43 [multiproc_executor.py:880] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-17 23:45:07] [INFO] INFO 01-17 23:45:07 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:35975 backend=nccl
[2026-01-17 23:45:07] [INFO] INFO 01-17 23:45:07 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:35975 backend=nccl
[2026-01-17 23:45:07] [INFO] INFO 01-17 23:45:07 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-17 23:45:08] [WARNING] WARNING 01-17 23:45:08 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-17 23:45:08] [WARNING] WARNING 01-17 23:45:08 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-17 23:45:08] [INFO] INFO 01-17 23:45:08 [parallel_state.py:1425] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
[2026-01-17 23:45:08] [INFO] INFO 01-17 23:45:08 [parallel_state.py:1425] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[2026-01-17 23:45:09] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m INFO 01-17 23:45:09 [gpu_model_runner.py:3789] Starting to load model /mnt/AI-Acer4T/AI-Chat/models/Devstral-2/Devstral-2-123B-Instruct-2512...
[2026-01-17 23:45:13] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m INFO 01-17 23:45:13 [cuda.py:351] Using FLASHINFER attention backend out of potential backends: ('FLASHINFER', 'TRITON_ATTN')
[2026-01-17 23:45:14] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:45:14] [INFO] Loading safetensors checkpoint shards:   0% Completed | 0/27 [00:00<?, ?it/s]
[2026-01-17 23:45:19] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:45:19] [INFO] Loading safetensors checkpoint shards:   4% Completed | 1/27 [00:04<02:09,  4.97s/it]
[2026-01-17 23:45:24] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:45:24] [INFO] Loading safetensors checkpoint shards:   7% Completed | 2/27 [00:09<02:03,  4.95s/it]
[2026-01-17 23:45:28] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:45:28] [INFO] Loading safetensors checkpoint shards:  11% Completed | 3/27 [00:14<01:57,  4.91s/it]
[2026-01-17 23:45:33] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:45:33] [INFO] Loading safetensors checkpoint shards:  15% Completed | 4/27 [00:19<01:49,  4.78s/it]
[2026-01-17 23:45:38] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:45:38] [INFO] Loading safetensors checkpoint shards:  19% Completed | 5/27 [00:24<01:49,  4.97s/it]
[2026-01-17 23:45:43] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:45:43] [INFO] Loading safetensors checkpoint shards:  22% Completed | 6/27 [00:29<01:45,  5.02s/it]
[2026-01-17 23:45:49] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:45:49] [INFO] Loading safetensors checkpoint shards:  26% Completed | 7/27 [00:34<01:41,  5.09s/it]
[2026-01-17 23:45:54] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:45:54] [INFO] Loading safetensors checkpoint shards:  30% Completed | 8/27 [00:39<01:35,  5.02s/it]
[2026-01-17 23:45:59] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:45:59] [INFO] Loading safetensors checkpoint shards:  33% Completed | 9/27 [00:45<01:32,  5.14s/it]
[2026-01-17 23:46:04] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:46:04] [INFO] Loading safetensors checkpoint shards:  37% Completed | 10/27 [00:50<01:27,  5.13s/it]
[2026-01-17 23:46:11] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:46:11] [INFO] Loading safetensors checkpoint shards:  41% Completed | 11/27 [00:56<01:28,  5.54s/it]
[2026-01-17 23:46:14] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:46:14] [INFO] Loading safetensors checkpoint shards:  44% Completed | 12/27 [01:00<01:13,  4.88s/it]
[2026-01-17 23:46:17] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:46:17] [INFO] Loading safetensors checkpoint shards:  48% Completed | 13/27 [01:03<00:59,  4.26s/it]
[2026-01-17 23:46:21] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:46:21] [INFO] Loading safetensors checkpoint shards:  52% Completed | 14/27 [01:07<00:57,  4.39s/it]
[2026-01-17 23:46:26] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:46:26] [INFO] Loading safetensors checkpoint shards:  56% Completed | 15/27 [01:12<00:53,  4.47s/it]
[2026-01-17 23:46:31] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:46:31] [INFO] Loading safetensors checkpoint shards:  59% Completed | 16/27 [01:17<00:50,  4.59s/it]
[2026-01-17 23:46:36] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:46:36] [INFO] Loading safetensors checkpoint shards:  63% Completed | 17/27 [01:22<00:47,  4.71s/it]
[2026-01-17 23:46:41] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:46:41] [INFO] Loading safetensors checkpoint shards:  67% Completed | 18/27 [01:26<00:42,  4.72s/it]
[2026-01-17 23:46:46] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:46:46] [INFO] Loading safetensors checkpoint shards:  70% Completed | 19/27 [01:31<00:37,  4.75s/it]
[2026-01-17 23:46:50] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:46:50] [INFO] Loading safetensors checkpoint shards:  74% Completed | 20/27 [01:36<00:32,  4.71s/it]
[2026-01-17 23:46:55] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:46:55] [INFO] Loading safetensors checkpoint shards:  78% Completed | 21/27 [01:41<00:29,  4.85s/it]
[2026-01-17 23:47:00] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:47:00] [INFO] Loading safetensors checkpoint shards:  81% Completed | 22/27 [01:46<00:23,  4.73s/it]
[2026-01-17 23:47:04] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:47:04] [INFO] Loading safetensors checkpoint shards:  85% Completed | 23/27 [01:50<00:18,  4.62s/it]
[2026-01-17 23:47:09] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:47:09] [INFO] Loading safetensors checkpoint shards:  89% Completed | 24/27 [01:54<00:13,  4.59s/it]
[2026-01-17 23:47:14] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:47:14] [INFO] Loading safetensors checkpoint shards:  93% Completed | 25/27 [01:59<00:09,  4.67s/it]
[2026-01-17 23:47:18] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:47:18] [INFO] Loading safetensors checkpoint shards:  96% Completed | 26/27 [02:04<00:04,  4.66s/it]
[2026-01-17 23:47:23] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:47:23] [INFO] Loading safetensors checkpoint shards: 100% Completed | 27/27 [02:09<00:00,  4.64s/it]
[2026-01-17 23:47:23] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:47:23] [INFO] Loading safetensors checkpoint shards: 100% Completed | 27/27 [02:09<00:00,  4.78s/it]
[2026-01-17 23:47:23] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:47:23] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m INFO 01-17 23:47:23 [default_loader.py:291] Loading weights took 129.12 seconds
[2026-01-17 23:47:23] [WARNING] [0;36m(Worker_TP0 pid=272861)[0;0m WARNING 01-17 23:47:23 [kv_cache.py:90] Checkpoint does not provide a q scaling factor. Setting it to k_scale. This only matters for FP8 Attention backends (flash-attn or flashinfer).
[2026-01-17 23:47:23] [WARNING] [0;36m(Worker_TP0 pid=272861)[0;0m WARNING 01-17 23:47:23 [kv_cache.py:104] Using KV cache scaling factor 1.0 for fp8_e4m3. If this is unintended, verify that k/v_scale scaling factors are properly set in the checkpoint.
[2026-01-17 23:47:23] [WARNING] [0;36m(Worker_TP0 pid=272861)[0;0m WARNING 01-17 23:47:23 [kv_cache.py:143] Using uncalibrated q_scale 1.0 and/or prob_scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure q/prob scaling factors are available in the fp8 checkpoint.
[2026-01-17 23:47:23] [WARNING] [0;36m(Worker_TP1 pid=272862)[0;0m WARNING 01-17 23:47:23 [kv_cache.py:90] Checkpoint does not provide a q scaling factor. Setting it to k_scale. This only matters for FP8 Attention backends (flash-attn or flashinfer).
[2026-01-17 23:47:23] [WARNING] [0;36m(Worker_TP1 pid=272862)[0;0m WARNING 01-17 23:47:23 [kv_cache.py:104] Using KV cache scaling factor 1.0 for fp8_e4m3. If this is unintended, verify that k/v_scale scaling factors are properly set in the checkpoint.
[2026-01-17 23:47:23] [WARNING] [0;36m(Worker_TP1 pid=272862)[0;0m WARNING 01-17 23:47:23 [kv_cache.py:143] Using uncalibrated q_scale 1.0 and/or prob_scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure q/prob scaling factors are available in the fp8 checkpoint.
[2026-01-17 23:47:24] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m INFO 01-17 23:47:24 [gpu_model_runner.py:3886] Model loading took 59.79 GiB memory and 134.241629 seconds
[2026-01-17 23:47:43] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m INFO 01-17 23:47:43 [backends.py:644] Using cache directory: /home/wuwen/.cache/vllm/torch_compile_cache/5dfa8b2b6f/rank_0_0/backbone for vLLM's torch.compile
[2026-01-17 23:47:43] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m INFO 01-17 23:47:43 [backends.py:704] Dynamo bytecode transform time: 19.02 s
[2026-01-17 23:48:01] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m INFO 01-17 23:48:01 [backends.py:261] Cache the graph of compile range (1, 8192) for later use
[2026-01-17 23:48:01] [INFO] [0;36m(Worker_TP1 pid=272862)[0;0m INFO 01-17 23:48:01 [backends.py:261] Cache the graph of compile range (1, 8192) for later use
[2026-01-17 23:48:24] [INFO] [0;36m(EngineCore_DP0 pid=272647)[0;0m INFO 01-17 23:48:24 [shm_broadcast.py:542] No available shared memory broadcast block found in 60 seconds. This typically happens when some processes are hanging or doing some time-consuming work (e.g. compilation, weight/kv cache quantization).
[2026-01-17 23:49:24] [INFO] [0;36m(EngineCore_DP0 pid=272647)[0;0m INFO 01-17 23:49:24 [shm_broadcast.py:542] No available shared memory broadcast block found in 60 seconds. This typically happens when some processes are hanging or doing some time-consuming work (e.g. compilation, weight/kv cache quantization).
[2026-01-17 23:50:23] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m INFO 01-17 23:50:23 [backends.py:278] Compiling a graph for compile range (1, 8192) takes 144.78 s
[2026-01-17 23:50:23] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m INFO 01-17 23:50:23 [monitor.py:34] torch.compile takes 163.80 s in total
[2026-01-17 23:50:24] [INFO] [0;36m(EngineCore_DP0 pid=272647)[0;0m INFO 01-17 23:50:24 [shm_broadcast.py:542] No available shared memory broadcast block found in 60 seconds. This typically happens when some processes are hanging or doing some time-consuming work (e.g. compilation, weight/kv cache quantization).
[2026-01-17 23:50:25] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m INFO 01-17 23:50:25 [gpu_worker.py:358] Available KV cache memory: 23.78 GiB
[2026-01-17 23:50:25] [INFO] [0;36m(EngineCore_DP0 pid=272647)[0;0m INFO 01-17 23:50:25 [kv_cache_utils.py:1305] GPU KV cache size: 283,360 tokens
[2026-01-17 23:50:25] [INFO] [0;36m(EngineCore_DP0 pid=272647)[0;0m INFO 01-17 23:50:25 [kv_cache_utils.py:1310] Maximum concurrency for 128,000 tokens per request: 2.21x
[2026-01-17 23:50:25] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m [0;36m(Worker_TP1 pid=272862)[0;0m 2026-01-17 23:50:25,694 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[2026-01-17 23:50:25] [INFO] 2026-01-17 23:50:25,694 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[2026-01-17 23:50:30] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m [0;36m(Worker_TP1 pid=272862)[0;0m 2026-01-17 23:50:30,835 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[2026-01-17 23:50:30] [INFO] 2026-01-17 23:50:30,835 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[2026-01-17 23:50:30] [INFO] [0;36m(Worker_TP1 pid=272862)[0;0m INFO 01-17 23:50:30 [kernel_warmup.py:64] Warming up FlashInfer attention.
[2026-01-17 23:50:30] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m INFO 01-17 23:50:30 [kernel_warmup.py:64] Warming up FlashInfer attention.
[2026-01-17 23:50:31] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:50:32] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
[2026-01-17 23:50:32] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:15,  3.27it/s]
[2026-01-17 23:50:32] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:14,  3.32it/s]
[2026-01-17 23:50:33] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:14,  3.39it/s]
[2026-01-17 23:50:33] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:01<00:13,  3.41it/s]
[2026-01-17 23:50:33] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:01<00:13,  3.41it/s]
[2026-01-17 23:50:33] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:01<00:13,  3.29it/s]
[2026-01-17 23:50:34] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▎        | 7/51 [00:02<00:13,  3.35it/s]
[2026-01-17 23:50:34] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:02<00:12,  3.41it/s]
[2026-01-17 23:50:34] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:02<00:11,  3.54it/s]
[2026-01-17 23:50:35] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|█▉        | 10/51 [00:02<00:11,  3.63it/s]
[2026-01-17 23:50:35] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:03<00:10,  3.70it/s]
[2026-01-17 23:50:35] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:03<00:09,  4.01it/s]
[2026-01-17 23:50:35] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:03<00:08,  4.26it/s]
[2026-01-17 23:50:35] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:03<00:08,  4.47it/s]
[2026-01-17 23:50:36] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:03<00:07,  4.61it/s]
[2026-01-17 23:50:36] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 16/51 [00:04<00:07,  4.71it/s]
[2026-01-17 23:50:36] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:04<00:07,  4.78it/s]
[2026-01-17 23:50:36] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:04<00:06,  4.85it/s]
[2026-01-17 23:50:36] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 19/51 [00:04<00:06,  4.87it/s]
[2026-01-17 23:50:37] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:04<00:06,  4.91it/s]
[2026-01-17 23:50:37] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:05<00:06,  4.91it/s]
[2026-01-17 23:50:37] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 22/51 [00:05<00:05,  4.93it/s]
[2026-01-17 23:50:37] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:05<00:05,  4.94it/s]
[2026-01-17 23:50:37] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:05<00:05,  4.92it/s]
[2026-01-17 23:50:38] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 25/51 [00:05<00:05,  4.93it/s]
[2026-01-17 23:50:38] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:06<00:05,  4.29it/s]
[2026-01-17 23:50:38] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:06<00:05,  4.46it/s]
[2026-01-17 23:50:38] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:06<00:05,  4.57it/s]
[2026-01-17 23:50:38] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:06<00:04,  4.67it/s]
[2026-01-17 23:50:39] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:07<00:04,  4.72it/s]
[2026-01-17 23:50:39] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 31/51 [00:07<00:04,  4.75it/s]
[2026-01-17 23:50:39] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:07<00:04,  3.99it/s]
[2026-01-17 23:50:39] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:07<00:04,  4.21it/s]
[2026-01-17 23:50:40] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:08<00:03,  4.29it/s]
[2026-01-17 23:50:40] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:08<00:03,  4.34it/s]
[2026-01-17 23:50:40] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:08<00:03,  4.37it/s]
[2026-01-17 23:50:40] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 37/51 [00:08<00:03,  4.39it/s]
[2026-01-17 23:50:41] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:08<00:02,  4.40it/s]
[2026-01-17 23:50:41] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▋  | 39/51 [00:09<00:03,  3.48it/s]
[2026-01-17 23:50:41] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:09<00:02,  3.72it/s]
[2026-01-17 23:50:41] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:09<00:02,  3.92it/s]
[2026-01-17 23:50:42] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:10<00:02,  4.06it/s]
[2026-01-17 23:50:42] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 43/51 [00:10<00:01,  4.14it/s]
[2026-01-17 23:50:42] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:10<00:01,  4.21it/s]
[2026-01-17 23:50:42] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 45/51 [00:10<00:01,  4.29it/s]
[2026-01-17 23:50:43] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|█████████ | 46/51 [00:10<00:01,  4.42it/s]
[2026-01-17 23:50:43] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:11<00:00,  4.53it/s]
[2026-01-17 23:50:43] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 48/51 [00:11<00:00,  4.49it/s]
[2026-01-17 23:50:43] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▌| 49/51 [00:11<00:00,  3.37it/s]
[2026-01-17 23:50:44] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:12<00:00,  3.66it/s]
[2026-01-17 23:50:44] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:12<00:00,  3.89it/s]
[2026-01-17 23:50:44] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:12<00:00,  4.14it/s]
[2026-01-17 23:50:44] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m
[2026-01-17 23:50:44] [INFO] Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
[2026-01-17 23:50:44] [INFO] Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:06,  5.04it/s]
[2026-01-17 23:50:44] [INFO] Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:00<00:06,  5.10it/s]
[2026-01-17 23:50:44] [INFO] Capturing CUDA graphs (decode, FULL):   9%|▊         | 3/35 [00:00<00:06,  5.13it/s]
[2026-01-17 23:50:45] [INFO] Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:06,  5.12it/s]
[2026-01-17 23:50:45] [INFO] Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:00<00:05,  5.10it/s]
[2026-01-17 23:50:45] [INFO] Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:01<00:05,  5.14it/s]
[2026-01-17 23:50:45] [INFO] Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:01<00:05,  5.15it/s]
[2026-01-17 23:50:45] [INFO] Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:01<00:05,  5.15it/s]
[2026-01-17 23:50:46] [INFO] Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:01<00:05,  5.18it/s]
[2026-01-17 23:50:46] [INFO] Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:02<00:06,  3.58it/s]
[2026-01-17 23:50:46] [INFO] Capturing CUDA graphs (decode, FULL):  31%|███▏      | 11/35 [00:02<00:06,  3.96it/s]
[2026-01-17 23:50:46] [INFO] Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:02<00:05,  4.28it/s]
[2026-01-17 23:50:47] [INFO] Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:02<00:04,  4.53it/s]
[2026-01-17 23:50:47] [INFO] Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:02<00:04,  4.72it/s]
[2026-01-17 23:50:47] [INFO] Capturing CUDA graphs (decode, FULL):  43%|████▎     | 15/35 [00:03<00:04,  4.86it/s]
[2026-01-17 23:50:47] [INFO] Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:03<00:03,  4.96it/s]
[2026-01-17 23:50:47] [INFO] Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:03<00:03,  5.04it/s]
[2026-01-17 23:50:48] [INFO] Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:03<00:03,  4.99it/s]
[2026-01-17 23:50:48] [INFO] Capturing CUDA graphs (decode, FULL):  54%|█████▍    | 19/35 [00:03<00:03,  4.91it/s]
[2026-01-17 23:50:48] [INFO] Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:04<00:03,  4.87it/s]
[2026-01-17 23:50:48] [INFO] Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:04<00:02,  4.83it/s]
[2026-01-17 23:50:48] [INFO] Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:04<00:02,  4.82it/s]
[2026-01-17 23:50:49] [INFO] Capturing CUDA graphs (decode, FULL):  66%|██████▌   | 23/35 [00:04<00:02,  4.80it/s]
[2026-01-17 23:50:49] [INFO] Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:05<00:02,  4.79it/s]
[2026-01-17 23:50:49] [INFO] Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:05<00:03,  3.16it/s]
[2026-01-17 23:50:50] [INFO] Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:05<00:02,  3.52it/s]
[2026-01-17 23:50:50] [INFO] Capturing CUDA graphs (decode, FULL):  77%|███████▋  | 27/35 [00:06<00:02,  3.83it/s]
[2026-01-17 23:50:50] [INFO] Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:06<00:01,  4.09it/s]
[2026-01-17 23:50:50] [INFO] Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:06<00:01,  4.25it/s]
[2026-01-17 23:50:50] [INFO] Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:06<00:01,  4.48it/s]
[2026-01-17 23:50:51] [INFO] Capturing CUDA graphs (decode, FULL):  89%|████████▊ | 31/35 [00:06<00:00,  4.69it/s]
[2026-01-17 23:50:51] [INFO] Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:07<00:00,  4.69it/s]
[2026-01-17 23:50:51] [INFO] Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:07<00:00,  4.71it/s]
[2026-01-17 23:50:51] [INFO] Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:07<00:00,  4.76it/s][0;36m(Worker_TP1 pid=272862)[0;0m INFO 01-17 23:50:51 [custom_all_reduce.py:216] Registering 13275 cuda graph addresses
[2026-01-17 23:50:51] [INFO] 
[2026-01-17 23:50:51] [INFO] Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:07<00:00,  4.90it/s]
[2026-01-17 23:50:51] [INFO] Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:07<00:00,  4.58it/s]
[2026-01-17 23:50:51] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m INFO 01-17 23:50:51 [custom_all_reduce.py:216] Registering 13275 cuda graph addresses
[2026-01-17 23:50:53] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m INFO 01-17 23:50:53 [gpu_model_runner.py:4837] Graph capturing finished in 22 secs, took -0.87 GiB
[2026-01-17 23:50:53] [INFO] [0;36m(EngineCore_DP0 pid=272647)[0;0m INFO 01-17 23:50:53 [core.py:273] init engine (profile, create kv cache, warmup model) took 208.83 seconds
[2026-01-17 23:50:53] [INFO] [0;36m(EngineCore_DP0 pid=272647)[0;0m [2026-01-17 23:50:53] INFO tekken.py:195: Non special vocabulary size is 130072 with 1000 special tokens.
[2026-01-17 23:50:54] [INFO] [0;36m(EngineCore_DP0 pid=272647)[0;0m /mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/mistral_common/tokens/tokenizers/tekken.py:462: FutureWarning: `get_control_token` is deprecated. Use `get_special_token` instead.
[2026-01-17 23:50:54] [INFO] [0;36m(EngineCore_DP0 pid=272647)[0;0m   warnings.warn("`get_control_token` is deprecated. Use `get_special_token` instead.", FutureWarning)
[2026-01-17 23:50:54] [INFO] [0;36m(EngineCore_DP0 pid=272647)[0;0m INFO 01-17 23:50:54 [core.py:186] Batch queue is enabled with size 2
[2026-01-17 23:50:54] [INFO] [0;36m(EngineCore_DP0 pid=272647)[0;0m INFO 01-17 23:50:54 [vllm.py:645] Asynchronous scheduling is enabled.
[2026-01-17 23:50:54] [INFO] [0;36m(APIServer pid=272181)[0;0m INFO 01-17 23:50:54 [api_server.py:1020] Supported tasks: ['generate']
[2026-01-17 23:50:54] [INFO] [0;36m(Worker_TP1 pid=272862)[0;0m INFO 01-17 23:50:54 [multiproc_executor.py:707] Parent process exited, terminating worker
[2026-01-17 23:50:54] [INFO] [0;36m(Worker_TP0 pid=272861)[0;0m INFO 01-17 23:50:54 [multiproc_executor.py:707] Parent process exited, terminating worker
[2026-01-17 23:50:59] [ERROR] [0;36m(APIServer pid=272181)[0;0m Traceback (most recent call last):
[2026-01-17 23:50:59] [INFO] [0;36m(APIServer pid=272181)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/bin/vllm", line 7, in <module>
[2026-01-17 23:50:59] [INFO] [0;36m(APIServer pid=272181)[0;0m     sys.exit(main())
[2026-01-17 23:50:59] [INFO] [0;36m(APIServer pid=272181)[0;0m              ^^^^^^
[2026-01-17 23:50:59] [INFO] [0;36m(APIServer pid=272181)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-17 23:50:59] [INFO] [0;36m(APIServer pid=272181)[0;0m     args.dispatch_function(args)
[2026-01-17 23:50:59] [INFO] [0;36m(APIServer pid=272181)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-17 23:50:59] [INFO] [0;36m(APIServer pid=272181)[0;0m     uvloop.run(run_server(args))
[2026-01-17 23:50:59] [INFO] [0;36m(APIServer pid=272181)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-17 23:50:59] [INFO] [0;36m(APIServer pid=272181)[0;0m     return __asyncio.run(
[2026-01-17 23:50:59] [INFO] [0;36m(APIServer pid=272181)[0;0m            ^^^^^^^^^^^^^^
[2026-01-17 23:50:59] [INFO] [0;36m(APIServer pid=272181)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-17 23:50:59] [INFO] [0;36m(APIServer pid=272181)[0;0m     return runner.run(main)
[2026-01-17 23:50:59] [INFO] [0;36m(APIServer pid=272181)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-17 23:50:59] [INFO] [0;36m(APIServer pid=272181)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-17 23:50:59] [INFO] [0;36m(APIServer pid=272181)[0;0m     return self._loop.run_until_complete(task)
[2026-01-17 23:50:59] [INFO] [0;36m(APIServer pid=272181)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-17 23:50:59] [INFO] [0;36m(APIServer pid=272181)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-17 23:50:59] [INFO] [0;36m(APIServer pid=272181)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-17 23:50:59] [INFO] [0;36m(APIServer pid=272181)[0;0m     return await main
[2026-01-17 23:50:59] [INFO] [0;36m(APIServer pid=272181)[0;0m            ^^^^^^^^^^
[2026-01-17 23:50:59] [INFO] [0;36m(APIServer pid=272181)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 1325, in run_server
[2026-01-17 23:50:59] [INFO] [0;36m(APIServer pid=272181)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-17 23:50:59] [INFO] [0;36m(APIServer pid=272181)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 1350, in run_server_worker
[2026-01-17 23:50:59] [INFO] [0;36m(APIServer pid=272181)[0;0m     await init_app_state(engine_client, app.state, args)
[2026-01-17 23:50:59] [INFO] [0;36m(APIServer pid=272181)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 1022, in init_app_state
[2026-01-17 23:50:59] [INFO] [0;36m(APIServer pid=272181)[0;0m     resolved_chat_template = await process_chat_template(
[2026-01-17 23:50:59] [INFO] [0;36m(APIServer pid=272181)[0;0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-17 23:50:59] [INFO] [0;36m(APIServer pid=272181)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/utils.py", line 301, in process_chat_template
[2026-01-17 23:50:59] [INFO] [0;36m(APIServer pid=272181)[0;0m     resolved_chat_template = resolve_mistral_chat_template(
[2026-01-17 23:50:59] [INFO] [0;36m(APIServer pid=272181)[0;0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-17 23:50:59] [INFO] [0;36m(APIServer pid=272181)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/chat_utils.py", line 461, in resolve_mistral_chat_template
[2026-01-17 23:50:59] [INFO] [0;36m(APIServer pid=272181)[0;0m     raise ValueError(
[2026-01-17 23:50:59] [INFO] [0;36m(APIServer pid=272181)[0;0m ValueError: 'chat_template' or 'chat_template_kwargs' cannot be overridden for mistral tokenizer.
[2026-01-17 23:51:04] [INFO] nvitop监控已启动
[2026-01-17 23:51:55] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-17 23:51:55] [INFO] nvitop监控已启动
[2026-01-17 23:51:55] [INFO] nvitop监控已停止
[2026-01-17 23:52:22] [INFO] [0;36m(APIServer pid=276815)[0;0m INFO 01-17 23:52:22 [api_server.py:1278] vLLM API server version 0.14.0rc1.dev492+g19504ac07
[2026-01-17 23:52:22] [INFO] [0;36m(APIServer pid=276815)[0;0m INFO 01-17 23:52:22 [utils.py:254] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/Devstral-2/Devstral-2-123B-Instruct-2512', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/AI-Acer4T/AI-Chat/models/Devstral-2/Devstral-2-123B-Instruct-2512/chat_template.jinja', 'enable_auto_tool_choice': True, 'tool_call_parser': 'mistral', 'model': '/mnt/AI-Acer4T/AI-Chat/models/Devstral-2/Devstral-2-123B-Instruct-2512', 'trust_remote_code': True, 'max_model_len': 128000, 'served_model_name': ['VLLM-MODEL'], 'tensor_parallel_size': 2, 'kv_cache_dtype': 'fp8', 'max_num_seqs': 256, 'async_scheduling': True}
[2026-01-17 23:52:22] [INFO] [0;36m(APIServer pid=276815)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-17 23:52:22] [INFO] [0;36m(APIServer pid=276815)[0;0m INFO 01-17 23:52:22 [model.py:528] Resolved architecture: MistralForCausalLM
[2026-01-17 23:52:22] [INFO] [0;36m(APIServer pid=276815)[0;0m INFO 01-17 23:52:22 [model.py:1543] Using max model len 128000
[2026-01-17 23:52:23] [INFO] [0;36m(APIServer pid=276815)[0;0m INFO 01-17 23:52:23 [cache.py:206] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor.
[2026-01-17 23:52:23] [INFO] [0;36m(APIServer pid=276815)[0;0m INFO 01-17 23:52:23 [scheduler.py:231] Chunked prefill is enabled with max_num_batched_tokens=8192.
[2026-01-17 23:52:23] [INFO] [0;36m(APIServer pid=276815)[0;0m INFO 01-17 23:52:23 [vllm.py:640] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-17 23:52:23] [INFO] [0;36m(APIServer pid=276815)[0;0m INFO 01-17 23:52:23 [vllm.py:645] Asynchronous scheduling is enabled.
[2026-01-17 23:52:23] [INFO] [0;36m(APIServer pid=276815)[0;0m [2026-01-17 23:52:23] INFO tekken.py:195: Non special vocabulary size is 130072 with 1000 special tokens.
[2026-01-17 23:52:46] [INFO] [0;36m(EngineCore_DP0 pid=277174)[0;0m INFO 01-17 23:52:46 [core.py:97] Initializing a V1 LLM engine (v0.14.0rc1.dev492+g19504ac07) with config: model='/mnt/AI-Acer4T/AI-Chat/models/Devstral-2/Devstral-2-123B-Instruct-2512', speculative_config=None, tokenizer='/mnt/AI-Acer4T/AI-Chat/models/Devstral-2/Devstral-2-123B-Instruct-2512', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=fp8, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-17 23:52:46] [WARNING] [0;36m(EngineCore_DP0 pid=277174)[0;0m WARNING 01-17 23:52:46 [multiproc_executor.py:880] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-17 23:53:10] [INFO] INFO 01-17 23:53:10 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:38697 backend=nccl
[2026-01-17 23:53:10] [INFO] INFO 01-17 23:53:10 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:38697 backend=nccl
[2026-01-17 23:53:10] [INFO] INFO 01-17 23:53:10 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-17 23:53:11] [WARNING] WARNING 01-17 23:53:11 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-17 23:53:11] [WARNING] WARNING 01-17 23:53:11 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-17 23:53:11] [INFO] INFO 01-17 23:53:11 [parallel_state.py:1425] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
[2026-01-17 23:53:11] [INFO] INFO 01-17 23:53:11 [parallel_state.py:1425] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[2026-01-17 23:53:12] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m INFO 01-17 23:53:12 [gpu_model_runner.py:3789] Starting to load model /mnt/AI-Acer4T/AI-Chat/models/Devstral-2/Devstral-2-123B-Instruct-2512...
[2026-01-17 23:53:16] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m INFO 01-17 23:53:16 [cuda.py:351] Using FLASHINFER attention backend out of potential backends: ('FLASHINFER', 'TRITON_ATTN')
[2026-01-17 23:53:17] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m
[2026-01-17 23:53:17] [INFO] Loading safetensors checkpoint shards:   0% Completed | 0/27 [00:00<?, ?it/s]
[2026-01-17 23:53:22] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m
[2026-01-17 23:53:22] [INFO] Loading safetensors checkpoint shards:   4% Completed | 1/27 [00:05<02:17,  5.29s/it]
[2026-01-17 23:53:27] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m
[2026-01-17 23:53:27] [INFO] Loading safetensors checkpoint shards:   7% Completed | 2/27 [00:09<02:02,  4.92s/it]
[2026-01-17 23:53:31] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m
[2026-01-17 23:53:31] [INFO] Loading safetensors checkpoint shards:  11% Completed | 3/27 [00:14<01:56,  4.85s/it]
[2026-01-17 23:53:34] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m
[2026-01-17 23:53:34] [INFO] Loading safetensors checkpoint shards:  15% Completed | 4/27 [00:17<01:34,  4.12s/it]
[2026-01-17 23:53:35] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m
[2026-01-17 23:53:35] [INFO] Loading safetensors checkpoint shards:  19% Completed | 5/27 [00:18<01:07,  3.07s/it]
[2026-01-17 23:53:37] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m
[2026-01-17 23:53:37] [INFO] Loading safetensors checkpoint shards:  22% Completed | 6/27 [00:20<00:50,  2.42s/it]
[2026-01-17 23:53:38] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m
[2026-01-17 23:53:38] [INFO] Loading safetensors checkpoint shards:  26% Completed | 7/27 [00:21<00:40,  2.02s/it]
[2026-01-17 23:53:39] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m
[2026-01-17 23:53:39] [INFO] Loading safetensors checkpoint shards:  30% Completed | 8/27 [00:22<00:33,  1.75s/it]
[2026-01-17 23:53:40] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m
[2026-01-17 23:53:40] [INFO] Loading safetensors checkpoint shards:  33% Completed | 9/27 [00:23<00:28,  1.59s/it]
[2026-01-17 23:53:41] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m
[2026-01-17 23:53:41] [INFO] Loading safetensors checkpoint shards:  37% Completed | 10/27 [00:24<00:25,  1.48s/it]
[2026-01-17 23:53:43] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m
[2026-01-17 23:53:43] [INFO] Loading safetensors checkpoint shards:  41% Completed | 11/27 [00:26<00:22,  1.43s/it]
[2026-01-17 23:53:43] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m
[2026-01-17 23:53:43] [INFO] Loading safetensors checkpoint shards:  44% Completed | 12/27 [00:26<00:16,  1.12s/it]
[2026-01-17 23:53:44] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m
[2026-01-17 23:53:44] [INFO] Loading safetensors checkpoint shards:  48% Completed | 13/27 [00:26<00:12,  1.15it/s]
[2026-01-17 23:53:45] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m
[2026-01-17 23:53:45] [INFO] Loading safetensors checkpoint shards:  52% Completed | 14/27 [00:28<00:12,  1.01it/s]
[2026-01-17 23:53:46] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m
[2026-01-17 23:53:46] [INFO] Loading safetensors checkpoint shards:  56% Completed | 15/27 [00:29<00:12,  1.07s/it]
[2026-01-17 23:53:47] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m
[2026-01-17 23:53:47] [INFO] Loading safetensors checkpoint shards:  59% Completed | 16/27 [00:30<00:12,  1.11s/it]
[2026-01-17 23:53:48] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m
[2026-01-17 23:53:48] [INFO] Loading safetensors checkpoint shards:  63% Completed | 17/27 [00:31<00:11,  1.14s/it]
[2026-01-17 23:53:50] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m
[2026-01-17 23:53:50] [INFO] Loading safetensors checkpoint shards:  67% Completed | 18/27 [00:33<00:10,  1.16s/it]
[2026-01-17 23:53:51] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m
[2026-01-17 23:53:51] [INFO] Loading safetensors checkpoint shards:  70% Completed | 19/27 [00:34<00:09,  1.18s/it]
[2026-01-17 23:53:52] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m
[2026-01-17 23:53:52] [INFO] Loading safetensors checkpoint shards:  74% Completed | 20/27 [00:35<00:08,  1.18s/it]
[2026-01-17 23:53:53] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m
[2026-01-17 23:53:53] [INFO] Loading safetensors checkpoint shards:  78% Completed | 21/27 [00:36<00:07,  1.19s/it]
[2026-01-17 23:53:54] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m
[2026-01-17 23:53:54] [INFO] Loading safetensors checkpoint shards:  81% Completed | 22/27 [00:37<00:05,  1.19s/it]
[2026-01-17 23:53:56] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m
[2026-01-17 23:53:56] [INFO] Loading safetensors checkpoint shards:  85% Completed | 23/27 [00:39<00:04,  1.20s/it]
[2026-01-17 23:53:57] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m
[2026-01-17 23:53:57] [INFO] Loading safetensors checkpoint shards:  89% Completed | 24/27 [00:40<00:03,  1.20s/it]
[2026-01-17 23:53:58] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m
[2026-01-17 23:53:58] [INFO] Loading safetensors checkpoint shards:  93% Completed | 25/27 [00:41<00:02,  1.21s/it]
[2026-01-17 23:53:59] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m
[2026-01-17 23:53:59] [INFO] Loading safetensors checkpoint shards:  96% Completed | 26/27 [00:42<00:01,  1.21s/it]
[2026-01-17 23:54:01] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m
[2026-01-17 23:54:01] [INFO] Loading safetensors checkpoint shards: 100% Completed | 27/27 [00:44<00:00,  1.23s/it]
[2026-01-17 23:54:01] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m
[2026-01-17 23:54:01] [INFO] Loading safetensors checkpoint shards: 100% Completed | 27/27 [00:44<00:00,  1.63s/it]
[2026-01-17 23:54:01] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m
[2026-01-17 23:54:01] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m INFO 01-17 23:54:01 [default_loader.py:291] Loading weights took 44.12 seconds
[2026-01-17 23:54:01] [WARNING] [0;36m(Worker_TP0 pid=277443)[0;0m WARNING 01-17 23:54:01 [kv_cache.py:90] Checkpoint does not provide a q scaling factor. Setting it to k_scale. This only matters for FP8 Attention backends (flash-attn or flashinfer).
[2026-01-17 23:54:01] [WARNING] [0;36m(Worker_TP0 pid=277443)[0;0m WARNING 01-17 23:54:01 [kv_cache.py:104] Using KV cache scaling factor 1.0 for fp8_e4m3. If this is unintended, verify that k/v_scale scaling factors are properly set in the checkpoint.
[2026-01-17 23:54:01] [WARNING] [0;36m(Worker_TP0 pid=277443)[0;0m WARNING 01-17 23:54:01 [kv_cache.py:143] Using uncalibrated q_scale 1.0 and/or prob_scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure q/prob scaling factors are available in the fp8 checkpoint.
[2026-01-17 23:54:01] [WARNING] [0;36m(Worker_TP1 pid=277444)[0;0m WARNING 01-17 23:54:01 [kv_cache.py:90] Checkpoint does not provide a q scaling factor. Setting it to k_scale. This only matters for FP8 Attention backends (flash-attn or flashinfer).
[2026-01-17 23:54:01] [WARNING] [0;36m(Worker_TP1 pid=277444)[0;0m WARNING 01-17 23:54:01 [kv_cache.py:104] Using KV cache scaling factor 1.0 for fp8_e4m3. If this is unintended, verify that k/v_scale scaling factors are properly set in the checkpoint.
[2026-01-17 23:54:01] [WARNING] [0;36m(Worker_TP1 pid=277444)[0;0m WARNING 01-17 23:54:01 [kv_cache.py:143] Using uncalibrated q_scale 1.0 and/or prob_scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure q/prob scaling factors are available in the fp8 checkpoint.
[2026-01-17 23:54:02] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m INFO 01-17 23:54:02 [gpu_model_runner.py:3886] Model loading took 59.79 GiB memory and 48.864837 seconds
[2026-01-17 23:54:22] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m INFO 01-17 23:54:22 [backends.py:644] Using cache directory: /home/wuwen/.cache/vllm/torch_compile_cache/5dfa8b2b6f/rank_0_0/backbone for vLLM's torch.compile
[2026-01-17 23:54:22] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m INFO 01-17 23:54:22 [backends.py:704] Dynamo bytecode transform time: 19.37 s
[2026-01-17 23:54:42] [INFO] [0;36m(Worker_TP1 pid=277444)[0;0m INFO 01-17 23:54:42 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 5.569 s
[2026-01-17 23:54:42] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m INFO 01-17 23:54:42 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 5.647 s
[2026-01-17 23:54:42] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m INFO 01-17 23:54:42 [monitor.py:34] torch.compile takes 25.02 s in total
[2026-01-17 23:54:43] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m INFO 01-17 23:54:43 [gpu_worker.py:358] Available KV cache memory: 24.17 GiB
[2026-01-17 23:54:44] [INFO] [0;36m(EngineCore_DP0 pid=277174)[0;0m INFO 01-17 23:54:44 [kv_cache_utils.py:1305] GPU KV cache size: 288,016 tokens
[2026-01-17 23:54:44] [INFO] [0;36m(EngineCore_DP0 pid=277174)[0;0m INFO 01-17 23:54:44 [kv_cache_utils.py:1310] Maximum concurrency for 128,000 tokens per request: 2.25x
[2026-01-17 23:54:44] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m 2026-01-17 23:54:44,443 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[2026-01-17 23:54:44] [INFO] [0;36m(Worker_TP1 pid=277444)[0;0m 2026-01-17 23:54:44,443 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[2026-01-17 23:54:49] [INFO] [0;36m(Worker_TP1 pid=277444)[0;0m 2026-01-17 23:54:49,543 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[2026-01-17 23:54:49] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m 2026-01-17 23:54:49,543 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[2026-01-17 23:54:49] [INFO] [0;36m(Worker_TP1 pid=277444)[0;0m INFO 01-17 23:54:49 [kernel_warmup.py:64] Warming up FlashInfer attention.
[2026-01-17 23:54:49] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m INFO 01-17 23:54:49 [kernel_warmup.py:64] Warming up FlashInfer attention.
[2026-01-17 23:54:50] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m
[2026-01-17 23:54:50] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
[2026-01-17 23:54:51] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:15,  3.29it/s]
[2026-01-17 23:54:51] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:14,  3.30it/s]
[2026-01-17 23:54:51] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:14,  3.35it/s]
[2026-01-17 23:54:52] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:01<00:13,  3.36it/s]
[2026-01-17 23:54:52] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:01<00:13,  3.38it/s]
[2026-01-17 23:54:52] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:01<00:13,  3.40it/s]
[2026-01-17 23:54:52] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▎        | 7/51 [00:02<00:12,  3.43it/s]
[2026-01-17 23:54:53] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:02<00:12,  3.46it/s]
[2026-01-17 23:54:53] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:02<00:11,  3.54it/s]
[2026-01-17 23:54:53] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|█▉        | 10/51 [00:02<00:11,  3.63it/s]
[2026-01-17 23:54:53] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:03<00:10,  3.69it/s]
[2026-01-17 23:54:54] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:03<00:09,  3.99it/s]
[2026-01-17 23:54:54] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:03<00:09,  4.21it/s]
[2026-01-17 23:54:54] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:03<00:08,  4.40it/s]
[2026-01-17 23:54:54] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:03<00:07,  4.55it/s]
[2026-01-17 23:54:54] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 16/51 [00:04<00:07,  4.66it/s]
[2026-01-17 23:54:55] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:04<00:07,  4.72it/s]
[2026-01-17 23:54:55] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:04<00:06,  4.78it/s]
[2026-01-17 23:54:55] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 19/51 [00:04<00:06,  4.82it/s]
[2026-01-17 23:54:55] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:04<00:06,  4.86it/s]
[2026-01-17 23:54:55] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:05<00:06,  4.86it/s]
[2026-01-17 23:54:56] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 22/51 [00:05<00:05,  4.89it/s]
[2026-01-17 23:54:56] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:05<00:06,  4.18it/s]
[2026-01-17 23:54:56] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:05<00:06,  4.38it/s]
[2026-01-17 23:54:56] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 25/51 [00:06<00:05,  4.52it/s]
[2026-01-17 23:54:57] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:06<00:05,  4.65it/s]
[2026-01-17 23:54:57] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:06<00:05,  4.73it/s]
[2026-01-17 23:54:57] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:06<00:05,  3.96it/s]
[2026-01-17 23:54:57] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:07<00:05,  4.22it/s]
[2026-01-17 23:54:58] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:07<00:04,  4.41it/s]
[2026-01-17 23:54:58] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 31/51 [00:07<00:04,  4.55it/s]
[2026-01-17 23:54:58] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:07<00:04,  4.63it/s]
[2026-01-17 23:54:58] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:07<00:03,  4.71it/s]
[2026-01-17 23:54:59] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:08<00:03,  4.65it/s]
[2026-01-17 23:54:59] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:08<00:04,  3.72it/s]
[2026-01-17 23:54:59] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:08<00:03,  3.93it/s]
[2026-01-17 23:54:59] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 37/51 [00:08<00:03,  4.08it/s]
[2026-01-17 23:54:59] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:09<00:03,  4.20it/s]
[2026-01-17 23:55:00] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▋  | 39/51 [00:09<00:02,  4.29it/s]
[2026-01-17 23:55:00] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:09<00:02,  4.33it/s]
[2026-01-17 23:55:00] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:09<00:02,  4.40it/s]
[2026-01-17 23:55:01] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:10<00:02,  4.42it/s]
[2026-01-17 23:55:01] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 43/51 [00:10<00:02,  3.44it/s]
[2026-01-17 23:55:01] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:10<00:01,  3.67it/s]
[2026-01-17 23:55:01] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 45/51 [00:10<00:01,  3.87it/s]
[2026-01-17 23:55:01] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|█████████ | 46/51 [00:11<00:01,  4.09it/s]
[2026-01-17 23:55:02] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:11<00:00,  4.26it/s]
[2026-01-17 23:55:02] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 48/51 [00:11<00:00,  4.27it/s]
[2026-01-17 23:55:02] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▌| 49/51 [00:11<00:00,  4.28it/s]
[2026-01-17 23:55:02] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:12<00:00,  4.34it/s]
[2026-01-17 23:55:02] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:12<00:00,  4.35it/s]
[2026-01-17 23:55:02] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:12<00:00,  4.15it/s]
[2026-01-17 23:55:02] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m
[2026-01-17 23:55:03] [INFO] Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
[2026-01-17 23:55:03] [INFO] Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:06,  4.88it/s]
[2026-01-17 23:55:03] [INFO] Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:00<00:12,  2.72it/s]
[2026-01-17 23:55:03] [INFO] Capturing CUDA graphs (decode, FULL):   9%|▊         | 3/35 [00:00<00:09,  3.47it/s]
[2026-01-17 23:55:04] [INFO] Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:01<00:07,  3.95it/s]
[2026-01-17 23:55:04] [INFO] Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:01<00:07,  4.28it/s]
[2026-01-17 23:55:04] [INFO] Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:01<00:06,  4.51it/s]
[2026-01-17 23:55:04] [INFO] Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:01<00:05,  4.71it/s]
[2026-01-17 23:55:04] [INFO] Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:01<00:05,  4.83it/s]
[2026-01-17 23:55:05] [INFO] Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:02<00:05,  4.95it/s]
[2026-01-17 23:55:05] [INFO] Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:02<00:04,  5.03it/s]
[2026-01-17 23:55:05] [INFO] Capturing CUDA graphs (decode, FULL):  31%|███▏      | 11/35 [00:02<00:04,  5.09it/s]
[2026-01-17 23:55:05] [INFO] Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:02<00:04,  5.10it/s]
[2026-01-17 23:55:05] [INFO] Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:02<00:04,  5.09it/s]
[2026-01-17 23:55:06] [INFO] Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:03<00:04,  5.10it/s]
[2026-01-17 23:55:06] [INFO] Capturing CUDA graphs (decode, FULL):  43%|████▎     | 15/35 [00:03<00:05,  3.43it/s]
[2026-01-17 23:55:06] [INFO] Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:03<00:04,  3.81it/s]
[2026-01-17 23:55:07] [INFO] Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:03<00:04,  4.15it/s]
[2026-01-17 23:55:07] [INFO] Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:04<00:03,  4.33it/s]
[2026-01-17 23:55:07] [INFO] Capturing CUDA graphs (decode, FULL):  54%|█████▍    | 19/35 [00:04<00:03,  4.46it/s]
[2026-01-17 23:55:07] [INFO] Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:04<00:03,  4.54it/s]
[2026-01-17 23:55:07] [INFO] Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:04<00:03,  4.59it/s]
[2026-01-17 23:55:08] [INFO] Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:04<00:02,  4.67it/s]
[2026-01-17 23:55:08] [INFO] Capturing CUDA graphs (decode, FULL):  66%|██████▌   | 23/35 [00:05<00:02,  4.71it/s]
[2026-01-17 23:55:08] [INFO] Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:05<00:02,  4.69it/s]
[2026-01-17 23:55:08] [INFO] Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:05<00:02,  4.71it/s]
[2026-01-17 23:55:08] [INFO] Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:05<00:01,  4.71it/s]
[2026-01-17 23:55:09] [INFO] Capturing CUDA graphs (decode, FULL):  77%|███████▋  | 27/35 [00:06<00:01,  4.74it/s]
[2026-01-17 23:55:09] [INFO] Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:06<00:01,  4.77it/s]
[2026-01-17 23:55:09] [INFO] Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:06<00:01,  4.76it/s]
[2026-01-17 23:55:09] [INFO] Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:06<00:01,  4.83it/s]
[2026-01-17 23:55:10] [INFO] Capturing CUDA graphs (decode, FULL):  89%|████████▊ | 31/35 [00:06<00:00,  4.90it/s]
[2026-01-17 23:55:10] [INFO] Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:07<00:00,  3.03it/s]
[2026-01-17 23:55:10] [INFO] Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:07<00:00,  3.39it/s]
[2026-01-17 23:55:10] [INFO] Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:07<00:00,  3.73it/s]
[2026-01-17 23:55:10] [INFO] Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:08<00:00,  4.09it/s]
[2026-01-17 23:55:10] [INFO] Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:08<00:00,  4.34it/s]
[2026-01-17 23:55:10] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m INFO 01-17 23:55:10 [custom_all_reduce.py:216] Registering 13275 cuda graph addresses
[2026-01-17 23:55:11] [INFO] [0;36m(Worker_TP1 pid=277444)[0;0m INFO 01-17 23:55:11 [custom_all_reduce.py:216] Registering 13275 cuda graph addresses
[2026-01-17 23:55:12] [INFO] [0;36m(Worker_TP0 pid=277443)[0;0m INFO 01-17 23:55:12 [gpu_model_runner.py:4837] Graph capturing finished in 23 secs, took -0.96 GiB
[2026-01-17 23:55:12] [INFO] [0;36m(EngineCore_DP0 pid=277174)[0;0m INFO 01-17 23:55:12 [core.py:273] init engine (profile, create kv cache, warmup model) took 70.12 seconds
[2026-01-17 23:55:12] [INFO] [0;36m(EngineCore_DP0 pid=277174)[0;0m [2026-01-17 23:55:12] INFO tekken.py:195: Non special vocabulary size is 130072 with 1000 special tokens.
[2026-01-17 23:55:13] [INFO] [0;36m(EngineCore_DP0 pid=277174)[0;0m INFO 01-17 23:55:13 [core.py:186] Batch queue is enabled with size 2
[2026-01-17 23:55:13] [INFO] [0;36m(EngineCore_DP0 pid=277174)[0;0m INFO 01-17 23:55:13 [vllm.py:645] Asynchronous scheduling is enabled.
[2026-01-17 23:55:14] [INFO] [0;36m(APIServer pid=276815)[0;0m INFO 01-17 23:55:14 [api_server.py:1020] Supported tasks: ['generate']
[2026-01-17 23:55:14] [INFO] [0;36m(Worker_TP1 pid=277444)[0;0m [0;36m(Worker_TP0 pid=277443)[0;0m INFO 01-17 23:55:14 [multiproc_executor.py:707] Parent process exited, terminating worker
[2026-01-17 23:55:14] [INFO] INFO 01-17 23:55:14 [multiproc_executor.py:707] Parent process exited, terminating worker
[2026-01-17 23:55:19] [ERROR] [0;36m(APIServer pid=276815)[0;0m Traceback (most recent call last):
[2026-01-17 23:55:19] [INFO] [0;36m(APIServer pid=276815)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/bin/vllm", line 7, in <module>
[2026-01-17 23:55:19] [INFO] [0;36m(APIServer pid=276815)[0;0m     sys.exit(main())
[2026-01-17 23:55:19] [INFO] [0;36m(APIServer pid=276815)[0;0m              ^^^^^^
[2026-01-17 23:55:19] [INFO] [0;36m(APIServer pid=276815)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-17 23:55:19] [INFO] [0;36m(APIServer pid=276815)[0;0m     args.dispatch_function(args)
[2026-01-17 23:55:19] [INFO] [0;36m(APIServer pid=276815)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-17 23:55:19] [INFO] [0;36m(APIServer pid=276815)[0;0m     uvloop.run(run_server(args))
[2026-01-17 23:55:19] [INFO] [0;36m(APIServer pid=276815)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-17 23:55:19] [INFO] [0;36m(APIServer pid=276815)[0;0m     return __asyncio.run(
[2026-01-17 23:55:19] [INFO] [0;36m(APIServer pid=276815)[0;0m            ^^^^^^^^^^^^^^
[2026-01-17 23:55:19] [INFO] [0;36m(APIServer pid=276815)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-17 23:55:19] [INFO] [0;36m(APIServer pid=276815)[0;0m     return runner.run(main)
[2026-01-17 23:55:19] [INFO] [0;36m(APIServer pid=276815)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-17 23:55:19] [INFO] [0;36m(APIServer pid=276815)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-17 23:55:19] [INFO] [0;36m(APIServer pid=276815)[0;0m     return self._loop.run_until_complete(task)
[2026-01-17 23:55:19] [INFO] [0;36m(APIServer pid=276815)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-17 23:55:19] [INFO] [0;36m(APIServer pid=276815)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-17 23:55:19] [INFO] [0;36m(APIServer pid=276815)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-17 23:55:19] [INFO] [0;36m(APIServer pid=276815)[0;0m     return await main
[2026-01-17 23:55:19] [INFO] [0;36m(APIServer pid=276815)[0;0m            ^^^^^^^^^^
[2026-01-17 23:55:19] [INFO] [0;36m(APIServer pid=276815)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 1325, in run_server
[2026-01-17 23:55:19] [INFO] [0;36m(APIServer pid=276815)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-17 23:55:19] [INFO] [0;36m(APIServer pid=276815)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 1350, in run_server_worker
[2026-01-17 23:55:19] [INFO] [0;36m(APIServer pid=276815)[0;0m     await init_app_state(engine_client, app.state, args)
[2026-01-17 23:55:19] [INFO] [0;36m(APIServer pid=276815)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 1022, in init_app_state
[2026-01-17 23:55:19] [INFO] [0;36m(APIServer pid=276815)[0;0m     resolved_chat_template = await process_chat_template(
[2026-01-17 23:55:19] [INFO] [0;36m(APIServer pid=276815)[0;0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-17 23:55:19] [INFO] [0;36m(APIServer pid=276815)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/utils.py", line 301, in process_chat_template
[2026-01-17 23:55:19] [INFO] [0;36m(APIServer pid=276815)[0;0m     resolved_chat_template = resolve_mistral_chat_template(
[2026-01-17 23:55:19] [INFO] [0;36m(APIServer pid=276815)[0;0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-17 23:55:19] [INFO] [0;36m(APIServer pid=276815)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/chat_utils.py", line 461, in resolve_mistral_chat_template
[2026-01-17 23:55:19] [INFO] [0;36m(APIServer pid=276815)[0;0m     raise ValueError(
[2026-01-17 23:55:19] [INFO] [0;36m(APIServer pid=276815)[0;0m ValueError: 'chat_template' or 'chat_template_kwargs' cannot be overridden for mistral tokenizer.
[2026-01-17 23:55:20] [INFO] nvitop监控已启动
[2026-01-17 23:58:33] [SUCCESS] 方案已保存: Seed-OSS-36B-Instruct
[2026-01-17 23:58:37] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-17 23:58:37] [INFO] nvitop监控已启动
[2026-01-17 23:58:37] [INFO] nvitop监控已停止
[2026-01-17 23:58:39] [INFO] /bin/bash: 第 1 行： export: " ": 不是有效的标识符
[2026-01-17 23:58:39] [INFO] nvitop监控已启动
[2026-01-18 00:00:21] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 00:00:21] [INFO] nvitop监控已启动
[2026-01-18 00:00:21] [INFO] nvitop监控已停止
[2026-01-18 00:00:47] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:00:47 [api_server.py:1278] vLLM API server version 0.14.0rc1.dev492+g19504ac07
[2026-01-18 00:00:47] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:00:47 [utils.py:254] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/Seed/Seed-OSS-36B-Instruct', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/AI-Acer4T/AI-Chat/models/Seed/Seed-OSS-36B-Instruct/chat_template.jinja', 'enable_auto_tool_choice': True, 'tool_call_parser': 'seed_oss', 'model': '/mnt/AI-Acer4T/AI-Chat/models/Seed/Seed-OSS-36B-Instruct', 'trust_remote_code': True, 'max_model_len': 128000, 'served_model_name': ['VLLM-MODEL'], 'reasoning_parser': 'seed_oss', 'tensor_parallel_size': 2, 'disable_custom_all_reduce': True, 'kv_cache_dtype': 'fp8', 'enable_prefix_caching': True, 'max_num_seqs': 256, 'async_scheduling': True, 'attention_config': AttentionConfig(backend=<AttentionBackendEnum.FLASHINFER: 'vllm.v1.attention.backends.flashinfer.FlashInferBackend'>, flash_attn_version=None, use_prefill_decode_attention=False, flash_attn_max_num_splits_for_cuda_graph=32, use_cudnn_prefill=False, use_trtllm_ragged_deepseek_prefill=False, use_trtllm_attention=None, disable_flashinfer_prefill=False, disable_flashinfer_q_quantization=False)}
[2026-01-18 00:00:47] [INFO] [0;36m(APIServer pid=280892)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 00:00:47] [INFO] [0;36m(APIServer pid=280892)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 00:00:47] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:00:47 [model.py:528] Resolved architecture: SeedOssForCausalLM
[2026-01-18 00:00:47] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:00:47 [model.py:1543] Using max model len 128000
[2026-01-18 00:00:47] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:00:47 [cache.py:206] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor.
[2026-01-18 00:00:47] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:00:47 [scheduler.py:231] Chunked prefill is enabled with max_num_batched_tokens=8192.
[2026-01-18 00:00:47] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:00:47 [vllm.py:640] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-18 00:00:47] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:00:47 [vllm.py:645] Asynchronous scheduling is enabled.
[2026-01-18 00:00:48] [INFO] [0;36m(APIServer pid=280892)[0;0m The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
[2026-01-18 00:01:11] [INFO] [0;36m(EngineCore_DP0 pid=281161)[0;0m INFO 01-18 00:01:11 [core.py:97] Initializing a V1 LLM engine (v0.14.0rc1.dev492+g19504ac07) with config: model='/mnt/AI-Acer4T/AI-Chat/models/Seed/Seed-OSS-36B-Instruct', speculative_config=None, tokenizer='/mnt/AI-Acer4T/AI-Chat/models/Seed/Seed-OSS-36B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=fp8, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='seed_oss', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-18 00:01:11] [WARNING] [0;36m(EngineCore_DP0 pid=281161)[0;0m WARNING 01-18 00:01:11 [multiproc_executor.py:880] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-18 00:01:35] [INFO] INFO 01-18 00:01:35 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:60513 backend=nccl
[2026-01-18 00:01:35] [INFO] INFO 01-18 00:01:35 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:60513 backend=nccl
[2026-01-18 00:01:35] [INFO] INFO 01-18 00:01:35 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-18 00:01:35] [WARNING] WARNING 01-18 00:01:35 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 00:01:35] [WARNING] WARNING 01-18 00:01:35 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 00:01:35] [INFO] INFO 01-18 00:01:35 [parallel_state.py:1425] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[2026-01-18 00:01:35] [INFO] INFO 01-18 00:01:35 [parallel_state.py:1425] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
[2026-01-18 00:01:39] [INFO] INFO 01-18 00:01:39 [topk_topp_sampler.py:47] Using FlashInfer for top-p & top-k sampling.
[2026-01-18 00:01:40] [INFO] [0;36m(Worker_TP0 pid=281460)[0;0m INFO 01-18 00:01:40 [gpu_model_runner.py:3789] Starting to load model /mnt/AI-Acer4T/AI-Chat/models/Seed/Seed-OSS-36B-Instruct...
[2026-01-18 00:01:40] [INFO] [0;36m(Worker_TP1 pid=281462)[0;0m INFO 01-18 00:01:40 [cuda.py:315] Using AttentionBackendEnum.FLASHINFER backend.
[2026-01-18 00:01:40] [INFO] [0;36m(Worker_TP0 pid=281460)[0;0m INFO 01-18 00:01:40 [cuda.py:315] Using AttentionBackendEnum.FLASHINFER backend.
[2026-01-18 00:01:41] [INFO] [0;36m(Worker_TP0 pid=281460)[0;0m
[2026-01-18 00:01:41] [INFO] Loading safetensors checkpoint shards:   0% Completed | 0/15 [00:00<?, ?it/s]
[2026-01-18 00:01:45] [INFO] [0;36m(Worker_TP0 pid=281460)[0;0m
[2026-01-18 00:01:45] [INFO] Loading safetensors checkpoint shards:   7% Completed | 1/15 [00:04<00:58,  4.15s/it]
[2026-01-18 00:01:49] [INFO] [0;36m(Worker_TP0 pid=281460)[0;0m
[2026-01-18 00:01:49] [INFO] Loading safetensors checkpoint shards:  13% Completed | 2/15 [00:08<00:57,  4.41s/it]
[2026-01-18 00:01:54] [INFO] [0;36m(Worker_TP0 pid=281460)[0;0m
[2026-01-18 00:01:54] [INFO] Loading safetensors checkpoint shards:  20% Completed | 3/15 [00:13<00:54,  4.52s/it]
[2026-01-18 00:01:59] [INFO] [0;36m(Worker_TP0 pid=281460)[0;0m
[2026-01-18 00:01:59] [INFO] Loading safetensors checkpoint shards:  27% Completed | 4/15 [00:18<00:50,  4.59s/it]
[2026-01-18 00:02:03] [INFO] [0;36m(Worker_TP0 pid=281460)[0;0m
[2026-01-18 00:02:03] [INFO] Loading safetensors checkpoint shards:  33% Completed | 5/15 [00:22<00:45,  4.58s/it]
[2026-01-18 00:02:08] [INFO] [0;36m(Worker_TP0 pid=281460)[0;0m
[2026-01-18 00:02:08] [INFO] Loading safetensors checkpoint shards:  40% Completed | 6/15 [00:27<00:41,  4.66s/it]
[2026-01-18 00:02:13] [INFO] [0;36m(Worker_TP0 pid=281460)[0;0m
[2026-01-18 00:02:13] [INFO] Loading safetensors checkpoint shards:  47% Completed | 7/15 [00:32<00:37,  4.71s/it]
[2026-01-18 00:02:18] [INFO] [0;36m(Worker_TP0 pid=281460)[0;0m
[2026-01-18 00:02:18] [INFO] Loading safetensors checkpoint shards:  53% Completed | 8/15 [00:37<00:33,  4.82s/it]
[2026-01-18 00:02:23] [INFO] [0;36m(Worker_TP0 pid=281460)[0;0m
[2026-01-18 00:02:23] [INFO] Loading safetensors checkpoint shards:  60% Completed | 9/15 [00:42<00:28,  4.80s/it]
[2026-01-18 00:02:28] [INFO] [0;36m(Worker_TP0 pid=281460)[0;0m
[2026-01-18 00:02:28] [INFO] Loading safetensors checkpoint shards:  67% Completed | 10/15 [00:46<00:24,  4.82s/it]
[2026-01-18 00:02:32] [INFO] [0;36m(Worker_TP0 pid=281460)[0;0m
[2026-01-18 00:02:32] [INFO] Loading safetensors checkpoint shards:  73% Completed | 11/15 [00:51<00:18,  4.69s/it]
[2026-01-18 00:02:37] [INFO] [0;36m(Worker_TP0 pid=281460)[0;0m
[2026-01-18 00:02:37] [INFO] Loading safetensors checkpoint shards:  80% Completed | 12/15 [00:56<00:14,  4.71s/it]
[2026-01-18 00:02:42] [INFO] [0;36m(Worker_TP0 pid=281460)[0;0m
[2026-01-18 00:02:42] [INFO] Loading safetensors checkpoint shards:  87% Completed | 13/15 [01:01<00:09,  4.91s/it]
[2026-01-18 00:02:47] [INFO] [0;36m(Worker_TP0 pid=281460)[0;0m
[2026-01-18 00:02:47] [INFO] Loading safetensors checkpoint shards:  93% Completed | 14/15 [01:06<00:04,  4.87s/it]
[2026-01-18 00:02:51] [INFO] [0;36m(Worker_TP0 pid=281460)[0;0m
[2026-01-18 00:02:51] [INFO] Loading safetensors checkpoint shards: 100% Completed | 15/15 [01:10<00:00,  4.59s/it]
[2026-01-18 00:02:51] [INFO] [0;36m(Worker_TP0 pid=281460)[0;0m
[2026-01-18 00:02:51] [INFO] Loading safetensors checkpoint shards: 100% Completed | 15/15 [01:10<00:00,  4.68s/it]
[2026-01-18 00:02:51] [INFO] [0;36m(Worker_TP0 pid=281460)[0;0m
[2026-01-18 00:02:51] [INFO] [0;36m(Worker_TP0 pid=281460)[0;0m INFO 01-18 00:02:51 [default_loader.py:291] Loading weights took 70.28 seconds
[2026-01-18 00:02:52] [INFO] [0;36m(Worker_TP0 pid=281460)[0;0m INFO 01-18 00:02:52 [gpu_model_runner.py:3886] Model loading took 33.86 GiB memory and 71.042125 seconds
[2026-01-18 00:03:04] [INFO] [0;36m(Worker_TP0 pid=281460)[0;0m INFO 01-18 00:03:04 [backends.py:644] Using cache directory: /home/wuwen/.cache/vllm/torch_compile_cache/e3a4a258bb/rank_0_0/backbone for vLLM's torch.compile
[2026-01-18 00:03:04] [INFO] [0;36m(Worker_TP0 pid=281460)[0;0m INFO 01-18 00:03:04 [backends.py:704] Dynamo bytecode transform time: 11.58 s
[2026-01-18 00:03:12] [INFO] [0;36m(Worker_TP1 pid=281462)[0;0m INFO 01-18 00:03:12 [backends.py:261] Cache the graph of compile range (1, 8192) for later use
[2026-01-18 00:03:12] [INFO] [0;36m(Worker_TP0 pid=281460)[0;0m INFO 01-18 00:03:12 [backends.py:261] Cache the graph of compile range (1, 8192) for later use
[2026-01-18 00:03:18] [INFO] [0;36m(Worker_TP0 pid=281460)[0;0m INFO 01-18 00:03:18 [backends.py:278] Compiling a graph for compile range (1, 8192) takes 10.40 s
[2026-01-18 00:03:18] [INFO] [0;36m(Worker_TP0 pid=281460)[0;0m INFO 01-18 00:03:18 [monitor.py:34] torch.compile takes 21.98 s in total
[2026-01-18 00:03:19] [INFO] [0;36m(Worker_TP0 pid=281460)[0;0m INFO 01-18 00:03:19 [gpu_worker.py:358] Available KV cache memory: 50.45 GiB
[2026-01-18 00:03:20] [INFO] [0;36m(EngineCore_DP0 pid=281161)[0;0m INFO 01-18 00:03:20 [kv_cache_utils.py:1305] GPU KV cache size: 826,528 tokens
[2026-01-18 00:03:20] [INFO] [0;36m(EngineCore_DP0 pid=281161)[0;0m INFO 01-18 00:03:20 [kv_cache_utils.py:1310] Maximum concurrency for 128,000 tokens per request: 6.46x
[2026-01-18 00:03:20] [INFO] [0;36m(Worker_TP0 pid=281460)[0;0m 2026-01-18 00:03:20,547 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[2026-01-18 00:03:20] [INFO] [0;36m(Worker_TP1 pid=281462)[0;0m 2026-01-18 00:03:20,547 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[2026-01-18 00:03:20] [INFO] [0;36m(Worker_TP0 pid=281460)[0;0m 2026-01-18 00:03:20,579 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[2026-01-18 00:03:20] [INFO] [0;36m(Worker_TP0 pid=281460)[0;0m INFO 01-18 00:03:20 [kernel_warmup.py:64] Warming up FlashInfer attention.
[2026-01-18 00:03:20] [INFO] [0;36m(Worker_TP1 pid=281462)[0;0m 2026-01-18 00:03:20,592 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[2026-01-18 00:03:20] [INFO] [0;36m(Worker_TP1 pid=281462)[0;0m INFO 01-18 00:03:20 [kernel_warmup.py:64] Warming up FlashInfer attention.
[2026-01-18 00:03:22] [INFO] [0;36m(Worker_TP0 pid=281460)[0;0m
[2026-01-18 00:03:22] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
[2026-01-18 00:03:22] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:06,  7.27it/s]
[2026-01-18 00:03:22] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:06,  7.37it/s]
[2026-01-18 00:03:22] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:06,  7.43it/s]
[2026-01-18 00:03:22] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:00<00:06,  7.40it/s]
[2026-01-18 00:03:23] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:00<00:06,  7.34it/s]
[2026-01-18 00:03:23] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:06,  7.31it/s]
[2026-01-18 00:03:23] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▎        | 7/51 [00:00<00:05,  7.37it/s]
[2026-01-18 00:03:23] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:01<00:05,  7.47it/s]
[2026-01-18 00:03:23] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:01<00:05,  7.73it/s]
[2026-01-18 00:03:23] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|█▉        | 10/51 [00:01<00:05,  7.87it/s]
[2026-01-18 00:03:23] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:01<00:04,  8.04it/s]
[2026-01-18 00:03:23] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:01<00:04,  8.16it/s]
[2026-01-18 00:03:24] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:01<00:04,  8.07it/s]
[2026-01-18 00:03:24] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:01<00:04,  8.04it/s]
[2026-01-18 00:03:24] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:01<00:04,  8.02it/s]
[2026-01-18 00:03:24] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 16/51 [00:02<00:04,  8.08it/s]
[2026-01-18 00:03:24] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:02<00:04,  8.37it/s]
[2026-01-18 00:03:24] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:02<00:03,  8.54it/s]
[2026-01-18 00:03:24] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 19/51 [00:02<00:03,  8.69it/s]
[2026-01-18 00:03:24] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:02<00:03,  8.23it/s]
[2026-01-18 00:03:25] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:02<00:03,  8.47it/s]
[2026-01-18 00:03:25] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 22/51 [00:02<00:03,  8.60it/s]
[2026-01-18 00:03:25] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:02<00:03,  8.74it/s]
[2026-01-18 00:03:25] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:02<00:03,  8.77it/s]
[2026-01-18 00:03:25] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 25/51 [00:03<00:02,  8.87it/s]
[2026-01-18 00:03:25] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:03<00:02,  8.99it/s]
[2026-01-18 00:03:25] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:03<00:02,  9.05it/s]
[2026-01-18 00:03:25] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:03<00:02,  9.12it/s]
[2026-01-18 00:03:25] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:03<00:02,  9.14it/s]
[2026-01-18 00:03:26] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:03<00:02,  9.08it/s]
[2026-01-18 00:03:26] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 31/51 [00:03<00:02,  9.15it/s]
[2026-01-18 00:03:26] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:03<00:02,  8.47it/s]
[2026-01-18 00:03:26] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:03<00:02,  8.74it/s]
[2026-01-18 00:03:26] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:04<00:01,  8.89it/s]
[2026-01-18 00:03:26] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:04<00:01,  9.01it/s]
[2026-01-18 00:03:26] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:04<00:01,  9.10it/s]
[2026-01-18 00:03:26] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 37/51 [00:04<00:01,  9.02it/s]
[2026-01-18 00:03:26] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:04<00:01,  8.98it/s]
[2026-01-18 00:03:27] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▋  | 39/51 [00:04<00:01,  8.86it/s]
[2026-01-18 00:03:27] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:04<00:01,  8.85it/s]
[2026-01-18 00:03:27] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:04<00:01,  8.84it/s]
[2026-01-18 00:03:27] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:04<00:01,  8.92it/s]
[2026-01-18 00:03:27] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 43/51 [00:05<00:00,  8.99it/s]
[2026-01-18 00:03:27] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:05<00:00,  9.04it/s]
[2026-01-18 00:03:27] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 45/51 [00:05<00:00,  9.19it/s]
[2026-01-18 00:03:27] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|█████████ | 46/51 [00:05<00:00,  9.20it/s]
[2026-01-18 00:03:27] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:05<00:00,  9.31it/s]
[2026-01-18 00:03:27] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 48/51 [00:05<00:00,  9.39it/s]
[2026-01-18 00:03:28] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▌| 49/51 [00:05<00:00,  9.50it/s]
[2026-01-18 00:03:28] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:05<00:00,  9.37it/s]
[2026-01-18 00:03:28] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:05<00:00,  8.47it/s]
[2026-01-18 00:03:28] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:05<00:00,  8.53it/s]
[2026-01-18 00:03:28] [INFO] [0;36m(Worker_TP0 pid=281460)[0;0m
[2026-01-18 00:03:28] [INFO] Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
[2026-01-18 00:03:28] [INFO] Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  8.21it/s]
[2026-01-18 00:03:28] [INFO] Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:00<00:03,  8.28it/s]
[2026-01-18 00:03:28] [INFO] Capturing CUDA graphs (decode, FULL):   9%|▊         | 3/35 [00:00<00:03,  8.65it/s]
[2026-01-18 00:03:28] [INFO] Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:03,  8.58it/s]
[2026-01-18 00:03:28] [INFO] Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:00<00:03,  8.84it/s]
[2026-01-18 00:03:29] [INFO] Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:00<00:03,  8.97it/s]
[2026-01-18 00:03:29] [INFO] Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:00<00:03,  9.09it/s]
[2026-01-18 00:03:29] [INFO] Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:00<00:02,  9.14it/s]
[2026-01-18 00:03:29] [INFO] Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:01<00:02,  9.31it/s]
[2026-01-18 00:03:29] [INFO] Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:01<00:02,  9.39it/s]
[2026-01-18 00:03:29] [INFO] Capturing CUDA graphs (decode, FULL):  31%|███▏      | 11/35 [00:01<00:02,  8.67it/s]
[2026-01-18 00:03:29] [INFO] Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:01<00:02,  8.98it/s]
[2026-01-18 00:03:29] [INFO] Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:01<00:02,  8.55it/s]
[2026-01-18 00:03:30] [INFO] Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:01<00:02,  8.93it/s]
[2026-01-18 00:03:30] [INFO] Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:01<00:02,  9.34it/s]
[2026-01-18 00:03:30] [INFO] Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:01<00:01,  9.72it/s]
[2026-01-18 00:03:30] [INFO] Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:02<00:01,  9.94it/s]
[2026-01-18 00:03:30] [INFO] Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:02<00:01,  9.96it/s]
[2026-01-18 00:03:31] [INFO] Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:02<00:01, 10.01it/s]
[2026-01-18 00:03:31] [INFO] Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:02<00:00, 10.09it/s]
[2026-01-18 00:03:31] [INFO] Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:02<00:00, 10.23it/s]
[2026-01-18 00:03:31] [INFO] Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:03<00:00, 10.24it/s]
[2026-01-18 00:03:31] [INFO] Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:03<00:00, 10.53it/s]
[2026-01-18 00:03:31] [INFO] Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:03<00:00, 10.75it/s]
[2026-01-18 00:03:31] [INFO] Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:03<00:00,  9.77it/s]
[2026-01-18 00:03:32] [INFO] [0;36m(Worker_TP0 pid=281460)[0;0m INFO 01-18 00:03:32 [gpu_model_runner.py:4837] Graph capturing finished in 11 secs, took 1.91 GiB
[2026-01-18 00:03:32] [INFO] [0;36m(EngineCore_DP0 pid=281161)[0;0m INFO 01-18 00:03:32 [core.py:273] init engine (profile, create kv cache, warmup model) took 40.32 seconds
[2026-01-18 00:03:33] [INFO] [0;36m(EngineCore_DP0 pid=281161)[0;0m INFO 01-18 00:03:33 [core.py:186] Batch queue is enabled with size 2
[2026-01-18 00:03:33] [INFO] [0;36m(EngineCore_DP0 pid=281161)[0;0m INFO 01-18 00:03:33 [vllm.py:645] Asynchronous scheduling is enabled.
[2026-01-18 00:03:33] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:33 [api_server.py:1020] Supported tasks: ['generate']
[2026-01-18 00:03:33] [WARNING] [0;36m(APIServer pid=280892)[0;0m WARNING 01-18 00:03:33 [model.py:1356] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[2026-01-18 00:03:33] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:33 [serving_responses.py:224] Using default chat sampling params from model: {'temperature': 1.1, 'top_p': 0.95}
[2026-01-18 00:03:33] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:33 [serving_engine.py:271] "auto" tool choice has been enabled.
[2026-01-18 00:03:33] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:33 [serving_engine.py:271] "auto" tool choice has been enabled.
[2026-01-18 00:03:33] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:33 [serving_chat.py:146] Using default chat sampling params from model: {'temperature': 1.1, 'top_p': 0.95}
[2026-01-18 00:03:33] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:33 [serving_chat.py:182] Warming up chat template processing...
[2026-01-18 00:03:33] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:33 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:34 [serving_chat.py:218] Chat template warmup completed in 56.8ms
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:34 [serving_completion.py:78] Using default completion sampling params from model: {'temperature': 1.1, 'top_p': 0.95}
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:34 [serving_engine.py:271] "auto" tool choice has been enabled.
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:34 [serving_chat.py:146] Using default chat sampling params from model: {'temperature': 1.1, 'top_p': 0.95}
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:34 [api_server.py:1352] Starting vLLM API server 0 on http://0.0.0.0:8005
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:34 [launcher.py:38] Available routes are:
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:34 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:34 [launcher.py:46] Route: /docs, Methods: HEAD, GET
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:34 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:34 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:34 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:34 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:34 [launcher.py:46] Route: /tokenize, Methods: POST
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:34 [launcher.py:46] Route: /detokenize, Methods: POST
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:34 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:34 [launcher.py:46] Route: /pause, Methods: POST
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:34 [launcher.py:46] Route: /resume, Methods: POST
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:34 [launcher.py:46] Route: /is_paused, Methods: GET
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:34 [launcher.py:46] Route: /metrics, Methods: GET
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:34 [launcher.py:46] Route: /health, Methods: GET
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:34 [launcher.py:46] Route: /load, Methods: GET
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:34 [launcher.py:46] Route: /v1/models, Methods: GET
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:34 [launcher.py:46] Route: /version, Methods: GET
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:34 [launcher.py:46] Route: /v1/responses, Methods: POST
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:34 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:34 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:34 [launcher.py:46] Route: /v1/messages, Methods: POST
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:34 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:34 [launcher.py:46] Route: /v1/completions, Methods: POST
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:34 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:34 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:34 [launcher.py:46] Route: /ping, Methods: GET
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:34 [launcher.py:46] Route: /ping, Methods: POST
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:34 [launcher.py:46] Route: /invocations, Methods: POST
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:34 [launcher.py:46] Route: /classify, Methods: POST
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:34 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:34 [launcher.py:46] Route: /score, Methods: POST
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:34 [launcher.py:46] Route: /v1/score, Methods: POST
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:34 [launcher.py:46] Route: /rerank, Methods: POST
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:34 [launcher.py:46] Route: /v1/rerank, Methods: POST
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:34 [launcher.py:46] Route: /v2/rerank, Methods: POST
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:03:34 [launcher.py:46] Route: /pooling, Methods: POST
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO:     Started server process [280892]
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO:     Waiting for application startup.
[2026-01-18 00:03:34] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO:     Application startup complete.
[2026-01-18 00:04:26] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO:     127.0.0.1:55790 - "GET /v1/models HTTP/1.1" 200 OK
[2026-01-18 00:04:35] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO:     127.0.0.1:40780 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-18 00:04:44] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:04:44 [loggers.py:257] Engine 000: Avg prompt throughput: 1.0 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[2026-01-18 00:04:54] [INFO] [0;36m(APIServer pid=280892)[0;0m INFO 01-18 00:04:54 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[2026-01-18 00:04:54] [INFO] [0;36m(Worker_TP1 pid=281462)[0;0m [0;36m(Worker_TP0 pid=281460)[0;0m INFO 01-18 00:04:54 [multiproc_executor.py:707] Parent process exited, terminating worker
[2026-01-18 00:04:54] [INFO] INFO 01-18 00:04:54 [multiproc_executor.py:707] Parent process exited, terminating worker
[2026-01-18 00:04:54] [ERROR] [0;36m(APIServer pid=280892)[0;0m ERROR 01-18 00:04:54 [core_client.py:610] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.
[2026-01-18 00:04:54] [INFO] [0;36m(Worker_TP1 pid=281462)[0;0m INFO 01-18 00:04:54 [multiproc_executor.py:751] WorkerProc shutting down.
[2026-01-18 00:04:54] [INFO] [0;36m(Worker_TP1 pid=281462)[0;0m /mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py:147: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.
[2026-01-18 00:04:54] [INFO] [0;36m(Worker_TP1 pid=281462)[0;0m   warnings.warn('resource_tracker: process died unexpectedly, '
[2026-01-18 00:04:54] [INFO] [0;36m(Worker_TP0 pid=281460)[0;0m INFO 01-18 00:04:54 [multiproc_executor.py:751] WorkerProc shutting down.
[2026-01-18 00:04:54] [INFO] [0;36m(Worker_TP0 pid=281460)[0;0m /mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py:147: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.
[2026-01-18 00:04:54] [INFO] [0;36m(Worker_TP0 pid=281460)[0;0m   warnings.warn('resource_tracker: process died unexpectedly, '
[2026-01-18 00:04:54] [INFO] 服务已停止
[2026-01-18 00:04:54] [INFO] nvitop监控已启动
[2026-01-18 00:04:54] [ERROR] Traceback (most recent call last):
[2026-01-18 00:04:54] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 00:04:54] [INFO] cache[rtype].remove(name)
[2026-01-18 00:04:54] [INFO] KeyError: '/psm_b5e91423'
[2026-01-18 00:04:54] [ERROR] Traceback (most recent call last):
[2026-01-18 00:04:54] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 00:04:54] [INFO] cache[rtype].remove(name)
[2026-01-18 00:04:54] [INFO] KeyError: '/mp-st5ug_2z'
[2026-01-18 00:04:54] [ERROR] Traceback (most recent call last):
[2026-01-18 00:04:54] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 00:04:54] [INFO] cache[rtype].remove(name)
[2026-01-18 00:04:54] [INFO] KeyError: '/psm_c935a537'
[2026-01-18 00:04:54] [ERROR] Traceback (most recent call last):
[2026-01-18 00:04:54] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 00:04:54] [INFO] cache[rtype].remove(name)
[2026-01-18 00:04:54] [INFO] KeyError: '/psm_026e3d53'
[2026-01-18 00:04:54] [ERROR] Traceback (most recent call last):
[2026-01-18 00:04:54] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 00:04:54] [INFO] cache[rtype].remove(name)
[2026-01-18 00:04:54] [INFO] KeyError: '/mp-aethvrkm'
[2026-01-18 00:04:55] [INFO] nvitop监控已启动
[2026-01-18 00:04:57] [INFO] nvitop监控已启动
[2026-01-18 00:07:07] [SUCCESS] 方案已保存: Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated
[2026-01-18 00:07:10] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 00:07:10] [INFO] nvitop监控已启动
[2026-01-18 00:07:10] [INFO] nvitop监控已停止
[2026-01-18 00:07:36] [INFO] usage: vllm serve [model_tag] [options]
[2026-01-18 00:07:36] [INFO] vllm serve: error: argument --speculative-config: Value {method: qwen3_next_mtp, num_speculative_tokens: 2} cannot be converted to <function loads at 0x7aa03800cb80>.
[2026-01-18 00:07:37] [INFO] nvitop监控已启动
[2026-01-18 00:08:29] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 00:08:29] [INFO] nvitop监控已启动
[2026-01-18 00:08:29] [INFO] nvitop监控已停止
[2026-01-18 00:08:53] [ERROR] Traceback (most recent call last):
[2026-01-18 00:08:53] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/bin/vllm", line 7, in <module>
[2026-01-18 00:08:53] [INFO] sys.exit(main())
[2026-01-18 00:08:53] [INFO] ^^^^^^
[2026-01-18 00:08:53] [INFO] File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/main.py", line 70, in main
[2026-01-18 00:08:53] [INFO] cmds[args.subparser].validate(args)
[2026-01-18 00:08:53] [INFO] File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/serve.py", line 63, in validate
[2026-01-18 00:08:53] [INFO] validate_parsed_serve_args(args)
[2026-01-18 00:08:53] [INFO] File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/cli_args.py", line 311, in validate_parsed_serve_args
[2026-01-18 00:08:53] [INFO] raise TypeError("Error: --enable-auto-tool-choice requires --tool-call-parser")
[2026-01-18 00:08:53] [INFO] TypeError: Error: --enable-auto-tool-choice requires --tool-call-parser
[2026-01-18 00:08:55] [INFO] nvitop监控已启动
[2026-01-18 00:10:43] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 00:10:43] [INFO] nvitop监控已启动
[2026-01-18 00:10:43] [INFO] nvitop监控已停止
[2026-01-18 00:11:07] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 00:11:07 [api_server.py:1278] vLLM API server version 0.14.0rc1.dev492+g19504ac07
[2026-01-18 00:11:07] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 00:11:07 [utils.py:254] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/chat_template.jinja', 'enable_auto_tool_choice': True, 'tool_call_parser': 'deepseek_r1', 'model': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/', 'trust_remote_code': True, 'max_model_len': 128000, 'served_model_name': ['VLLM-MODEL'], 'reasoning_parser': 'deepseek_r1', 'tensor_parallel_size': 2, 'kv_cache_dtype': 'fp8', 'max_num_seqs': 256, 'enable_chunked_prefill': False, 'async_scheduling': True, 'compilation_config': {'level': None, 'mode': None, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': [], 'splitting_ops': None, 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': None, 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.PIECEWISE: 1>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': None, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}}
[2026-01-18 00:11:07] [ERROR] [0;36m(APIServer pid=286251)[0;0m Traceback (most recent call last):
[2026-01-18 00:11:07] [INFO] [0;36m(APIServer pid=286251)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/bin/vllm", line 7, in <module>
[2026-01-18 00:11:07] [INFO] [0;36m(APIServer pid=286251)[0;0m     sys.exit(main())
[2026-01-18 00:11:07] [INFO] [0;36m(APIServer pid=286251)[0;0m              ^^^^^^
[2026-01-18 00:11:07] [INFO] [0;36m(APIServer pid=286251)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-18 00:11:07] [INFO] [0;36m(APIServer pid=286251)[0;0m     args.dispatch_function(args)
[2026-01-18 00:11:07] [INFO] [0;36m(APIServer pid=286251)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-18 00:11:07] [INFO] [0;36m(APIServer pid=286251)[0;0m     uvloop.run(run_server(args))
[2026-01-18 00:11:07] [INFO] [0;36m(APIServer pid=286251)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-18 00:11:07] [INFO] [0;36m(APIServer pid=286251)[0;0m     return __asyncio.run(
[2026-01-18 00:11:07] [INFO] [0;36m(APIServer pid=286251)[0;0m            ^^^^^^^^^^^^^^
[2026-01-18 00:11:07] [INFO] [0;36m(APIServer pid=286251)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-18 00:11:07] [INFO] [0;36m(APIServer pid=286251)[0;0m     return runner.run(main)
[2026-01-18 00:11:07] [INFO] [0;36m(APIServer pid=286251)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-18 00:11:07] [INFO] [0;36m(APIServer pid=286251)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-18 00:11:07] [INFO] [0;36m(APIServer pid=286251)[0;0m     return self._loop.run_until_complete(task)
[2026-01-18 00:11:07] [INFO] [0;36m(APIServer pid=286251)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 00:11:07] [INFO] [0;36m(APIServer pid=286251)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-18 00:11:07] [INFO] [0;36m(APIServer pid=286251)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-18 00:11:07] [INFO] [0;36m(APIServer pid=286251)[0;0m     return await main
[2026-01-18 00:11:07] [INFO] [0;36m(APIServer pid=286251)[0;0m            ^^^^^^^^^^
[2026-01-18 00:11:07] [INFO] [0;36m(APIServer pid=286251)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 1324, in run_server
[2026-01-18 00:11:07] [INFO] [0;36m(APIServer pid=286251)[0;0m     listen_address, sock = setup_server(args)
[2026-01-18 00:11:07] [INFO] [0;36m(APIServer pid=286251)[0;0m                            ^^^^^^^^^^^^^^^^^^
[2026-01-18 00:11:07] [INFO] [0;36m(APIServer pid=286251)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 1287, in setup_server
[2026-01-18 00:11:07] [INFO] [0;36m(APIServer pid=286251)[0;0m     validate_api_server_args(args)
[2026-01-18 00:11:07] [INFO] [0;36m(APIServer pid=286251)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 1259, in validate_api_server_args
[2026-01-18 00:11:07] [INFO] [0;36m(APIServer pid=286251)[0;0m     raise KeyError(
[2026-01-18 00:11:07] [INFO] [0;36m(APIServer pid=286251)[0;0m KeyError: 'invalid tool call parser: deepseek_r1 (chose from { deepseek_v3,deepseek_v31,deepseek_v32,ernie45,functiongemma,gigachat3,glm45,glm47,granite,granite-20b-fc,hermes,hunyuan_a13b,internlm,jamba,kimi_k2,llama3_json,llama4_json,llama4_pythonic,longcat,minimax,minimax_m2,mistral,olmo3,openai,phi4_mini_json,pythonic,qwen3_coder,qwen3_xml,seed_oss,step3,xlam })'
[2026-01-18 00:11:08] [INFO] nvitop监控已启动
[2026-01-18 00:12:45] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 00:12:45] [INFO] nvitop监控已启动
[2026-01-18 00:12:45] [INFO] nvitop监控已停止
[2026-01-18 00:13:10] [INFO] [0;36m(APIServer pid=287286)[0;0m INFO 01-18 00:13:10 [api_server.py:1278] vLLM API server version 0.14.0rc1.dev492+g19504ac07
[2026-01-18 00:13:11] [INFO] [0;36m(APIServer pid=287286)[0;0m INFO 01-18 00:13:11 [utils.py:254] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/chat_template.jinja', 'model': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/', 'trust_remote_code': True, 'max_model_len': 128000, 'served_model_name': ['VLLM-MODEL'], 'tensor_parallel_size': 2, 'kv_cache_dtype': 'fp8', 'max_num_seqs': 256, 'enable_chunked_prefill': False, 'async_scheduling': True, 'compilation_config': {'level': None, 'mode': None, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': [], 'splitting_ops': None, 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': None, 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.PIECEWISE: 1>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': None, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}}
[2026-01-18 00:13:11] [INFO] [0;36m(APIServer pid=287286)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 00:13:11] [INFO] [0;36m(APIServer pid=287286)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 00:13:11] [INFO] [0;36m(APIServer pid=287286)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 00:13:11] [INFO] [0;36m(APIServer pid=287286)[0;0m INFO 01-18 00:13:11 [model.py:528] Resolved architecture: Qwen3NextForCausalLM
[2026-01-18 00:13:11] [INFO] [0;36m(APIServer pid=287286)[0;0m INFO 01-18 00:13:11 [model.py:1543] Using max model len 128000
[2026-01-18 00:13:11] [WARNING] [0;36m(APIServer pid=287286)[0;0m WARNING 01-18 00:13:11 [arg_utils.py:1914] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[2026-01-18 00:13:11] [INFO] [0;36m(APIServer pid=287286)[0;0m INFO 01-18 00:13:11 [cache.py:206] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor.
[2026-01-18 00:13:12] [INFO] [0;36m(APIServer pid=287286)[0;0m INFO 01-18 00:13:12 [config.py:476] Setting attention block size to 1072 tokens to ensure that attention page size is >= mamba page size.
[2026-01-18 00:13:12] [INFO] [0;36m(APIServer pid=287286)[0;0m INFO 01-18 00:13:12 [vllm.py:640] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-18 00:13:12] [INFO] [0;36m(APIServer pid=287286)[0;0m INFO 01-18 00:13:12 [vllm.py:645] Asynchronous scheduling is enabled.
[2026-01-18 00:13:35] [INFO] [0;36m(EngineCore_DP0 pid=287633)[0;0m INFO 01-18 00:13:35 [core.py:97] Initializing a V1 LLM engine (v0.14.0rc1.dev492+g19504ac07) with config: model='/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/', speculative_config=None, tokenizer='/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=fp8, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=False, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [128000], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.PIECEWISE: 1>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-18 00:13:35] [WARNING] [0;36m(EngineCore_DP0 pid=287633)[0;0m WARNING 01-18 00:13:35 [multiproc_executor.py:880] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-18 00:14:00] [INFO] INFO 01-18 00:14:00 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:47419 backend=nccl
[2026-01-18 00:14:00] [INFO] INFO 01-18 00:14:00 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:47419 backend=nccl
[2026-01-18 00:14:01] [INFO] INFO 01-18 00:14:01 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-18 00:14:02] [WARNING] WARNING 01-18 00:14:02 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 00:14:02] [WARNING] WARNING 01-18 00:14:02 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 00:14:02] [INFO] INFO 01-18 00:14:02 [parallel_state.py:1425] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[2026-01-18 00:14:02] [INFO] INFO 01-18 00:14:02 [parallel_state.py:1425] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
[2026-01-18 00:14:03] [INFO] [0;36m(Worker_TP0 pid=287835)[0;0m INFO 01-18 00:14:03 [gpu_model_runner.py:3789] Starting to load model /mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/...
[2026-01-18 00:14:08] [INFO] [0;36m(Worker_TP0 pid=287835)[0;0m INFO 01-18 00:14:08 [cuda.py:351] Using FLASHINFER attention backend out of potential backends: ('FLASHINFER', 'TRITON_ATTN')
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749] WorkerProc failed to start.
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749] Traceback (most recent call last):
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 720, in worker_main
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]     worker = WorkerProc(*args, **kwargs)
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 560, in __init__
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]     self.worker.load_model()
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/worker/gpu_worker.py", line 274, in load_model
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/worker/gpu_model_runner.py", line 3808, in load_model
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]     self.model = model_loader.load_model(
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/model_loader/base_loader.py", line 58, in load_model
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]     self.load_weights(model, model_config)
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/model_loader/default_loader.py", line 288, in load_weights
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/qwen3_next.py", line 1287, in load_weights
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]     return loader.load_weights(weights)
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/utils.py", line 335, in load_weights
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]     autoloaded_weights = set(self._load_module("", self.module, weights))
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/utils.py", line 279, in _load_module
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]     for child_prefix, child_weights in self._groupby_prefix(weights):
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/utils.py", line 163, in _groupby_prefix
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]     for prefix, group in itertools.groupby(weights_by_parts, key=lambda x: x[0][0]):
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/utils.py", line 160, in <genexpr>
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]     for weight_name, weight_data in weights
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]                                     ^^^^^^^
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/utils.py", line 332, in <genexpr>
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]     (name, weight) for name, weight in weights if not self._can_skip(name)
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]                                        ^^^^^^^
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/model_loader/default_loader.py", line 260, in get_all_weights
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]     yield from self._get_weights_iterator(primary_weights)
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/model_loader/default_loader.py", line 189, in _get_weights_iterator
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]     hf_folder, hf_weights_files, use_safetensors = self._prepare_weights(
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]                                                    ^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/model_loader/default_loader.py", line 171, in _prepare_weights
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]     hf_weights_files = filter_duplicate_safetensors_files(
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/model_loader/weight_utils.py", line 570, in filter_duplicate_safetensors_files
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]     weight_map = json.load(f)["weight_map"]
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]                  ^^^^^^^^^^^^
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/json/__init__.py", line 293, in load
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]     return loads(fp.read(),
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]            ^^^^^^^^^^^^^^^^
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/json/__init__.py", line 346, in loads
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]     return _default_decoder.decode(s)
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]            ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/json/decoder.py", line 338, in decode
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]     obj, end = self.raw_decode(s, idx=_w(s, 0).end())
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/json/decoder.py", line 354, in raw_decode
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]     obj, end = self.scan_once(s, idx)
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749]                ^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 00:14:08] [ERROR] [0;36m(Worker_TP0 pid=287835)[0;0m ERROR 01-18 00:14:08 [multiproc_executor.py:749] json.decoder.JSONDecodeError: Unterminated string starting at: line 41299 column 59 (char 3919835)
[2026-01-18 00:14:08] [INFO] [0;36m(Worker_TP0 pid=287835)[0;0m INFO 01-18 00:14:08 [multiproc_executor.py:707] Parent process exited, terminating worker
[2026-01-18 00:14:09] [INFO] [0;36m(Worker_TP1 pid=287839)[0;0m INFO 01-18 00:14:09 [multiproc_executor.py:707] Parent process exited, terminating worker
[2026-01-18 00:14:09] [WARNING] [rank0]:[W118 00:14:09.017364969 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2026-01-18 00:14:11] [ERROR] [0;36m(EngineCore_DP0 pid=287633)[0;0m ERROR 01-18 00:14:11 [core.py:936] EngineCore failed to start.
[2026-01-18 00:14:11] [ERROR] [0;36m(EngineCore_DP0 pid=287633)[0;0m ERROR 01-18 00:14:11 [core.py:936] Traceback (most recent call last):
[2026-01-18 00:14:11] [ERROR] [0;36m(EngineCore_DP0 pid=287633)[0;0m ERROR 01-18 00:14:11 [core.py:936]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core.py", line 927, in run_engine_core
[2026-01-18 00:14:11] [ERROR] [0;36m(EngineCore_DP0 pid=287633)[0;0m ERROR 01-18 00:14:11 [core.py:936]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 00:14:11] [ERROR] [0;36m(EngineCore_DP0 pid=287633)[0;0m ERROR 01-18 00:14:11 [core.py:936]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 00:14:11] [ERROR] [0;36m(EngineCore_DP0 pid=287633)[0;0m ERROR 01-18 00:14:11 [core.py:936]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core.py", line 692, in __init__
[2026-01-18 00:14:11] [ERROR] [0;36m(EngineCore_DP0 pid=287633)[0;0m ERROR 01-18 00:14:11 [core.py:936]     super().__init__(
[2026-01-18 00:14:11] [ERROR] [0;36m(EngineCore_DP0 pid=287633)[0;0m ERROR 01-18 00:14:11 [core.py:936]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core.py", line 106, in __init__
[2026-01-18 00:14:11] [ERROR] [0;36m(EngineCore_DP0 pid=287633)[0;0m ERROR 01-18 00:14:11 [core.py:936]     self.model_executor = executor_class(vllm_config)
[2026-01-18 00:14:11] [ERROR] [0;36m(EngineCore_DP0 pid=287633)[0;0m ERROR 01-18 00:14:11 [core.py:936]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 00:14:11] [ERROR] [0;36m(EngineCore_DP0 pid=287633)[0;0m ERROR 01-18 00:14:11 [core.py:936]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[2026-01-18 00:14:11] [ERROR] [0;36m(EngineCore_DP0 pid=287633)[0;0m ERROR 01-18 00:14:11 [core.py:936]     super().__init__(vllm_config)
[2026-01-18 00:14:11] [ERROR] [0;36m(EngineCore_DP0 pid=287633)[0;0m ERROR 01-18 00:14:11 [core.py:936]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/abstract.py", line 101, in __init__
[2026-01-18 00:14:11] [ERROR] [0;36m(EngineCore_DP0 pid=287633)[0;0m ERROR 01-18 00:14:11 [core.py:936]     self._init_executor()
[2026-01-18 00:14:11] [ERROR] [0;36m(EngineCore_DP0 pid=287633)[0;0m ERROR 01-18 00:14:11 [core.py:936]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 172, in _init_executor
[2026-01-18 00:14:11] [ERROR] [0;36m(EngineCore_DP0 pid=287633)[0;0m ERROR 01-18 00:14:11 [core.py:936]     self.workers = WorkerProc.wait_for_ready(unready_workers)
[2026-01-18 00:14:11] [ERROR] [0;36m(EngineCore_DP0 pid=287633)[0;0m ERROR 01-18 00:14:11 [core.py:936]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 00:14:11] [ERROR] [0;36m(EngineCore_DP0 pid=287633)[0;0m ERROR 01-18 00:14:11 [core.py:936]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 658, in wait_for_ready
[2026-01-18 00:14:11] [ERROR] [0;36m(EngineCore_DP0 pid=287633)[0;0m ERROR 01-18 00:14:11 [core.py:936]     raise e from None
[2026-01-18 00:14:11] [ERROR] [0;36m(EngineCore_DP0 pid=287633)[0;0m ERROR 01-18 00:14:11 [core.py:936] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[2026-01-18 00:14:11] [INFO] [0;36m(EngineCore_DP0 pid=287633)[0;0m Process EngineCore_DP0:
[2026-01-18 00:14:11] [ERROR] [0;36m(EngineCore_DP0 pid=287633)[0;0m Traceback (most recent call last):
[2026-01-18 00:14:11] [INFO] [0;36m(EngineCore_DP0 pid=287633)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[2026-01-18 00:14:11] [INFO] [0;36m(EngineCore_DP0 pid=287633)[0;0m     self.run()
[2026-01-18 00:14:11] [INFO] [0;36m(EngineCore_DP0 pid=287633)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/process.py", line 108, in run
[2026-01-18 00:14:11] [INFO] [0;36m(EngineCore_DP0 pid=287633)[0;0m     self._target(*self._args, **self._kwargs)
[2026-01-18 00:14:11] [INFO] [0;36m(EngineCore_DP0 pid=287633)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core.py", line 940, in run_engine_core
[2026-01-18 00:14:11] [INFO] [0;36m(EngineCore_DP0 pid=287633)[0;0m     raise e
[2026-01-18 00:14:11] [INFO] [0;36m(EngineCore_DP0 pid=287633)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core.py", line 927, in run_engine_core
[2026-01-18 00:14:11] [INFO] [0;36m(EngineCore_DP0 pid=287633)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 00:14:11] [INFO] [0;36m(EngineCore_DP0 pid=287633)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 00:14:11] [INFO] [0;36m(EngineCore_DP0 pid=287633)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core.py", line 692, in __init__
[2026-01-18 00:14:11] [INFO] [0;36m(EngineCore_DP0 pid=287633)[0;0m     super().__init__(
[2026-01-18 00:14:11] [INFO] [0;36m(EngineCore_DP0 pid=287633)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core.py", line 106, in __init__
[2026-01-18 00:14:11] [INFO] [0;36m(EngineCore_DP0 pid=287633)[0;0m     self.model_executor = executor_class(vllm_config)
[2026-01-18 00:14:11] [INFO] [0;36m(EngineCore_DP0 pid=287633)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 00:14:11] [INFO] [0;36m(EngineCore_DP0 pid=287633)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[2026-01-18 00:14:11] [INFO] [0;36m(EngineCore_DP0 pid=287633)[0;0m     super().__init__(vllm_config)
[2026-01-18 00:14:11] [INFO] [0;36m(EngineCore_DP0 pid=287633)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/abstract.py", line 101, in __init__
[2026-01-18 00:14:11] [INFO] [0;36m(EngineCore_DP0 pid=287633)[0;0m     self._init_executor()
[2026-01-18 00:14:11] [INFO] [0;36m(EngineCore_DP0 pid=287633)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 172, in _init_executor
[2026-01-18 00:14:11] [INFO] [0;36m(EngineCore_DP0 pid=287633)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
[2026-01-18 00:14:11] [INFO] [0;36m(EngineCore_DP0 pid=287633)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 00:14:11] [INFO] [0;36m(EngineCore_DP0 pid=287633)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 658, in wait_for_ready
[2026-01-18 00:14:11] [INFO] [0;36m(EngineCore_DP0 pid=287633)[0;0m     raise e from None
[2026-01-18 00:14:11] [INFO] [0;36m(EngineCore_DP0 pid=287633)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[2026-01-18 00:14:12] [ERROR] [0;36m(APIServer pid=287286)[0;0m Traceback (most recent call last):
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/bin/vllm", line 7, in <module>
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m     sys.exit(main())
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m              ^^^^^^
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m     args.dispatch_function(args)
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m     uvloop.run(run_server(args))
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m     return __asyncio.run(
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m            ^^^^^^^^^^^^^^
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m     return runner.run(main)
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m     return self._loop.run_until_complete(task)
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m     return await main
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m            ^^^^^^^^^^
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 1325, in run_server
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 1344, in run_server_worker
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m     async with build_async_engine_client(
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m     return await anext(self.gen)
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 173, in build_async_engine_client
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m     return await anext(self.gen)
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 214, in build_async_engine_client_from_engine_args
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/async_llm.py", line 205, in from_vllm_config
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m     return cls(
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m            ^^^^
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/async_llm.py", line 132, in __init__
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m     return AsyncMPClient(*client_args)
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core_client.py", line 824, in __init__
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m     super().__init__(
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core_client.py", line 479, in __init__
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/contextlib.py", line 144, in __exit__
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m     next(self.gen)
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/utils.py", line 921, in launch_core_engines
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m     wait_for_engine_startup(
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/utils.py", line 980, in wait_for_engine_startup
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m     raise RuntimeError(
[2026-01-18 00:14:12] [INFO] [0;36m(APIServer pid=287286)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[2026-01-18 00:14:13] [INFO] /mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py:279: UserWarning: resource_tracker: There appear to be 3 leaked shared_memory objects to clean up at shutdown
[2026-01-18 00:14:13] [INFO] warnings.warn('resource_tracker: There appear to be %d '
[2026-01-18 00:14:13] [INFO] nvitop监控已启动
[2026-01-18 00:15:32] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 00:15:32] [INFO] nvitop监控已启动
[2026-01-18 00:15:32] [INFO] nvitop监控已停止
[2026-01-18 00:15:56] [INFO] [0;36m(APIServer pid=288636)[0;0m INFO 01-18 00:15:56 [api_server.py:1278] vLLM API server version 0.14.0rc1.dev492+g19504ac07
[2026-01-18 00:15:56] [INFO] [0;36m(APIServer pid=288636)[0;0m INFO 01-18 00:15:56 [utils.py:254] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/chat_template.jinja', 'enable_auto_tool_choice': True, 'tool_call_parser': 'qwen3_next', 'model': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/', 'trust_remote_code': True, 'max_model_len': 128000, 'served_model_name': ['VLLM-MODEL'], 'reasoning_parser': 'qwen3_next', 'tensor_parallel_size': 2, 'kv_cache_dtype': 'fp8', 'max_num_seqs': 256, 'enable_chunked_prefill': False, 'async_scheduling': True, 'compilation_config': {'level': None, 'mode': None, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': [], 'splitting_ops': None, 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': None, 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.PIECEWISE: 1>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': None, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}}
[2026-01-18 00:15:56] [ERROR] [0;36m(APIServer pid=288636)[0;0m Traceback (most recent call last):
[2026-01-18 00:15:56] [INFO] [0;36m(APIServer pid=288636)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/bin/vllm", line 7, in <module>
[2026-01-18 00:15:56] [INFO] [0;36m(APIServer pid=288636)[0;0m     sys.exit(main())
[2026-01-18 00:15:56] [INFO] [0;36m(APIServer pid=288636)[0;0m              ^^^^^^
[2026-01-18 00:15:56] [INFO] [0;36m(APIServer pid=288636)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-18 00:15:56] [INFO] [0;36m(APIServer pid=288636)[0;0m     args.dispatch_function(args)
[2026-01-18 00:15:56] [INFO] [0;36m(APIServer pid=288636)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-18 00:15:56] [INFO] [0;36m(APIServer pid=288636)[0;0m     uvloop.run(run_server(args))
[2026-01-18 00:15:56] [INFO] [0;36m(APIServer pid=288636)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-18 00:15:56] [INFO] [0;36m(APIServer pid=288636)[0;0m     return __asyncio.run(
[2026-01-18 00:15:56] [INFO] [0;36m(APIServer pid=288636)[0;0m            ^^^^^^^^^^^^^^
[2026-01-18 00:15:56] [INFO] [0;36m(APIServer pid=288636)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-18 00:15:56] [INFO] [0;36m(APIServer pid=288636)[0;0m     return runner.run(main)
[2026-01-18 00:15:56] [INFO] [0;36m(APIServer pid=288636)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-18 00:15:56] [INFO] [0;36m(APIServer pid=288636)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-18 00:15:56] [INFO] [0;36m(APIServer pid=288636)[0;0m     return self._loop.run_until_complete(task)
[2026-01-18 00:15:56] [INFO] [0;36m(APIServer pid=288636)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 00:15:56] [INFO] [0;36m(APIServer pid=288636)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-18 00:15:56] [INFO] [0;36m(APIServer pid=288636)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-18 00:15:56] [INFO] [0;36m(APIServer pid=288636)[0;0m     return await main
[2026-01-18 00:15:56] [INFO] [0;36m(APIServer pid=288636)[0;0m            ^^^^^^^^^^
[2026-01-18 00:15:56] [INFO] [0;36m(APIServer pid=288636)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 1324, in run_server
[2026-01-18 00:15:56] [INFO] [0;36m(APIServer pid=288636)[0;0m     listen_address, sock = setup_server(args)
[2026-01-18 00:15:56] [INFO] [0;36m(APIServer pid=288636)[0;0m                            ^^^^^^^^^^^^^^^^^^
[2026-01-18 00:15:56] [INFO] [0;36m(APIServer pid=288636)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 1287, in setup_server
[2026-01-18 00:15:56] [INFO] [0;36m(APIServer pid=288636)[0;0m     validate_api_server_args(args)
[2026-01-18 00:15:56] [INFO] [0;36m(APIServer pid=288636)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 1259, in validate_api_server_args
[2026-01-18 00:15:56] [INFO] [0;36m(APIServer pid=288636)[0;0m     raise KeyError(
[2026-01-18 00:15:56] [INFO] [0;36m(APIServer pid=288636)[0;0m KeyError: 'invalid tool call parser: qwen3_next (chose from { deepseek_v3,deepseek_v31,deepseek_v32,ernie45,functiongemma,gigachat3,glm45,glm47,granite,granite-20b-fc,hermes,hunyuan_a13b,internlm,jamba,kimi_k2,llama3_json,llama4_json,llama4_pythonic,longcat,minimax,minimax_m2,mistral,olmo3,openai,phi4_mini_json,pythonic,qwen3_coder,qwen3_xml,seed_oss,step3,xlam })'
[2026-01-18 00:15:58] [INFO] nvitop监控已启动
[2026-01-18 00:16:32] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 00:16:32] [INFO] nvitop监控已启动
[2026-01-18 00:16:32] [INFO] nvitop监控已停止
[2026-01-18 00:16:34] [INFO] 
[2026-01-18 00:16:34] [INFO] EnvironmentNameNotFound: Could not find conda environment: vllm-tool
[2026-01-18 00:16:34] [INFO] You can list all discoverable environments with `conda info --envs`.
[2026-01-18 00:16:34] [INFO] 
[2026-01-18 00:16:34] [INFO] 
[2026-01-18 00:16:34] [INFO] nvitop监控已启动
[2026-01-18 00:16:55] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 00:16:55] [INFO] nvitop监控已启动
[2026-01-18 00:16:55] [INFO] nvitop监控已停止
[2026-01-18 00:16:57] [INFO] /bin/bash: 行 1: vllm: 未找到命令
[2026-01-18 00:16:57] [INFO] nvitop监控已启动
[2026-01-18 00:31:26] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 00:31:26] [INFO] nvitop监控已启动
[2026-01-18 00:31:26] [INFO] nvitop监控已停止
[2026-01-18 00:31:28] [INFO] /bin/bash: 行 1: vllm: 未找到命令
[2026-01-18 00:31:28] [INFO] nvitop监控已启动
[2026-01-18 00:32:39] [INFO] VLLM GUI 服务器启动，端口: 5003
[2026-01-18 00:32:39] [INFO] nvitop监控已启动
[2026-01-18 00:32:40] [INFO] nvitop监控已启动
[2026-01-18 00:32:41] [INFO] nvitop监控已启动
[2026-01-18 00:32:41] [INFO] 从conda info --base自动检测到conda路径: /mnt/AI-Acer4T/miniconda3
[2026-01-18 00:33:15] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 00:33:15] [INFO] nvitop监控已启动
[2026-01-18 00:33:15] [INFO] /bin/bash: -c: 行 1: 寻找匹配的 `'' 时遇到了未预期的 EOF
[2026-01-18 00:33:15] [INFO] nvitop监控已停止
[2026-01-18 00:33:15] [INFO] nvitop监控已启动
[2026-01-18 00:34:32] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 00:34:32] [INFO] nvitop监控已启动
[2026-01-18 00:34:32] [INFO] nvitop监控已停止
[2026-01-18 00:34:34] [INFO] /bin/bash: 行 1: vllm: 未找到命令
[2026-01-18 00:34:34] [INFO] nvitop监控已启动
[2026-01-18 00:52:24] [SUCCESS] 方案已保存: gpt-oss-120b
[2026-01-18 00:53:17] [SUCCESS] 方案已保存: gpt-oss-120b-Derestricted-MXFP4
[2026-01-18 00:53:28] [SUCCESS] 方案已保存: Seed-OSS-36B-Instruct-TP2
[2026-01-18 00:53:38] [SUCCESS] 方案已保存: Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated-TP2
[2026-01-18 00:53:43] [SUCCESS] 方案已保存: gpt-oss-120b-TP2
[2026-01-18 00:53:49] [SUCCESS] 方案已保存: gpt-oss-120b-Derestricted-MXFP4-TP2
[2026-01-18 00:55:25] [SUCCESS] 方案已保存: GLM-4.6V-FP8-TP2
[2026-01-18 00:57:36] [SUCCESS] 方案已保存: GLM-4.7-REAP-40-W4A16-TP2
[2026-01-18 00:59:13] [SUCCESS] 方案已保存: gemma-3-27b-abliterated
[2026-01-18 01:00:19] [SUCCESS] 方案已保存: Qwen3-Next-80B-A3B-Thinking-FP8-TP2
[2026-01-18 01:02:04] [SUCCESS] 方案已保存: Qwen3-235B-A22B-Thinking-2507-AWQ
[2026-01-18 01:04:31] [SUCCESS] 方案已保存: Qwen3-30B-A3B-Instruct-2507-FP8
[2026-01-18 01:04:48] [SUCCESS] 方案已保存: gemma-3-27b-abliterated-TP2
[2026-01-18 01:05:01] [SUCCESS] 方案已保存: Qwen3-235B-A22B-Thinking-2507-AWQ-TP2
[2026-01-18 01:05:12] [SUCCESS] 方案已保存: Qwen3-30B-A3B-Instruct-2507-FP8
[2026-01-18 01:05:31] [SUCCESS] 方案已保存: Qwen3-30B-A3B-Instruct-2507-FP8-GPU0
[2026-01-18 01:06:44] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 01:06:44] [INFO] nvitop监控已启动
[2026-01-18 01:06:44] [INFO] nvitop监控已停止
[2026-01-18 01:07:11] [INFO] [0;36m(APIServer pid=306388)[0;0m INFO 01-18 01:07:11 [api_server.py:872] vLLM API server version 0.14.0rc2.dev117+g9e078d058
[2026-01-18 01:07:11] [INFO] [0;36m(APIServer pid=306388)[0;0m INFO 01-18 01:07:11 [utils.py:267] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/chat_template.jinja', 'enable_auto_tool_choice': True, 'tool_call_parser': 'deepseek-v3', 'model': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/', 'trust_remote_code': True, 'max_model_len': 128000, 'served_model_name': ['VLLM-MODEL'], 'reasoning_parser': 'deepseek-v3', 'tensor_parallel_size': 2, 'kv_cache_dtype': 'fp8', 'max_num_seqs': 256, 'enable_chunked_prefill': False, 'async_scheduling': True, 'compilation_config': {'level': None, 'mode': None, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': [], 'splitting_ops': None, 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': None, 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.PIECEWISE: 1>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': None, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}}
[2026-01-18 01:07:11] [ERROR] [0;36m(APIServer pid=306388)[0;0m Traceback (most recent call last):
[2026-01-18 01:07:11] [INFO] [0;36m(APIServer pid=306388)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/bin/vllm", line 7, in <module>
[2026-01-18 01:07:11] [INFO] [0;36m(APIServer pid=306388)[0;0m     sys.exit(main())
[2026-01-18 01:07:11] [INFO] [0;36m(APIServer pid=306388)[0;0m              ^^^^^^
[2026-01-18 01:07:11] [INFO] [0;36m(APIServer pid=306388)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-18 01:07:11] [INFO] [0;36m(APIServer pid=306388)[0;0m     args.dispatch_function(args)
[2026-01-18 01:07:11] [INFO] [0;36m(APIServer pid=306388)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-18 01:07:11] [INFO] [0;36m(APIServer pid=306388)[0;0m     uvloop.run(run_server(args))
[2026-01-18 01:07:11] [INFO] [0;36m(APIServer pid=306388)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-18 01:07:11] [INFO] [0;36m(APIServer pid=306388)[0;0m     return __asyncio.run(
[2026-01-18 01:07:11] [INFO] [0;36m(APIServer pid=306388)[0;0m            ^^^^^^^^^^^^^^
[2026-01-18 01:07:11] [INFO] [0;36m(APIServer pid=306388)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-18 01:07:11] [INFO] [0;36m(APIServer pid=306388)[0;0m     return runner.run(main)
[2026-01-18 01:07:11] [INFO] [0;36m(APIServer pid=306388)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-18 01:07:11] [INFO] [0;36m(APIServer pid=306388)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-18 01:07:11] [INFO] [0;36m(APIServer pid=306388)[0;0m     return self._loop.run_until_complete(task)
[2026-01-18 01:07:11] [INFO] [0;36m(APIServer pid=306388)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 01:07:11] [INFO] [0;36m(APIServer pid=306388)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-18 01:07:11] [INFO] [0;36m(APIServer pid=306388)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-18 01:07:11] [INFO] [0;36m(APIServer pid=306388)[0;0m     return await main
[2026-01-18 01:07:11] [INFO] [0;36m(APIServer pid=306388)[0;0m            ^^^^^^^^^^
[2026-01-18 01:07:11] [INFO] [0;36m(APIServer pid=306388)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 918, in run_server
[2026-01-18 01:07:11] [INFO] [0;36m(APIServer pid=306388)[0;0m     listen_address, sock = setup_server(args)
[2026-01-18 01:07:11] [INFO] [0;36m(APIServer pid=306388)[0;0m                            ^^^^^^^^^^^^^^^^^^
[2026-01-18 01:07:11] [INFO] [0;36m(APIServer pid=306388)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 881, in setup_server
[2026-01-18 01:07:11] [INFO] [0;36m(APIServer pid=306388)[0;0m     validate_api_server_args(args)
[2026-01-18 01:07:11] [INFO] [0;36m(APIServer pid=306388)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 853, in validate_api_server_args
[2026-01-18 01:07:11] [INFO] [0;36m(APIServer pid=306388)[0;0m     raise KeyError(
[2026-01-18 01:07:11] [INFO] [0;36m(APIServer pid=306388)[0;0m KeyError: 'invalid tool call parser: deepseek-v3 (chose from { deepseek_v3,deepseek_v31,deepseek_v32,ernie45,functiongemma,gigachat3,glm45,glm47,granite,granite-20b-fc,hermes,hunyuan_a13b,internlm,jamba,kimi_k2,llama3_json,llama4_json,llama4_pythonic,longcat,minimax,minimax_m2,mistral,olmo3,openai,phi4_mini_json,pythonic,qwen3_coder,qwen3_xml,seed_oss,step3,xlam })'
[2026-01-18 01:07:12] [INFO] nvitop监控已启动
[2026-01-18 01:07:27] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 01:07:27] [INFO] nvitop监控已启动
[2026-01-18 01:07:27] [INFO] nvitop监控已停止
[2026-01-18 01:07:53] [INFO] [0;36m(APIServer pid=306728)[0;0m INFO 01-18 01:07:53 [api_server.py:872] vLLM API server version 0.14.0rc2.dev117+g9e078d058
[2026-01-18 01:07:53] [INFO] [0;36m(APIServer pid=306728)[0;0m INFO 01-18 01:07:53 [utils.py:267] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/chat_template.jinja', 'enable_auto_tool_choice': True, 'tool_call_parser': 'deepseek_v3', 'model': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/', 'trust_remote_code': True, 'max_model_len': 128000, 'served_model_name': ['VLLM-MODEL'], 'reasoning_parser': 'deepseek_v3', 'tensor_parallel_size': 2, 'kv_cache_dtype': 'fp8', 'max_num_seqs': 256, 'enable_chunked_prefill': False, 'async_scheduling': True, 'compilation_config': {'level': None, 'mode': None, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': [], 'splitting_ops': None, 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': None, 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.PIECEWISE: 1>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': None, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}}
[2026-01-18 01:07:53] [INFO] [0;36m(APIServer pid=306728)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 01:07:53] [INFO] [0;36m(APIServer pid=306728)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 01:07:53] [INFO] [0;36m(APIServer pid=306728)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 01:08:16] [INFO] [0;36m(APIServer pid=306728)[0;0m INFO 01-18 01:08:16 [model.py:530] Resolved architecture: Qwen3NextForCausalLM
[2026-01-18 01:08:16] [INFO] [0;36m(APIServer pid=306728)[0;0m INFO 01-18 01:08:16 [model.py:1547] Using max model len 128000
[2026-01-18 01:08:16] [WARNING] [0;36m(APIServer pid=306728)[0;0m WARNING 01-18 01:08:16 [arg_utils.py:1913] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[2026-01-18 01:08:16] [INFO] [0;36m(APIServer pid=306728)[0;0m INFO 01-18 01:08:16 [cache.py:206] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor.
[2026-01-18 01:08:17] [INFO] [0;36m(APIServer pid=306728)[0;0m INFO 01-18 01:08:17 [config.py:476] Setting attention block size to 1072 tokens to ensure that attention page size is >= mamba page size.
[2026-01-18 01:08:17] [INFO] [0;36m(APIServer pid=306728)[0;0m INFO 01-18 01:08:17 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 01:08:17] [INFO] [0;36m(APIServer pid=306728)[0;0m INFO 01-18 01:08:17 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-18 01:08:42] [INFO] [0;36m(EngineCore_DP0 pid=307175)[0;0m INFO 01-18 01:08:42 [core.py:96] Initializing a V1 LLM engine (v0.14.0rc2.dev117+g9e078d058) with config: model='/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/', speculative_config=None, tokenizer='/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=fp8, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='deepseek_v3', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=False, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [128000], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.PIECEWISE: 1>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-18 01:08:42] [WARNING] [0;36m(EngineCore_DP0 pid=307175)[0;0m WARNING 01-18 01:08:42 [multiproc_executor.py:897] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-18 01:09:07] [INFO] INFO 01-18 01:09:07 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:37551 backend=nccl
[2026-01-18 01:09:08] [INFO] INFO 01-18 01:09:08 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:37551 backend=nccl
[2026-01-18 01:09:08] [INFO] INFO 01-18 01:09:08 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-18 01:09:08] [WARNING] WARNING 01-18 01:09:08 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 01:09:08] [WARNING] WARNING 01-18 01:09:08 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 01:09:09] [INFO] INFO 01-18 01:09:09 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[2026-01-18 01:09:09] [INFO] INFO 01-18 01:09:09 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
[2026-01-18 01:09:10] [INFO] [0;36m(Worker_TP0 pid=307381)[0;0m INFO 01-18 01:09:10 [gpu_model_runner.py:3803] Starting to load model /mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/...
[2026-01-18 01:09:14] [INFO] [0;36m(Worker_TP0 pid=307381)[0;0m INFO 01-18 01:09:14 [unquantized.py:99] Using TRITON backend for Unquantized MoE
[2026-01-18 01:09:14] [INFO] [0;36m(Worker_TP0 pid=307381)[0;0m INFO 01-18 01:09:14 [cuda.py:351] Using FLASHINFER attention backend out of potential backends: ('FLASHINFER', 'TRITON_ATTN')
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766] WorkerProc failed to start.
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766] Traceback (most recent call last):
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 737, in worker_main
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]     worker = WorkerProc(*args, **kwargs)
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 575, in __init__
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]     self.worker.load_model()
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 274, in load_model
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3822, in load_model
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]     self.model = model_loader.load_model(
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 58, in load_model
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]     self.load_weights(model, model_config)
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 288, in load_weights
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_next.py", line 1294, in load_weights
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]     return loader.load_weights(weights)
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 335, in load_weights
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]     autoloaded_weights = set(self._load_module("", self.module, weights))
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 279, in _load_module
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]     for child_prefix, child_weights in self._groupby_prefix(weights):
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 163, in _groupby_prefix
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]     for prefix, group in itertools.groupby(weights_by_parts, key=lambda x: x[0][0]):
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 160, in <genexpr>
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]     for weight_name, weight_data in weights
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]                                     ^^^^^^^
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 332, in <genexpr>
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]     (name, weight) for name, weight in weights if not self._can_skip(name)
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]                                        ^^^^^^^
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 260, in get_all_weights
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]     yield from self._get_weights_iterator(primary_weights)
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 189, in _get_weights_iterator
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]     hf_folder, hf_weights_files, use_safetensors = self._prepare_weights(
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]                                                    ^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 171, in _prepare_weights
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]     hf_weights_files = filter_duplicate_safetensors_files(
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py", line 570, in filter_duplicate_safetensors_files
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]     weight_map = json.load(f)["weight_map"]
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]                  ^^^^^^^^^^^^
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/json/__init__.py", line 293, in load
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]     return loads(fp.read(),
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/json/__init__.py", line 346, in loads
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]     return _default_decoder.decode(s)
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/json/decoder.py", line 338, in decode
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]     obj, end = self.raw_decode(s, idx=_w(s, 0).end())
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/json/decoder.py", line 354, in raw_decode
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]     obj, end = self.scan_once(s, idx)
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766]                ^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 01:09:15] [ERROR] [0;36m(Worker_TP0 pid=307381)[0;0m ERROR 01-18 01:09:15 [multiproc_executor.py:766] json.decoder.JSONDecodeError: Unterminated string starting at: line 41299 column 59 (char 3919835)
[2026-01-18 01:09:15] [INFO] [0;36m(Worker_TP0 pid=307381)[0;0m INFO 01-18 01:09:15 [multiproc_executor.py:724] Parent process exited, terminating worker
[2026-01-18 01:09:15] [INFO] [0;36m(Worker_TP1 pid=307382)[0;0m INFO 01-18 01:09:15 [multiproc_executor.py:724] Parent process exited, terminating worker
[2026-01-18 01:09:16] [WARNING] [rank0]:[W118 01:09:16.797518828 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2026-01-18 01:09:17] [ERROR] [0;36m(EngineCore_DP0 pid=307175)[0;0m ERROR 01-18 01:09:17 [core.py:935] EngineCore failed to start.
[2026-01-18 01:09:17] [ERROR] [0;36m(EngineCore_DP0 pid=307175)[0;0m ERROR 01-18 01:09:17 [core.py:935] Traceback (most recent call last):
[2026-01-18 01:09:17] [ERROR] [0;36m(EngineCore_DP0 pid=307175)[0;0m ERROR 01-18 01:09:17 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-18 01:09:17] [ERROR] [0;36m(EngineCore_DP0 pid=307175)[0;0m ERROR 01-18 01:09:17 [core.py:935]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 01:09:17] [ERROR] [0;36m(EngineCore_DP0 pid=307175)[0;0m ERROR 01-18 01:09:17 [core.py:935]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 01:09:17] [ERROR] [0;36m(EngineCore_DP0 pid=307175)[0;0m ERROR 01-18 01:09:17 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-18 01:09:17] [ERROR] [0;36m(EngineCore_DP0 pid=307175)[0;0m ERROR 01-18 01:09:17 [core.py:935]     super().__init__(
[2026-01-18 01:09:17] [ERROR] [0;36m(EngineCore_DP0 pid=307175)[0;0m ERROR 01-18 01:09:17 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[2026-01-18 01:09:17] [ERROR] [0;36m(EngineCore_DP0 pid=307175)[0;0m ERROR 01-18 01:09:17 [core.py:935]     self.model_executor = executor_class(vllm_config)
[2026-01-18 01:09:17] [ERROR] [0;36m(EngineCore_DP0 pid=307175)[0;0m ERROR 01-18 01:09:17 [core.py:935]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 01:09:17] [ERROR] [0;36m(EngineCore_DP0 pid=307175)[0;0m ERROR 01-18 01:09:17 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[2026-01-18 01:09:17] [ERROR] [0;36m(EngineCore_DP0 pid=307175)[0;0m ERROR 01-18 01:09:17 [core.py:935]     super().__init__(vllm_config)
[2026-01-18 01:09:17] [ERROR] [0;36m(EngineCore_DP0 pid=307175)[0;0m ERROR 01-18 01:09:17 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[2026-01-18 01:09:17] [ERROR] [0;36m(EngineCore_DP0 pid=307175)[0;0m ERROR 01-18 01:09:17 [core.py:935]     self._init_executor()
[2026-01-18 01:09:17] [ERROR] [0;36m(EngineCore_DP0 pid=307175)[0;0m ERROR 01-18 01:09:17 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[2026-01-18 01:09:17] [ERROR] [0;36m(EngineCore_DP0 pid=307175)[0;0m ERROR 01-18 01:09:17 [core.py:935]     self.workers = WorkerProc.wait_for_ready(unready_workers)
[2026-01-18 01:09:17] [ERROR] [0;36m(EngineCore_DP0 pid=307175)[0;0m ERROR 01-18 01:09:17 [core.py:935]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 01:09:17] [ERROR] [0;36m(EngineCore_DP0 pid=307175)[0;0m ERROR 01-18 01:09:17 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 675, in wait_for_ready
[2026-01-18 01:09:17] [ERROR] [0;36m(EngineCore_DP0 pid=307175)[0;0m ERROR 01-18 01:09:17 [core.py:935]     raise e from None
[2026-01-18 01:09:17] [ERROR] [0;36m(EngineCore_DP0 pid=307175)[0;0m ERROR 01-18 01:09:17 [core.py:935] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[2026-01-18 01:09:17] [INFO] [0;36m(EngineCore_DP0 pid=307175)[0;0m Process EngineCore_DP0:
[2026-01-18 01:09:17] [ERROR] [0;36m(EngineCore_DP0 pid=307175)[0;0m Traceback (most recent call last):
[2026-01-18 01:09:17] [INFO] [0;36m(EngineCore_DP0 pid=307175)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[2026-01-18 01:09:17] [INFO] [0;36m(EngineCore_DP0 pid=307175)[0;0m     self.run()
[2026-01-18 01:09:17] [INFO] [0;36m(EngineCore_DP0 pid=307175)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/process.py", line 108, in run
[2026-01-18 01:09:17] [INFO] [0;36m(EngineCore_DP0 pid=307175)[0;0m     self._target(*self._args, **self._kwargs)
[2026-01-18 01:09:17] [INFO] [0;36m(EngineCore_DP0 pid=307175)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 939, in run_engine_core
[2026-01-18 01:09:17] [INFO] [0;36m(EngineCore_DP0 pid=307175)[0;0m     raise e
[2026-01-18 01:09:17] [INFO] [0;36m(EngineCore_DP0 pid=307175)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-18 01:09:17] [INFO] [0;36m(EngineCore_DP0 pid=307175)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 01:09:17] [INFO] [0;36m(EngineCore_DP0 pid=307175)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 01:09:17] [INFO] [0;36m(EngineCore_DP0 pid=307175)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-18 01:09:17] [INFO] [0;36m(EngineCore_DP0 pid=307175)[0;0m     super().__init__(
[2026-01-18 01:09:17] [INFO] [0;36m(EngineCore_DP0 pid=307175)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[2026-01-18 01:09:17] [INFO] [0;36m(EngineCore_DP0 pid=307175)[0;0m     self.model_executor = executor_class(vllm_config)
[2026-01-18 01:09:17] [INFO] [0;36m(EngineCore_DP0 pid=307175)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 01:09:17] [INFO] [0;36m(EngineCore_DP0 pid=307175)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[2026-01-18 01:09:17] [INFO] [0;36m(EngineCore_DP0 pid=307175)[0;0m     super().__init__(vllm_config)
[2026-01-18 01:09:17] [INFO] [0;36m(EngineCore_DP0 pid=307175)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[2026-01-18 01:09:17] [INFO] [0;36m(EngineCore_DP0 pid=307175)[0;0m     self._init_executor()
[2026-01-18 01:09:17] [INFO] [0;36m(EngineCore_DP0 pid=307175)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[2026-01-18 01:09:17] [INFO] [0;36m(EngineCore_DP0 pid=307175)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
[2026-01-18 01:09:17] [INFO] [0;36m(EngineCore_DP0 pid=307175)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 01:09:17] [INFO] [0;36m(EngineCore_DP0 pid=307175)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 675, in wait_for_ready
[2026-01-18 01:09:17] [INFO] [0;36m(EngineCore_DP0 pid=307175)[0;0m     raise e from None
[2026-01-18 01:09:17] [INFO] [0;36m(EngineCore_DP0 pid=307175)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[2026-01-18 01:09:18] [ERROR] [0;36m(APIServer pid=306728)[0;0m Traceback (most recent call last):
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/bin/vllm", line 7, in <module>
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m     sys.exit(main())
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m              ^^^^^^
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m     args.dispatch_function(args)
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m     uvloop.run(run_server(args))
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m     return __asyncio.run(
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m            ^^^^^^^^^^^^^^
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m     return runner.run(main)
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m     return self._loop.run_until_complete(task)
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m     return await main
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m            ^^^^^^^^^^
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m     async with build_async_engine_client(
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m     return await anext(self.gen)
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 146, in build_async_engine_client
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m     return await anext(self.gen)
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 187, in build_async_engine_client_from_engine_args
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 202, in from_vllm_config
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m     return cls(
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m            ^^^^
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 129, in __init__
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m     return AsyncMPClient(*client_args)
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m     super().__init__(
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 144, in __exit__
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m     next(self.gen)
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 918, in launch_core_engines
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m     wait_for_engine_startup(
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 977, in wait_for_engine_startup
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m     raise RuntimeError(
[2026-01-18 01:09:18] [INFO] [0;36m(APIServer pid=306728)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[2026-01-18 01:09:19] [INFO] /mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/resource_tracker.py:279: UserWarning: resource_tracker: There appear to be 3 leaked shared_memory objects to clean up at shutdown
[2026-01-18 01:09:19] [INFO] warnings.warn('resource_tracker: There appear to be %d '
[2026-01-18 01:09:20] [INFO] nvitop监控已启动
[2026-01-18 15:54:42] [INFO] VLLM GUI 服务器启动，端口: 5003
[2026-01-18 15:54:45] [ERROR] nvitop启动异常: [Errno 2] No such file or directory: 'wsl'
[2026-01-18 15:54:45] [ERROR] nvitop启动异常: [Errno 2] No such file or directory: 'wsl'
[2026-01-18 15:54:45] [INFO] nvitop监控已启动
[2026-01-18 15:54:46] [INFO] nvitop监控已启动
[2026-01-18 15:54:46] [SUCCESS] Script saved: /mnt/AI-Acer4T/AI-Chat/vllm-gui启动器/start_vllmsh.sh
[2026-01-18 15:54:47] [INFO] 从conda info --base自动检测到conda路径: /mnt/AI-Acer4T/miniconda3
[2026-01-18 15:54:53] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 15:54:54] [INFO] nvitop监控已启动
[2026-01-18 15:54:54] [INFO] nvitop监控已停止
[2026-01-18 15:55:15] [INFO] 服务已停止
[2026-01-18 15:55:15] [INFO] nvitop监控已启动
[2026-01-18 15:55:16] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 15:55:16] [INFO] nvitop监控已启动
[2026-01-18 15:55:16] [INFO] nvitop监控已停止
[2026-01-18 15:55:16] [INFO] nvitop监控已启动
[2026-01-18 15:55:42] [INFO] [0;36m(APIServer pid=12625)[0;0m INFO 01-18 15:55:42 [api_server.py:872] vLLM API server version 0.14.0rc2.dev117+g9e078d058
[2026-01-18 15:55:42] [INFO] [0;36m(APIServer pid=12625)[0;0m INFO 01-18 15:55:42 [utils.py:267] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/chat_template.jinja', 'model': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/', 'trust_remote_code': True, 'max_model_len': 128000, 'served_model_name': ['VLLM-MODEL'], 'tensor_parallel_size': 2, 'kv_cache_dtype': 'fp8', 'max_num_seqs': 256, 'enable_chunked_prefill': False, 'compilation_config': {'level': None, 'mode': None, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': [], 'splitting_ops': None, 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': None, 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.PIECEWISE: 1>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': None, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}}
[2026-01-18 15:55:42] [INFO] [0;36m(APIServer pid=12625)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 15:55:42] [INFO] [0;36m(APIServer pid=12625)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 15:55:42] [INFO] [0;36m(APIServer pid=12625)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 15:55:42] [INFO] [0;36m(APIServer pid=12625)[0;0m INFO 01-18 15:55:42 [model.py:530] Resolved architecture: Qwen3NextForCausalLM
[2026-01-18 15:55:42] [INFO] [0;36m(APIServer pid=12625)[0;0m INFO 01-18 15:55:42 [model.py:1547] Using max model len 128000
[2026-01-18 15:55:42] [WARNING] [0;36m(APIServer pid=12625)[0;0m WARNING 01-18 15:55:42 [arg_utils.py:1913] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[2026-01-18 15:55:42] [INFO] [0;36m(APIServer pid=12625)[0;0m INFO 01-18 15:55:42 [cache.py:206] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor.
[2026-01-18 15:55:44] [INFO] [0;36m(APIServer pid=12625)[0;0m INFO 01-18 15:55:44 [config.py:476] Setting attention block size to 1072 tokens to ensure that attention page size is >= mamba page size.
[2026-01-18 15:55:44] [INFO] [0;36m(APIServer pid=12625)[0;0m INFO 01-18 15:55:44 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 15:55:44] [INFO] [0;36m(APIServer pid=12625)[0;0m INFO 01-18 15:55:44 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-18 15:56:07] [INFO] [0;36m(EngineCore_DP0 pid=12965)[0;0m INFO 01-18 15:56:07 [core.py:96] Initializing a V1 LLM engine (v0.14.0rc2.dev117+g9e078d058) with config: model='/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/', speculative_config=None, tokenizer='/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=fp8, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=False, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [128000], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.PIECEWISE: 1>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-18 15:56:07] [WARNING] [0;36m(EngineCore_DP0 pid=12965)[0;0m WARNING 01-18 15:56:07 [multiproc_executor.py:897] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-18 15:56:33] [INFO] INFO 01-18 15:56:33 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:52789 backend=nccl
[2026-01-18 15:56:33] [INFO] INFO 01-18 15:56:33 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:52789 backend=nccl
[2026-01-18 15:56:34] [INFO] INFO 01-18 15:56:34 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-18 15:56:34] [WARNING] WARNING 01-18 15:56:34 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 15:56:34] [WARNING] WARNING 01-18 15:56:34 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 15:56:34] [INFO] INFO 01-18 15:56:34 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[2026-01-18 15:56:34] [INFO] INFO 01-18 15:56:34 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
[2026-01-18 15:56:36] [INFO] [0;36m(Worker_TP0 pid=13283)[0;0m INFO 01-18 15:56:36 [gpu_model_runner.py:3803] Starting to load model /mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/...
[2026-01-18 15:56:40] [INFO] [0;36m(Worker_TP0 pid=13283)[0;0m INFO 01-18 15:56:40 [unquantized.py:99] Using TRITON backend for Unquantized MoE
[2026-01-18 15:56:40] [INFO] [0;36m(Worker_TP0 pid=13283)[0;0m INFO 01-18 15:56:40 [cuda.py:351] Using FLASHINFER attention backend out of potential backends: ('FLASHINFER', 'TRITON_ATTN')
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766] WorkerProc failed to start.
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766] Traceback (most recent call last):
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 737, in worker_main
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]     worker = WorkerProc(*args, **kwargs)
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 575, in __init__
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]     self.worker.load_model()
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 274, in load_model
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3822, in load_model
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]     self.model = model_loader.load_model(
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 58, in load_model
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]     self.load_weights(model, model_config)
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 288, in load_weights
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_next.py", line 1294, in load_weights
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]     return loader.load_weights(weights)
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 335, in load_weights
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]     autoloaded_weights = set(self._load_module("", self.module, weights))
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 279, in _load_module
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]     for child_prefix, child_weights in self._groupby_prefix(weights):
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 163, in _groupby_prefix
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]     for prefix, group in itertools.groupby(weights_by_parts, key=lambda x: x[0][0]):
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 160, in <genexpr>
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]     for weight_name, weight_data in weights
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]                                     ^^^^^^^
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 332, in <genexpr>
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]     (name, weight) for name, weight in weights if not self._can_skip(name)
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]                                        ^^^^^^^
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 260, in get_all_weights
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]     yield from self._get_weights_iterator(primary_weights)
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 189, in _get_weights_iterator
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]     hf_folder, hf_weights_files, use_safetensors = self._prepare_weights(
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]                                                    ^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 171, in _prepare_weights
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]     hf_weights_files = filter_duplicate_safetensors_files(
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py", line 570, in filter_duplicate_safetensors_files
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]     weight_map = json.load(f)["weight_map"]
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]                  ^^^^^^^^^^^^
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/json/__init__.py", line 293, in load
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]     return loads(fp.read(),
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/json/__init__.py", line 346, in loads
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]     return _default_decoder.decode(s)
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/json/decoder.py", line 338, in decode
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]     obj, end = self.raw_decode(s, idx=_w(s, 0).end())
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/json/decoder.py", line 354, in raw_decode
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]     obj, end = self.scan_once(s, idx)
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766]                ^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 15:56:40] [ERROR] [0;36m(Worker_TP1 pid=13284)[0;0m ERROR 01-18 15:56:40 [multiproc_executor.py:766] json.decoder.JSONDecodeError: Unterminated string starting at: line 41299 column 59 (char 3919835)
[2026-01-18 15:56:40] [INFO] [0;36m(Worker_TP1 pid=13284)[0;0m INFO 01-18 15:56:40 [multiproc_executor.py:724] Parent process exited, terminating worker
[2026-01-18 15:56:40] [INFO] [0;36m(Worker_TP0 pid=13283)[0;0m INFO 01-18 15:56:40 [multiproc_executor.py:724] Parent process exited, terminating worker
[2026-01-18 15:56:41] [WARNING] [rank0]:[W118 15:56:41.812749852 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2026-01-18 15:56:43] [ERROR] [0;36m(EngineCore_DP0 pid=12965)[0;0m ERROR 01-18 15:56:43 [core.py:935] EngineCore failed to start.
[2026-01-18 15:56:43] [ERROR] [0;36m(EngineCore_DP0 pid=12965)[0;0m ERROR 01-18 15:56:43 [core.py:935] Traceback (most recent call last):
[2026-01-18 15:56:43] [ERROR] [0;36m(EngineCore_DP0 pid=12965)[0;0m ERROR 01-18 15:56:43 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-18 15:56:43] [ERROR] [0;36m(EngineCore_DP0 pid=12965)[0;0m ERROR 01-18 15:56:43 [core.py:935]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 15:56:43] [ERROR] [0;36m(EngineCore_DP0 pid=12965)[0;0m ERROR 01-18 15:56:43 [core.py:935]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 15:56:43] [ERROR] [0;36m(EngineCore_DP0 pid=12965)[0;0m ERROR 01-18 15:56:43 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-18 15:56:43] [ERROR] [0;36m(EngineCore_DP0 pid=12965)[0;0m ERROR 01-18 15:56:43 [core.py:935]     super().__init__(
[2026-01-18 15:56:43] [ERROR] [0;36m(EngineCore_DP0 pid=12965)[0;0m ERROR 01-18 15:56:43 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[2026-01-18 15:56:43] [ERROR] [0;36m(EngineCore_DP0 pid=12965)[0;0m ERROR 01-18 15:56:43 [core.py:935]     self.model_executor = executor_class(vllm_config)
[2026-01-18 15:56:43] [ERROR] [0;36m(EngineCore_DP0 pid=12965)[0;0m ERROR 01-18 15:56:43 [core.py:935]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 15:56:43] [ERROR] [0;36m(EngineCore_DP0 pid=12965)[0;0m ERROR 01-18 15:56:43 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[2026-01-18 15:56:43] [ERROR] [0;36m(EngineCore_DP0 pid=12965)[0;0m ERROR 01-18 15:56:43 [core.py:935]     super().__init__(vllm_config)
[2026-01-18 15:56:43] [ERROR] [0;36m(EngineCore_DP0 pid=12965)[0;0m ERROR 01-18 15:56:43 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[2026-01-18 15:56:43] [ERROR] [0;36m(EngineCore_DP0 pid=12965)[0;0m ERROR 01-18 15:56:43 [core.py:935]     self._init_executor()
[2026-01-18 15:56:43] [ERROR] [0;36m(EngineCore_DP0 pid=12965)[0;0m ERROR 01-18 15:56:43 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[2026-01-18 15:56:43] [ERROR] [0;36m(EngineCore_DP0 pid=12965)[0;0m ERROR 01-18 15:56:43 [core.py:935]     self.workers = WorkerProc.wait_for_ready(unready_workers)
[2026-01-18 15:56:43] [ERROR] [0;36m(EngineCore_DP0 pid=12965)[0;0m ERROR 01-18 15:56:43 [core.py:935]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 15:56:43] [ERROR] [0;36m(EngineCore_DP0 pid=12965)[0;0m ERROR 01-18 15:56:43 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 675, in wait_for_ready
[2026-01-18 15:56:43] [ERROR] [0;36m(EngineCore_DP0 pid=12965)[0;0m ERROR 01-18 15:56:43 [core.py:935]     raise e from None
[2026-01-18 15:56:43] [ERROR] [0;36m(EngineCore_DP0 pid=12965)[0;0m ERROR 01-18 15:56:43 [core.py:935] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[2026-01-18 15:56:43] [INFO] [0;36m(EngineCore_DP0 pid=12965)[0;0m Process EngineCore_DP0:
[2026-01-18 15:56:43] [ERROR] [0;36m(EngineCore_DP0 pid=12965)[0;0m Traceback (most recent call last):
[2026-01-18 15:56:43] [INFO] [0;36m(EngineCore_DP0 pid=12965)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[2026-01-18 15:56:43] [INFO] [0;36m(EngineCore_DP0 pid=12965)[0;0m     self.run()
[2026-01-18 15:56:43] [INFO] [0;36m(EngineCore_DP0 pid=12965)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/process.py", line 108, in run
[2026-01-18 15:56:43] [INFO] [0;36m(EngineCore_DP0 pid=12965)[0;0m     self._target(*self._args, **self._kwargs)
[2026-01-18 15:56:43] [INFO] [0;36m(EngineCore_DP0 pid=12965)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 939, in run_engine_core
[2026-01-18 15:56:43] [INFO] [0;36m(EngineCore_DP0 pid=12965)[0;0m     raise e
[2026-01-18 15:56:43] [INFO] [0;36m(EngineCore_DP0 pid=12965)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-18 15:56:43] [INFO] [0;36m(EngineCore_DP0 pid=12965)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 15:56:43] [INFO] [0;36m(EngineCore_DP0 pid=12965)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 15:56:43] [INFO] [0;36m(EngineCore_DP0 pid=12965)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-18 15:56:43] [INFO] [0;36m(EngineCore_DP0 pid=12965)[0;0m     super().__init__(
[2026-01-18 15:56:43] [INFO] [0;36m(EngineCore_DP0 pid=12965)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[2026-01-18 15:56:43] [INFO] [0;36m(EngineCore_DP0 pid=12965)[0;0m     self.model_executor = executor_class(vllm_config)
[2026-01-18 15:56:43] [INFO] [0;36m(EngineCore_DP0 pid=12965)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 15:56:43] [INFO] [0;36m(EngineCore_DP0 pid=12965)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[2026-01-18 15:56:43] [INFO] [0;36m(EngineCore_DP0 pid=12965)[0;0m     super().__init__(vllm_config)
[2026-01-18 15:56:43] [INFO] [0;36m(EngineCore_DP0 pid=12965)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[2026-01-18 15:56:43] [INFO] [0;36m(EngineCore_DP0 pid=12965)[0;0m     self._init_executor()
[2026-01-18 15:56:43] [INFO] [0;36m(EngineCore_DP0 pid=12965)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[2026-01-18 15:56:43] [INFO] [0;36m(EngineCore_DP0 pid=12965)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
[2026-01-18 15:56:43] [INFO] [0;36m(EngineCore_DP0 pid=12965)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 15:56:43] [INFO] [0;36m(EngineCore_DP0 pid=12965)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 675, in wait_for_ready
[2026-01-18 15:56:43] [INFO] [0;36m(EngineCore_DP0 pid=12965)[0;0m     raise e from None
[2026-01-18 15:56:43] [INFO] [0;36m(EngineCore_DP0 pid=12965)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[2026-01-18 15:56:44] [ERROR] [0;36m(APIServer pid=12625)[0;0m Traceback (most recent call last):
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/bin/vllm", line 7, in <module>
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m     sys.exit(main())
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m              ^^^^^^
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m     args.dispatch_function(args)
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m     uvloop.run(run_server(args))
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m     return __asyncio.run(
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m            ^^^^^^^^^^^^^^
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m     return runner.run(main)
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m     return self._loop.run_until_complete(task)
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m     return await main
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m            ^^^^^^^^^^
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m     async with build_async_engine_client(
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m     return await anext(self.gen)
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 146, in build_async_engine_client
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m     return await anext(self.gen)
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 187, in build_async_engine_client_from_engine_args
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 202, in from_vllm_config
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m     return cls(
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m            ^^^^
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 129, in __init__
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m     return AsyncMPClient(*client_args)
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m     super().__init__(
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 144, in __exit__
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m     next(self.gen)
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 918, in launch_core_engines
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m     wait_for_engine_startup(
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 977, in wait_for_engine_startup
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m     raise RuntimeError(
[2026-01-18 15:56:44] [INFO] [0;36m(APIServer pid=12625)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[2026-01-18 15:56:44] [INFO] /mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/resource_tracker.py:279: UserWarning: resource_tracker: There appear to be 3 leaked shared_memory objects to clean up at shutdown
[2026-01-18 15:56:44] [INFO] warnings.warn('resource_tracker: There appear to be %d '
[2026-01-18 15:56:45] [INFO] nvitop监控已启动
[2026-01-18 15:57:30] [SUCCESS] 正在关闭服务器...
[2026-01-18 16:24:07] [INFO] VLLM GUI 服务器启动，端口: 5003
[2026-01-18 16:24:08] [INFO] nvitop监控已启动
[2026-01-18 16:24:09] [INFO] nvitop监控已启动
[2026-01-18 16:24:10] [INFO] 从conda info --base自动检测到conda路径: /mnt/AI-Acer4T/miniconda3
[2026-01-18 16:24:14] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 16:24:14] [INFO] nvitop监控已启动
[2026-01-18 16:24:14] [INFO] nvitop监控已停止
[2026-01-18 16:24:14] [INFO] /bin/bash: -c: 行 1: 寻找匹配的 `'' 时遇到了未预期的 EOF
[2026-01-18 16:24:14] [INFO] nvitop监控已启动
[2026-01-18 16:24:41] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 16:24:41] [INFO] nvitop监控已启动
[2026-01-18 16:24:41] [INFO] nvitop监控已停止
[2026-01-18 16:25:08] [INFO] [0;36m(APIServer pid=24745)[0;0m INFO 01-18 16:25:08 [api_server.py:872] vLLM API server version 0.14.0rc2.dev117+g9e078d058
[2026-01-18 16:25:08] [INFO] [0;36m(APIServer pid=24745)[0;0m INFO 01-18 16:25:08 [utils.py:267] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-Next-80B-A3B-Thinking-FP8/', 'host': '0.0.0.0', 'port': 8005, 'enable_auto_tool_choice': True, 'tool_call_parser': 'deepseek-v3', 'model': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-Next-80B-A3B-Thinking-FP8/', 'trust_remote_code': True, 'max_model_len': 128000, 'served_model_name': ['VLLM-MODEL'], 'reasoning_parser': 'deepseek-v3', 'tensor_parallel_size': 2, 'kv_cache_dtype': 'fp8', 'max_num_seqs': 256, 'enable_chunked_prefill': False, 'async_scheduling': True, 'compilation_config': {'level': None, 'mode': None, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': [], 'splitting_ops': None, 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': None, 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.PIECEWISE: 1>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': None, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}}
[2026-01-18 16:25:08] [ERROR] [0;36m(APIServer pid=24745)[0;0m Traceback (most recent call last):
[2026-01-18 16:25:08] [INFO] [0;36m(APIServer pid=24745)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/bin/vllm", line 7, in <module>
[2026-01-18 16:25:08] [INFO] [0;36m(APIServer pid=24745)[0;0m     sys.exit(main())
[2026-01-18 16:25:08] [INFO] [0;36m(APIServer pid=24745)[0;0m              ^^^^^^
[2026-01-18 16:25:08] [INFO] [0;36m(APIServer pid=24745)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-18 16:25:08] [INFO] [0;36m(APIServer pid=24745)[0;0m     args.dispatch_function(args)
[2026-01-18 16:25:08] [INFO] [0;36m(APIServer pid=24745)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-18 16:25:08] [INFO] [0;36m(APIServer pid=24745)[0;0m     uvloop.run(run_server(args))
[2026-01-18 16:25:08] [INFO] [0;36m(APIServer pid=24745)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-18 16:25:08] [INFO] [0;36m(APIServer pid=24745)[0;0m     return __asyncio.run(
[2026-01-18 16:25:08] [INFO] [0;36m(APIServer pid=24745)[0;0m            ^^^^^^^^^^^^^^
[2026-01-18 16:25:08] [INFO] [0;36m(APIServer pid=24745)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-18 16:25:08] [INFO] [0;36m(APIServer pid=24745)[0;0m     return runner.run(main)
[2026-01-18 16:25:08] [INFO] [0;36m(APIServer pid=24745)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-18 16:25:08] [INFO] [0;36m(APIServer pid=24745)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-18 16:25:08] [INFO] [0;36m(APIServer pid=24745)[0;0m     return self._loop.run_until_complete(task)
[2026-01-18 16:25:08] [INFO] [0;36m(APIServer pid=24745)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:25:08] [INFO] [0;36m(APIServer pid=24745)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-18 16:25:08] [INFO] [0;36m(APIServer pid=24745)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-18 16:25:08] [INFO] [0;36m(APIServer pid=24745)[0;0m     return await main
[2026-01-18 16:25:08] [INFO] [0;36m(APIServer pid=24745)[0;0m            ^^^^^^^^^^
[2026-01-18 16:25:08] [INFO] [0;36m(APIServer pid=24745)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 918, in run_server
[2026-01-18 16:25:08] [INFO] [0;36m(APIServer pid=24745)[0;0m     listen_address, sock = setup_server(args)
[2026-01-18 16:25:08] [INFO] [0;36m(APIServer pid=24745)[0;0m                            ^^^^^^^^^^^^^^^^^^
[2026-01-18 16:25:08] [INFO] [0;36m(APIServer pid=24745)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 881, in setup_server
[2026-01-18 16:25:08] [INFO] [0;36m(APIServer pid=24745)[0;0m     validate_api_server_args(args)
[2026-01-18 16:25:08] [INFO] [0;36m(APIServer pid=24745)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 853, in validate_api_server_args
[2026-01-18 16:25:08] [INFO] [0;36m(APIServer pid=24745)[0;0m     raise KeyError(
[2026-01-18 16:25:08] [INFO] [0;36m(APIServer pid=24745)[0;0m KeyError: 'invalid tool call parser: deepseek-v3 (chose from { deepseek_v3,deepseek_v31,deepseek_v32,ernie45,functiongemma,gigachat3,glm45,glm47,granite,granite-20b-fc,hermes,hunyuan_a13b,internlm,jamba,kimi_k2,llama3_json,llama4_json,llama4_pythonic,longcat,minimax,minimax_m2,mistral,olmo3,openai,phi4_mini_json,pythonic,qwen3_coder,qwen3_xml,seed_oss,step3,xlam })'
[2026-01-18 16:25:10] [INFO] nvitop监控已启动
[2026-01-18 16:27:27] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 16:27:27] [INFO] nvitop监控已启动
[2026-01-18 16:27:27] [INFO] nvitop监控已停止
[2026-01-18 16:27:55] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:27:55 [api_server.py:872] vLLM API server version 0.14.0rc2.dev117+g9e078d058
[2026-01-18 16:27:55] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:27:55 [utils.py:267] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-Next-80B-A3B-Thinking-FP8/', 'host': '0.0.0.0', 'port': 8005, 'enable_auto_tool_choice': True, 'tool_call_parser': 'deepseek_v3', 'model': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-Next-80B-A3B-Thinking-FP8/', 'trust_remote_code': True, 'max_model_len': 128000, 'served_model_name': ['VLLM-MODEL'], 'reasoning_parser': 'deepseek_v3', 'tensor_parallel_size': 2, 'kv_cache_dtype': 'fp8', 'max_num_seqs': 256, 'enable_chunked_prefill': False, 'async_scheduling': True, 'compilation_config': {'level': None, 'mode': None, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': [], 'splitting_ops': None, 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': None, 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.PIECEWISE: 1>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': None, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}}
[2026-01-18 16:27:55] [INFO] [0;36m(APIServer pid=25871)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 16:27:55] [INFO] [0;36m(APIServer pid=25871)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 16:27:55] [INFO] [0;36m(APIServer pid=25871)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 16:27:55] [INFO] [0;36m(APIServer pid=25871)[0;0m Unrecognized keys in `rope_parameters` for 'rope_type'='default': {'partial_rotary_factor'}
[2026-01-18 16:27:55] [INFO] [0;36m(APIServer pid=25871)[0;0m Unrecognized keys in `rope_parameters` for 'rope_type'='default': {'partial_rotary_factor'}
[2026-01-18 16:27:55] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:27:55 [model.py:530] Resolved architecture: Qwen3NextForCausalLM
[2026-01-18 16:27:55] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:27:55 [model.py:1547] Using max model len 128000
[2026-01-18 16:27:55] [WARNING] [0;36m(APIServer pid=25871)[0;0m WARNING 01-18 16:27:55 [quark_ocp_mx.py:157] AITER is not found or QuarkOCP_MX is not supported on the current platform. QuarkOCP_MX quantization will not be available.
[2026-01-18 16:27:56] [WARNING] [0;36m(APIServer pid=25871)[0;0m WARNING 01-18 16:27:56 [arg_utils.py:1913] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[2026-01-18 16:27:56] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:27:56 [cache.py:206] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor.
[2026-01-18 16:27:57] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:27:57 [config.py:476] Setting attention block size to 1072 tokens to ensure that attention page size is >= mamba page size.
[2026-01-18 16:27:57] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:27:57 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 16:27:57] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:27:57 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-18 16:28:23] [INFO] [0;36m(EngineCore_DP0 pid=26208)[0;0m INFO 01-18 16:28:23 [core.py:96] Initializing a V1 LLM engine (v0.14.0rc2.dev117+g9e078d058) with config: model='/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-Next-80B-A3B-Thinking-FP8/', speculative_config=None, tokenizer='/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-Next-80B-A3B-Thinking-FP8/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=fp8, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='deepseek_v3', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=False, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['+quant_fp8', 'none', '+quant_fp8'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [128000], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.PIECEWISE: 1>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': True, 'fuse_act_quant': True, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-18 16:28:23] [WARNING] [0;36m(EngineCore_DP0 pid=26208)[0;0m WARNING 01-18 16:28:23 [multiproc_executor.py:897] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-18 16:28:53] [INFO] INFO 01-18 16:28:53 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:36181 backend=nccl
[2026-01-18 16:28:53] [INFO] INFO 01-18 16:28:53 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:36181 backend=nccl
[2026-01-18 16:28:53] [INFO] INFO 01-18 16:28:53 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-18 16:28:54] [WARNING] WARNING 01-18 16:28:54 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 16:28:54] [WARNING] WARNING 01-18 16:28:54 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 16:28:54] [INFO] INFO 01-18 16:28:54 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[2026-01-18 16:28:54] [INFO] INFO 01-18 16:28:54 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
[2026-01-18 16:28:55] [INFO] [0;36m(Worker_TP0 pid=26422)[0;0m INFO 01-18 16:28:55 [gpu_model_runner.py:3803] Starting to load model /mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-Next-80B-A3B-Thinking-FP8/...
[2026-01-18 16:28:55] [INFO] [0;36m(Worker_TP0 pid=26422)[0;0m INFO 01-18 16:28:55 [fp8.py:126] DeepGEMM is disabled because the platform does not support it.
[2026-01-18 16:28:55] [INFO] [0;36m(Worker_TP0 pid=26422)[0;0m INFO 01-18 16:28:55 [fp8.py:154] Using Triton backend for FP8 MoE
[2026-01-18 16:28:59] [INFO] [0;36m(Worker_TP0 pid=26422)[0;0m INFO 01-18 16:28:59 [cuda.py:351] Using FLASHINFER attention backend out of potential backends: ('FLASHINFER', 'TRITON_ATTN')
[2026-01-18 16:28:59] [INFO] [0;36m(Worker_TP0 pid=26422)[0;0m
[2026-01-18 16:28:59] [INFO] Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]
[2026-01-18 16:29:09] [INFO] [0;36m(Worker_TP0 pid=26422)[0;0m
[2026-01-18 16:29:09] [INFO] Loading safetensors checkpoint shards:  12% Completed | 1/8 [00:10<01:11, 10.24s/it]
[2026-01-18 16:29:20] [INFO] [0;36m(Worker_TP0 pid=26422)[0;0m
[2026-01-18 16:29:20] [INFO] Loading safetensors checkpoint shards:  25% Completed | 2/8 [00:20<01:02, 10.38s/it]
[2026-01-18 16:29:30] [INFO] [0;36m(Worker_TP0 pid=26422)[0;0m
[2026-01-18 16:29:30] [INFO] Loading safetensors checkpoint shards:  38% Completed | 3/8 [00:31<00:52, 10.49s/it]
[2026-01-18 16:29:40] [INFO] [0;36m(Worker_TP0 pid=26422)[0;0m
[2026-01-18 16:29:40] [INFO] Loading safetensors checkpoint shards:  50% Completed | 4/8 [00:41<00:40, 10.17s/it]
[2026-01-18 16:29:50] [INFO] [0;36m(Worker_TP0 pid=26422)[0;0m
[2026-01-18 16:29:50] [INFO] Loading safetensors checkpoint shards:  62% Completed | 5/8 [00:50<00:30, 10.02s/it]
[2026-01-18 16:30:00] [INFO] [0;36m(Worker_TP0 pid=26422)[0;0m
[2026-01-18 16:30:00] [INFO] Loading safetensors checkpoint shards:  75% Completed | 6/8 [01:00<00:20, 10.00s/it]
[2026-01-18 16:30:11] [INFO] [0;36m(Worker_TP0 pid=26422)[0;0m
[2026-01-18 16:30:11] [INFO] Loading safetensors checkpoint shards:  88% Completed | 7/8 [01:11<00:10, 10.29s/it]
[2026-01-18 16:30:17] [INFO] [0;36m(Worker_TP0 pid=26422)[0;0m
[2026-01-18 16:30:17] [INFO] Loading safetensors checkpoint shards: 100% Completed | 8/8 [01:17<00:00,  8.88s/it]
[2026-01-18 16:30:17] [INFO] [0;36m(Worker_TP0 pid=26422)[0;0m
[2026-01-18 16:30:17] [INFO] Loading safetensors checkpoint shards: 100% Completed | 8/8 [01:17<00:00,  9.69s/it]
[2026-01-18 16:30:17] [INFO] [0;36m(Worker_TP0 pid=26422)[0;0m
[2026-01-18 16:30:17] [WARNING] [0;36m(Worker_TP1 pid=26423)[0;0m WARNING 01-18 16:30:17 [kv_cache.py:90] Checkpoint does not provide a q scaling factor. Setting it to k_scale. This only matters for FP8 Attention backends (flash-attn or flashinfer).
[2026-01-18 16:30:17] [WARNING] [0;36m(Worker_TP1 pid=26423)[0;0m WARNING 01-18 16:30:17 [kv_cache.py:104] Using KV cache scaling factor 1.0 for fp8_e4m3. If this is unintended, verify that k/v_scale scaling factors are properly set in the checkpoint.
[2026-01-18 16:30:17] [WARNING] [0;36m(Worker_TP1 pid=26423)[0;0m WARNING 01-18 16:30:17 [kv_cache.py:143] Using uncalibrated q_scale 1.0 and/or prob_scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure q/prob scaling factors are available in the fp8 checkpoint.
[2026-01-18 16:30:17] [INFO] [0;36m(Worker_TP0 pid=26422)[0;0m INFO 01-18 16:30:17 [default_loader.py:291] Loading weights took 77.62 seconds
[2026-01-18 16:30:17] [WARNING] [0;36m(Worker_TP0 pid=26422)[0;0m WARNING 01-18 16:30:17 [kv_cache.py:90] Checkpoint does not provide a q scaling factor. Setting it to k_scale. This only matters for FP8 Attention backends (flash-attn or flashinfer).
[2026-01-18 16:30:17] [WARNING] [0;36m(Worker_TP0 pid=26422)[0;0m WARNING 01-18 16:30:17 [kv_cache.py:104] Using KV cache scaling factor 1.0 for fp8_e4m3. If this is unintended, verify that k/v_scale scaling factors are properly set in the checkpoint.
[2026-01-18 16:30:17] [WARNING] [0;36m(Worker_TP0 pid=26422)[0;0m WARNING 01-18 16:30:17 [kv_cache.py:143] Using uncalibrated q_scale 1.0 and/or prob_scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure q/prob scaling factors are available in the fp8 checkpoint.
[2026-01-18 16:30:17] [INFO] [0;36m(Worker_TP0 pid=26422)[0;0m INFO 01-18 16:30:17 [gpu_model_runner.py:3900] Model loading took 37.52 GiB memory and 81.521747 seconds
[2026-01-18 16:30:31] [INFO] [0;36m(Worker_TP0 pid=26422)[0;0m INFO 01-18 16:30:31 [backends.py:644] Using cache directory: /home/wuwen/.cache/vllm/torch_compile_cache/ba339cf788/rank_0_0/backbone for vLLM's torch.compile
[2026-01-18 16:30:31] [INFO] [0;36m(Worker_TP0 pid=26422)[0;0m INFO 01-18 16:30:31 [backends.py:704] Dynamo bytecode transform time: 13.23 s
[2026-01-18 16:30:44] [INFO] [0;36m(Worker_TP1 pid=26423)[0;0m INFO 01-18 16:30:44 [backends.py:261] Cache the graph of compile range (1, 128000) for later use
[2026-01-18 16:30:44] [INFO] [0;36m(Worker_TP0 pid=26422)[0;0m INFO 01-18 16:30:44 [backends.py:261] Cache the graph of compile range (1, 128000) for later use
[2026-01-18 16:30:49] [WARNING] [0;36m(Worker_TP0 pid=26422)[0;0m WARNING 01-18 16:30:49 [fused_moe.py:1090] Using default MoE config. Performance might be sub-optimal! Config file not found at /mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/configs/E=512,N=256,device_name=NVIDIA_RTX_PRO_6000_Blackwell_Workstation_Edition,dtype=fp8_w8a8,block_shape=[128,128].json
[2026-01-18 16:31:17] [INFO] [0;36m(EngineCore_DP0 pid=26208)[0;0m INFO 01-18 16:31:17 [shm_broadcast.py:542] No available shared memory broadcast block found in 60 seconds. This typically happens when some processes are hanging or doing some time-consuming work (e.g. compilation, weight/kv cache quantization).
[2026-01-18 16:32:17] [INFO] [0;36m(EngineCore_DP0 pid=26208)[0;0m INFO 01-18 16:32:17 [shm_broadcast.py:542] No available shared memory broadcast block found in 60 seconds. This typically happens when some processes are hanging or doing some time-consuming work (e.g. compilation, weight/kv cache quantization).
[2026-01-18 16:33:06] [INFO] [0;36m(Worker_TP0 pid=26422)[0;0m INFO 01-18 16:33:06 [backends.py:278] Compiling a graph for compile range (1, 128000) takes 147.05 s
[2026-01-18 16:33:06] [INFO] [0;36m(Worker_TP0 pid=26422)[0;0m INFO 01-18 16:33:06 [monitor.py:34] torch.compile takes 160.27 s in total
[2026-01-18 16:33:07] [INFO] [0;36m(Worker_TP0 pid=26422)[0;0m INFO 01-18 16:33:07 [gpu_worker.py:355] Available KV cache memory: 40.13 GiB
[2026-01-18 16:33:07] [INFO] [0;36m(EngineCore_DP0 pid=26208)[0;0m INFO 01-18 16:33:07 [kv_cache_utils.py:1307] GPU KV cache size: 1,752,720 tokens
[2026-01-18 16:33:07] [INFO] [0;36m(EngineCore_DP0 pid=26208)[0;0m INFO 01-18 16:33:07 [kv_cache_utils.py:1312] Maximum concurrency for 128,000 tokens per request: 53.19x
[2026-01-18 16:33:07] [INFO] [0;36m(Worker_TP1 pid=26423)[0;0m [0;36m(Worker_TP0 pid=26422)[0;0m 2026-01-18 16:33:07,770 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[2026-01-18 16:33:07] [INFO] 2026-01-18 16:33:07,770 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[2026-01-18 16:33:10] [INFO] [0;36m(Worker_TP1 pid=26423)[0;0m 2026-01-18 16:33:10,097 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[2026-01-18 16:33:10] [INFO] [0;36m(Worker_TP0 pid=26422)[0;0m 2026-01-18 16:33:10,097 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[2026-01-18 16:33:10] [INFO] [0;36m(Worker_TP0 pid=26422)[0;0m
[2026-01-18 16:33:10] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
[2026-01-18 16:33:11] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:18,  2.77it/s]
[2026-01-18 16:33:11] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:10,  4.63it/s]
[2026-01-18 16:33:11] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:08,  5.90it/s]
[2026-01-18 16:33:11] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:00<00:06,  6.79it/s]
[2026-01-18 16:33:11] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:00<00:06,  7.36it/s]
[2026-01-18 16:33:11] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:05,  7.78it/s]
[2026-01-18 16:33:11] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▎        | 7/51 [00:01<00:05,  8.07it/s]
[2026-01-18 16:33:11] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:01<00:05,  8.25it/s]
[2026-01-18 16:33:12] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:01<00:05,  8.34it/s]
[2026-01-18 16:33:12] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|█▉        | 10/51 [00:01<00:04,  8.43it/s]
[2026-01-18 16:33:12] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:01<00:04,  8.46it/s]
[2026-01-18 16:33:12] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:01<00:04,  8.48it/s]
[2026-01-18 16:33:12] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:01<00:04,  8.01it/s]
[2026-01-18 16:33:12] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:01<00:04,  8.11it/s]
[2026-01-18 16:33:12] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:02<00:04,  8.17it/s]
[2026-01-18 16:33:12] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 16/51 [00:02<00:04,  8.31it/s]
[2026-01-18 16:33:12] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:02<00:04,  8.05it/s]
[2026-01-18 16:33:13] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:02<00:04,  7.98it/s]
[2026-01-18 16:33:13] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 19/51 [00:02<00:04,  7.88it/s]
[2026-01-18 16:33:13] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:02<00:03,  7.80it/s]
[2026-01-18 16:33:13] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:02<00:03,  7.71it/s]
[2026-01-18 16:33:13] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 22/51 [00:02<00:03,  7.64it/s]
[2026-01-18 16:33:13] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:03<00:03,  7.65it/s]
[2026-01-18 16:33:13] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:03<00:03,  7.40it/s]
[2026-01-18 16:33:14] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 25/51 [00:03<00:03,  7.48it/s]
[2026-01-18 16:33:14] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:03<00:03,  7.54it/s]
[2026-01-18 16:33:14] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:03<00:03,  7.59it/s]
[2026-01-18 16:33:14] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:03<00:03,  7.43it/s]
[2026-01-18 16:33:14] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:03<00:02,  7.35it/s]
[2026-01-18 16:33:14] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:03<00:02,  7.46it/s]
[2026-01-18 16:33:14] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 31/51 [00:04<00:02,  7.58it/s]
[2026-01-18 16:33:14] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:04<00:02,  7.63it/s]
[2026-01-18 16:33:15] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:04<00:02,  7.71it/s]
[2026-01-18 16:33:15] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:04<00:02,  7.74it/s]
[2026-01-18 16:33:15] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:04<00:02,  7.77it/s]
[2026-01-18 16:33:15] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:04<00:01,  7.76it/s]
[2026-01-18 16:33:15] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 37/51 [00:04<00:01,  7.79it/s]
[2026-01-18 16:33:15] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:05<00:01,  7.79it/s]
[2026-01-18 16:33:15] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▋  | 39/51 [00:05<00:01,  7.81it/s]
[2026-01-18 16:33:16] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:05<00:01,  7.70it/s]
[2026-01-18 16:33:16] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:05<00:01,  7.68it/s]
[2026-01-18 16:33:16] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:05<00:01,  7.58it/s]
[2026-01-18 16:33:16] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 43/51 [00:05<00:01,  7.48it/s]
[2026-01-18 16:33:16] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:05<00:00,  7.51it/s]
[2026-01-18 16:33:16] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 45/51 [00:05<00:00,  7.55it/s]
[2026-01-18 16:33:16] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|█████████ | 46/51 [00:06<00:00,  7.56it/s]
[2026-01-18 16:33:16] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:06<00:00,  7.57it/s]
[2026-01-18 16:33:17] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 48/51 [00:06<00:00,  7.57it/s]
[2026-01-18 16:33:18] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▌| 49/51 [00:07<00:00,  2.55it/s]
[2026-01-18 16:33:18] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:07<00:00,  3.18it/s]
[2026-01-18 16:33:18] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:07<00:00,  3.76it/s]
[2026-01-18 16:33:18] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:07<00:00,  6.69it/s]
[2026-01-18 16:33:18] [INFO] [0;36m(Worker_TP0 pid=26422)[0;0m INFO 01-18 16:33:18 [custom_all_reduce.py:216] Registering 4947 cuda graph addresses
[2026-01-18 16:33:18] [INFO] [0;36m(Worker_TP1 pid=26423)[0;0m INFO 01-18 16:33:18 [custom_all_reduce.py:216] Registering 4947 cuda graph addresses
[2026-01-18 16:33:18] [INFO] [0;36m(Worker_TP0 pid=26422)[0;0m INFO 01-18 16:33:18 [gpu_model_runner.py:4852] Graph capturing finished in 9 secs, took -6.74 GiB
[2026-01-18 16:33:19] [INFO] [0;36m(EngineCore_DP0 pid=26208)[0;0m INFO 01-18 16:33:19 [core.py:272] init engine (profile, create kv cache, warmup model) took 181.35 seconds
[2026-01-18 16:33:19] [INFO] [0;36m(EngineCore_DP0 pid=26208)[0;0m INFO 01-18 16:33:19 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [api_server.py:663] Supported tasks: ['generate']
[2026-01-18 16:33:20] [WARNING] [0;36m(APIServer pid=25871)[0;0m WARNING 01-18 16:33:20 [model.py:1360] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [serving.py:281] "auto" tool choice has been enabled.
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [serving.py:281] "auto" tool choice has been enabled.
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [serving.py:185] Warming up chat template processing...
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [serving.py:221] Chat template warmup completed in 530.4ms
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [serving.py:281] "auto" tool choice has been enabled.
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:8005
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [launcher.py:38] Available routes are:
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [launcher.py:46] Route: /docs, Methods: HEAD, GET
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [launcher.py:46] Route: /tokenize, Methods: POST
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [launcher.py:46] Route: /detokenize, Methods: POST
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [launcher.py:46] Route: /pause, Methods: POST
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [launcher.py:46] Route: /resume, Methods: POST
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [launcher.py:46] Route: /is_paused, Methods: GET
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [launcher.py:46] Route: /metrics, Methods: GET
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [launcher.py:46] Route: /health, Methods: GET
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [launcher.py:46] Route: /v1/responses, Methods: POST
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [launcher.py:46] Route: /v1/completions, Methods: POST
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [launcher.py:46] Route: /v1/messages, Methods: POST
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [launcher.py:46] Route: /v1/models, Methods: GET
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [launcher.py:46] Route: /load, Methods: GET
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [launcher.py:46] Route: /version, Methods: GET
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [launcher.py:46] Route: /ping, Methods: GET
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [launcher.py:46] Route: /ping, Methods: POST
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [launcher.py:46] Route: /invocations, Methods: POST
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [launcher.py:46] Route: /classify, Methods: POST
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [launcher.py:46] Route: /score, Methods: POST
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [launcher.py:46] Route: /v1/score, Methods: POST
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [launcher.py:46] Route: /rerank, Methods: POST
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [launcher.py:46] Route: /v1/rerank, Methods: POST
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [launcher.py:46] Route: /v2/rerank, Methods: POST
[2026-01-18 16:33:20] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:33:20 [launcher.py:46] Route: /pooling, Methods: POST
[2026-01-18 16:33:21] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO:     Started server process [25871]
[2026-01-18 16:33:21] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO:     Waiting for application startup.
[2026-01-18 16:33:21] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO:     Application startup complete.
[2026-01-18 16:33:33] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO:     127.0.0.1:43786 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-18 16:34:01] [INFO] [0;36m(Worker_TP0 pid=26422)[0;0m /mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fla/ops/utils.py:113: UserWarning: Input tensor shape suggests potential format mismatch: seq_len (11) < num_heads (16). This may indicate the inputs were passed in head-first format [B, H, T, ...] when head_first=False was specified. Please verify your input tensor format matches the expected shape [B, T, H, ...].
[2026-01-18 16:34:01] [INFO] [0;36m(Worker_TP0 pid=26422)[0;0m   return fn(*contiguous_args, **contiguous_kwargs)
[2026-01-18 16:34:01] [INFO] [0;36m(Worker_TP1 pid=26423)[0;0m /mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fla/ops/utils.py:113: UserWarning: Input tensor shape suggests potential format mismatch: seq_len (11) < num_heads (16). This may indicate the inputs were passed in head-first format [B, H, T, ...] when head_first=False was specified. Please verify your input tensor format matches the expected shape [B, T, H, ...].
[2026-01-18 16:34:01] [INFO] [0;36m(Worker_TP1 pid=26423)[0;0m   return fn(*contiguous_args, **contiguous_kwargs)
[2026-01-18 16:34:33] [INFO] [0;36m(EngineCore_DP0 pid=26208)[0;0m INFO 01-18 16:34:33 [shm_broadcast.py:542] No available shared memory broadcast block found in 60 seconds. This typically happens when some processes are hanging or doing some time-consuming work (e.g. compilation, weight/kv cache quantization).
[2026-01-18 16:34:45] [INFO] [0;36m(Worker_TP0 pid=26422)[0;0m /mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fla/ops/utils.py:113: UserWarning: Input tensor shape suggests potential format mismatch: seq_len (11) < num_heads (16). This may indicate the inputs were passed in head-first format [B, H, T, ...] when head_first=False was specified. Please verify your input tensor format matches the expected shape [B, T, H, ...].
[2026-01-18 16:34:45] [INFO] [0;36m(Worker_TP0 pid=26422)[0;0m   return fn(*contiguous_args, **contiguous_kwargs)
[2026-01-18 16:34:46] [INFO] [0;36m(Worker_TP1 pid=26423)[0;0m /mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fla/ops/utils.py:113: UserWarning: Input tensor shape suggests potential format mismatch: seq_len (11) < num_heads (16). This may indicate the inputs were passed in head-first format [B, H, T, ...] when head_first=False was specified. Please verify your input tensor format matches the expected shape [B, T, H, ...].
[2026-01-18 16:34:46] [INFO] [0;36m(Worker_TP1 pid=26423)[0;0m   return fn(*contiguous_args, **contiguous_kwargs)
[2026-01-18 16:34:51] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:34:51 [loggers.py:257] Engine 000: Avg prompt throughput: 1.1 tokens/s, Avg generation throughput: 25.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
[2026-01-18 16:35:01] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:35:01 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 40.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[2026-01-18 16:35:02] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO:     127.0.0.1:57728 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-18 16:35:02] [INFO] [0;36m(Worker_TP1 pid=26423)[0;0m [0;36m(Worker_TP0 pid=26422)[0;0m /mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fla/ops/utils.py:113: UserWarning: Input tensor shape suggests potential format mismatch: seq_len (11) < num_heads (16). This may indicate the inputs were passed in head-first format [B, H, T, ...] when head_first=False was specified. Please verify your input tensor format matches the expected shape [B, T, H, ...].
[2026-01-18 16:35:02] [INFO] [0;36m(Worker_TP0 pid=26422)[0;0m /mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fla/ops/utils.py:113: UserWarning: Input tensor shape suggests potential format mismatch: seq_len (11) < num_heads (16). This may indicate the inputs were passed in head-first format [B, H, T, ...] when head_first=False was specified. Please verify your input tensor format matches the expected shape [B, T, H, ...].
[2026-01-18 16:35:02] [INFO] return fn(*contiguous_args, **contiguous_kwargs)
[2026-01-18 16:35:02] [INFO] [0;36m(Worker_TP1 pid=26423)[0;0m   return fn(*contiguous_args, **contiguous_kwargs)
[2026-01-18 16:35:11] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:35:11 [loggers.py:257] Engine 000: Avg prompt throughput: 1.1 tokens/s, Avg generation throughput: 69.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[2026-01-18 16:35:21] [INFO] [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:35:21 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[2026-01-18 16:37:21] [INFO] [0;36m(Worker_TP1 pid=26423)[0;0m [0;36m(APIServer pid=25871)[0;0m INFO 01-18 16:37:21 [multiproc_executor.py:724] Parent process exited, terminating worker
[2026-01-18 16:37:21] [ERROR] ERROR 01-18 16:37:21 [core_client.py:605] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.
[2026-01-18 16:37:21] [INFO] [0;36m(Worker_TP0 pid=26422)[0;0m INFO 01-18 16:37:21 [multiproc_executor.py:724] Parent process exited, terminating worker
[2026-01-18 16:37:21] [INFO] [0;36m(Worker_TP1 pid=26423)[0;0m INFO 01-18 16:37:21 [multiproc_executor.py:768] WorkerProc shutting down.
[2026-01-18 16:37:21] [INFO] [0;36m(Worker_TP0 pid=26422)[0;0m INFO 01-18 16:37:21 [multiproc_executor.py:768] WorkerProc shutting down.
[2026-01-18 16:37:21] [INFO] [0;36m(Worker_TP0 pid=26422)[0;0m /mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/resource_tracker.py:147: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.
[2026-01-18 16:37:21] [INFO] [0;36m(Worker_TP0 pid=26422)[0;0m   warnings.warn('resource_tracker: process died unexpectedly, '
[2026-01-18 16:37:21] [ERROR] Traceback (most recent call last):
[2026-01-18 16:37:21] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 16:37:21] [INFO] cache[rtype].remove(name)
[2026-01-18 16:37:21] [INFO] KeyError: '/psm_1c9aad9a'
[2026-01-18 16:37:21] [ERROR] Traceback (most recent call last):
[2026-01-18 16:37:21] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 16:37:21] [INFO] cache[rtype].remove(name)
[2026-01-18 16:37:21] [INFO] KeyError: '/psm_9b43ef8a'
[2026-01-18 16:37:21] [INFO] 服务已停止
[2026-01-18 16:37:21] [INFO] nvitop监控已启动
[2026-01-18 16:37:21] [INFO] [0;36m(Worker_TP1 pid=26423)[0;0m /mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/resource_tracker.py:147: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.
[2026-01-18 16:37:21] [INFO] [0;36m(Worker_TP1 pid=26423)[0;0m   warnings.warn('resource_tracker: process died unexpectedly, '
[2026-01-18 16:37:21] [ERROR] Traceback (most recent call last):
[2026-01-18 16:37:21] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 16:37:21] [INFO] cache[rtype].remove(name)
[2026-01-18 16:37:21] [INFO] KeyError: '/mp-6rglvpae'
[2026-01-18 16:37:21] [ERROR] Traceback (most recent call last):
[2026-01-18 16:37:21] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 16:37:21] [INFO] cache[rtype].remove(name)
[2026-01-18 16:37:21] [INFO] KeyError: '/mp-ex0v8zsk'
[2026-01-18 16:37:22] [INFO] nvitop监控已启动
[2026-01-18 16:37:24] [INFO] nvitop监控已启动
[2026-01-18 16:37:26] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 16:37:26] [INFO] nvitop监控已启动
[2026-01-18 16:37:26] [INFO] nvitop监控已停止
[2026-01-18 16:37:55] [INFO] [0;36m(APIServer pid=34991)[0;0m INFO 01-18 16:37:55 [api_server.py:872] vLLM API server version 0.14.0rc2.dev117+g9e078d058
[2026-01-18 16:37:55] [INFO] [0;36m(APIServer pid=34991)[0;0m INFO 01-18 16:37:55 [utils.py:267] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/chat_template.jinja', 'enable_auto_tool_choice': True, 'tool_call_parser': 'deepseek_v3', 'model': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/', 'trust_remote_code': True, 'max_model_len': 128000, 'served_model_name': ['VLLM-MODEL'], 'reasoning_parser': 'deepseek_v3', 'tensor_parallel_size': 2, 'kv_cache_dtype': 'fp8', 'max_num_seqs': 256, 'enable_chunked_prefill': False, 'async_scheduling': True, 'compilation_config': {'level': None, 'mode': None, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': [], 'splitting_ops': None, 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': None, 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.PIECEWISE: 1>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': None, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}}
[2026-01-18 16:37:55] [INFO] [0;36m(APIServer pid=34991)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 16:37:55] [INFO] [0;36m(APIServer pid=34991)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 16:37:55] [INFO] [0;36m(APIServer pid=34991)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 16:37:55] [INFO] [0;36m(APIServer pid=34991)[0;0m Unrecognized keys in `rope_parameters` for 'rope_type'='default': {'partial_rotary_factor'}
[2026-01-18 16:37:55] [INFO] [0;36m(APIServer pid=34991)[0;0m Unrecognized keys in `rope_parameters` for 'rope_type'='default': {'partial_rotary_factor'}
[2026-01-18 16:37:55] [INFO] [0;36m(APIServer pid=34991)[0;0m INFO 01-18 16:37:55 [model.py:530] Resolved architecture: Qwen3NextForCausalLM
[2026-01-18 16:37:55] [INFO] [0;36m(APIServer pid=34991)[0;0m INFO 01-18 16:37:55 [model.py:1547] Using max model len 128000
[2026-01-18 16:37:55] [WARNING] [0;36m(APIServer pid=34991)[0;0m WARNING 01-18 16:37:55 [arg_utils.py:1913] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[2026-01-18 16:37:55] [INFO] [0;36m(APIServer pid=34991)[0;0m INFO 01-18 16:37:55 [cache.py:206] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor.
[2026-01-18 16:37:56] [INFO] [0;36m(APIServer pid=34991)[0;0m INFO 01-18 16:37:56 [config.py:476] Setting attention block size to 1072 tokens to ensure that attention page size is >= mamba page size.
[2026-01-18 16:37:56] [INFO] [0;36m(APIServer pid=34991)[0;0m INFO 01-18 16:37:56 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 16:37:56] [INFO] [0;36m(APIServer pid=34991)[0;0m INFO 01-18 16:37:56 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-18 16:38:26] [INFO] [0;36m(EngineCore_DP0 pid=35301)[0;0m INFO 01-18 16:38:26 [core.py:96] Initializing a V1 LLM engine (v0.14.0rc2.dev117+g9e078d058) with config: model='/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/', speculative_config=None, tokenizer='/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=fp8, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='deepseek_v3', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=False, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [128000], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.PIECEWISE: 1>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-18 16:38:26] [WARNING] [0;36m(EngineCore_DP0 pid=35301)[0;0m WARNING 01-18 16:38:26 [multiproc_executor.py:897] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-18 16:38:55] [INFO] INFO 01-18 16:38:55 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:60147 backend=nccl
[2026-01-18 16:38:56] [INFO] INFO 01-18 16:38:56 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:60147 backend=nccl
[2026-01-18 16:38:56] [INFO] INFO 01-18 16:38:56 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-18 16:38:56] [WARNING] WARNING 01-18 16:38:56 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 16:38:56] [WARNING] WARNING 01-18 16:38:56 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 16:38:56] [INFO] INFO 01-18 16:38:56 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
[2026-01-18 16:38:56] [INFO] INFO 01-18 16:38:56 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[2026-01-18 16:38:58] [INFO] [0;36m(Worker_TP0 pid=35539)[0;0m INFO 01-18 16:38:58 [gpu_model_runner.py:3803] Starting to load model /mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/...
[2026-01-18 16:39:02] [INFO] [0;36m(Worker_TP0 pid=35539)[0;0m INFO 01-18 16:39:02 [unquantized.py:99] Using TRITON backend for Unquantized MoE
[2026-01-18 16:39:02] [INFO] [0;36m(Worker_TP0 pid=35539)[0;0m INFO 01-18 16:39:02 [cuda.py:351] Using FLASHINFER attention backend out of potential backends: ('FLASHINFER', 'TRITON_ATTN')
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766] WorkerProc failed to start.
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766] Traceback (most recent call last):
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 737, in worker_main
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]     worker = WorkerProc(*args, **kwargs)
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 575, in __init__
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]     self.worker.load_model()
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 274, in load_model
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3822, in load_model
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]     self.model = model_loader.load_model(
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 58, in load_model
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]     self.load_weights(model, model_config)
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 288, in load_weights
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_next.py", line 1294, in load_weights
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]     return loader.load_weights(weights)
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 335, in load_weights
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]     autoloaded_weights = set(self._load_module("", self.module, weights))
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 279, in _load_module
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]     for child_prefix, child_weights in self._groupby_prefix(weights):
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 163, in _groupby_prefix
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]     for prefix, group in itertools.groupby(weights_by_parts, key=lambda x: x[0][0]):
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 160, in <genexpr>
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]     for weight_name, weight_data in weights
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]                                     ^^^^^^^
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 332, in <genexpr>
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]     (name, weight) for name, weight in weights if not self._can_skip(name)
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]                                        ^^^^^^^
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 260, in get_all_weights
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]     yield from self._get_weights_iterator(primary_weights)
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 189, in _get_weights_iterator
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]     hf_folder, hf_weights_files, use_safetensors = self._prepare_weights(
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]                                                    ^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 171, in _prepare_weights
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]     hf_weights_files = filter_duplicate_safetensors_files(
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py", line 570, in filter_duplicate_safetensors_files
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]     weight_map = json.load(f)["weight_map"]
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]                  ^^^^^^^^^^^^
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/json/__init__.py", line 293, in load
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]     return loads(fp.read(),
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/json/__init__.py", line 346, in loads
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]     return _default_decoder.decode(s)
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/json/decoder.py", line 338, in decode
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]     obj, end = self.raw_decode(s, idx=_w(s, 0).end())
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/json/decoder.py", line 354, in raw_decode
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]     obj, end = self.scan_once(s, idx)
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766]                ^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:39:03] [ERROR] [0;36m(Worker_TP0 pid=35539)[0;0m ERROR 01-18 16:39:03 [multiproc_executor.py:766] json.decoder.JSONDecodeError: Unterminated string starting at: line 41299 column 59 (char 3919835)
[2026-01-18 16:39:03] [INFO] [0;36m(Worker_TP0 pid=35539)[0;0m INFO 01-18 16:39:03 [multiproc_executor.py:724] Parent process exited, terminating worker
[2026-01-18 16:39:03] [INFO] [0;36m(Worker_TP1 pid=35540)[0;0m INFO 01-18 16:39:03 [multiproc_executor.py:724] Parent process exited, terminating worker
[2026-01-18 16:39:03] [WARNING] [rank0]:[W118 16:39:03.240583239 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2026-01-18 16:39:05] [ERROR] [0;36m(EngineCore_DP0 pid=35301)[0;0m ERROR 01-18 16:39:05 [core.py:935] EngineCore failed to start.
[2026-01-18 16:39:05] [ERROR] [0;36m(EngineCore_DP0 pid=35301)[0;0m ERROR 01-18 16:39:05 [core.py:935] Traceback (most recent call last):
[2026-01-18 16:39:05] [ERROR] [0;36m(EngineCore_DP0 pid=35301)[0;0m ERROR 01-18 16:39:05 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-18 16:39:05] [ERROR] [0;36m(EngineCore_DP0 pid=35301)[0;0m ERROR 01-18 16:39:05 [core.py:935]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 16:39:05] [ERROR] [0;36m(EngineCore_DP0 pid=35301)[0;0m ERROR 01-18 16:39:05 [core.py:935]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:39:05] [ERROR] [0;36m(EngineCore_DP0 pid=35301)[0;0m ERROR 01-18 16:39:05 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-18 16:39:05] [ERROR] [0;36m(EngineCore_DP0 pid=35301)[0;0m ERROR 01-18 16:39:05 [core.py:935]     super().__init__(
[2026-01-18 16:39:05] [ERROR] [0;36m(EngineCore_DP0 pid=35301)[0;0m ERROR 01-18 16:39:05 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[2026-01-18 16:39:05] [ERROR] [0;36m(EngineCore_DP0 pid=35301)[0;0m ERROR 01-18 16:39:05 [core.py:935]     self.model_executor = executor_class(vllm_config)
[2026-01-18 16:39:05] [ERROR] [0;36m(EngineCore_DP0 pid=35301)[0;0m ERROR 01-18 16:39:05 [core.py:935]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:39:05] [ERROR] [0;36m(EngineCore_DP0 pid=35301)[0;0m ERROR 01-18 16:39:05 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[2026-01-18 16:39:05] [ERROR] [0;36m(EngineCore_DP0 pid=35301)[0;0m ERROR 01-18 16:39:05 [core.py:935]     super().__init__(vllm_config)
[2026-01-18 16:39:05] [ERROR] [0;36m(EngineCore_DP0 pid=35301)[0;0m ERROR 01-18 16:39:05 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[2026-01-18 16:39:05] [ERROR] [0;36m(EngineCore_DP0 pid=35301)[0;0m ERROR 01-18 16:39:05 [core.py:935]     self._init_executor()
[2026-01-18 16:39:05] [ERROR] [0;36m(EngineCore_DP0 pid=35301)[0;0m ERROR 01-18 16:39:05 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[2026-01-18 16:39:05] [ERROR] [0;36m(EngineCore_DP0 pid=35301)[0;0m ERROR 01-18 16:39:05 [core.py:935]     self.workers = WorkerProc.wait_for_ready(unready_workers)
[2026-01-18 16:39:05] [ERROR] [0;36m(EngineCore_DP0 pid=35301)[0;0m ERROR 01-18 16:39:05 [core.py:935]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:39:05] [ERROR] [0;36m(EngineCore_DP0 pid=35301)[0;0m ERROR 01-18 16:39:05 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 675, in wait_for_ready
[2026-01-18 16:39:05] [ERROR] [0;36m(EngineCore_DP0 pid=35301)[0;0m ERROR 01-18 16:39:05 [core.py:935]     raise e from None
[2026-01-18 16:39:05] [ERROR] [0;36m(EngineCore_DP0 pid=35301)[0;0m ERROR 01-18 16:39:05 [core.py:935] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[2026-01-18 16:39:05] [INFO] [0;36m(EngineCore_DP0 pid=35301)[0;0m Process EngineCore_DP0:
[2026-01-18 16:39:05] [ERROR] [0;36m(EngineCore_DP0 pid=35301)[0;0m Traceback (most recent call last):
[2026-01-18 16:39:05] [INFO] [0;36m(EngineCore_DP0 pid=35301)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[2026-01-18 16:39:05] [INFO] [0;36m(EngineCore_DP0 pid=35301)[0;0m     self.run()
[2026-01-18 16:39:05] [INFO] [0;36m(EngineCore_DP0 pid=35301)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/process.py", line 108, in run
[2026-01-18 16:39:05] [INFO] [0;36m(EngineCore_DP0 pid=35301)[0;0m     self._target(*self._args, **self._kwargs)
[2026-01-18 16:39:05] [INFO] [0;36m(EngineCore_DP0 pid=35301)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 939, in run_engine_core
[2026-01-18 16:39:05] [INFO] [0;36m(EngineCore_DP0 pid=35301)[0;0m     raise e
[2026-01-18 16:39:05] [INFO] [0;36m(EngineCore_DP0 pid=35301)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-18 16:39:05] [INFO] [0;36m(EngineCore_DP0 pid=35301)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 16:39:05] [INFO] [0;36m(EngineCore_DP0 pid=35301)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:39:05] [INFO] [0;36m(EngineCore_DP0 pid=35301)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-18 16:39:05] [INFO] [0;36m(EngineCore_DP0 pid=35301)[0;0m     super().__init__(
[2026-01-18 16:39:05] [INFO] [0;36m(EngineCore_DP0 pid=35301)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[2026-01-18 16:39:05] [INFO] [0;36m(EngineCore_DP0 pid=35301)[0;0m     self.model_executor = executor_class(vllm_config)
[2026-01-18 16:39:05] [INFO] [0;36m(EngineCore_DP0 pid=35301)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:39:05] [INFO] [0;36m(EngineCore_DP0 pid=35301)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[2026-01-18 16:39:05] [INFO] [0;36m(EngineCore_DP0 pid=35301)[0;0m     super().__init__(vllm_config)
[2026-01-18 16:39:05] [INFO] [0;36m(EngineCore_DP0 pid=35301)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[2026-01-18 16:39:05] [INFO] [0;36m(EngineCore_DP0 pid=35301)[0;0m     self._init_executor()
[2026-01-18 16:39:05] [INFO] [0;36m(EngineCore_DP0 pid=35301)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[2026-01-18 16:39:05] [INFO] [0;36m(EngineCore_DP0 pid=35301)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
[2026-01-18 16:39:05] [INFO] [0;36m(EngineCore_DP0 pid=35301)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:39:05] [INFO] [0;36m(EngineCore_DP0 pid=35301)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 675, in wait_for_ready
[2026-01-18 16:39:05] [INFO] [0;36m(EngineCore_DP0 pid=35301)[0;0m     raise e from None
[2026-01-18 16:39:05] [INFO] [0;36m(EngineCore_DP0 pid=35301)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[2026-01-18 16:39:06] [ERROR] [0;36m(APIServer pid=34991)[0;0m Traceback (most recent call last):
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/bin/vllm", line 7, in <module>
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m     sys.exit(main())
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m              ^^^^^^
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m     args.dispatch_function(args)
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m     uvloop.run(run_server(args))
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m     return __asyncio.run(
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m            ^^^^^^^^^^^^^^
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m     return runner.run(main)
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m     return self._loop.run_until_complete(task)
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m     return await main
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m            ^^^^^^^^^^
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m     async with build_async_engine_client(
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m     return await anext(self.gen)
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 146, in build_async_engine_client
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m     return await anext(self.gen)
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 187, in build_async_engine_client_from_engine_args
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 202, in from_vllm_config
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m     return cls(
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m            ^^^^
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 129, in __init__
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m     return AsyncMPClient(*client_args)
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m     super().__init__(
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 144, in __exit__
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m     next(self.gen)
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 918, in launch_core_engines
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m     wait_for_engine_startup(
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 977, in wait_for_engine_startup
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m     raise RuntimeError(
[2026-01-18 16:39:06] [INFO] [0;36m(APIServer pid=34991)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[2026-01-18 16:39:07] [INFO] /mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/resource_tracker.py:279: UserWarning: resource_tracker: There appear to be 3 leaked shared_memory objects to clean up at shutdown
[2026-01-18 16:39:07] [INFO] warnings.warn('resource_tracker: There appear to be %d '
[2026-01-18 16:39:07] [INFO] nvitop监控已启动
[2026-01-18 16:43:35] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 16:43:35] [INFO] nvitop监控已启动
[2026-01-18 16:43:35] [INFO] nvitop监控已停止
[2026-01-18 16:44:05] [INFO] [0;36m(APIServer pid=37436)[0;0m INFO 01-18 16:44:05 [api_server.py:872] vLLM API server version 0.14.0rc2.dev117+g9e078d058
[2026-01-18 16:44:05] [INFO] [0;36m(APIServer pid=37436)[0;0m INFO 01-18 16:44:05 [utils.py:267] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/chat_template.jinja', 'enable_auto_tool_choice': True, 'tool_call_parser': 'deepseek_v3', 'model': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/', 'trust_remote_code': True, 'max_model_len': 128000, 'served_model_name': ['VLLM-MODEL'], 'reasoning_parser': 'deepseek_v3', 'tensor_parallel_size': 2, 'kv_cache_dtype': 'fp8', 'max_num_seqs': 256, 'enable_chunked_prefill': False, 'async_scheduling': True, 'compilation_config': {'level': None, 'mode': None, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': [], 'splitting_ops': None, 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': None, 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.PIECEWISE: 1>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': None, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}}
[2026-01-18 16:44:05] [INFO] [0;36m(APIServer pid=37436)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 16:44:05] [INFO] [0;36m(APIServer pid=37436)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 16:44:05] [INFO] [0;36m(APIServer pid=37436)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 16:44:05] [INFO] [0;36m(APIServer pid=37436)[0;0m Unrecognized keys in `rope_parameters` for 'rope_type'='default': {'partial_rotary_factor'}
[2026-01-18 16:44:05] [INFO] [0;36m(APIServer pid=37436)[0;0m Unrecognized keys in `rope_parameters` for 'rope_type'='default': {'partial_rotary_factor'}
[2026-01-18 16:44:05] [INFO] [0;36m(APIServer pid=37436)[0;0m INFO 01-18 16:44:05 [model.py:530] Resolved architecture: Qwen3NextForCausalLM
[2026-01-18 16:44:05] [INFO] [0;36m(APIServer pid=37436)[0;0m INFO 01-18 16:44:05 [model.py:1547] Using max model len 128000
[2026-01-18 16:44:05] [WARNING] [0;36m(APIServer pid=37436)[0;0m WARNING 01-18 16:44:05 [arg_utils.py:1913] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[2026-01-18 16:44:05] [INFO] [0;36m(APIServer pid=37436)[0;0m INFO 01-18 16:44:05 [cache.py:206] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor.
[2026-01-18 16:44:06] [INFO] [0;36m(APIServer pid=37436)[0;0m INFO 01-18 16:44:06 [config.py:476] Setting attention block size to 1072 tokens to ensure that attention page size is >= mamba page size.
[2026-01-18 16:44:06] [INFO] [0;36m(APIServer pid=37436)[0;0m INFO 01-18 16:44:06 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 16:44:06] [INFO] [0;36m(APIServer pid=37436)[0;0m INFO 01-18 16:44:06 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-18 16:44:34] [INFO] [0;36m(EngineCore_DP0 pid=37763)[0;0m INFO 01-18 16:44:34 [core.py:96] Initializing a V1 LLM engine (v0.14.0rc2.dev117+g9e078d058) with config: model='/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/', speculative_config=None, tokenizer='/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=fp8, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='deepseek_v3', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=False, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [128000], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.PIECEWISE: 1>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-18 16:44:34] [WARNING] [0;36m(EngineCore_DP0 pid=37763)[0;0m WARNING 01-18 16:44:34 [multiproc_executor.py:897] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-18 16:45:03] [INFO] INFO 01-18 16:45:03 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:59051 backend=nccl
[2026-01-18 16:45:03] [INFO] INFO 01-18 16:45:03 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:59051 backend=nccl
[2026-01-18 16:45:04] [INFO] INFO 01-18 16:45:04 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-18 16:45:04] [WARNING] WARNING 01-18 16:45:04 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 16:45:04] [WARNING] WARNING 01-18 16:45:04 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 16:45:04] [INFO] INFO 01-18 16:45:04 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[2026-01-18 16:45:05] [INFO] INFO 01-18 16:45:04 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
[2026-01-18 16:45:06] [INFO] [0;36m(Worker_TP0 pid=38004)[0;0m INFO 01-18 16:45:06 [gpu_model_runner.py:3803] Starting to load model /mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/...
[2026-01-18 16:45:10] [INFO] [0;36m(Worker_TP0 pid=38004)[0;0m INFO 01-18 16:45:10 [unquantized.py:99] Using TRITON backend for Unquantized MoE
[2026-01-18 16:45:11] [INFO] [0;36m(Worker_TP0 pid=38004)[0;0m INFO 01-18 16:45:11 [cuda.py:351] Using FLASHINFER attention backend out of potential backends: ('FLASHINFER', 'TRITON_ATTN')
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766] WorkerProc failed to start.
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766] Traceback (most recent call last):
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 737, in worker_main
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]     worker = WorkerProc(*args, **kwargs)
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 575, in __init__
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]     self.worker.load_model()
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 274, in load_model
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3822, in load_model
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]     self.model = model_loader.load_model(
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 58, in load_model
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]     self.load_weights(model, model_config)
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 288, in load_weights
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_next.py", line 1294, in load_weights
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]     return loader.load_weights(weights)
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 335, in load_weights
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]     autoloaded_weights = set(self._load_module("", self.module, weights))
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 279, in _load_module
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]     for child_prefix, child_weights in self._groupby_prefix(weights):
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 163, in _groupby_prefix
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]     for prefix, group in itertools.groupby(weights_by_parts, key=lambda x: x[0][0]):
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 160, in <genexpr>
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]     for weight_name, weight_data in weights
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]                                     ^^^^^^^
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 332, in <genexpr>
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]     (name, weight) for name, weight in weights if not self._can_skip(name)
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]                                        ^^^^^^^
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 260, in get_all_weights
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]     yield from self._get_weights_iterator(primary_weights)
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 189, in _get_weights_iterator
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]     hf_folder, hf_weights_files, use_safetensors = self._prepare_weights(
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]                                                    ^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 171, in _prepare_weights
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]     hf_weights_files = filter_duplicate_safetensors_files(
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py", line 570, in filter_duplicate_safetensors_files
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]     weight_map = json.load(f)["weight_map"]
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]                  ^^^^^^^^^^^^
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/json/__init__.py", line 293, in load
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]     return loads(fp.read(),
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/json/__init__.py", line 346, in loads
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]     return _default_decoder.decode(s)
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/json/decoder.py", line 338, in decode
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]     obj, end = self.raw_decode(s, idx=_w(s, 0).end())
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/json/decoder.py", line 354, in raw_decode
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]     obj, end = self.scan_once(s, idx)
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766]                ^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:45:11] [ERROR] [0;36m(Worker_TP1 pid=38005)[0;0m ERROR 01-18 16:45:11 [multiproc_executor.py:766] json.decoder.JSONDecodeError: Unterminated string starting at: line 41299 column 59 (char 3919835)
[2026-01-18 16:45:11] [INFO] [0;36m(Worker_TP1 pid=38005)[0;0m INFO 01-18 16:45:11 [multiproc_executor.py:724] Parent process exited, terminating worker
[2026-01-18 16:45:11] [INFO] [0;36m(Worker_TP0 pid=38004)[0;0m INFO 01-18 16:45:11 [multiproc_executor.py:724] Parent process exited, terminating worker
[2026-01-18 16:45:12] [WARNING] [rank0]:[W118 16:45:12.865061332 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2026-01-18 16:45:13] [ERROR] [0;36m(EngineCore_DP0 pid=37763)[0;0m ERROR 01-18 16:45:13 [core.py:935] EngineCore failed to start.
[2026-01-18 16:45:13] [ERROR] [0;36m(EngineCore_DP0 pid=37763)[0;0m ERROR 01-18 16:45:13 [core.py:935] Traceback (most recent call last):
[2026-01-18 16:45:13] [ERROR] [0;36m(EngineCore_DP0 pid=37763)[0;0m ERROR 01-18 16:45:13 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-18 16:45:13] [ERROR] [0;36m(EngineCore_DP0 pid=37763)[0;0m ERROR 01-18 16:45:13 [core.py:935]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 16:45:13] [ERROR] [0;36m(EngineCore_DP0 pid=37763)[0;0m ERROR 01-18 16:45:13 [core.py:935]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:45:13] [ERROR] [0;36m(EngineCore_DP0 pid=37763)[0;0m ERROR 01-18 16:45:13 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-18 16:45:13] [ERROR] [0;36m(EngineCore_DP0 pid=37763)[0;0m ERROR 01-18 16:45:13 [core.py:935]     super().__init__(
[2026-01-18 16:45:13] [ERROR] [0;36m(EngineCore_DP0 pid=37763)[0;0m ERROR 01-18 16:45:13 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[2026-01-18 16:45:13] [ERROR] [0;36m(EngineCore_DP0 pid=37763)[0;0m ERROR 01-18 16:45:13 [core.py:935]     self.model_executor = executor_class(vllm_config)
[2026-01-18 16:45:13] [ERROR] [0;36m(EngineCore_DP0 pid=37763)[0;0m ERROR 01-18 16:45:13 [core.py:935]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:45:13] [ERROR] [0;36m(EngineCore_DP0 pid=37763)[0;0m ERROR 01-18 16:45:13 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[2026-01-18 16:45:13] [ERROR] [0;36m(EngineCore_DP0 pid=37763)[0;0m ERROR 01-18 16:45:13 [core.py:935]     super().__init__(vllm_config)
[2026-01-18 16:45:13] [ERROR] [0;36m(EngineCore_DP0 pid=37763)[0;0m ERROR 01-18 16:45:13 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[2026-01-18 16:45:13] [ERROR] [0;36m(EngineCore_DP0 pid=37763)[0;0m ERROR 01-18 16:45:13 [core.py:935]     self._init_executor()
[2026-01-18 16:45:13] [ERROR] [0;36m(EngineCore_DP0 pid=37763)[0;0m ERROR 01-18 16:45:13 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[2026-01-18 16:45:13] [ERROR] [0;36m(EngineCore_DP0 pid=37763)[0;0m ERROR 01-18 16:45:13 [core.py:935]     self.workers = WorkerProc.wait_for_ready(unready_workers)
[2026-01-18 16:45:13] [ERROR] [0;36m(EngineCore_DP0 pid=37763)[0;0m ERROR 01-18 16:45:13 [core.py:935]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:45:13] [ERROR] [0;36m(EngineCore_DP0 pid=37763)[0;0m ERROR 01-18 16:45:13 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 675, in wait_for_ready
[2026-01-18 16:45:13] [ERROR] [0;36m(EngineCore_DP0 pid=37763)[0;0m ERROR 01-18 16:45:13 [core.py:935]     raise e from None
[2026-01-18 16:45:13] [ERROR] [0;36m(EngineCore_DP0 pid=37763)[0;0m ERROR 01-18 16:45:13 [core.py:935] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[2026-01-18 16:45:13] [INFO] [0;36m(EngineCore_DP0 pid=37763)[0;0m Process EngineCore_DP0:
[2026-01-18 16:45:13] [ERROR] [0;36m(EngineCore_DP0 pid=37763)[0;0m Traceback (most recent call last):
[2026-01-18 16:45:13] [INFO] [0;36m(EngineCore_DP0 pid=37763)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[2026-01-18 16:45:13] [INFO] [0;36m(EngineCore_DP0 pid=37763)[0;0m     self.run()
[2026-01-18 16:45:13] [INFO] [0;36m(EngineCore_DP0 pid=37763)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/process.py", line 108, in run
[2026-01-18 16:45:13] [INFO] [0;36m(EngineCore_DP0 pid=37763)[0;0m     self._target(*self._args, **self._kwargs)
[2026-01-18 16:45:13] [INFO] [0;36m(EngineCore_DP0 pid=37763)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 939, in run_engine_core
[2026-01-18 16:45:13] [INFO] [0;36m(EngineCore_DP0 pid=37763)[0;0m     raise e
[2026-01-18 16:45:13] [INFO] [0;36m(EngineCore_DP0 pid=37763)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-18 16:45:13] [INFO] [0;36m(EngineCore_DP0 pid=37763)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 16:45:13] [INFO] [0;36m(EngineCore_DP0 pid=37763)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:45:13] [INFO] [0;36m(EngineCore_DP0 pid=37763)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-18 16:45:13] [INFO] [0;36m(EngineCore_DP0 pid=37763)[0;0m     super().__init__(
[2026-01-18 16:45:13] [INFO] [0;36m(EngineCore_DP0 pid=37763)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[2026-01-18 16:45:13] [INFO] [0;36m(EngineCore_DP0 pid=37763)[0;0m     self.model_executor = executor_class(vllm_config)
[2026-01-18 16:45:13] [INFO] [0;36m(EngineCore_DP0 pid=37763)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:45:13] [INFO] [0;36m(EngineCore_DP0 pid=37763)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[2026-01-18 16:45:13] [INFO] [0;36m(EngineCore_DP0 pid=37763)[0;0m     super().__init__(vllm_config)
[2026-01-18 16:45:13] [INFO] [0;36m(EngineCore_DP0 pid=37763)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[2026-01-18 16:45:13] [INFO] [0;36m(EngineCore_DP0 pid=37763)[0;0m     self._init_executor()
[2026-01-18 16:45:13] [INFO] [0;36m(EngineCore_DP0 pid=37763)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[2026-01-18 16:45:13] [INFO] [0;36m(EngineCore_DP0 pid=37763)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
[2026-01-18 16:45:13] [INFO] [0;36m(EngineCore_DP0 pid=37763)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:45:13] [INFO] [0;36m(EngineCore_DP0 pid=37763)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 675, in wait_for_ready
[2026-01-18 16:45:13] [INFO] [0;36m(EngineCore_DP0 pid=37763)[0;0m     raise e from None
[2026-01-18 16:45:13] [INFO] [0;36m(EngineCore_DP0 pid=37763)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[2026-01-18 16:45:14] [ERROR] [0;36m(APIServer pid=37436)[0;0m Traceback (most recent call last):
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/bin/vllm", line 7, in <module>
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m     sys.exit(main())
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m              ^^^^^^
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m     args.dispatch_function(args)
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m     uvloop.run(run_server(args))
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m     return __asyncio.run(
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m            ^^^^^^^^^^^^^^
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m     return runner.run(main)
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m     return self._loop.run_until_complete(task)
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m     return await main
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m            ^^^^^^^^^^
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m     async with build_async_engine_client(
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m     return await anext(self.gen)
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 146, in build_async_engine_client
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m     return await anext(self.gen)
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 187, in build_async_engine_client_from_engine_args
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 202, in from_vllm_config
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m     return cls(
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m            ^^^^
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 129, in __init__
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m     return AsyncMPClient(*client_args)
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m     super().__init__(
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 144, in __exit__
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m     next(self.gen)
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 918, in launch_core_engines
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m     wait_for_engine_startup(
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 977, in wait_for_engine_startup
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m     raise RuntimeError(
[2026-01-18 16:45:14] [INFO] [0;36m(APIServer pid=37436)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[2026-01-18 16:45:15] [INFO] /mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/resource_tracker.py:279: UserWarning: resource_tracker: There appear to be 3 leaked shared_memory objects to clean up at shutdown
[2026-01-18 16:45:15] [INFO] warnings.warn('resource_tracker: There appear to be %d '
[2026-01-18 16:45:15] [INFO] nvitop监控已启动
[2026-01-18 16:45:15] [INFO] 服务已停止
[2026-01-18 16:45:16] [INFO] nvitop监控已启动
[2026-01-18 16:45:17] [INFO] nvitop监控已启动
[2026-01-18 16:45:32] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 16:45:32] [INFO] nvitop监控已启动
[2026-01-18 16:45:32] [INFO] nvitop监控已停止
[2026-01-18 16:46:01] [INFO] [0;36m(APIServer pid=38495)[0;0m INFO 01-18 16:46:01 [api_server.py:872] vLLM API server version 0.14.0rc2.dev117+g9e078d058
[2026-01-18 16:46:01] [INFO] [0;36m(APIServer pid=38495)[0;0m INFO 01-18 16:46:01 [utils.py:267] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/chat_template.jinja', 'enable_auto_tool_choice': True, 'tool_call_parser': 'deepseek_v3', 'model': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/', 'trust_remote_code': True, 'max_model_len': 128000, 'served_model_name': ['VLLM-MODEL'], 'reasoning_parser': 'deepseek_v3', 'tensor_parallel_size': 2, 'kv_cache_dtype': 'fp8', 'max_num_seqs': 256, 'async_scheduling': True, 'compilation_config': {'level': None, 'mode': None, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': [], 'splitting_ops': None, 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': None, 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.PIECEWISE: 1>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': None, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}}
[2026-01-18 16:46:01] [INFO] [0;36m(APIServer pid=38495)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 16:46:01] [INFO] [0;36m(APIServer pid=38495)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 16:46:01] [INFO] [0;36m(APIServer pid=38495)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 16:46:01] [INFO] [0;36m(APIServer pid=38495)[0;0m Unrecognized keys in `rope_parameters` for 'rope_type'='default': {'partial_rotary_factor'}
[2026-01-18 16:46:01] [INFO] [0;36m(APIServer pid=38495)[0;0m Unrecognized keys in `rope_parameters` for 'rope_type'='default': {'partial_rotary_factor'}
[2026-01-18 16:46:01] [INFO] [0;36m(APIServer pid=38495)[0;0m INFO 01-18 16:46:01 [model.py:530] Resolved architecture: Qwen3NextForCausalLM
[2026-01-18 16:46:01] [INFO] [0;36m(APIServer pid=38495)[0;0m INFO 01-18 16:46:01 [model.py:1547] Using max model len 128000
[2026-01-18 16:46:01] [INFO] [0;36m(APIServer pid=38495)[0;0m INFO 01-18 16:46:01 [cache.py:206] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor.
[2026-01-18 16:46:02] [INFO] [0;36m(APIServer pid=38495)[0;0m INFO 01-18 16:46:02 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[2026-01-18 16:46:03] [INFO] [0;36m(APIServer pid=38495)[0;0m INFO 01-18 16:46:03 [config.py:476] Setting attention block size to 1072 tokens to ensure that attention page size is >= mamba page size.
[2026-01-18 16:46:03] [INFO] [0;36m(APIServer pid=38495)[0;0m INFO 01-18 16:46:03 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 16:46:03] [INFO] [0;36m(APIServer pid=38495)[0;0m INFO 01-18 16:46:03 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-18 16:46:33] [INFO] [0;36m(EngineCore_DP0 pid=38845)[0;0m INFO 01-18 16:46:33 [core.py:96] Initializing a V1 LLM engine (v0.14.0rc2.dev117+g9e078d058) with config: model='/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/', speculative_config=None, tokenizer='/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=fp8, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='deepseek_v3', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.PIECEWISE: 1>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-18 16:46:33] [WARNING] [0;36m(EngineCore_DP0 pid=38845)[0;0m WARNING 01-18 16:46:33 [multiproc_executor.py:897] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-18 16:46:46] [SUCCESS] 方案已删除: Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated
[2026-01-18 16:47:03] [INFO] INFO 01-18 16:47:03 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:43415 backend=nccl
[2026-01-18 16:47:03] [INFO] INFO 01-18 16:47:03 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:43415 backend=nccl
[2026-01-18 16:47:03] [INFO] INFO 01-18 16:47:03 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-18 16:47:04] [WARNING] WARNING 01-18 16:47:04 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 16:47:04] [WARNING] WARNING 01-18 16:47:04 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 16:47:04] [INFO] INFO 01-18 16:47:04 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
[2026-01-18 16:47:04] [INFO] INFO 01-18 16:47:04 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[2026-01-18 16:47:05] [INFO] [0;36m(Worker_TP0 pid=39099)[0;0m INFO 01-18 16:47:05 [gpu_model_runner.py:3803] Starting to load model /mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/...
[2026-01-18 16:47:09] [INFO] [0;36m(Worker_TP0 pid=39099)[0;0m INFO 01-18 16:47:09 [unquantized.py:99] Using TRITON backend for Unquantized MoE
[2026-01-18 16:47:10] [INFO] [0;36m(Worker_TP0 pid=39099)[0;0m INFO 01-18 16:47:10 [cuda.py:351] Using FLASHINFER attention backend out of potential backends: ('FLASHINFER', 'TRITON_ATTN')
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766] WorkerProc failed to start.
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766] Traceback (most recent call last):
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 737, in worker_main
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]     worker = WorkerProc(*args, **kwargs)
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 575, in __init__
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]     self.worker.load_model()
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 274, in load_model
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3822, in load_model
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]     self.model = model_loader.load_model(
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 58, in load_model
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]     self.load_weights(model, model_config)
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 288, in load_weights
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_next.py", line 1294, in load_weights
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]     return loader.load_weights(weights)
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 335, in load_weights
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]     autoloaded_weights = set(self._load_module("", self.module, weights))
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 279, in _load_module
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]     for child_prefix, child_weights in self._groupby_prefix(weights):
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 163, in _groupby_prefix
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]     for prefix, group in itertools.groupby(weights_by_parts, key=lambda x: x[0][0]):
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 160, in <genexpr>
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]     for weight_name, weight_data in weights
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]                                     ^^^^^^^
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 332, in <genexpr>
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]     (name, weight) for name, weight in weights if not self._can_skip(name)
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]                                        ^^^^^^^
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 260, in get_all_weights
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]     yield from self._get_weights_iterator(primary_weights)
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 189, in _get_weights_iterator
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]     hf_folder, hf_weights_files, use_safetensors = self._prepare_weights(
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]                                                    ^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 171, in _prepare_weights
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]     hf_weights_files = filter_duplicate_safetensors_files(
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py", line 570, in filter_duplicate_safetensors_files
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]     weight_map = json.load(f)["weight_map"]
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]                  ^^^^^^^^^^^^
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/json/__init__.py", line 293, in load
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]     return loads(fp.read(),
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/json/__init__.py", line 346, in loads
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]     return _default_decoder.decode(s)
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/json/decoder.py", line 338, in decode
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]     obj, end = self.raw_decode(s, idx=_w(s, 0).end())
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/json/decoder.py", line 354, in raw_decode
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]     obj, end = self.scan_once(s, idx)
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766]                ^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:47:10] [ERROR] [0;36m(Worker_TP1 pid=39100)[0;0m ERROR 01-18 16:47:10 [multiproc_executor.py:766] json.decoder.JSONDecodeError: Unterminated string starting at: line 41299 column 59 (char 3919835)
[2026-01-18 16:47:10] [INFO] [0;36m(Worker_TP1 pid=39100)[0;0m INFO 01-18 16:47:10 [multiproc_executor.py:724] Parent process exited, terminating worker
[2026-01-18 16:47:10] [INFO] [0;36m(Worker_TP0 pid=39099)[0;0m INFO 01-18 16:47:10 [multiproc_executor.py:724] Parent process exited, terminating worker
[2026-01-18 16:47:11] [WARNING] [rank0]:[W118 16:47:11.754294588 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2026-01-18 16:47:12] [ERROR] [0;36m(EngineCore_DP0 pid=38845)[0;0m ERROR 01-18 16:47:12 [core.py:935] EngineCore failed to start.
[2026-01-18 16:47:12] [ERROR] [0;36m(EngineCore_DP0 pid=38845)[0;0m ERROR 01-18 16:47:12 [core.py:935] Traceback (most recent call last):
[2026-01-18 16:47:12] [ERROR] [0;36m(EngineCore_DP0 pid=38845)[0;0m ERROR 01-18 16:47:12 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-18 16:47:12] [ERROR] [0;36m(EngineCore_DP0 pid=38845)[0;0m ERROR 01-18 16:47:12 [core.py:935]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 16:47:12] [ERROR] [0;36m(EngineCore_DP0 pid=38845)[0;0m ERROR 01-18 16:47:12 [core.py:935]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:47:12] [ERROR] [0;36m(EngineCore_DP0 pid=38845)[0;0m ERROR 01-18 16:47:12 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-18 16:47:12] [ERROR] [0;36m(EngineCore_DP0 pid=38845)[0;0m ERROR 01-18 16:47:12 [core.py:935]     super().__init__(
[2026-01-18 16:47:12] [ERROR] [0;36m(EngineCore_DP0 pid=38845)[0;0m ERROR 01-18 16:47:12 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[2026-01-18 16:47:12] [ERROR] [0;36m(EngineCore_DP0 pid=38845)[0;0m ERROR 01-18 16:47:12 [core.py:935]     self.model_executor = executor_class(vllm_config)
[2026-01-18 16:47:12] [ERROR] [0;36m(EngineCore_DP0 pid=38845)[0;0m ERROR 01-18 16:47:12 [core.py:935]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:47:12] [ERROR] [0;36m(EngineCore_DP0 pid=38845)[0;0m ERROR 01-18 16:47:12 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[2026-01-18 16:47:12] [ERROR] [0;36m(EngineCore_DP0 pid=38845)[0;0m ERROR 01-18 16:47:12 [core.py:935]     super().__init__(vllm_config)
[2026-01-18 16:47:12] [ERROR] [0;36m(EngineCore_DP0 pid=38845)[0;0m ERROR 01-18 16:47:12 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[2026-01-18 16:47:12] [ERROR] [0;36m(EngineCore_DP0 pid=38845)[0;0m ERROR 01-18 16:47:12 [core.py:935]     self._init_executor()
[2026-01-18 16:47:12] [ERROR] [0;36m(EngineCore_DP0 pid=38845)[0;0m ERROR 01-18 16:47:12 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[2026-01-18 16:47:12] [ERROR] [0;36m(EngineCore_DP0 pid=38845)[0;0m ERROR 01-18 16:47:12 [core.py:935]     self.workers = WorkerProc.wait_for_ready(unready_workers)
[2026-01-18 16:47:12] [ERROR] [0;36m(EngineCore_DP0 pid=38845)[0;0m ERROR 01-18 16:47:12 [core.py:935]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:47:12] [ERROR] [0;36m(EngineCore_DP0 pid=38845)[0;0m ERROR 01-18 16:47:12 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 675, in wait_for_ready
[2026-01-18 16:47:12] [ERROR] [0;36m(EngineCore_DP0 pid=38845)[0;0m ERROR 01-18 16:47:12 [core.py:935]     raise e from None
[2026-01-18 16:47:12] [ERROR] [0;36m(EngineCore_DP0 pid=38845)[0;0m ERROR 01-18 16:47:12 [core.py:935] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[2026-01-18 16:47:12] [INFO] [0;36m(EngineCore_DP0 pid=38845)[0;0m Process EngineCore_DP0:
[2026-01-18 16:47:12] [ERROR] [0;36m(EngineCore_DP0 pid=38845)[0;0m Traceback (most recent call last):
[2026-01-18 16:47:12] [INFO] [0;36m(EngineCore_DP0 pid=38845)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[2026-01-18 16:47:12] [INFO] [0;36m(EngineCore_DP0 pid=38845)[0;0m     self.run()
[2026-01-18 16:47:12] [INFO] [0;36m(EngineCore_DP0 pid=38845)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/process.py", line 108, in run
[2026-01-18 16:47:12] [INFO] [0;36m(EngineCore_DP0 pid=38845)[0;0m     self._target(*self._args, **self._kwargs)
[2026-01-18 16:47:12] [INFO] [0;36m(EngineCore_DP0 pid=38845)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 939, in run_engine_core
[2026-01-18 16:47:12] [INFO] [0;36m(EngineCore_DP0 pid=38845)[0;0m     raise e
[2026-01-18 16:47:12] [INFO] [0;36m(EngineCore_DP0 pid=38845)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-18 16:47:12] [INFO] [0;36m(EngineCore_DP0 pid=38845)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 16:47:12] [INFO] [0;36m(EngineCore_DP0 pid=38845)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:47:12] [INFO] [0;36m(EngineCore_DP0 pid=38845)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-18 16:47:12] [INFO] [0;36m(EngineCore_DP0 pid=38845)[0;0m     super().__init__(
[2026-01-18 16:47:12] [INFO] [0;36m(EngineCore_DP0 pid=38845)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[2026-01-18 16:47:12] [INFO] [0;36m(EngineCore_DP0 pid=38845)[0;0m     self.model_executor = executor_class(vllm_config)
[2026-01-18 16:47:12] [INFO] [0;36m(EngineCore_DP0 pid=38845)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:47:12] [INFO] [0;36m(EngineCore_DP0 pid=38845)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[2026-01-18 16:47:12] [INFO] [0;36m(EngineCore_DP0 pid=38845)[0;0m     super().__init__(vllm_config)
[2026-01-18 16:47:12] [INFO] [0;36m(EngineCore_DP0 pid=38845)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[2026-01-18 16:47:12] [INFO] [0;36m(EngineCore_DP0 pid=38845)[0;0m     self._init_executor()
[2026-01-18 16:47:12] [INFO] [0;36m(EngineCore_DP0 pid=38845)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[2026-01-18 16:47:12] [INFO] [0;36m(EngineCore_DP0 pid=38845)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
[2026-01-18 16:47:12] [INFO] [0;36m(EngineCore_DP0 pid=38845)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:47:12] [INFO] [0;36m(EngineCore_DP0 pid=38845)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 675, in wait_for_ready
[2026-01-18 16:47:12] [INFO] [0;36m(EngineCore_DP0 pid=38845)[0;0m     raise e from None
[2026-01-18 16:47:12] [INFO] [0;36m(EngineCore_DP0 pid=38845)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[2026-01-18 16:47:13] [ERROR] [0;36m(APIServer pid=38495)[0;0m Traceback (most recent call last):
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/bin/vllm", line 7, in <module>
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m     sys.exit(main())
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m              ^^^^^^
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m     args.dispatch_function(args)
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m     uvloop.run(run_server(args))
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m     return __asyncio.run(
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m            ^^^^^^^^^^^^^^
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m     return runner.run(main)
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m     return self._loop.run_until_complete(task)
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m     return await main
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m            ^^^^^^^^^^
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m     async with build_async_engine_client(
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m     return await anext(self.gen)
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 146, in build_async_engine_client
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m     return await anext(self.gen)
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 187, in build_async_engine_client_from_engine_args
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 202, in from_vllm_config
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m     return cls(
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m            ^^^^
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 129, in __init__
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m     return AsyncMPClient(*client_args)
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m     super().__init__(
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 144, in __exit__
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m     next(self.gen)
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 918, in launch_core_engines
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m     wait_for_engine_startup(
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 977, in wait_for_engine_startup
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m     raise RuntimeError(
[2026-01-18 16:47:13] [INFO] [0;36m(APIServer pid=38495)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[2026-01-18 16:47:14] [INFO] /mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/resource_tracker.py:279: UserWarning: resource_tracker: There appear to be 3 leaked shared_memory objects to clean up at shutdown
[2026-01-18 16:47:14] [INFO] warnings.warn('resource_tracker: There appear to be %d '
[2026-01-18 16:47:14] [INFO] nvitop监控已启动
[2026-01-18 16:51:38] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 16:51:38] [INFO] nvitop监控已启动
[2026-01-18 16:51:38] [INFO] nvitop监控已停止
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m INFO 01-18 16:52:07 [api_server.py:872] vLLM API server version 0.14.0rc2.dev117+g9e078d058
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m INFO 01-18 16:52:07 [utils.py:267] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/chat_template.jinja', 'enable_auto_tool_choice': True, 'tool_call_parser': 'deepseek_v3', 'model': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/', 'trust_remote_code': True, 'max_model_len': 128000, 'served_model_name': ['VLLM-MODEL'], 'attention_backend': 'flash_attn2', 'reasoning_parser': 'deepseek_v3', 'tensor_parallel_size': 2, 'max_num_seqs': 256, 'compilation_config': {'level': None, 'mode': None, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': [], 'splitting_ops': None, 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': None, 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.PIECEWISE: 1>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': None, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}}
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m Unrecognized keys in `rope_parameters` for 'rope_type'='default': {'partial_rotary_factor'}
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m Unrecognized keys in `rope_parameters` for 'rope_type'='default': {'partial_rotary_factor'}
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m INFO 01-18 16:52:07 [model.py:530] Resolved architecture: Qwen3NextForCausalLM
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m INFO 01-18 16:52:07 [model.py:1547] Using max model len 128000
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m INFO 01-18 16:52:07 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[2026-01-18 16:52:07] [ERROR] [0;36m(APIServer pid=41108)[0;0m Traceback (most recent call last):
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/bin/vllm", line 7, in <module>
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m     sys.exit(main())
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m              ^^^^^^
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m     args.dispatch_function(args)
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m     uvloop.run(run_server(args))
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m     return __asyncio.run(
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m            ^^^^^^^^^^^^^^
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m     return runner.run(main)
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m     return self._loop.run_until_complete(task)
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m     return await main
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m            ^^^^^^^^^^
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m     async with build_async_engine_client(
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m     return await anext(self.gen)
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 146, in build_async_engine_client
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m     return await anext(self.gen)
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 172, in build_async_engine_client_from_engine_args
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/engine/arg_utils.py", line 1695, in create_engine_config
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m     attention_config.backend = AttentionBackendEnum[
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m                                ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/attention/backends/registry.py", line 28, in __getitem__
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m     raise ValueError(
[2026-01-18 16:52:07] [INFO] [0;36m(APIServer pid=41108)[0;0m ValueError: Unknown attention backend: 'FLASH_ATTN2'. Valid options are: FLASH_ATTN, FLASH_ATTN_DIFFKV, TRITON_ATTN, ROCM_ATTN, ROCM_AITER_MLA, ROCM_AITER_TRITON_MLA, ROCM_AITER_FA, ROCM_AITER_MLA_SPARSE, TORCH_SDPA, FLASHINFER, FLASHINFER_MLA, TRITON_MLA, CUTLASS_MLA, FLASHMLA, FLASHMLA_SPARSE, FLASH_ATTN_MLA, IPEX, NO_ATTENTION, FLEX_ATTENTION, TREE_ATTN, ROCM_AITER_UNIFIED_ATTN, CPU_ATTN, CUSTOM
[2026-01-18 16:52:09] [INFO] nvitop监控已启动
[2026-01-18 16:52:42] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 16:52:42] [INFO] nvitop监控已启动
[2026-01-18 16:52:42] [INFO] nvitop监控已停止
[2026-01-18 16:53:11] [INFO] [0;36m(APIServer pid=41601)[0;0m INFO 01-18 16:53:11 [api_server.py:872] vLLM API server version 0.14.0rc2.dev117+g9e078d058
[2026-01-18 16:53:11] [INFO] [0;36m(APIServer pid=41601)[0;0m INFO 01-18 16:53:11 [utils.py:267] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/chat_template.jinja', 'enable_auto_tool_choice': True, 'tool_call_parser': 'deepseek_v3', 'model': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/', 'trust_remote_code': True, 'max_model_len': 128000, 'served_model_name': ['VLLM-MODEL'], 'attention_backend': 'FLASH_ATTN', 'reasoning_parser': 'deepseek_v3', 'tensor_parallel_size': 2, 'max_num_seqs': 256, 'compilation_config': {'level': None, 'mode': None, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': [], 'splitting_ops': None, 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': None, 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.PIECEWISE: 1>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': None, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}}
[2026-01-18 16:53:11] [INFO] [0;36m(APIServer pid=41601)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 16:53:11] [INFO] [0;36m(APIServer pid=41601)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 16:53:11] [INFO] [0;36m(APIServer pid=41601)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 16:53:11] [INFO] [0;36m(APIServer pid=41601)[0;0m Unrecognized keys in `rope_parameters` for 'rope_type'='default': {'partial_rotary_factor'}
[2026-01-18 16:53:11] [INFO] [0;36m(APIServer pid=41601)[0;0m Unrecognized keys in `rope_parameters` for 'rope_type'='default': {'partial_rotary_factor'}
[2026-01-18 16:53:11] [INFO] [0;36m(APIServer pid=41601)[0;0m INFO 01-18 16:53:11 [model.py:530] Resolved architecture: Qwen3NextForCausalLM
[2026-01-18 16:53:11] [INFO] [0;36m(APIServer pid=41601)[0;0m INFO 01-18 16:53:11 [model.py:1547] Using max model len 128000
[2026-01-18 16:53:11] [INFO] [0;36m(APIServer pid=41601)[0;0m INFO 01-18 16:53:11 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[2026-01-18 16:53:12] [INFO] [0;36m(APIServer pid=41601)[0;0m INFO 01-18 16:53:12 [config.py:476] Setting attention block size to 544 tokens to ensure that attention page size is >= mamba page size.
[2026-01-18 16:53:12] [INFO] [0;36m(APIServer pid=41601)[0;0m INFO 01-18 16:53:12 [config.py:500] Padding mamba page size by 1.49% to ensure that mamba page size and attention page size are exactly equal.
[2026-01-18 16:53:12] [INFO] [0;36m(APIServer pid=41601)[0;0m INFO 01-18 16:53:12 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 16:53:12] [INFO] [0;36m(APIServer pid=41601)[0;0m INFO 01-18 16:53:12 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-18 16:53:40] [INFO] [0;36m(EngineCore_DP0 pid=41919)[0;0m INFO 01-18 16:53:40 [core.py:96] Initializing a V1 LLM engine (v0.14.0rc2.dev117+g9e078d058) with config: model='/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/', speculative_config=None, tokenizer='/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='deepseek_v3', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.PIECEWISE: 1>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-18 16:53:40] [WARNING] [0;36m(EngineCore_DP0 pid=41919)[0;0m WARNING 01-18 16:53:40 [multiproc_executor.py:897] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-18 16:54:10] [INFO] INFO 01-18 16:54:10 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:59459 backend=nccl
[2026-01-18 16:54:10] [INFO] INFO 01-18 16:54:10 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:59459 backend=nccl
[2026-01-18 16:54:10] [INFO] INFO 01-18 16:54:10 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-18 16:54:11] [WARNING] WARNING 01-18 16:54:11 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 16:54:11] [WARNING] WARNING 01-18 16:54:11 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 16:54:11] [INFO] INFO 01-18 16:54:11 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
[2026-01-18 16:54:11] [INFO] INFO 01-18 16:54:11 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[2026-01-18 16:54:12] [INFO] [0;36m(Worker_TP0 pid=42203)[0;0m INFO 01-18 16:54:12 [gpu_model_runner.py:3803] Starting to load model /mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/...
[2026-01-18 16:54:16] [INFO] [0;36m(Worker_TP0 pid=42203)[0;0m INFO 01-18 16:54:16 [unquantized.py:99] Using TRITON backend for Unquantized MoE
[2026-01-18 16:54:16] [INFO] [0;36m(Worker_TP0 pid=42203)[0;0m INFO 01-18 16:54:16 [cuda.py:315] Using AttentionBackendEnum.FLASH_ATTN backend.
[2026-01-18 16:54:16] [INFO] [0;36m(Worker_TP1 pid=42204)[0;0m INFO 01-18 16:54:16 [cuda.py:315] Using AttentionBackendEnum.FLASH_ATTN backend.
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766] WorkerProc failed to start.
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766] Traceback (most recent call last):
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 737, in worker_main
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]     worker = WorkerProc(*args, **kwargs)
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 575, in __init__
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]     self.worker.load_model()
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 274, in load_model
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3822, in load_model
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]     self.model = model_loader.load_model(
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 58, in load_model
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]     self.load_weights(model, model_config)
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 288, in load_weights
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_next.py", line 1294, in load_weights
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]     return loader.load_weights(weights)
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 335, in load_weights
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]     autoloaded_weights = set(self._load_module("", self.module, weights))
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 279, in _load_module
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]     for child_prefix, child_weights in self._groupby_prefix(weights):
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 163, in _groupby_prefix
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]     for prefix, group in itertools.groupby(weights_by_parts, key=lambda x: x[0][0]):
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 160, in <genexpr>
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]     for weight_name, weight_data in weights
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]                                     ^^^^^^^
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 332, in <genexpr>
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]     (name, weight) for name, weight in weights if not self._can_skip(name)
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]                                        ^^^^^^^
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 260, in get_all_weights
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]     yield from self._get_weights_iterator(primary_weights)
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 189, in _get_weights_iterator
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]     hf_folder, hf_weights_files, use_safetensors = self._prepare_weights(
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]                                                    ^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 171, in _prepare_weights
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]     hf_weights_files = filter_duplicate_safetensors_files(
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py", line 570, in filter_duplicate_safetensors_files
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]     weight_map = json.load(f)["weight_map"]
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]                  ^^^^^^^^^^^^
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/json/__init__.py", line 293, in load
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]     return loads(fp.read(),
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/json/__init__.py", line 346, in loads
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]     return _default_decoder.decode(s)
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/json/decoder.py", line 338, in decode
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]     obj, end = self.raw_decode(s, idx=_w(s, 0).end())
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/json/decoder.py", line 354, in raw_decode
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]     obj, end = self.scan_once(s, idx)
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766]                ^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:54:17] [ERROR] [0;36m(Worker_TP0 pid=42203)[0;0m ERROR 01-18 16:54:17 [multiproc_executor.py:766] json.decoder.JSONDecodeError: Unterminated string starting at: line 41299 column 59 (char 3919835)
[2026-01-18 16:54:17] [INFO] [0;36m(Worker_TP0 pid=42203)[0;0m INFO 01-18 16:54:17 [multiproc_executor.py:724] Parent process exited, terminating worker
[2026-01-18 16:54:17] [INFO] [0;36m(Worker_TP1 pid=42204)[0;0m INFO 01-18 16:54:17 [multiproc_executor.py:724] Parent process exited, terminating worker
[2026-01-18 16:54:18] [WARNING] [rank0]:[W118 16:54:18.682480647 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2026-01-18 16:54:19] [ERROR] [0;36m(EngineCore_DP0 pid=41919)[0;0m ERROR 01-18 16:54:19 [core.py:935] EngineCore failed to start.
[2026-01-18 16:54:19] [ERROR] [0;36m(EngineCore_DP0 pid=41919)[0;0m ERROR 01-18 16:54:19 [core.py:935] Traceback (most recent call last):
[2026-01-18 16:54:19] [ERROR] [0;36m(EngineCore_DP0 pid=41919)[0;0m ERROR 01-18 16:54:19 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-18 16:54:19] [ERROR] [0;36m(EngineCore_DP0 pid=41919)[0;0m ERROR 01-18 16:54:19 [core.py:935]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 16:54:19] [ERROR] [0;36m(EngineCore_DP0 pid=41919)[0;0m ERROR 01-18 16:54:19 [core.py:935]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:54:19] [ERROR] [0;36m(EngineCore_DP0 pid=41919)[0;0m ERROR 01-18 16:54:19 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-18 16:54:19] [ERROR] [0;36m(EngineCore_DP0 pid=41919)[0;0m ERROR 01-18 16:54:19 [core.py:935]     super().__init__(
[2026-01-18 16:54:19] [ERROR] [0;36m(EngineCore_DP0 pid=41919)[0;0m ERROR 01-18 16:54:19 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[2026-01-18 16:54:19] [ERROR] [0;36m(EngineCore_DP0 pid=41919)[0;0m ERROR 01-18 16:54:19 [core.py:935]     self.model_executor = executor_class(vllm_config)
[2026-01-18 16:54:19] [ERROR] [0;36m(EngineCore_DP0 pid=41919)[0;0m ERROR 01-18 16:54:19 [core.py:935]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:54:19] [ERROR] [0;36m(EngineCore_DP0 pid=41919)[0;0m ERROR 01-18 16:54:19 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[2026-01-18 16:54:19] [ERROR] [0;36m(EngineCore_DP0 pid=41919)[0;0m ERROR 01-18 16:54:19 [core.py:935]     super().__init__(vllm_config)
[2026-01-18 16:54:19] [ERROR] [0;36m(EngineCore_DP0 pid=41919)[0;0m ERROR 01-18 16:54:19 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[2026-01-18 16:54:19] [ERROR] [0;36m(EngineCore_DP0 pid=41919)[0;0m ERROR 01-18 16:54:19 [core.py:935]     self._init_executor()
[2026-01-18 16:54:19] [ERROR] [0;36m(EngineCore_DP0 pid=41919)[0;0m ERROR 01-18 16:54:19 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[2026-01-18 16:54:19] [ERROR] [0;36m(EngineCore_DP0 pid=41919)[0;0m ERROR 01-18 16:54:19 [core.py:935]     self.workers = WorkerProc.wait_for_ready(unready_workers)
[2026-01-18 16:54:19] [ERROR] [0;36m(EngineCore_DP0 pid=41919)[0;0m ERROR 01-18 16:54:19 [core.py:935]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:54:19] [ERROR] [0;36m(EngineCore_DP0 pid=41919)[0;0m ERROR 01-18 16:54:19 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 675, in wait_for_ready
[2026-01-18 16:54:19] [ERROR] [0;36m(EngineCore_DP0 pid=41919)[0;0m ERROR 01-18 16:54:19 [core.py:935]     raise e from None
[2026-01-18 16:54:19] [ERROR] [0;36m(EngineCore_DP0 pid=41919)[0;0m ERROR 01-18 16:54:19 [core.py:935] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[2026-01-18 16:54:19] [INFO] [0;36m(EngineCore_DP0 pid=41919)[0;0m Process EngineCore_DP0:
[2026-01-18 16:54:19] [ERROR] [0;36m(EngineCore_DP0 pid=41919)[0;0m Traceback (most recent call last):
[2026-01-18 16:54:19] [INFO] [0;36m(EngineCore_DP0 pid=41919)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[2026-01-18 16:54:19] [INFO] [0;36m(EngineCore_DP0 pid=41919)[0;0m     self.run()
[2026-01-18 16:54:19] [INFO] [0;36m(EngineCore_DP0 pid=41919)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/process.py", line 108, in run
[2026-01-18 16:54:19] [INFO] [0;36m(EngineCore_DP0 pid=41919)[0;0m     self._target(*self._args, **self._kwargs)
[2026-01-18 16:54:19] [INFO] [0;36m(EngineCore_DP0 pid=41919)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 939, in run_engine_core
[2026-01-18 16:54:19] [INFO] [0;36m(EngineCore_DP0 pid=41919)[0;0m     raise e
[2026-01-18 16:54:19] [INFO] [0;36m(EngineCore_DP0 pid=41919)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-18 16:54:19] [INFO] [0;36m(EngineCore_DP0 pid=41919)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 16:54:19] [INFO] [0;36m(EngineCore_DP0 pid=41919)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:54:19] [INFO] [0;36m(EngineCore_DP0 pid=41919)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-18 16:54:19] [INFO] [0;36m(EngineCore_DP0 pid=41919)[0;0m     super().__init__(
[2026-01-18 16:54:19] [INFO] [0;36m(EngineCore_DP0 pid=41919)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[2026-01-18 16:54:19] [INFO] [0;36m(EngineCore_DP0 pid=41919)[0;0m     self.model_executor = executor_class(vllm_config)
[2026-01-18 16:54:19] [INFO] [0;36m(EngineCore_DP0 pid=41919)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:54:19] [INFO] [0;36m(EngineCore_DP0 pid=41919)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[2026-01-18 16:54:19] [INFO] [0;36m(EngineCore_DP0 pid=41919)[0;0m     super().__init__(vllm_config)
[2026-01-18 16:54:19] [INFO] [0;36m(EngineCore_DP0 pid=41919)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[2026-01-18 16:54:19] [INFO] [0;36m(EngineCore_DP0 pid=41919)[0;0m     self._init_executor()
[2026-01-18 16:54:19] [INFO] [0;36m(EngineCore_DP0 pid=41919)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[2026-01-18 16:54:19] [INFO] [0;36m(EngineCore_DP0 pid=41919)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
[2026-01-18 16:54:19] [INFO] [0;36m(EngineCore_DP0 pid=41919)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:54:19] [INFO] [0;36m(EngineCore_DP0 pid=41919)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 675, in wait_for_ready
[2026-01-18 16:54:19] [INFO] [0;36m(EngineCore_DP0 pid=41919)[0;0m     raise e from None
[2026-01-18 16:54:19] [INFO] [0;36m(EngineCore_DP0 pid=41919)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[2026-01-18 16:54:20] [ERROR] [0;36m(APIServer pid=41601)[0;0m Traceback (most recent call last):
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/bin/vllm", line 7, in <module>
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m     sys.exit(main())
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m              ^^^^^^
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m     args.dispatch_function(args)
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m     uvloop.run(run_server(args))
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m     return __asyncio.run(
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m            ^^^^^^^^^^^^^^
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m     return runner.run(main)
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m     return self._loop.run_until_complete(task)
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m     return await main
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m            ^^^^^^^^^^
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m     async with build_async_engine_client(
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m     return await anext(self.gen)
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 146, in build_async_engine_client
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m     return await anext(self.gen)
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 187, in build_async_engine_client_from_engine_args
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 202, in from_vllm_config
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m     return cls(
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m            ^^^^
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 129, in __init__
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m     return AsyncMPClient(*client_args)
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m     super().__init__(
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 144, in __exit__
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m     next(self.gen)
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 918, in launch_core_engines
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m     wait_for_engine_startup(
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 977, in wait_for_engine_startup
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m     raise RuntimeError(
[2026-01-18 16:54:20] [INFO] [0;36m(APIServer pid=41601)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[2026-01-18 16:54:21] [INFO] /mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/resource_tracker.py:279: UserWarning: resource_tracker: There appear to be 3 leaked shared_memory objects to clean up at shutdown
[2026-01-18 16:54:21] [INFO] warnings.warn('resource_tracker: There appear to be %d '
[2026-01-18 16:54:22] [INFO] nvitop监控已启动
[2026-01-18 16:54:33] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 16:54:33] [INFO] nvitop监控已启动
[2026-01-18 16:54:33] [INFO] nvitop监控已停止
[2026-01-18 16:55:00] [INFO] [0;36m(APIServer pid=42714)[0;0m INFO 01-18 16:55:00 [api_server.py:1278] vLLM API server version 0.14.0rc1.dev492+g19504ac07
[2026-01-18 16:55:00] [INFO] [0;36m(APIServer pid=42714)[0;0m INFO 01-18 16:55:00 [utils.py:254] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/chat_template.jinja', 'enable_auto_tool_choice': True, 'tool_call_parser': 'deepseek_v3', 'model': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/', 'trust_remote_code': True, 'max_model_len': 128000, 'served_model_name': ['VLLM-MODEL'], 'attention_backend': 'FLASH_ATTN', 'reasoning_parser': 'deepseek_v3', 'tensor_parallel_size': 2, 'max_num_seqs': 256, 'compilation_config': {'level': None, 'mode': None, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': [], 'splitting_ops': None, 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': None, 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.PIECEWISE: 1>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': None, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}}
[2026-01-18 16:55:00] [INFO] [0;36m(APIServer pid=42714)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 16:55:00] [INFO] [0;36m(APIServer pid=42714)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 16:55:00] [INFO] [0;36m(APIServer pid=42714)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 16:55:20] [INFO] [0;36m(APIServer pid=42714)[0;0m INFO 01-18 16:55:20 [model.py:528] Resolved architecture: Qwen3NextForCausalLM
[2026-01-18 16:55:20] [INFO] [0;36m(APIServer pid=42714)[0;0m INFO 01-18 16:55:20 [model.py:1543] Using max model len 128000
[2026-01-18 16:55:21] [INFO] [0;36m(APIServer pid=42714)[0;0m INFO 01-18 16:55:21 [scheduler.py:231] Chunked prefill is enabled with max_num_batched_tokens=8192.
[2026-01-18 16:55:21] [INFO] [0;36m(APIServer pid=42714)[0;0m INFO 01-18 16:55:21 [config.py:476] Setting attention block size to 544 tokens to ensure that attention page size is >= mamba page size.
[2026-01-18 16:55:21] [INFO] [0;36m(APIServer pid=42714)[0;0m INFO 01-18 16:55:21 [config.py:500] Padding mamba page size by 1.49% to ensure that mamba page size and attention page size are exactly equal.
[2026-01-18 16:55:21] [INFO] [0;36m(APIServer pid=42714)[0;0m INFO 01-18 16:55:21 [vllm.py:640] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-18 16:55:21] [INFO] [0;36m(APIServer pid=42714)[0;0m INFO 01-18 16:55:21 [vllm.py:645] Asynchronous scheduling is enabled.
[2026-01-18 16:55:45] [INFO] [0;36m(EngineCore_DP0 pid=43217)[0;0m INFO 01-18 16:55:45 [core.py:97] Initializing a V1 LLM engine (v0.14.0rc1.dev492+g19504ac07) with config: model='/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/', speculative_config=None, tokenizer='/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='deepseek_v3', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.PIECEWISE: 1>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-18 16:55:45] [WARNING] [0;36m(EngineCore_DP0 pid=43217)[0;0m WARNING 01-18 16:55:45 [multiproc_executor.py:880] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-18 16:56:09] [INFO] INFO 01-18 16:56:09 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:40827 backend=nccl
[2026-01-18 16:56:09] [INFO] INFO 01-18 16:56:09 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:40827 backend=nccl
[2026-01-18 16:56:10] [INFO] INFO 01-18 16:56:10 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-18 16:56:10] [WARNING] WARNING 01-18 16:56:10 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 16:56:10] [WARNING] WARNING 01-18 16:56:10 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 16:56:10] [INFO] INFO 01-18 16:56:10 [parallel_state.py:1425] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[2026-01-18 16:56:10] [INFO] INFO 01-18 16:56:10 [parallel_state.py:1425] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
[2026-01-18 16:56:11] [INFO] [0;36m(Worker_TP0 pid=43455)[0;0m INFO 01-18 16:56:11 [gpu_model_runner.py:3789] Starting to load model /mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/...
[2026-01-18 16:56:15] [INFO] [0;36m(Worker_TP0 pid=43455)[0;0m INFO 01-18 16:56:15 [cuda.py:315] Using AttentionBackendEnum.FLASH_ATTN backend.
[2026-01-18 16:56:15] [INFO] [0;36m(Worker_TP1 pid=43456)[0;0m INFO 01-18 16:56:15 [cuda.py:315] Using AttentionBackendEnum.FLASH_ATTN backend.
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749] WorkerProc failed to start.
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749] Traceback (most recent call last):
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 720, in worker_main
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]     worker = WorkerProc(*args, **kwargs)
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 560, in __init__
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]     self.worker.load_model()
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/worker/gpu_worker.py", line 274, in load_model
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/worker/gpu_model_runner.py", line 3808, in load_model
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]     self.model = model_loader.load_model(
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/model_loader/base_loader.py", line 58, in load_model
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]     self.load_weights(model, model_config)
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/model_loader/default_loader.py", line 288, in load_weights
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/qwen3_next.py", line 1287, in load_weights
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]     return loader.load_weights(weights)
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/utils.py", line 335, in load_weights
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]     autoloaded_weights = set(self._load_module("", self.module, weights))
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/utils.py", line 279, in _load_module
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]     for child_prefix, child_weights in self._groupby_prefix(weights):
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/utils.py", line 163, in _groupby_prefix
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]     for prefix, group in itertools.groupby(weights_by_parts, key=lambda x: x[0][0]):
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/utils.py", line 160, in <genexpr>
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]     for weight_name, weight_data in weights
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]                                     ^^^^^^^
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/utils.py", line 332, in <genexpr>
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]     (name, weight) for name, weight in weights if not self._can_skip(name)
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]                                        ^^^^^^^
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/model_loader/default_loader.py", line 260, in get_all_weights
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]     yield from self._get_weights_iterator(primary_weights)
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/model_loader/default_loader.py", line 189, in _get_weights_iterator
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]     hf_folder, hf_weights_files, use_safetensors = self._prepare_weights(
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]                                                    ^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/model_loader/default_loader.py", line 171, in _prepare_weights
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]     hf_weights_files = filter_duplicate_safetensors_files(
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/model_loader/weight_utils.py", line 570, in filter_duplicate_safetensors_files
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]     weight_map = json.load(f)["weight_map"]
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]                  ^^^^^^^^^^^^
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/json/__init__.py", line 293, in load
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]     return loads(fp.read(),
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]            ^^^^^^^^^^^^^^^^
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/json/__init__.py", line 346, in loads
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]     return _default_decoder.decode(s)
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]            ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/json/decoder.py", line 338, in decode
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]     obj, end = self.raw_decode(s, idx=_w(s, 0).end())
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/json/decoder.py", line 354, in raw_decode
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]     obj, end = self.scan_once(s, idx)
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749]                ^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:56:16] [ERROR] [0;36m(Worker_TP0 pid=43455)[0;0m ERROR 01-18 16:56:16 [multiproc_executor.py:749] json.decoder.JSONDecodeError: Unterminated string starting at: line 41299 column 59 (char 3919835)
[2026-01-18 16:56:16] [INFO] [0;36m(Worker_TP0 pid=43455)[0;0m INFO 01-18 16:56:16 [multiproc_executor.py:707] Parent process exited, terminating worker
[2026-01-18 16:56:16] [INFO] [0;36m(Worker_TP1 pid=43456)[0;0m INFO 01-18 16:56:16 [multiproc_executor.py:707] Parent process exited, terminating worker
[2026-01-18 16:56:17] [WARNING] [rank0]:[W118 16:56:17.727311147 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2026-01-18 16:56:18] [ERROR] [0;36m(EngineCore_DP0 pid=43217)[0;0m ERROR 01-18 16:56:18 [core.py:936] EngineCore failed to start.
[2026-01-18 16:56:18] [ERROR] [0;36m(EngineCore_DP0 pid=43217)[0;0m ERROR 01-18 16:56:18 [core.py:936] Traceback (most recent call last):
[2026-01-18 16:56:18] [ERROR] [0;36m(EngineCore_DP0 pid=43217)[0;0m ERROR 01-18 16:56:18 [core.py:936]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core.py", line 927, in run_engine_core
[2026-01-18 16:56:18] [ERROR] [0;36m(EngineCore_DP0 pid=43217)[0;0m ERROR 01-18 16:56:18 [core.py:936]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 16:56:18] [ERROR] [0;36m(EngineCore_DP0 pid=43217)[0;0m ERROR 01-18 16:56:18 [core.py:936]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:56:18] [ERROR] [0;36m(EngineCore_DP0 pid=43217)[0;0m ERROR 01-18 16:56:18 [core.py:936]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core.py", line 692, in __init__
[2026-01-18 16:56:18] [ERROR] [0;36m(EngineCore_DP0 pid=43217)[0;0m ERROR 01-18 16:56:18 [core.py:936]     super().__init__(
[2026-01-18 16:56:18] [ERROR] [0;36m(EngineCore_DP0 pid=43217)[0;0m ERROR 01-18 16:56:18 [core.py:936]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core.py", line 106, in __init__
[2026-01-18 16:56:18] [ERROR] [0;36m(EngineCore_DP0 pid=43217)[0;0m ERROR 01-18 16:56:18 [core.py:936]     self.model_executor = executor_class(vllm_config)
[2026-01-18 16:56:18] [ERROR] [0;36m(EngineCore_DP0 pid=43217)[0;0m ERROR 01-18 16:56:18 [core.py:936]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:56:18] [ERROR] [0;36m(EngineCore_DP0 pid=43217)[0;0m ERROR 01-18 16:56:18 [core.py:936]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[2026-01-18 16:56:18] [ERROR] [0;36m(EngineCore_DP0 pid=43217)[0;0m ERROR 01-18 16:56:18 [core.py:936]     super().__init__(vllm_config)
[2026-01-18 16:56:18] [ERROR] [0;36m(EngineCore_DP0 pid=43217)[0;0m ERROR 01-18 16:56:18 [core.py:936]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/abstract.py", line 101, in __init__
[2026-01-18 16:56:18] [ERROR] [0;36m(EngineCore_DP0 pid=43217)[0;0m ERROR 01-18 16:56:18 [core.py:936]     self._init_executor()
[2026-01-18 16:56:18] [ERROR] [0;36m(EngineCore_DP0 pid=43217)[0;0m ERROR 01-18 16:56:18 [core.py:936]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 172, in _init_executor
[2026-01-18 16:56:18] [ERROR] [0;36m(EngineCore_DP0 pid=43217)[0;0m ERROR 01-18 16:56:18 [core.py:936]     self.workers = WorkerProc.wait_for_ready(unready_workers)
[2026-01-18 16:56:18] [ERROR] [0;36m(EngineCore_DP0 pid=43217)[0;0m ERROR 01-18 16:56:18 [core.py:936]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:56:18] [ERROR] [0;36m(EngineCore_DP0 pid=43217)[0;0m ERROR 01-18 16:56:18 [core.py:936]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 658, in wait_for_ready
[2026-01-18 16:56:18] [ERROR] [0;36m(EngineCore_DP0 pid=43217)[0;0m ERROR 01-18 16:56:18 [core.py:936]     raise e from None
[2026-01-18 16:56:18] [ERROR] [0;36m(EngineCore_DP0 pid=43217)[0;0m ERROR 01-18 16:56:18 [core.py:936] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[2026-01-18 16:56:18] [INFO] [0;36m(EngineCore_DP0 pid=43217)[0;0m Process EngineCore_DP0:
[2026-01-18 16:56:18] [ERROR] [0;36m(EngineCore_DP0 pid=43217)[0;0m Traceback (most recent call last):
[2026-01-18 16:56:18] [INFO] [0;36m(EngineCore_DP0 pid=43217)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[2026-01-18 16:56:18] [INFO] [0;36m(EngineCore_DP0 pid=43217)[0;0m     self.run()
[2026-01-18 16:56:18] [INFO] [0;36m(EngineCore_DP0 pid=43217)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/process.py", line 108, in run
[2026-01-18 16:56:18] [INFO] [0;36m(EngineCore_DP0 pid=43217)[0;0m     self._target(*self._args, **self._kwargs)
[2026-01-18 16:56:18] [INFO] [0;36m(EngineCore_DP0 pid=43217)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core.py", line 940, in run_engine_core
[2026-01-18 16:56:18] [INFO] [0;36m(EngineCore_DP0 pid=43217)[0;0m     raise e
[2026-01-18 16:56:18] [INFO] [0;36m(EngineCore_DP0 pid=43217)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core.py", line 927, in run_engine_core
[2026-01-18 16:56:18] [INFO] [0;36m(EngineCore_DP0 pid=43217)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 16:56:18] [INFO] [0;36m(EngineCore_DP0 pid=43217)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:56:18] [INFO] [0;36m(EngineCore_DP0 pid=43217)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core.py", line 692, in __init__
[2026-01-18 16:56:18] [INFO] [0;36m(EngineCore_DP0 pid=43217)[0;0m     super().__init__(
[2026-01-18 16:56:18] [INFO] [0;36m(EngineCore_DP0 pid=43217)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core.py", line 106, in __init__
[2026-01-18 16:56:18] [INFO] [0;36m(EngineCore_DP0 pid=43217)[0;0m     self.model_executor = executor_class(vllm_config)
[2026-01-18 16:56:18] [INFO] [0;36m(EngineCore_DP0 pid=43217)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:56:18] [INFO] [0;36m(EngineCore_DP0 pid=43217)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[2026-01-18 16:56:18] [INFO] [0;36m(EngineCore_DP0 pid=43217)[0;0m     super().__init__(vllm_config)
[2026-01-18 16:56:18] [INFO] [0;36m(EngineCore_DP0 pid=43217)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/abstract.py", line 101, in __init__
[2026-01-18 16:56:18] [INFO] [0;36m(EngineCore_DP0 pid=43217)[0;0m     self._init_executor()
[2026-01-18 16:56:18] [INFO] [0;36m(EngineCore_DP0 pid=43217)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 172, in _init_executor
[2026-01-18 16:56:18] [INFO] [0;36m(EngineCore_DP0 pid=43217)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
[2026-01-18 16:56:18] [INFO] [0;36m(EngineCore_DP0 pid=43217)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:56:18] [INFO] [0;36m(EngineCore_DP0 pid=43217)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 658, in wait_for_ready
[2026-01-18 16:56:18] [INFO] [0;36m(EngineCore_DP0 pid=43217)[0;0m     raise e from None
[2026-01-18 16:56:18] [INFO] [0;36m(EngineCore_DP0 pid=43217)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[2026-01-18 16:56:19] [ERROR] [0;36m(APIServer pid=42714)[0;0m Traceback (most recent call last):
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/bin/vllm", line 7, in <module>
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m     sys.exit(main())
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m              ^^^^^^
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m     args.dispatch_function(args)
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m     uvloop.run(run_server(args))
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m     return __asyncio.run(
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m            ^^^^^^^^^^^^^^
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m     return runner.run(main)
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m     return self._loop.run_until_complete(task)
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m     return await main
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m            ^^^^^^^^^^
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 1325, in run_server
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 1344, in run_server_worker
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m     async with build_async_engine_client(
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m     return await anext(self.gen)
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 173, in build_async_engine_client
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m     return await anext(self.gen)
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 214, in build_async_engine_client_from_engine_args
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/async_llm.py", line 205, in from_vllm_config
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m     return cls(
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m            ^^^^
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/async_llm.py", line 132, in __init__
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m     return AsyncMPClient(*client_args)
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core_client.py", line 824, in __init__
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m     super().__init__(
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core_client.py", line 479, in __init__
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/contextlib.py", line 144, in __exit__
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m     next(self.gen)
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/utils.py", line 921, in launch_core_engines
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m     wait_for_engine_startup(
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/utils.py", line 980, in wait_for_engine_startup
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m     raise RuntimeError(
[2026-01-18 16:56:19] [INFO] [0;36m(APIServer pid=42714)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[2026-01-18 16:56:20] [INFO] /mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py:279: UserWarning: resource_tracker: There appear to be 3 leaked shared_memory objects to clean up at shutdown
[2026-01-18 16:56:20] [INFO] warnings.warn('resource_tracker: There appear to be %d '
[2026-01-18 16:56:23] [INFO] nvitop监控已启动
[2026-01-18 16:57:33] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 16:57:33] [INFO] nvitop监控已启动
[2026-01-18 16:57:33] [INFO] nvitop监控已停止
[2026-01-18 16:58:02] [INFO] [0;36m(APIServer pid=44347)[0;0m INFO 01-18 16:58:02 [api_server.py:872] vLLM API server version 0.14.0rc2.dev117+g9e078d058
[2026-01-18 16:58:02] [INFO] [0;36m(APIServer pid=44347)[0;0m INFO 01-18 16:58:02 [utils.py:267] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/', 'host': '0.0.0.0', 'port': 8005, 'model': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/', 'max_model_len': 128000, 'served_model_name': ['VLLM-MODEL'], 'tensor_parallel_size': 2, 'max_num_seqs': 256}
[2026-01-18 16:58:02] [INFO] [0;36m(APIServer pid=44347)[0;0m Unrecognized keys in `rope_parameters` for 'rope_type'='default': {'partial_rotary_factor'}
[2026-01-18 16:58:02] [INFO] [0;36m(APIServer pid=44347)[0;0m Unrecognized keys in `rope_parameters` for 'rope_type'='default': {'partial_rotary_factor'}
[2026-01-18 16:58:29] [INFO] [0;36m(APIServer pid=44347)[0;0m INFO 01-18 16:58:29 [model.py:530] Resolved architecture: Qwen3NextForCausalLM
[2026-01-18 16:58:29] [INFO] [0;36m(APIServer pid=44347)[0;0m INFO 01-18 16:58:29 [model.py:1547] Using max model len 128000
[2026-01-18 16:58:29] [INFO] [0;36m(APIServer pid=44347)[0;0m INFO 01-18 16:58:29 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[2026-01-18 16:58:30] [INFO] [0;36m(APIServer pid=44347)[0;0m INFO 01-18 16:58:30 [config.py:476] Setting attention block size to 544 tokens to ensure that attention page size is >= mamba page size.
[2026-01-18 16:58:30] [INFO] [0;36m(APIServer pid=44347)[0;0m INFO 01-18 16:58:30 [config.py:500] Padding mamba page size by 1.49% to ensure that mamba page size and attention page size are exactly equal.
[2026-01-18 16:58:30] [INFO] [0;36m(APIServer pid=44347)[0;0m INFO 01-18 16:58:30 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 16:58:30] [INFO] [0;36m(APIServer pid=44347)[0;0m INFO 01-18 16:58:30 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-18 16:59:00] [INFO] [0;36m(EngineCore_DP0 pid=45007)[0;0m INFO 01-18 16:59:00 [core.py:96] Initializing a V1 LLM engine (v0.14.0rc2.dev117+g9e078d058) with config: model='/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/', speculative_config=None, tokenizer='/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-18 16:59:00] [WARNING] [0;36m(EngineCore_DP0 pid=45007)[0;0m WARNING 01-18 16:59:00 [multiproc_executor.py:897] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-18 16:59:29] [INFO] INFO 01-18 16:59:29 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:34739 backend=nccl
[2026-01-18 16:59:30] [INFO] INFO 01-18 16:59:30 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:34739 backend=nccl
[2026-01-18 16:59:30] [INFO] INFO 01-18 16:59:30 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-18 16:59:30] [WARNING] WARNING 01-18 16:59:30 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 16:59:30] [WARNING] WARNING 01-18 16:59:30 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 16:59:30] [INFO] INFO 01-18 16:59:30 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[2026-01-18 16:59:30] [INFO] INFO 01-18 16:59:30 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
[2026-01-18 16:59:31] [INFO] [0;36m(Worker_TP0 pid=45285)[0;0m INFO 01-18 16:59:31 [gpu_model_runner.py:3803] Starting to load model /mnt/AI-Acer4T/AI-Chat/models/Qwen3/Huihui-Qwen3-Next-80B-A3B-Thinking-abliterated/...
[2026-01-18 16:59:35] [INFO] [0;36m(Worker_TP0 pid=45285)[0;0m INFO 01-18 16:59:35 [unquantized.py:99] Using TRITON backend for Unquantized MoE
[2026-01-18 16:59:35] [INFO] [0;36m(Worker_TP0 pid=45285)[0;0m INFO 01-18 16:59:35 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766] WorkerProc failed to start.
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766] Traceback (most recent call last):
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 737, in worker_main
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]     worker = WorkerProc(*args, **kwargs)
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 575, in __init__
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]     self.worker.load_model()
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 274, in load_model
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3822, in load_model
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]     self.model = model_loader.load_model(
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 58, in load_model
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]     self.load_weights(model, model_config)
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 288, in load_weights
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_next.py", line 1294, in load_weights
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]     return loader.load_weights(weights)
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 335, in load_weights
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]     autoloaded_weights = set(self._load_module("", self.module, weights))
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 279, in _load_module
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]     for child_prefix, child_weights in self._groupby_prefix(weights):
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 163, in _groupby_prefix
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]     for prefix, group in itertools.groupby(weights_by_parts, key=lambda x: x[0][0]):
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 160, in <genexpr>
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]     for weight_name, weight_data in weights
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]                                     ^^^^^^^
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 332, in <genexpr>
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]     (name, weight) for name, weight in weights if not self._can_skip(name)
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]                                        ^^^^^^^
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 260, in get_all_weights
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]     yield from self._get_weights_iterator(primary_weights)
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 189, in _get_weights_iterator
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]     hf_folder, hf_weights_files, use_safetensors = self._prepare_weights(
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]                                                    ^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 171, in _prepare_weights
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]     hf_weights_files = filter_duplicate_safetensors_files(
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py", line 570, in filter_duplicate_safetensors_files
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]     weight_map = json.load(f)["weight_map"]
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]                  ^^^^^^^^^^^^
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/json/__init__.py", line 293, in load
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]     return loads(fp.read(),
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/json/__init__.py", line 346, in loads
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]     return _default_decoder.decode(s)
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/json/decoder.py", line 338, in decode
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]     obj, end = self.raw_decode(s, idx=_w(s, 0).end())
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/json/decoder.py", line 354, in raw_decode
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]     obj, end = self.scan_once(s, idx)
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766]                ^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:59:36] [ERROR] [0;36m(Worker_TP0 pid=45285)[0;0m ERROR 01-18 16:59:36 [multiproc_executor.py:766] json.decoder.JSONDecodeError: Unterminated string starting at: line 41299 column 59 (char 3919835)
[2026-01-18 16:59:36] [INFO] [0;36m(Worker_TP0 pid=45285)[0;0m [0;36m(Worker_TP1 pid=45286)[0;0m INFO 01-18 16:59:36 [multiproc_executor.py:724] Parent process exited, terminating worker
[2026-01-18 16:59:36] [INFO] INFO 01-18 16:59:36 [multiproc_executor.py:724] Parent process exited, terminating worker
[2026-01-18 16:59:37] [WARNING] [rank0]:[W118 16:59:37.670013607 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2026-01-18 16:59:38] [ERROR] [0;36m(EngineCore_DP0 pid=45007)[0;0m ERROR 01-18 16:59:38 [core.py:935] EngineCore failed to start.
[2026-01-18 16:59:38] [ERROR] [0;36m(EngineCore_DP0 pid=45007)[0;0m ERROR 01-18 16:59:38 [core.py:935] Traceback (most recent call last):
[2026-01-18 16:59:38] [ERROR] [0;36m(EngineCore_DP0 pid=45007)[0;0m ERROR 01-18 16:59:38 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-18 16:59:38] [ERROR] [0;36m(EngineCore_DP0 pid=45007)[0;0m ERROR 01-18 16:59:38 [core.py:935]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 16:59:38] [ERROR] [0;36m(EngineCore_DP0 pid=45007)[0;0m ERROR 01-18 16:59:38 [core.py:935]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:59:38] [ERROR] [0;36m(EngineCore_DP0 pid=45007)[0;0m ERROR 01-18 16:59:38 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-18 16:59:38] [ERROR] [0;36m(EngineCore_DP0 pid=45007)[0;0m ERROR 01-18 16:59:38 [core.py:935]     super().__init__(
[2026-01-18 16:59:38] [ERROR] [0;36m(EngineCore_DP0 pid=45007)[0;0m ERROR 01-18 16:59:38 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[2026-01-18 16:59:38] [ERROR] [0;36m(EngineCore_DP0 pid=45007)[0;0m ERROR 01-18 16:59:38 [core.py:935]     self.model_executor = executor_class(vllm_config)
[2026-01-18 16:59:38] [ERROR] [0;36m(EngineCore_DP0 pid=45007)[0;0m ERROR 01-18 16:59:38 [core.py:935]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:59:38] [ERROR] [0;36m(EngineCore_DP0 pid=45007)[0;0m ERROR 01-18 16:59:38 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[2026-01-18 16:59:38] [ERROR] [0;36m(EngineCore_DP0 pid=45007)[0;0m ERROR 01-18 16:59:38 [core.py:935]     super().__init__(vllm_config)
[2026-01-18 16:59:38] [ERROR] [0;36m(EngineCore_DP0 pid=45007)[0;0m ERROR 01-18 16:59:38 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[2026-01-18 16:59:38] [ERROR] [0;36m(EngineCore_DP0 pid=45007)[0;0m ERROR 01-18 16:59:38 [core.py:935]     self._init_executor()
[2026-01-18 16:59:38] [ERROR] [0;36m(EngineCore_DP0 pid=45007)[0;0m ERROR 01-18 16:59:38 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[2026-01-18 16:59:38] [ERROR] [0;36m(EngineCore_DP0 pid=45007)[0;0m ERROR 01-18 16:59:38 [core.py:935]     self.workers = WorkerProc.wait_for_ready(unready_workers)
[2026-01-18 16:59:38] [ERROR] [0;36m(EngineCore_DP0 pid=45007)[0;0m ERROR 01-18 16:59:38 [core.py:935]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:59:38] [ERROR] [0;36m(EngineCore_DP0 pid=45007)[0;0m ERROR 01-18 16:59:38 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 675, in wait_for_ready
[2026-01-18 16:59:38] [ERROR] [0;36m(EngineCore_DP0 pid=45007)[0;0m ERROR 01-18 16:59:38 [core.py:935]     raise e from None
[2026-01-18 16:59:38] [ERROR] [0;36m(EngineCore_DP0 pid=45007)[0;0m ERROR 01-18 16:59:38 [core.py:935] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[2026-01-18 16:59:38] [INFO] [0;36m(EngineCore_DP0 pid=45007)[0;0m Process EngineCore_DP0:
[2026-01-18 16:59:38] [ERROR] [0;36m(EngineCore_DP0 pid=45007)[0;0m Traceback (most recent call last):
[2026-01-18 16:59:38] [INFO] [0;36m(EngineCore_DP0 pid=45007)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[2026-01-18 16:59:38] [INFO] [0;36m(EngineCore_DP0 pid=45007)[0;0m     self.run()
[2026-01-18 16:59:38] [INFO] [0;36m(EngineCore_DP0 pid=45007)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/process.py", line 108, in run
[2026-01-18 16:59:38] [INFO] [0;36m(EngineCore_DP0 pid=45007)[0;0m     self._target(*self._args, **self._kwargs)
[2026-01-18 16:59:38] [INFO] [0;36m(EngineCore_DP0 pid=45007)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 939, in run_engine_core
[2026-01-18 16:59:38] [INFO] [0;36m(EngineCore_DP0 pid=45007)[0;0m     raise e
[2026-01-18 16:59:38] [INFO] [0;36m(EngineCore_DP0 pid=45007)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-18 16:59:38] [INFO] [0;36m(EngineCore_DP0 pid=45007)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 16:59:38] [INFO] [0;36m(EngineCore_DP0 pid=45007)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:59:38] [INFO] [0;36m(EngineCore_DP0 pid=45007)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-18 16:59:38] [INFO] [0;36m(EngineCore_DP0 pid=45007)[0;0m     super().__init__(
[2026-01-18 16:59:38] [INFO] [0;36m(EngineCore_DP0 pid=45007)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[2026-01-18 16:59:38] [INFO] [0;36m(EngineCore_DP0 pid=45007)[0;0m     self.model_executor = executor_class(vllm_config)
[2026-01-18 16:59:38] [INFO] [0;36m(EngineCore_DP0 pid=45007)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:59:38] [INFO] [0;36m(EngineCore_DP0 pid=45007)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[2026-01-18 16:59:38] [INFO] [0;36m(EngineCore_DP0 pid=45007)[0;0m     super().__init__(vllm_config)
[2026-01-18 16:59:38] [INFO] [0;36m(EngineCore_DP0 pid=45007)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[2026-01-18 16:59:38] [INFO] [0;36m(EngineCore_DP0 pid=45007)[0;0m     self._init_executor()
[2026-01-18 16:59:38] [INFO] [0;36m(EngineCore_DP0 pid=45007)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[2026-01-18 16:59:38] [INFO] [0;36m(EngineCore_DP0 pid=45007)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
[2026-01-18 16:59:38] [INFO] [0;36m(EngineCore_DP0 pid=45007)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:59:38] [INFO] [0;36m(EngineCore_DP0 pid=45007)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 675, in wait_for_ready
[2026-01-18 16:59:38] [INFO] [0;36m(EngineCore_DP0 pid=45007)[0;0m     raise e from None
[2026-01-18 16:59:38] [INFO] [0;36m(EngineCore_DP0 pid=45007)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[2026-01-18 16:59:39] [ERROR] [0;36m(APIServer pid=44347)[0;0m Traceback (most recent call last):
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/bin/vllm", line 7, in <module>
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m     sys.exit(main())
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m              ^^^^^^
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m     args.dispatch_function(args)
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m     uvloop.run(run_server(args))
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m     return __asyncio.run(
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m            ^^^^^^^^^^^^^^
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m     return runner.run(main)
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m     return self._loop.run_until_complete(task)
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m     return await main
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m            ^^^^^^^^^^
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m     async with build_async_engine_client(
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m     return await anext(self.gen)
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 146, in build_async_engine_client
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m     return await anext(self.gen)
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 187, in build_async_engine_client_from_engine_args
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 202, in from_vllm_config
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m     return cls(
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m            ^^^^
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 129, in __init__
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m     return AsyncMPClient(*client_args)
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m     super().__init__(
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 144, in __exit__
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m     next(self.gen)
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 918, in launch_core_engines
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m     wait_for_engine_startup(
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 977, in wait_for_engine_startup
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m     raise RuntimeError(
[2026-01-18 16:59:39] [INFO] [0;36m(APIServer pid=44347)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[2026-01-18 16:59:40] [INFO] /mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/resource_tracker.py:279: UserWarning: resource_tracker: There appear to be 3 leaked shared_memory objects to clean up at shutdown
[2026-01-18 16:59:40] [INFO] warnings.warn('resource_tracker: There appear to be %d '
[2026-01-18 16:59:44] [INFO] nvitop监控已启动
[2026-01-18 17:02:27] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 17:02:27] [INFO] nvitop监控已启动
[2026-01-18 17:02:27] [INFO] nvitop监控已停止
[2026-01-18 17:02:51] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:02:51 [api_server.py:1278] vLLM API server version 0.14.0rc1.dev492+g19504ac07
[2026-01-18 17:02:51] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:02:51 [utils.py:254] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/GPT-OSS/gpt-oss-120b-Derestricted-MXFP4/', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/AI-Acer4T/AI-Chat/models/GPT-OSS/gpt-oss-120b-Derestricted-MXFP4/chat_template.jinja', 'enable_auto_tool_choice': True, 'tool_call_parser': 'openai', 'model': '/mnt/AI-Acer4T/AI-Chat/models/GPT-OSS/gpt-oss-120b-Derestricted-MXFP4/', 'trust_remote_code': True, 'max_model_len': 128000, 'quantization': 'mxfp4', 'served_model_name': ['VLLM-MODEL'], 'reasoning_parser': 'openai_gptoss', 'tensor_parallel_size': 2, 'kv_cache_dtype': 'fp8', 'max_num_seqs': 256, 'async_scheduling': True}
[2026-01-18 17:02:51] [INFO] [0;36m(APIServer pid=46679)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 17:02:51] [INFO] [0;36m(APIServer pid=46679)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 17:02:51] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:02:51 [model.py:528] Resolved architecture: GptOssForCausalLM
[2026-01-18 17:02:51] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:02:51 [model.py:1543] Using max model len 128000
[2026-01-18 17:02:52] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:02:52 [cache.py:206] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor.
[2026-01-18 17:02:52] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:02:52 [scheduler.py:231] Chunked prefill is enabled with max_num_batched_tokens=8192.
[2026-01-18 17:02:52] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:02:52 [config.py:314] Overriding max cuda graph capture size to 1024 for performance.
[2026-01-18 17:02:52] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:02:52 [vllm.py:640] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-18 17:02:52] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:02:52 [vllm.py:645] Asynchronous scheduling is enabled.
[2026-01-18 17:02:53] [INFO] [0;36m(APIServer pid=46679)[0;0m The tokenizer you are loading from '/mnt/AI-Acer4T/AI-Chat/models/GPT-OSS/gpt-oss-120b-Derestricted-MXFP4/' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[2026-01-18 17:03:16] [INFO] [0;36m(EngineCore_DP0 pid=46993)[0;0m INFO 01-18 17:03:16 [core.py:97] Initializing a V1 LLM engine (v0.14.0rc1.dev492+g19504ac07) with config: model='/mnt/AI-Acer4T/AI-Chat/models/GPT-OSS/gpt-oss-120b-Derestricted-MXFP4/', speculative_config=None, tokenizer='/mnt/AI-Acer4T/AI-Chat/models/GPT-OSS/gpt-oss-120b-Derestricted-MXFP4/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=mxfp4, enforce_eager=False, kv_cache_dtype=fp8, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='openai_gptoss', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512, 528, 544, 560, 576, 592, 608, 624, 640, 656, 672, 688, 704, 720, 736, 752, 768, 784, 800, 816, 832, 848, 864, 880, 896, 912, 928, 944, 960, 976, 992, 1008, 1024], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 1024, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-18 17:03:16] [WARNING] [0;36m(EngineCore_DP0 pid=46993)[0;0m WARNING 01-18 17:03:16 [multiproc_executor.py:880] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-18 17:03:40] [INFO] INFO 01-18 17:03:40 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:48373 backend=nccl
[2026-01-18 17:03:40] [INFO] INFO 01-18 17:03:40 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:48373 backend=nccl
[2026-01-18 17:03:41] [INFO] INFO 01-18 17:03:41 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-18 17:03:41] [WARNING] WARNING 01-18 17:03:41 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 17:03:41] [WARNING] WARNING 01-18 17:03:41 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 17:03:41] [INFO] INFO 01-18 17:03:41 [parallel_state.py:1425] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[2026-01-18 17:03:41] [INFO] INFO 01-18 17:03:41 [parallel_state.py:1425] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
[2026-01-18 17:03:42] [INFO] [0;36m(Worker_TP0 pid=47233)[0;0m INFO 01-18 17:03:42 [gpu_model_runner.py:3789] Starting to load model /mnt/AI-Acer4T/AI-Chat/models/GPT-OSS/gpt-oss-120b-Derestricted-MXFP4/...
[2026-01-18 17:03:46] [INFO] [0;36m(Worker_TP0 pid=47233)[0;0m INFO 01-18 17:03:46 [cuda.py:351] Using TRITON_ATTN attention backend out of potential backends: ('TRITON_ATTN',)
[2026-01-18 17:03:46] [INFO] [0;36m(Worker_TP0 pid=47233)[0;0m INFO 01-18 17:03:46 [mxfp4.py:162] Using Marlin backend
[2026-01-18 17:03:46] [INFO] [0;36m(Worker_TP0 pid=47233)[0;0m
[2026-01-18 17:03:46] [INFO] Loading safetensors checkpoint shards:   0% Completed | 0/16 [00:00<?, ?it/s]
[2026-01-18 17:03:47] [INFO] [0;36m(Worker_TP1 pid=47234)[0;0m INFO 01-18 17:03:47 [mxfp4.py:162] Using Marlin backend
[2026-01-18 17:03:50] [INFO] [0;36m(Worker_TP0 pid=47233)[0;0m
[2026-01-18 17:03:50] [INFO] Loading safetensors checkpoint shards:   6% Completed | 1/16 [00:04<01:02,  4.19s/it]
[2026-01-18 17:03:54] [INFO] [0;36m(Worker_TP0 pid=47233)[0;0m
[2026-01-18 17:03:54] [INFO] Loading safetensors checkpoint shards:  12% Completed | 2/16 [00:08<00:56,  4.07s/it]
[2026-01-18 17:03:58] [INFO] [0;36m(Worker_TP0 pid=47233)[0;0m
[2026-01-18 17:03:58] [INFO] Loading safetensors checkpoint shards:  19% Completed | 3/16 [00:12<00:53,  4.13s/it]
[2026-01-18 17:04:02] [INFO] [0;36m(Worker_TP0 pid=47233)[0;0m
[2026-01-18 17:04:02] [INFO] Loading safetensors checkpoint shards:  25% Completed | 4/16 [00:16<00:50,  4.23s/it]
[2026-01-18 17:04:08] [INFO] [0;36m(Worker_TP0 pid=47233)[0;0m
[2026-01-18 17:04:08] [INFO] Loading safetensors checkpoint shards:  31% Completed | 5/16 [00:22<00:50,  4.60s/it]
[2026-01-18 17:04:13] [INFO] [0;36m(Worker_TP0 pid=47233)[0;0m
[2026-01-18 17:04:13] [INFO] Loading safetensors checkpoint shards:  38% Completed | 6/16 [00:27<00:48,  4.90s/it]
[2026-01-18 17:04:18] [INFO] [0;36m(Worker_TP0 pid=47233)[0;0m
[2026-01-18 17:04:18] [INFO] Loading safetensors checkpoint shards:  44% Completed | 7/16 [00:32<00:44,  4.94s/it]
[2026-01-18 17:04:23] [INFO] [0;36m(Worker_TP0 pid=47233)[0;0m
[2026-01-18 17:04:23] [INFO] Loading safetensors checkpoint shards:  50% Completed | 8/16 [00:37<00:39,  4.88s/it]
[2026-01-18 17:04:28] [INFO] [0;36m(Worker_TP0 pid=47233)[0;0m
[2026-01-18 17:04:28] [INFO] Loading safetensors checkpoint shards:  56% Completed | 9/16 [00:42<00:33,  4.84s/it]
[2026-01-18 17:04:33] [INFO] [0;36m(Worker_TP0 pid=47233)[0;0m
[2026-01-18 17:04:33] [INFO] Loading safetensors checkpoint shards:  62% Completed | 10/16 [00:47<00:29,  4.98s/it]
[2026-01-18 17:04:37] [INFO] [0;36m(Worker_TP0 pid=47233)[0;0m
[2026-01-18 17:04:37] [INFO] Loading safetensors checkpoint shards:  69% Completed | 11/16 [00:51<00:23,  4.78s/it]
[2026-01-18 17:04:42] [INFO] [0;36m(Worker_TP0 pid=47233)[0;0m
[2026-01-18 17:04:42] [INFO] Loading safetensors checkpoint shards:  75% Completed | 12/16 [00:56<00:19,  4.86s/it]
[2026-01-18 17:04:47] [INFO] [0;36m(Worker_TP0 pid=47233)[0;0m
[2026-01-18 17:04:47] [INFO] Loading safetensors checkpoint shards:  81% Completed | 13/16 [01:01<00:14,  4.75s/it]
[2026-01-18 17:04:52] [INFO] [0;36m(Worker_TP0 pid=47233)[0;0m
[2026-01-18 17:04:52] [INFO] Loading safetensors checkpoint shards:  88% Completed | 14/16 [01:06<00:09,  4.81s/it]
[2026-01-18 17:04:56] [INFO] [0;36m(Worker_TP0 pid=47233)[0;0m
[2026-01-18 17:04:56] [INFO] Loading safetensors checkpoint shards:  94% Completed | 15/16 [01:10<00:04,  4.74s/it]
[2026-01-18 17:04:58] [INFO] [0;36m(Worker_TP0 pid=47233)[0;0m
[2026-01-18 17:04:58] [INFO] Loading safetensors checkpoint shards: 100% Completed | 16/16 [01:12<00:00,  3.93s/it]
[2026-01-18 17:04:58] [INFO] [0;36m(Worker_TP0 pid=47233)[0;0m
[2026-01-18 17:04:58] [INFO] Loading safetensors checkpoint shards: 100% Completed | 16/16 [01:12<00:00,  4.55s/it]
[2026-01-18 17:04:58] [INFO] [0;36m(Worker_TP0 pid=47233)[0;0m
[2026-01-18 17:04:58] [WARNING] [0;36m(Worker_TP1 pid=47234)[0;0m WARNING 01-18 17:04:58 [marlin_utils_fp4.py:333] Your GPU does not have native support for FP4 computation but FP4 quantization is being used. Weight-only FP4 compression will be used leveraging the Marlin kernel. This may degrade performance for compute-heavy workloads.
[2026-01-18 17:04:59] [INFO] [0;36m(Worker_TP0 pid=47233)[0;0m INFO 01-18 17:04:59 [default_loader.py:291] Loading weights took 72.79 seconds
[2026-01-18 17:04:59] [WARNING] [0;36m(Worker_TP0 pid=47233)[0;0m WARNING 01-18 17:04:59 [marlin_utils_fp4.py:333] Your GPU does not have native support for FP4 computation but FP4 quantization is being used. Weight-only FP4 compression will be used leveraging the Marlin kernel. This may degrade performance for compute-heavy workloads.
[2026-01-18 17:05:00] [INFO] [0;36m(Worker_TP0 pid=47233)[0;0m INFO 01-18 17:05:00 [gpu_model_runner.py:3886] Model loading took 34.38 GiB memory and 77.654434 seconds
[2026-01-18 17:05:10] [INFO] [0;36m(Worker_TP0 pid=47233)[0;0m INFO 01-18 17:05:10 [backends.py:644] Using cache directory: /home/wuwen/.cache/vllm/torch_compile_cache/779f183e29/rank_0_0/backbone for vLLM's torch.compile
[2026-01-18 17:05:10] [INFO] [0;36m(Worker_TP0 pid=47233)[0;0m INFO 01-18 17:05:10 [backends.py:704] Dynamo bytecode transform time: 8.95 s
[2026-01-18 17:05:17] [INFO] [0;36m(Worker_TP1 pid=47234)[0;0m INFO 01-18 17:05:17 [backends.py:261] Cache the graph of compile range (1, 8192) for later use
[2026-01-18 17:05:17] [INFO] [0;36m(Worker_TP0 pid=47233)[0;0m INFO 01-18 17:05:17 [backends.py:261] Cache the graph of compile range (1, 8192) for later use
[2026-01-18 17:06:00] [INFO] [0;36m(EngineCore_DP0 pid=46993)[0;0m INFO 01-18 17:06:00 [shm_broadcast.py:542] No available shared memory broadcast block found in 60 seconds. This typically happens when some processes are hanging or doing some time-consuming work (e.g. compilation, weight/kv cache quantization).
[2026-01-18 17:06:07] [INFO] [0;36m(Worker_TP0 pid=47233)[0;0m INFO 01-18 17:06:07 [backends.py:278] Compiling a graph for compile range (1, 8192) takes 55.18 s
[2026-01-18 17:06:07] [INFO] [0;36m(Worker_TP0 pid=47233)[0;0m INFO 01-18 17:06:07 [monitor.py:34] torch.compile takes 64.13 s in total
[2026-01-18 17:06:08] [INFO] [0;36m(Worker_TP0 pid=47233)[0;0m INFO 01-18 17:06:08 [gpu_worker.py:358] Available KV cache memory: 49.13 GiB
[2026-01-18 17:06:09] [INFO] [0;36m(EngineCore_DP0 pid=46993)[0;0m INFO 01-18 17:06:09 [kv_cache_utils.py:1305] GPU KV cache size: 2,862,096 tokens
[2026-01-18 17:06:09] [INFO] [0;36m(EngineCore_DP0 pid=46993)[0;0m INFO 01-18 17:06:09 [kv_cache_utils.py:1310] Maximum concurrency for 128,000 tokens per request: 41.99x
[2026-01-18 17:06:09] [INFO] [0;36m(Worker_TP0 pid=47233)[0;0m 2026-01-18 17:06:09,335 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[2026-01-18 17:06:09] [INFO] [0;36m(Worker_TP1 pid=47234)[0;0m 2026-01-18 17:06:09,335 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[2026-01-18 17:06:09] [INFO] [0;36m(Worker_TP0 pid=47233)[0;0m 2026-01-18 17:06:09,360 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[2026-01-18 17:06:09] [INFO] [0;36m(Worker_TP1 pid=47234)[0;0m 2026-01-18 17:06:09,362 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[2026-01-18 17:06:09] [INFO] [0;36m(Worker_TP0 pid=47233)[0;0m
[2026-01-18 17:06:09] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/83 [00:00<?, ?it/s]
[2026-01-18 17:06:10] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 2/83 [00:00<00:05, 14.58it/s]
[2026-01-18 17:06:10] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▍         | 4/83 [00:00<00:05, 14.92it/s]
[2026-01-18 17:06:10] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   7%|▋         | 6/83 [00:00<00:05, 15.20it/s]
[2026-01-18 17:06:10] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 8/83 [00:00<00:04, 15.44it/s]
[2026-01-18 17:06:10] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 10/83 [00:00<00:04, 15.59it/s]
[2026-01-18 17:06:10] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 12/83 [00:00<00:04, 15.70it/s]
[2026-01-18 17:06:10] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 14/83 [00:00<00:04, 15.83it/s]
[2026-01-18 17:06:10] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|█▉        | 16/83 [00:01<00:04, 15.55it/s]
[2026-01-18 17:06:11] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 18/83 [00:01<00:04, 15.50it/s]
[2026-01-18 17:06:11] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▍       | 20/83 [00:01<00:04, 15.26it/s]
[2026-01-18 17:06:11] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 22/83 [00:01<00:04, 14.74it/s]
[2026-01-18 17:06:11] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 24/83 [00:01<00:04, 13.92it/s]
[2026-01-18 17:06:11] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 26/83 [00:01<00:03, 14.46it/s]
[2026-01-18 17:06:11] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▎      | 28/83 [00:01<00:03, 14.87it/s]
[2026-01-18 17:06:11] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▌      | 30/83 [00:01<00:03, 15.27it/s]
[2026-01-18 17:06:12] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▊      | 32/83 [00:02<00:03, 15.61it/s]
[2026-01-18 17:06:12] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 34/83 [00:02<00:03, 15.89it/s]
[2026-01-18 17:06:12] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 36/83 [00:02<00:03, 15.35it/s]
[2026-01-18 17:06:12] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▌     | 38/83 [00:02<00:02, 15.85it/s]
[2026-01-18 17:06:12] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  48%|████▊     | 40/83 [00:02<00:02, 16.09it/s]
[2026-01-18 17:06:12] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 42/83 [00:02<00:02, 16.27it/s]
[2026-01-18 17:06:12] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 44/83 [00:02<00:02, 16.48it/s]
[2026-01-18 17:06:12] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▌    | 46/83 [00:02<00:02, 16.77it/s]
[2026-01-18 17:06:12] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 48/83 [00:03<00:02, 16.83it/s]
[2026-01-18 17:06:13] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 50/83 [00:03<00:01, 16.93it/s]
[2026-01-18 17:06:13] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 52/83 [00:03<00:01, 16.99it/s]
[2026-01-18 17:06:13] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▌   | 54/83 [00:03<00:01, 17.08it/s]
[2026-01-18 17:06:13] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 56/83 [00:03<00:01, 17.40it/s]
[2026-01-18 17:06:13] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  70%|██████▉   | 58/83 [00:03<00:01, 17.64it/s]
[2026-01-18 17:06:13] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|███████▏  | 60/83 [00:03<00:01, 17.93it/s]
[2026-01-18 17:06:13] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 62/83 [00:03<00:01, 18.25it/s]
[2026-01-18 17:06:13] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 64/83 [00:03<00:01, 18.48it/s]
[2026-01-18 17:06:13] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|███████▉  | 66/83 [00:04<00:00, 18.57it/s]
[2026-01-18 17:06:14] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 68/83 [00:04<00:00, 18.65it/s]
[2026-01-18 17:06:14] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 70/83 [00:04<00:00, 18.62it/s]
[2026-01-18 17:06:14] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  87%|████████▋ | 72/83 [00:04<00:00, 18.53it/s]
[2026-01-18 17:06:14] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 74/83 [00:04<00:00, 18.68it/s]
[2026-01-18 17:06:14] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 76/83 [00:04<00:00, 18.80it/s]
[2026-01-18 17:06:14] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 78/83 [00:04<00:00, 18.87it/s]
[2026-01-18 17:06:14] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▋| 80/83 [00:04<00:00, 18.84it/s]
[2026-01-18 17:06:14] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  99%|█████████▉| 82/83 [00:04<00:00, 18.51it/s]
[2026-01-18 17:06:14] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 83/83 [00:04<00:00, 16.62it/s]
[2026-01-18 17:06:14] [INFO] [0;36m(Worker_TP0 pid=47233)[0;0m
[2026-01-18 17:06:16] [INFO] Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
[2026-01-18 17:06:17] [INFO] Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:01<00:57,  1.69s/it]
[2026-01-18 17:06:17] [INFO] Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:02<00:38,  1.16s/it]
[2026-01-18 17:06:17] [INFO] Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:02<00:14,  2.14it/s]
[2026-01-18 17:06:17] [INFO] Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:02<00:07,  3.65it/s]
[2026-01-18 17:06:17] [INFO] Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:02<00:05,  5.39it/s]
[2026-01-18 17:06:17] [INFO] Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:02<00:03,  7.23it/s]
[2026-01-18 17:06:17] [INFO] Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:03<00:02,  9.08it/s]
[2026-01-18 17:06:18] [INFO] Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:03<00:01, 10.81it/s]
[2026-01-18 17:06:18] [INFO] Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:03<00:01, 12.30it/s]
[2026-01-18 17:06:18] [INFO] Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:03<00:01, 13.63it/s]
[2026-01-18 17:06:18] [INFO] Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:03<00:01, 14.69it/s]
[2026-01-18 17:06:18] [INFO] Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:03<00:00, 15.48it/s]
[2026-01-18 17:06:18] [INFO] Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:03<00:00, 16.10it/s]
[2026-01-18 17:06:18] [INFO] Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:03<00:00, 16.71it/s]
[2026-01-18 17:06:21] [INFO] Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:03<00:00, 17.10it/s]
[2026-01-18 17:06:21] [INFO] Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:06<00:02,  2.18it/s]
[2026-01-18 17:06:21] [INFO] Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:06<00:01,  2.96it/s]
[2026-01-18 17:06:23] [INFO] Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:06<00:00,  3.94it/s]
[2026-01-18 17:06:23] [INFO] Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:08<00:00,  4.21it/s]
[2026-01-18 17:06:23] [INFO] [0;36m(Worker_TP0 pid=47233)[0;0m INFO 01-18 17:06:23 [custom_all_reduce.py:216] Registering 8614 cuda graph addresses
[2026-01-18 17:06:23] [INFO] [0;36m(Worker_TP1 pid=47234)[0;0m INFO 01-18 17:06:23 [custom_all_reduce.py:216] Registering 8614 cuda graph addresses
[2026-01-18 17:06:23] [INFO] [0;36m(Worker_TP0 pid=47233)[0;0m INFO 01-18 17:06:23 [gpu_model_runner.py:4837] Graph capturing finished in 15 secs, took 0.21 GiB
[2026-01-18 17:06:23] [INFO] [0;36m(EngineCore_DP0 pid=46993)[0;0m INFO 01-18 17:06:23 [core.py:273] init engine (profile, create kv cache, warmup model) took 83.17 seconds
[2026-01-18 17:06:24] [INFO] [0;36m(EngineCore_DP0 pid=46993)[0;0m The tokenizer you are loading from '/mnt/AI-Acer4T/AI-Chat/models/GPT-OSS/gpt-oss-120b-Derestricted-MXFP4/' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[2026-01-18 17:06:25] [INFO] [0;36m(EngineCore_DP0 pid=46993)[0;0m INFO 01-18 17:06:25 [core.py:186] Batch queue is enabled with size 2
[2026-01-18 17:06:25] [INFO] [0;36m(EngineCore_DP0 pid=46993)[0;0m INFO 01-18 17:06:25 [vllm.py:645] Asynchronous scheduling is enabled.
[2026-01-18 17:06:26] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:06:26 [api_server.py:1020] Supported tasks: ['generate']
[2026-01-18 17:06:26] [INFO] [0;36m(APIServer pid=46679)[0;0m The tokenizer you are loading from '/mnt/AI-Acer4T/AI-Chat/models/GPT-OSS/gpt-oss-120b-Derestricted-MXFP4/' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[2026-01-18 17:06:26] [WARNING] [0;36m(APIServer pid=46679)[0;0m WARNING 01-18 17:06:26 [serving_responses.py:245] For gpt-oss, we ignore --enable-auto-tool-choice and always enable tool use.
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:08:06 [serving_engine.py:271] "auto" tool choice has been enabled.
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:08:06 [serving_engine.py:271] "auto" tool choice has been enabled.
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:08:06 [serving_chat.py:182] Warming up chat template processing...
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:08:06 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:08:06 [serving_chat.py:218] Chat template warmup completed in 69.4ms
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:08:06 [serving_engine.py:271] "auto" tool choice has been enabled.
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:08:06 [api_server.py:1352] Starting vLLM API server 0 on http://0.0.0.0:8005
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:08:06 [launcher.py:38] Available routes are:
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:08:06 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:08:06 [launcher.py:46] Route: /docs, Methods: HEAD, GET
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:08:06 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:08:06 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:08:06 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:08:06 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:08:06 [launcher.py:46] Route: /tokenize, Methods: POST
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:08:06 [launcher.py:46] Route: /detokenize, Methods: POST
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:08:06 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:08:06 [launcher.py:46] Route: /pause, Methods: POST
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:08:06 [launcher.py:46] Route: /resume, Methods: POST
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:08:06 [launcher.py:46] Route: /is_paused, Methods: GET
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:08:06 [launcher.py:46] Route: /metrics, Methods: GET
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:08:06 [launcher.py:46] Route: /health, Methods: GET
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:08:06 [launcher.py:46] Route: /load, Methods: GET
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:08:06 [launcher.py:46] Route: /v1/models, Methods: GET
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:08:06 [launcher.py:46] Route: /version, Methods: GET
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:08:06 [launcher.py:46] Route: /v1/responses, Methods: POST
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:08:06 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:08:06 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:08:06 [launcher.py:46] Route: /v1/messages, Methods: POST
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:08:06 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:08:06 [launcher.py:46] Route: /v1/completions, Methods: POST
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:08:06 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:08:06 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:08:06 [launcher.py:46] Route: /ping, Methods: GET
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:08:06 [launcher.py:46] Route: /ping, Methods: POST
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:08:06 [launcher.py:46] Route: /invocations, Methods: POST
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:08:06 [launcher.py:46] Route: /classify, Methods: POST
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:08:06 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:08:06 [launcher.py:46] Route: /score, Methods: POST
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:08:06 [launcher.py:46] Route: /v1/score, Methods: POST
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:08:06 [launcher.py:46] Route: /rerank, Methods: POST
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:08:06 [launcher.py:46] Route: /v1/rerank, Methods: POST
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:08:06 [launcher.py:46] Route: /v2/rerank, Methods: POST
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:08:06 [launcher.py:46] Route: /pooling, Methods: POST
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO:     Started server process [46679]
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO:     Waiting for application startup.
[2026-01-18 17:08:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO:     Application startup complete.
[2026-01-18 17:12:56] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO:     127.0.0.1:53550 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-18 17:13:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:13:06 [loggers.py:257] Engine 000: Avg prompt throughput: 6.6 tokens/s, Avg generation throughput: 6.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[2026-01-18 17:13:16] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:13:16 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[2026-01-18 17:13:16] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO:     127.0.0.1:37942 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-18 17:13:26] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:13:26 [loggers.py:257] Engine 000: Avg prompt throughput: 12.6 tokens/s, Avg generation throughput: 124.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 33.3%
[2026-01-18 17:13:36] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:13:36 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 33.3%
[2026-01-18 17:13:38] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO:     127.0.0.1:58374 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-18 17:13:46] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:13:46 [loggers.py:257] Engine 000: Avg prompt throughput: 12.7 tokens/s, Avg generation throughput: 136.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 50.2%
[2026-01-18 17:13:56] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:13:56 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 50.2%
[2026-01-18 17:14:05] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO:     127.0.0.1:41828 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-18 17:14:06] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:14:06 [loggers.py:257] Engine 000: Avg prompt throughput: 13.0 tokens/s, Avg generation throughput: 17.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 57.0%
[2026-01-18 17:14:16] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:14:16 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 119.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 57.0%
[2026-01-18 17:14:26] [INFO] [0;36m(APIServer pid=46679)[0;0m INFO 01-18 17:14:26 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 57.0%
[2026-01-18 17:14:49] [ERROR] [0;36m(APIServer pid=46679)[0;0m ERROR 01-18 17:14:49 [core_client.py:610] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.
[2026-01-18 17:14:49] [INFO] [0;36m(Worker_TP0 pid=47233)[0;0m [0;36m(Worker_TP1 pid=47234)[0;0m INFO 01-18 17:14:49 [multiproc_executor.py:707] Parent process exited, terminating worker
[2026-01-18 17:14:49] [INFO] INFO 01-18 17:14:49 [multiproc_executor.py:707] Parent process exited, terminating worker
[2026-01-18 17:14:49] [INFO] [0;36m(Worker_TP1 pid=47234)[0;0m INFO 01-18 17:14:49 [multiproc_executor.py:751] WorkerProc shutting down.
[2026-01-18 17:14:49] [INFO] [0;36m(Worker_TP0 pid=47233)[0;0m INFO 01-18 17:14:49 [multiproc_executor.py:751] WorkerProc shutting down.
[2026-01-18 17:14:49] [INFO] [0;36m(Worker_TP1 pid=47234)[0;0m /mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py:147: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.
[2026-01-18 17:14:49] [INFO] [0;36m(Worker_TP1 pid=47234)[0;0m   warnings.warn('resource_tracker: process died unexpectedly, '
[2026-01-18 17:14:49] [INFO] [0;36m(Worker_TP0 pid=47233)[0;0m /mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py:147: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.
[2026-01-18 17:14:49] [INFO] [0;36m(Worker_TP0 pid=47233)[0;0m   warnings.warn('resource_tracker: process died unexpectedly, '
[2026-01-18 17:14:49] [ERROR] Traceback (most recent call last):
[2026-01-18 17:14:49] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 17:14:49] [ERROR] Traceback (most recent call last):
[2026-01-18 17:14:49] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 17:14:49] [INFO] cache[rtype].remove(name)
[2026-01-18 17:14:49] [INFO] cache[rtype].remove(name)
[2026-01-18 17:14:49] [INFO] KeyError: '/psm_ec33fb25'
[2026-01-18 17:14:49] [INFO] KeyError: '/psm_e8f8ed8d'
[2026-01-18 17:14:49] [INFO] 服务已停止
[2026-01-18 17:14:49] [ERROR] Traceback (most recent call last):
[2026-01-18 17:14:49] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 17:14:49] [INFO] cache[rtype].remove(name)
[2026-01-18 17:14:49] [INFO] KeyError: '/psm_dac10da1'
[2026-01-18 17:14:49] [ERROR] Traceback (most recent call last):
[2026-01-18 17:14:49] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 17:14:49] [INFO] cache[rtype].remove(name)
[2026-01-18 17:14:49] [INFO] KeyError: '/mp-ulsv1jj2'
[2026-01-18 17:14:49] [ERROR] Traceback (most recent call last):
[2026-01-18 17:14:49] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 17:14:49] [INFO] cache[rtype].remove(name)
[2026-01-18 17:14:49] [INFO] KeyError: '/mp-odqiikx4'
[2026-01-18 17:14:50] [INFO] nvitop监控已启动
[2026-01-18 17:14:50] [INFO] nvitop监控已启动
[2026-01-18 17:14:52] [INFO] nvitop监控已启动
[2026-01-18 17:14:56] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 17:14:56] [INFO] nvitop监控已启动
[2026-01-18 17:14:56] [INFO] nvitop监控已停止
[2026-01-18 17:14:57] [INFO] /bin/bash: 第 1 行： export: " ": 不是有效的标识符
[2026-01-18 17:14:58] [INFO] nvitop监控已启动
[2026-01-18 17:18:12] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 17:18:12] [INFO] nvitop监控已启动
[2026-01-18 17:18:12] [INFO] nvitop监控已停止
[2026-01-18 17:18:35] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:18:35 [api_server.py:1278] vLLM API server version 0.14.0rc1.dev492+g19504ac07
[2026-01-18 17:18:35] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:18:35 [utils.py:254] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/Seed/Seed-OSS-36B-Instruct', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/AI-Acer4T/AI-Chat/models/Seed/Seed-OSS-36B-Instruct/chat_template.jinja', 'enable_auto_tool_choice': True, 'tool_call_parser': 'seed_oss', 'model': '/mnt/AI-Acer4T/AI-Chat/models/Seed/Seed-OSS-36B-Instruct', 'trust_remote_code': True, 'max_model_len': 128000, 'served_model_name': ['VLLM-MODEL'], 'reasoning_parser': 'seed_oss', 'tensor_parallel_size': 2, 'disable_custom_all_reduce': True, 'kv_cache_dtype': 'fp8', 'enable_prefix_caching': True, 'max_num_seqs': 256, 'async_scheduling': True, 'attention_config': AttentionConfig(backend=<AttentionBackendEnum.FLASHINFER: 'vllm.v1.attention.backends.flashinfer.FlashInferBackend'>, flash_attn_version=None, use_prefill_decode_attention=False, flash_attn_max_num_splits_for_cuda_graph=32, use_cudnn_prefill=False, use_trtllm_ragged_deepseek_prefill=False, use_trtllm_attention=None, disable_flashinfer_prefill=False, disable_flashinfer_q_quantization=False)}
[2026-01-18 17:18:35] [INFO] [0;36m(APIServer pid=53805)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 17:18:35] [INFO] [0;36m(APIServer pid=53805)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 17:18:35] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:18:35 [model.py:528] Resolved architecture: SeedOssForCausalLM
[2026-01-18 17:18:35] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:18:35 [model.py:1543] Using max model len 128000
[2026-01-18 17:18:35] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:18:35 [cache.py:206] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor.
[2026-01-18 17:18:35] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:18:35 [scheduler.py:231] Chunked prefill is enabled with max_num_batched_tokens=8192.
[2026-01-18 17:18:35] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:18:35 [vllm.py:640] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-18 17:18:35] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:18:35 [vllm.py:645] Asynchronous scheduling is enabled.
[2026-01-18 17:18:36] [INFO] [0;36m(APIServer pid=53805)[0;0m The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
[2026-01-18 17:19:00] [INFO] [0;36m(EngineCore_DP0 pid=54124)[0;0m INFO 01-18 17:19:00 [core.py:97] Initializing a V1 LLM engine (v0.14.0rc1.dev492+g19504ac07) with config: model='/mnt/AI-Acer4T/AI-Chat/models/Seed/Seed-OSS-36B-Instruct', speculative_config=None, tokenizer='/mnt/AI-Acer4T/AI-Chat/models/Seed/Seed-OSS-36B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=fp8, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='seed_oss', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-18 17:19:00] [WARNING] [0;36m(EngineCore_DP0 pid=54124)[0;0m WARNING 01-18 17:19:00 [multiproc_executor.py:880] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-18 17:19:23] [INFO] INFO 01-18 17:19:23 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:54133 backend=nccl
[2026-01-18 17:19:24] [INFO] INFO 01-18 17:19:24 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:54133 backend=nccl
[2026-01-18 17:19:24] [INFO] INFO 01-18 17:19:24 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-18 17:19:25] [WARNING] WARNING 01-18 17:19:25 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 17:19:25] [WARNING] WARNING 01-18 17:19:25 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 17:19:25] [INFO] INFO 01-18 17:19:25 [parallel_state.py:1425] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
[2026-01-18 17:19:25] [INFO] INFO 01-18 17:19:25 [parallel_state.py:1425] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[2026-01-18 17:19:29] [INFO] INFO 01-18 17:19:29 [topk_topp_sampler.py:47] Using FlashInfer for top-p & top-k sampling.
[2026-01-18 17:19:29] [INFO] [0;36m(Worker_TP0 pid=54365)[0;0m INFO 01-18 17:19:29 [gpu_model_runner.py:3789] Starting to load model /mnt/AI-Acer4T/AI-Chat/models/Seed/Seed-OSS-36B-Instruct...
[2026-01-18 17:19:29] [INFO] [0;36m(Worker_TP1 pid=54366)[0;0m INFO 01-18 17:19:29 [cuda.py:315] Using AttentionBackendEnum.FLASHINFER backend.
[2026-01-18 17:19:29] [INFO] [0;36m(Worker_TP0 pid=54365)[0;0m INFO 01-18 17:19:29 [cuda.py:315] Using AttentionBackendEnum.FLASHINFER backend.
[2026-01-18 17:19:30] [INFO] [0;36m(Worker_TP0 pid=54365)[0;0m
[2026-01-18 17:19:30] [INFO] Loading safetensors checkpoint shards:   0% Completed | 0/15 [00:00<?, ?it/s]
[2026-01-18 17:19:34] [INFO] [0;36m(Worker_TP0 pid=54365)[0;0m
[2026-01-18 17:19:34] [INFO] Loading safetensors checkpoint shards:   7% Completed | 1/15 [00:03<00:50,  3.63s/it]
[2026-01-18 17:19:39] [INFO] [0;36m(Worker_TP0 pid=54365)[0;0m
[2026-01-18 17:19:39] [INFO] Loading safetensors checkpoint shards:  13% Completed | 2/15 [00:09<01:00,  4.67s/it]
[2026-01-18 17:19:44] [INFO] [0;36m(Worker_TP0 pid=54365)[0;0m
[2026-01-18 17:19:44] [INFO] Loading safetensors checkpoint shards:  20% Completed | 3/15 [00:14<00:58,  4.89s/it]
[2026-01-18 17:19:50] [INFO] [0;36m(Worker_TP0 pid=54365)[0;0m
[2026-01-18 17:19:50] [INFO] Loading safetensors checkpoint shards:  27% Completed | 4/15 [00:19<00:56,  5.12s/it]
[2026-01-18 17:19:54] [INFO] [0;36m(Worker_TP0 pid=54365)[0;0m
[2026-01-18 17:19:54] [INFO] Loading safetensors checkpoint shards:  33% Completed | 5/15 [00:24<00:50,  5.01s/it]
[2026-01-18 17:20:00] [INFO] [0;36m(Worker_TP0 pid=54365)[0;0m
[2026-01-18 17:20:00] [INFO] Loading safetensors checkpoint shards:  40% Completed | 6/15 [00:29<00:46,  5.13s/it]
[2026-01-18 17:20:06] [INFO] [0;36m(Worker_TP0 pid=54365)[0;0m
[2026-01-18 17:20:06] [INFO] Loading safetensors checkpoint shards:  47% Completed | 7/15 [00:35<00:42,  5.37s/it]
[2026-01-18 17:20:11] [INFO] [0;36m(Worker_TP0 pid=54365)[0;0m
[2026-01-18 17:20:11] [INFO] Loading safetensors checkpoint shards:  53% Completed | 8/15 [00:41<00:38,  5.47s/it]
[2026-01-18 17:20:16] [INFO] [0;36m(Worker_TP0 pid=54365)[0;0m
[2026-01-18 17:20:16] [INFO] Loading safetensors checkpoint shards:  60% Completed | 9/15 [00:46<00:31,  5.31s/it]
[2026-01-18 17:20:21] [INFO] [0;36m(Worker_TP0 pid=54365)[0;0m
[2026-01-18 17:20:21] [INFO] Loading safetensors checkpoint shards:  67% Completed | 10/15 [00:51<00:25,  5.14s/it]
[2026-01-18 17:20:26] [INFO] [0;36m(Worker_TP0 pid=54365)[0;0m
[2026-01-18 17:20:26] [INFO] Loading safetensors checkpoint shards:  73% Completed | 11/15 [00:56<00:20,  5.11s/it]
[2026-01-18 17:20:32] [INFO] [0;36m(Worker_TP0 pid=54365)[0;0m
[2026-01-18 17:20:32] [INFO] Loading safetensors checkpoint shards:  80% Completed | 12/15 [01:01<00:15,  5.22s/it]
[2026-01-18 17:20:36] [INFO] [0;36m(Worker_TP0 pid=54365)[0;0m
[2026-01-18 17:20:36] [INFO] Loading safetensors checkpoint shards:  87% Completed | 13/15 [01:06<00:09,  4.98s/it]
[2026-01-18 17:20:41] [INFO] [0;36m(Worker_TP0 pid=54365)[0;0m
[2026-01-18 17:20:41] [INFO] Loading safetensors checkpoint shards:  93% Completed | 14/15 [01:10<00:04,  4.84s/it]
[2026-01-18 17:20:44] [INFO] [0;36m(Worker_TP0 pid=54365)[0;0m
[2026-01-18 17:20:44] [INFO] Loading safetensors checkpoint shards: 100% Completed | 15/15 [01:14<00:00,  4.49s/it]
[2026-01-18 17:20:44] [INFO] [0;36m(Worker_TP0 pid=54365)[0;0m
[2026-01-18 17:20:44] [INFO] Loading safetensors checkpoint shards: 100% Completed | 15/15 [01:14<00:00,  4.95s/it]
[2026-01-18 17:20:44] [INFO] [0;36m(Worker_TP0 pid=54365)[0;0m
[2026-01-18 17:20:44] [INFO] [0;36m(Worker_TP0 pid=54365)[0;0m INFO 01-18 17:20:44 [default_loader.py:291] Loading weights took 74.34 seconds
[2026-01-18 17:20:45] [INFO] [0;36m(Worker_TP0 pid=54365)[0;0m INFO 01-18 17:20:45 [gpu_model_runner.py:3886] Model loading took 33.86 GiB memory and 75.042524 seconds
[2026-01-18 17:20:57] [INFO] [0;36m(Worker_TP0 pid=54365)[0;0m INFO 01-18 17:20:57 [backends.py:644] Using cache directory: /home/wuwen/.cache/vllm/torch_compile_cache/e3a4a258bb/rank_0_0/backbone for vLLM's torch.compile
[2026-01-18 17:20:57] [INFO] [0;36m(Worker_TP0 pid=54365)[0;0m INFO 01-18 17:20:57 [backends.py:704] Dynamo bytecode transform time: 11.79 s
[2026-01-18 17:21:05] [INFO] [0;36m(Worker_TP1 pid=54366)[0;0m INFO 01-18 17:21:05 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 3.639 s
[2026-01-18 17:21:05] [INFO] [0;36m(Worker_TP0 pid=54365)[0;0m INFO 01-18 17:21:05 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 3.664 s
[2026-01-18 17:21:05] [INFO] [0;36m(Worker_TP0 pid=54365)[0;0m INFO 01-18 17:21:05 [monitor.py:34] torch.compile takes 15.45 s in total
[2026-01-18 17:21:06] [INFO] [0;36m(Worker_TP0 pid=54365)[0;0m INFO 01-18 17:21:06 [gpu_worker.py:358] Available KV cache memory: 50.65 GiB
[2026-01-18 17:21:06] [INFO] [0;36m(EngineCore_DP0 pid=54124)[0;0m INFO 01-18 17:21:06 [kv_cache_utils.py:1305] GPU KV cache size: 829,888 tokens
[2026-01-18 17:21:06] [INFO] [0;36m(EngineCore_DP0 pid=54124)[0;0m INFO 01-18 17:21:06 [kv_cache_utils.py:1310] Maximum concurrency for 128,000 tokens per request: 6.48x
[2026-01-18 17:21:06] [INFO] [0;36m(Worker_TP0 pid=54365)[0;0m 2026-01-18 17:21:06,843 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[2026-01-18 17:21:06] [INFO] [0;36m(Worker_TP1 pid=54366)[0;0m 2026-01-18 17:21:06,843 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[2026-01-18 17:21:06] [INFO] [0;36m(Worker_TP0 pid=54365)[0;0m 2026-01-18 17:21:06,872 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[2026-01-18 17:21:06] [INFO] [0;36m(Worker_TP0 pid=54365)[0;0m INFO 01-18 17:21:06 [kernel_warmup.py:64] Warming up FlashInfer attention.
[2026-01-18 17:21:06] [INFO] [0;36m(Worker_TP1 pid=54366)[0;0m 2026-01-18 17:21:06,873 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[2026-01-18 17:21:06] [INFO] [0;36m(Worker_TP1 pid=54366)[0;0m INFO 01-18 17:21:06 [kernel_warmup.py:64] Warming up FlashInfer attention.
[2026-01-18 17:21:08] [INFO] [0;36m(Worker_TP0 pid=54365)[0;0m
[2026-01-18 17:21:08] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
[2026-01-18 17:21:08] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:09,  5.40it/s]
[2026-01-18 17:21:08] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:07,  6.46it/s]
[2026-01-18 17:21:09] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:06,  6.90it/s]
[2026-01-18 17:21:09] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:00<00:06,  7.12it/s]
[2026-01-18 17:21:09] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:00<00:06,  7.38it/s]
[2026-01-18 17:21:09] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:05,  7.54it/s]
[2026-01-18 17:21:09] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▎        | 7/51 [00:00<00:05,  7.62it/s]
[2026-01-18 17:21:09] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:01<00:05,  7.70it/s]
[2026-01-18 17:21:09] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:01<00:05,  7.90it/s]
[2026-01-18 17:21:09] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|█▉        | 10/51 [00:01<00:05,  8.05it/s]
[2026-01-18 17:21:10] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:01<00:04,  8.16it/s]
[2026-01-18 17:21:10] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:01<00:04,  8.25it/s]
[2026-01-18 17:21:10] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:01<00:04,  8.20it/s]
[2026-01-18 17:21:10] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:01<00:04,  8.13it/s]
[2026-01-18 17:21:10] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:01<00:04,  8.15it/s]
[2026-01-18 17:21:10] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 16/51 [00:02<00:04,  8.18it/s]
[2026-01-18 17:21:10] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:02<00:04,  8.46it/s]
[2026-01-18 17:21:10] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:02<00:03,  8.63it/s]
[2026-01-18 17:21:11] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 19/51 [00:02<00:03,  8.76it/s]
[2026-01-18 17:21:11] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:02<00:03,  8.47it/s]
[2026-01-18 17:21:11] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:02<00:03,  8.58it/s]
[2026-01-18 17:21:11] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 22/51 [00:02<00:03,  8.74it/s]
[2026-01-18 17:21:11] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:02<00:03,  8.84it/s]
[2026-01-18 17:21:11] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:02<00:03,  8.85it/s]
[2026-01-18 17:21:11] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 25/51 [00:03<00:02,  8.99it/s]
[2026-01-18 17:21:11] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:03<00:02,  9.04it/s]
[2026-01-18 17:21:11] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:03<00:02,  9.09it/s]
[2026-01-18 17:21:12] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:03<00:02,  9.15it/s]
[2026-01-18 17:21:12] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:03<00:02,  9.20it/s]
[2026-01-18 17:21:12] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:03<00:02,  9.19it/s]
[2026-01-18 17:21:12] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 31/51 [00:03<00:02,  9.24it/s]
[2026-01-18 17:21:12] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:03<00:02,  8.60it/s]
[2026-01-18 17:21:12] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:03<00:02,  8.81it/s]
[2026-01-18 17:21:12] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:04<00:01,  8.96it/s]
[2026-01-18 17:21:12] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:04<00:01,  9.08it/s]
[2026-01-18 17:21:12] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:04<00:01,  9.09it/s]
[2026-01-18 17:21:13] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 37/51 [00:04<00:01,  9.01it/s]
[2026-01-18 17:21:13] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:04<00:01,  8.97it/s]
[2026-01-18 17:21:13] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▋  | 39/51 [00:04<00:01,  8.85it/s]
[2026-01-18 17:21:13] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:04<00:01,  8.75it/s]
[2026-01-18 17:21:13] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:04<00:01,  8.74it/s]
[2026-01-18 17:21:13] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:04<00:01,  8.81it/s]
[2026-01-18 17:21:13] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 43/51 [00:05<00:00,  8.79it/s]
[2026-01-18 17:21:13] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:05<00:00,  8.87it/s]
[2026-01-18 17:21:13] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 45/51 [00:05<00:00,  8.83it/s]
[2026-01-18 17:21:14] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|█████████ | 46/51 [00:05<00:00,  8.71it/s]
[2026-01-18 17:21:14] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:05<00:00,  8.91it/s]
[2026-01-18 17:21:14] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 48/51 [00:05<00:00,  9.01it/s]
[2026-01-18 17:21:14] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▌| 49/51 [00:05<00:00,  9.25it/s]
[2026-01-18 17:21:14] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:05<00:00,  9.32it/s]
[2026-01-18 17:21:14] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:05<00:00,  8.69it/s]
[2026-01-18 17:21:14] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:05<00:00,  8.52it/s]
[2026-01-18 17:21:14] [INFO] [0;36m(Worker_TP0 pid=54365)[0;0m
[2026-01-18 17:21:14] [INFO] Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
[2026-01-18 17:21:14] [INFO] Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  7.87it/s]
[2026-01-18 17:21:14] [INFO] Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:00<00:04,  8.18it/s]
[2026-01-18 17:21:14] [INFO] Capturing CUDA graphs (decode, FULL):   9%|▊         | 3/35 [00:00<00:03,  8.66it/s]
[2026-01-18 17:21:15] [INFO] Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:03,  8.90it/s]
[2026-01-18 17:21:15] [INFO] Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:00<00:03,  9.07it/s]
[2026-01-18 17:21:15] [INFO] Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:00<00:03,  9.14it/s]
[2026-01-18 17:21:15] [INFO] Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:00<00:03,  9.04it/s]
[2026-01-18 17:21:15] [INFO] Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:00<00:02,  9.16it/s]
[2026-01-18 17:21:15] [INFO] Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:00<00:02,  9.40it/s]
[2026-01-18 17:21:15] [INFO] Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:01<00:02,  9.53it/s]
[2026-01-18 17:21:16] [INFO] Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:01<00:02,  9.71it/s]
[2026-01-18 17:21:16] [INFO] Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:01<00:02,  9.83it/s]
[2026-01-18 17:21:16] [INFO] Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:01<00:01,  9.93it/s]
[2026-01-18 17:21:16] [INFO] Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:01<00:01, 10.12it/s]
[2026-01-18 17:21:16] [INFO] Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:02<00:01, 10.22it/s]
[2026-01-18 17:21:16] [INFO] Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:02<00:01, 10.12it/s]
[2026-01-18 17:21:17] [INFO] Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:02<00:01, 10.19it/s]
[2026-01-18 17:21:17] [INFO] Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:02<00:00, 10.31it/s]
[2026-01-18 17:21:17] [INFO] Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:02<00:00, 10.49it/s]
[2026-01-18 17:21:17] [INFO] Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:03<00:00, 10.65it/s]
[2026-01-18 17:21:17] [INFO] Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:03<00:00, 10.95it/s]
[2026-01-18 17:21:17] [INFO] Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:03<00:00, 11.14it/s]
[2026-01-18 17:21:17] [INFO] Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:03<00:00, 10.14it/s]
[2026-01-18 17:21:18] [INFO] [0;36m(Worker_TP0 pid=54365)[0;0m INFO 01-18 17:21:18 [gpu_model_runner.py:4837] Graph capturing finished in 10 secs, took 1.91 GiB
[2026-01-18 17:21:18] [INFO] [0;36m(EngineCore_DP0 pid=54124)[0;0m INFO 01-18 17:21:18 [core.py:273] init engine (profile, create kv cache, warmup model) took 33.04 seconds
[2026-01-18 17:21:19] [INFO] [0;36m(EngineCore_DP0 pid=54124)[0;0m INFO 01-18 17:21:19 [core.py:186] Batch queue is enabled with size 2
[2026-01-18 17:21:19] [INFO] [0;36m(EngineCore_DP0 pid=54124)[0;0m INFO 01-18 17:21:19 [vllm.py:645] Asynchronous scheduling is enabled.
[2026-01-18 17:21:19] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:19 [api_server.py:1020] Supported tasks: ['generate']
[2026-01-18 17:21:19] [WARNING] [0;36m(APIServer pid=53805)[0;0m WARNING 01-18 17:21:19 [model.py:1356] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[2026-01-18 17:21:19] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:19 [serving_responses.py:224] Using default chat sampling params from model: {'temperature': 1.1, 'top_p': 0.95}
[2026-01-18 17:21:19] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:19 [serving_engine.py:271] "auto" tool choice has been enabled.
[2026-01-18 17:21:19] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:19 [serving_engine.py:271] "auto" tool choice has been enabled.
[2026-01-18 17:21:19] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:19 [serving_chat.py:146] Using default chat sampling params from model: {'temperature': 1.1, 'top_p': 0.95}
[2026-01-18 17:21:19] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:19 [serving_chat.py:182] Warming up chat template processing...
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:20 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:20 [serving_chat.py:218] Chat template warmup completed in 57.5ms
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:20 [serving_completion.py:78] Using default completion sampling params from model: {'temperature': 1.1, 'top_p': 0.95}
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:20 [serving_engine.py:271] "auto" tool choice has been enabled.
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:20 [serving_chat.py:146] Using default chat sampling params from model: {'temperature': 1.1, 'top_p': 0.95}
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:20 [api_server.py:1352] Starting vLLM API server 0 on http://0.0.0.0:8005
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:20 [launcher.py:38] Available routes are:
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:20 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:20 [launcher.py:46] Route: /docs, Methods: GET, HEAD
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:20 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:20 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:20 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:20 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:20 [launcher.py:46] Route: /tokenize, Methods: POST
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:20 [launcher.py:46] Route: /detokenize, Methods: POST
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:20 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:20 [launcher.py:46] Route: /pause, Methods: POST
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:20 [launcher.py:46] Route: /resume, Methods: POST
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:20 [launcher.py:46] Route: /is_paused, Methods: GET
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:20 [launcher.py:46] Route: /metrics, Methods: GET
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:20 [launcher.py:46] Route: /health, Methods: GET
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:20 [launcher.py:46] Route: /load, Methods: GET
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:20 [launcher.py:46] Route: /v1/models, Methods: GET
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:20 [launcher.py:46] Route: /version, Methods: GET
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:20 [launcher.py:46] Route: /v1/responses, Methods: POST
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:20 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:20 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:20 [launcher.py:46] Route: /v1/messages, Methods: POST
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:20 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:20 [launcher.py:46] Route: /v1/completions, Methods: POST
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:20 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:20 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:20 [launcher.py:46] Route: /ping, Methods: GET
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:20 [launcher.py:46] Route: /ping, Methods: POST
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:20 [launcher.py:46] Route: /invocations, Methods: POST
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:20 [launcher.py:46] Route: /classify, Methods: POST
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:20 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:20 [launcher.py:46] Route: /score, Methods: POST
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:20 [launcher.py:46] Route: /v1/score, Methods: POST
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:20 [launcher.py:46] Route: /rerank, Methods: POST
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:20 [launcher.py:46] Route: /v1/rerank, Methods: POST
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:20 [launcher.py:46] Route: /v2/rerank, Methods: POST
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO 01-18 17:21:20 [launcher.py:46] Route: /pooling, Methods: POST
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO:     Started server process [53805]
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO:     Waiting for application startup.
[2026-01-18 17:21:20] [INFO] [0;36m(APIServer pid=53805)[0;0m INFO:     Application startup complete.
[2026-01-18 17:21:39] [INFO] [0;36m(Worker_TP0 pid=54365)[0;0m [0;36m(Worker_TP1 pid=54366)[0;0m INFO 01-18 17:21:39 [multiproc_executor.py:707] Parent process exited, terminating worker
[2026-01-18 17:21:39] [INFO] INFO 01-18 17:21:39 [multiproc_executor.py:707] Parent process exited, terminating worker
[2026-01-18 17:21:39] [ERROR] [0;36m(APIServer pid=53805)[0;0m ERROR 01-18 17:21:39 [core_client.py:610] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.
[2026-01-18 17:21:39] [INFO] [0;36m(Worker_TP1 pid=54366)[0;0m INFO 01-18 17:21:39 [multiproc_executor.py:751] WorkerProc shutting down.
[2026-01-18 17:21:39] [INFO] [0;36m(Worker_TP0 pid=54365)[0;0m INFO 01-18 17:21:39 [multiproc_executor.py:751] WorkerProc shutting down.
[2026-01-18 17:21:39] [INFO] [0;36m(Worker_TP1 pid=54366)[0;0m /mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py:147: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.
[2026-01-18 17:21:39] [INFO] [0;36m(Worker_TP1 pid=54366)[0;0m   warnings.warn('resource_tracker: process died unexpectedly, '
[2026-01-18 17:21:39] [INFO] [0;36m(Worker_TP0 pid=54365)[0;0m /mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py:147: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.
[2026-01-18 17:21:39] [INFO] [0;36m(Worker_TP0 pid=54365)[0;0m   warnings.warn('resource_tracker: process died unexpectedly, '
[2026-01-18 17:21:40] [INFO] 服务已停止
[2026-01-18 17:21:40] [ERROR] Traceback (most recent call last):
[2026-01-18 17:21:40] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 17:21:40] [INFO] cache[rtype].remove(name)
[2026-01-18 17:21:40] [INFO] KeyError: '/psm_57fe1258'
[2026-01-18 17:21:40] [ERROR] Traceback (most recent call last):
[2026-01-18 17:21:40] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 17:21:40] [INFO] cache[rtype].remove(name)
[2026-01-18 17:21:40] [INFO] KeyError: '/psm_c3ea2db1'
[2026-01-18 17:21:40] [INFO] nvitop监控已启动
[2026-01-18 17:21:40] [ERROR] Traceback (most recent call last):
[2026-01-18 17:21:40] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 17:21:40] [INFO] cache[rtype].remove(name)
[2026-01-18 17:21:40] [INFO] KeyError: '/psm_64aa65d6'
[2026-01-18 17:21:40] [ERROR] Traceback (most recent call last):
[2026-01-18 17:21:40] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 17:21:40] [INFO] cache[rtype].remove(name)
[2026-01-18 17:21:40] [INFO] KeyError: '/mp-pf74ck0q'
[2026-01-18 17:21:40] [ERROR] Traceback (most recent call last):
[2026-01-18 17:21:40] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 17:21:40] [INFO] cache[rtype].remove(name)
[2026-01-18 17:21:40] [INFO] KeyError: '/mp-mt7g8gky'
[2026-01-18 17:21:41] [INFO] nvitop监控已启动
[2026-01-18 17:21:42] [INFO] nvitop监控已启动
[2026-01-18 17:21:45] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 17:21:45] [INFO] nvitop监控已启动
[2026-01-18 17:21:45] [INFO] nvitop监控已停止
[2026-01-18 17:22:11] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:22:11 [api_server.py:1278] vLLM API server version 0.14.0rc1.dev492+g19504ac07
[2026-01-18 17:22:11] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:22:11 [utils.py:254] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/GPT-OSS/gpt-oss-120b/', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/AI-Acer4T/AI-Chat/models/GPT-OSS/gpt-oss-120b/chat_template.jinja', 'enable_auto_tool_choice': True, 'tool_call_parser': 'openai', 'model': '/mnt/AI-Acer4T/AI-Chat/models/GPT-OSS/gpt-oss-120b/', 'trust_remote_code': True, 'max_model_len': 128000, 'quantization': 'mxfp4', 'served_model_name': ['VLLM-MODEL'], 'reasoning_parser': 'openai_gptoss', 'tensor_parallel_size': 2, 'kv_cache_dtype': 'fp8', 'max_num_seqs': 256, 'async_scheduling': True}
[2026-01-18 17:22:11] [INFO] [0;36m(APIServer pid=55535)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 17:22:11] [INFO] [0;36m(APIServer pid=55535)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 17:22:11] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:22:11 [model.py:528] Resolved architecture: GptOssForCausalLM
[2026-01-18 17:22:11] [ERROR] [0;36m(APIServer pid=55535)[0;0m ERROR 01-18 17:22:11 [repo_utils.py:65] Error retrieving safetensors: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/mnt/AI-Acer4T/AI-Chat/models/GPT-OSS/gpt-oss-120b/'. Use `repo_type` argument if needed., retrying 1 of 2
[2026-01-18 17:22:13] [ERROR] [0;36m(APIServer pid=55535)[0;0m ERROR 01-18 17:22:13 [repo_utils.py:63] Error retrieving safetensors: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/mnt/AI-Acer4T/AI-Chat/models/GPT-OSS/gpt-oss-120b/'. Use `repo_type` argument if needed.
[2026-01-18 17:22:13] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:22:13 [model.py:1864] Downcasting torch.float32 to torch.bfloat16.
[2026-01-18 17:22:13] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:22:13 [model.py:1543] Using max model len 128000
[2026-01-18 17:22:13] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:22:13 [cache.py:206] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor.
[2026-01-18 17:22:14] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:22:14 [scheduler.py:231] Chunked prefill is enabled with max_num_batched_tokens=8192.
[2026-01-18 17:22:14] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:22:14 [config.py:314] Overriding max cuda graph capture size to 1024 for performance.
[2026-01-18 17:22:14] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:22:14 [vllm.py:640] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-18 17:22:14] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:22:14 [vllm.py:645] Asynchronous scheduling is enabled.
[2026-01-18 17:22:37] [INFO] [0;36m(EngineCore_DP0 pid=55894)[0;0m INFO 01-18 17:22:37 [core.py:97] Initializing a V1 LLM engine (v0.14.0rc1.dev492+g19504ac07) with config: model='/mnt/AI-Acer4T/AI-Chat/models/GPT-OSS/gpt-oss-120b/', speculative_config=None, tokenizer='/mnt/AI-Acer4T/AI-Chat/models/GPT-OSS/gpt-oss-120b/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=mxfp4, enforce_eager=False, kv_cache_dtype=fp8, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='openai_gptoss', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512, 528, 544, 560, 576, 592, 608, 624, 640, 656, 672, 688, 704, 720, 736, 752, 768, 784, 800, 816, 832, 848, 864, 880, 896, 912, 928, 944, 960, 976, 992, 1008, 1024], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 1024, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-18 17:22:37] [WARNING] [0;36m(EngineCore_DP0 pid=55894)[0;0m WARNING 01-18 17:22:37 [multiproc_executor.py:880] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-18 17:23:01] [INFO] INFO 01-18 17:23:01 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:35263 backend=nccl
[2026-01-18 17:23:01] [INFO] INFO 01-18 17:23:01 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:35263 backend=nccl
[2026-01-18 17:23:01] [INFO] INFO 01-18 17:23:01 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-18 17:23:02] [WARNING] WARNING 01-18 17:23:02 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 17:23:02] [WARNING] WARNING 01-18 17:23:02 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 17:23:02] [INFO] INFO 01-18 17:23:02 [parallel_state.py:1425] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
[2026-01-18 17:23:02] [INFO] INFO 01-18 17:23:02 [parallel_state.py:1425] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[2026-01-18 17:23:03] [INFO] [0;36m(Worker_TP0 pid=56128)[0;0m INFO 01-18 17:23:03 [gpu_model_runner.py:3789] Starting to load model /mnt/AI-Acer4T/AI-Chat/models/GPT-OSS/gpt-oss-120b/...
[2026-01-18 17:23:06] [INFO] [0;36m(Worker_TP0 pid=56128)[0;0m INFO 01-18 17:23:06 [cuda.py:351] Using TRITON_ATTN attention backend out of potential backends: ('TRITON_ATTN',)
[2026-01-18 17:23:06] [INFO] [0;36m(Worker_TP1 pid=56129)[0;0m INFO 01-18 17:23:06 [mxfp4.py:162] Using Marlin backend
[2026-01-18 17:23:06] [INFO] [0;36m(Worker_TP0 pid=56128)[0;0m INFO 01-18 17:23:06 [mxfp4.py:162] Using Marlin backend
[2026-01-18 17:23:07] [INFO] [0;36m(Worker_TP0 pid=56128)[0;0m
[2026-01-18 17:23:07] [INFO] Loading safetensors checkpoint shards:   0% Completed | 0/15 [00:00<?, ?it/s]
[2026-01-18 17:23:11] [INFO] [0;36m(Worker_TP0 pid=56128)[0;0m
[2026-01-18 17:23:11] [INFO] Loading safetensors checkpoint shards:   7% Completed | 1/15 [00:04<01:04,  4.60s/it]
[2026-01-18 17:23:16] [INFO] [0;36m(Worker_TP0 pid=56128)[0;0m
[2026-01-18 17:23:16] [INFO] Loading safetensors checkpoint shards:  13% Completed | 2/15 [00:09<00:59,  4.55s/it]
[2026-01-18 17:23:21] [INFO] [0;36m(Worker_TP0 pid=56128)[0;0m
[2026-01-18 17:23:21] [INFO] Loading safetensors checkpoint shards:  20% Completed | 3/15 [00:13<00:55,  4.66s/it]
[2026-01-18 17:23:25] [INFO] [0;36m(Worker_TP0 pid=56128)[0;0m
[2026-01-18 17:23:25] [INFO] Loading safetensors checkpoint shards:  27% Completed | 4/15 [00:17<00:48,  4.43s/it]
[2026-01-18 17:23:29] [INFO] [0;36m(Worker_TP0 pid=56128)[0;0m
[2026-01-18 17:23:29] [INFO] Loading safetensors checkpoint shards:  33% Completed | 5/15 [00:22<00:43,  4.39s/it]
[2026-01-18 17:23:33] [INFO] [0;36m(Worker_TP0 pid=56128)[0;0m
[2026-01-18 17:23:33] [INFO] Loading safetensors checkpoint shards:  40% Completed | 6/15 [00:26<00:38,  4.32s/it]
[2026-01-18 17:23:38] [INFO] [0;36m(Worker_TP0 pid=56128)[0;0m
[2026-01-18 17:23:38] [INFO] Loading safetensors checkpoint shards:  47% Completed | 7/15 [00:31<00:35,  4.43s/it]
[2026-01-18 17:23:43] [INFO] [0;36m(Worker_TP0 pid=56128)[0;0m
[2026-01-18 17:23:43] [INFO] Loading safetensors checkpoint shards:  53% Completed | 8/15 [00:36<00:32,  4.61s/it]
[2026-01-18 17:23:48] [INFO] [0;36m(Worker_TP0 pid=56128)[0;0m
[2026-01-18 17:23:48] [INFO] Loading safetensors checkpoint shards:  60% Completed | 9/15 [00:41<00:28,  4.71s/it]
[2026-01-18 17:23:52] [INFO] [0;36m(Worker_TP0 pid=56128)[0;0m
[2026-01-18 17:23:52] [INFO] Loading safetensors checkpoint shards:  67% Completed | 10/15 [00:45<00:23,  4.67s/it]
[2026-01-18 17:23:57] [INFO] [0;36m(Worker_TP0 pid=56128)[0;0m
[2026-01-18 17:23:57] [INFO] Loading safetensors checkpoint shards:  73% Completed | 11/15 [00:50<00:18,  4.72s/it]
[2026-01-18 17:24:02] [INFO] [0;36m(Worker_TP0 pid=56128)[0;0m
[2026-01-18 17:24:02] [INFO] Loading safetensors checkpoint shards:  80% Completed | 12/15 [00:54<00:13,  4.60s/it]
[2026-01-18 17:24:06] [INFO] [0;36m(Worker_TP0 pid=56128)[0;0m
[2026-01-18 17:24:06] [INFO] Loading safetensors checkpoint shards:  87% Completed | 13/15 [00:58<00:08,  4.40s/it]
[2026-01-18 17:24:11] [INFO] [0;36m(Worker_TP0 pid=56128)[0;0m
[2026-01-18 17:24:11] [INFO] Loading safetensors checkpoint shards:  93% Completed | 14/15 [01:03<00:04,  4.63s/it]
[2026-01-18 17:24:15] [INFO] [0;36m(Worker_TP0 pid=56128)[0;0m
[2026-01-18 17:24:15] [INFO] Loading safetensors checkpoint shards: 100% Completed | 15/15 [01:08<00:00,  4.47s/it]
[2026-01-18 17:24:15] [INFO] [0;36m(Worker_TP0 pid=56128)[0;0m
[2026-01-18 17:24:15] [INFO] Loading safetensors checkpoint shards: 100% Completed | 15/15 [01:08<00:00,  4.53s/it]
[2026-01-18 17:24:15] [INFO] [0;36m(Worker_TP0 pid=56128)[0;0m
[2026-01-18 17:24:15] [INFO] [0;36m(Worker_TP0 pid=56128)[0;0m INFO 01-18 17:24:15 [default_loader.py:291] Loading weights took 68.09 seconds
[2026-01-18 17:24:15] [WARNING] [0;36m(Worker_TP0 pid=56128)[0;0m WARNING 01-18 17:24:15 [marlin_utils_fp4.py:333] Your GPU does not have native support for FP4 computation but FP4 quantization is being used. Weight-only FP4 compression will be used leveraging the Marlin kernel. This may degrade performance for compute-heavy workloads.
[2026-01-18 17:24:15] [WARNING] [0;36m(Worker_TP1 pid=56129)[0;0m WARNING 01-18 17:24:15 [marlin_utils_fp4.py:333] Your GPU does not have native support for FP4 computation but FP4 quantization is being used. Weight-only FP4 compression will be used leveraging the Marlin kernel. This may degrade performance for compute-heavy workloads.
[2026-01-18 17:24:17] [INFO] [0;36m(Worker_TP0 pid=56128)[0;0m INFO 01-18 17:24:17 [gpu_model_runner.py:3886] Model loading took 34.38 GiB memory and 73.257753 seconds
[2026-01-18 17:24:25] [INFO] [0;36m(Worker_TP0 pid=56128)[0;0m INFO 01-18 17:24:25 [backends.py:644] Using cache directory: /home/wuwen/.cache/vllm/torch_compile_cache/f9b58ab175/rank_0_0/backbone for vLLM's torch.compile
[2026-01-18 17:24:25] [INFO] [0;36m(Worker_TP0 pid=56128)[0;0m INFO 01-18 17:24:25 [backends.py:704] Dynamo bytecode transform time: 8.15 s
[2026-01-18 17:24:28] [INFO] [0;36m(Worker_TP1 pid=56129)[0;0m INFO 01-18 17:24:28 [backends.py:261] Cache the graph of compile range (1, 8192) for later use
[2026-01-18 17:24:28] [INFO] [0;36m(Worker_TP0 pid=56128)[0;0m INFO 01-18 17:24:28 [backends.py:261] Cache the graph of compile range (1, 8192) for later use
[2026-01-18 17:24:30] [INFO] [0;36m(Worker_TP0 pid=56128)[0;0m INFO 01-18 17:24:30 [backends.py:278] Compiling a graph for compile range (1, 8192) takes 2.64 s
[2026-01-18 17:24:30] [INFO] [0;36m(Worker_TP0 pid=56128)[0;0m INFO 01-18 17:24:30 [monitor.py:34] torch.compile takes 10.80 s in total
[2026-01-18 17:24:31] [INFO] [0;36m(Worker_TP0 pid=56128)[0;0m INFO 01-18 17:24:31 [gpu_worker.py:358] Available KV cache memory: 49.13 GiB
[2026-01-18 17:24:31] [INFO] [0;36m(EngineCore_DP0 pid=55894)[0;0m INFO 01-18 17:24:31 [kv_cache_utils.py:1305] GPU KV cache size: 2,862,096 tokens
[2026-01-18 17:24:31] [INFO] [0;36m(EngineCore_DP0 pid=55894)[0;0m INFO 01-18 17:24:31 [kv_cache_utils.py:1310] Maximum concurrency for 128,000 tokens per request: 41.99x
[2026-01-18 17:24:32] [INFO] [0;36m(Worker_TP0 pid=56128)[0;0m 2026-01-18 17:24:32,068 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[2026-01-18 17:24:32] [INFO] [0;36m(Worker_TP1 pid=56129)[0;0m 2026-01-18 17:24:32,068 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[2026-01-18 17:24:32] [INFO] [0;36m(Worker_TP0 pid=56128)[0;0m 2026-01-18 17:24:32,094 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[2026-01-18 17:24:32] [INFO] [0;36m(Worker_TP1 pid=56129)[0;0m 2026-01-18 17:24:32,098 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[2026-01-18 17:24:32] [INFO] [0;36m(Worker_TP0 pid=56128)[0;0m
[2026-01-18 17:24:32] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/83 [00:00<?, ?it/s]
[2026-01-18 17:24:32] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 2/83 [00:00<00:05, 14.00it/s]
[2026-01-18 17:24:32] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▍         | 4/83 [00:00<00:05, 14.61it/s]
[2026-01-18 17:24:33] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   7%|▋         | 6/83 [00:00<00:05, 14.94it/s]
[2026-01-18 17:24:33] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 8/83 [00:00<00:04, 15.15it/s]
[2026-01-18 17:24:33] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 10/83 [00:00<00:04, 15.15it/s]
[2026-01-18 17:24:33] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 12/83 [00:00<00:04, 15.26it/s]
[2026-01-18 17:24:33] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 14/83 [00:00<00:04, 15.44it/s]
[2026-01-18 17:24:33] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|█▉        | 16/83 [00:01<00:04, 15.60it/s]
[2026-01-18 17:24:33] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 18/83 [00:01<00:04, 15.73it/s]
[2026-01-18 17:24:33] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▍       | 20/83 [00:01<00:03, 15.85it/s]
[2026-01-18 17:24:34] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 22/83 [00:01<00:03, 15.93it/s]
[2026-01-18 17:24:34] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 24/83 [00:01<00:03, 15.73it/s]
[2026-01-18 17:24:34] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 26/83 [00:01<00:03, 15.75it/s]
[2026-01-18 17:24:34] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▎      | 28/83 [00:01<00:03, 15.74it/s]
[2026-01-18 17:24:34] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▌      | 30/83 [00:01<00:03, 15.88it/s]
[2026-01-18 17:24:34] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▊      | 32/83 [00:02<00:03, 15.98it/s]
[2026-01-18 17:24:34] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 34/83 [00:02<00:03, 16.14it/s]
[2026-01-18 17:24:34] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 36/83 [00:02<00:03, 15.50it/s]
[2026-01-18 17:24:35] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▌     | 38/83 [00:02<00:02, 15.87it/s]
[2026-01-18 17:24:35] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  48%|████▊     | 40/83 [00:02<00:02, 16.00it/s]
[2026-01-18 17:24:35] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 42/83 [00:02<00:02, 16.12it/s]
[2026-01-18 17:24:35] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 44/83 [00:02<00:02, 16.18it/s]
[2026-01-18 17:24:35] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▌    | 46/83 [00:02<00:02, 16.30it/s]
[2026-01-18 17:24:35] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 48/83 [00:03<00:02, 16.40it/s]
[2026-01-18 17:24:35] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 50/83 [00:03<00:02, 16.41it/s]
[2026-01-18 17:24:35] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 52/83 [00:03<00:01, 16.17it/s]
[2026-01-18 17:24:36] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▌   | 54/83 [00:03<00:01, 16.39it/s]
[2026-01-18 17:24:36] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 56/83 [00:03<00:01, 16.71it/s]
[2026-01-18 17:24:36] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  70%|██████▉   | 58/83 [00:03<00:01, 17.01it/s]
[2026-01-18 17:24:36] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|███████▏  | 60/83 [00:03<00:01, 17.22it/s]
[2026-01-18 17:24:36] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 62/83 [00:03<00:01, 17.45it/s]
[2026-01-18 17:24:36] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 64/83 [00:03<00:01, 17.67it/s]
[2026-01-18 17:24:36] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|███████▉  | 66/83 [00:04<00:00, 17.86it/s]
[2026-01-18 17:24:36] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 68/83 [00:04<00:00, 18.06it/s]
[2026-01-18 17:24:36] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 70/83 [00:04<00:00, 18.05it/s]
[2026-01-18 17:24:36] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  87%|████████▋ | 72/83 [00:04<00:00, 18.05it/s]
[2026-01-18 17:24:37] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 74/83 [00:04<00:00, 18.23it/s]
[2026-01-18 17:24:37] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 76/83 [00:04<00:00, 18.37it/s]
[2026-01-18 17:24:37] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 78/83 [00:04<00:00, 18.33it/s]
[2026-01-18 17:24:37] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▋| 80/83 [00:04<00:00, 18.48it/s]
[2026-01-18 17:24:37] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  99%|█████████▉| 82/83 [00:04<00:00, 18.64it/s]
[2026-01-18 17:24:37] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 83/83 [00:05<00:00, 16.55it/s]
[2026-01-18 17:24:37] [INFO] [0;36m(Worker_TP0 pid=56128)[0;0m
[2026-01-18 17:24:38] [INFO] Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
[2026-01-18 17:24:39] [INFO] Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:01<00:46,  1.38s/it]
[2026-01-18 17:24:39] [INFO] Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:02<00:33,  1.02s/it]
[2026-01-18 17:24:39] [INFO] Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:02<00:12,  2.41it/s]
[2026-01-18 17:24:39] [INFO] Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:02<00:07,  4.07it/s]
[2026-01-18 17:24:40] [INFO] Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:02<00:04,  5.92it/s]
[2026-01-18 17:24:40] [INFO] Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:02<00:03,  7.83it/s]
[2026-01-18 17:24:40] [INFO] Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:02<00:02,  9.67it/s]
[2026-01-18 17:24:40] [INFO] Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:02<00:01, 11.38it/s]
[2026-01-18 17:24:40] [INFO] Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:02<00:01, 12.83it/s]
[2026-01-18 17:24:40] [INFO] Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:03<00:01, 14.05it/s]
[2026-01-18 17:24:40] [INFO] Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:03<00:01, 14.96it/s]
[2026-01-18 17:24:40] [INFO] Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:03<00:00, 15.54it/s]
[2026-01-18 17:24:41] [INFO] Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:03<00:00, 16.07it/s]
[2026-01-18 17:24:41] [INFO] Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:03<00:00, 16.41it/s]
[2026-01-18 17:24:42] [INFO] Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:03<00:00, 16.62it/s]
[2026-01-18 17:24:43] [INFO] Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:05<00:01,  3.08it/s]
[2026-01-18 17:24:43] [INFO] Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:05<00:00,  4.09it/s]
[2026-01-18 17:24:44] [INFO] Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:05<00:00,  5.30it/s]
[2026-01-18 17:24:44] [INFO] Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:06<00:00,  5.24it/s]
[2026-01-18 17:24:44] [INFO] [0;36m(Worker_TP0 pid=56128)[0;0m INFO 01-18 17:24:44 [custom_all_reduce.py:216] Registering 8614 cuda graph addresses
[2026-01-18 17:24:44] [INFO] [0;36m(Worker_TP1 pid=56129)[0;0m INFO 01-18 17:24:44 [custom_all_reduce.py:216] Registering 8614 cuda graph addresses
[2026-01-18 17:24:44] [INFO] [0;36m(Worker_TP0 pid=56128)[0;0m INFO 01-18 17:24:44 [gpu_model_runner.py:4837] Graph capturing finished in 13 secs, took 0.21 GiB
[2026-01-18 17:24:44] [INFO] [0;36m(EngineCore_DP0 pid=55894)[0;0m INFO 01-18 17:24:44 [core.py:273] init engine (profile, create kv cache, warmup model) took 27.63 seconds
[2026-01-18 17:24:46] [INFO] [0;36m(EngineCore_DP0 pid=55894)[0;0m INFO 01-18 17:24:46 [core.py:186] Batch queue is enabled with size 2
[2026-01-18 17:24:46] [INFO] [0;36m(EngineCore_DP0 pid=55894)[0;0m INFO 01-18 17:24:46 [vllm.py:645] Asynchronous scheduling is enabled.
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [api_server.py:1020] Supported tasks: ['generate']
[2026-01-18 17:24:47] [WARNING] [0;36m(APIServer pid=55535)[0;0m WARNING 01-18 17:24:47 [serving_responses.py:245] For gpt-oss, we ignore --enable-auto-tool-choice and always enable tool use.
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [serving_engine.py:271] "auto" tool choice has been enabled.
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [serving_engine.py:271] "auto" tool choice has been enabled.
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [serving_chat.py:182] Warming up chat template processing...
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [serving_chat.py:218] Chat template warmup completed in 70.0ms
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [serving_engine.py:271] "auto" tool choice has been enabled.
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [api_server.py:1352] Starting vLLM API server 0 on http://0.0.0.0:8005
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [launcher.py:38] Available routes are:
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [launcher.py:46] Route: /docs, Methods: GET, HEAD
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [launcher.py:46] Route: /tokenize, Methods: POST
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [launcher.py:46] Route: /detokenize, Methods: POST
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [launcher.py:46] Route: /pause, Methods: POST
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [launcher.py:46] Route: /resume, Methods: POST
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [launcher.py:46] Route: /is_paused, Methods: GET
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [launcher.py:46] Route: /metrics, Methods: GET
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [launcher.py:46] Route: /health, Methods: GET
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [launcher.py:46] Route: /load, Methods: GET
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [launcher.py:46] Route: /v1/models, Methods: GET
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [launcher.py:46] Route: /version, Methods: GET
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [launcher.py:46] Route: /v1/responses, Methods: POST
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [launcher.py:46] Route: /v1/messages, Methods: POST
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [launcher.py:46] Route: /v1/completions, Methods: POST
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [launcher.py:46] Route: /ping, Methods: GET
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [launcher.py:46] Route: /ping, Methods: POST
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [launcher.py:46] Route: /invocations, Methods: POST
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [launcher.py:46] Route: /classify, Methods: POST
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [launcher.py:46] Route: /score, Methods: POST
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [launcher.py:46] Route: /v1/score, Methods: POST
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [launcher.py:46] Route: /rerank, Methods: POST
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [launcher.py:46] Route: /v1/rerank, Methods: POST
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [launcher.py:46] Route: /v2/rerank, Methods: POST
[2026-01-18 17:24:47] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO 01-18 17:24:47 [launcher.py:46] Route: /pooling, Methods: POST
[2026-01-18 17:24:48] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO:     Started server process [55535]
[2026-01-18 17:24:48] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO:     Waiting for application startup.
[2026-01-18 17:24:48] [INFO] [0;36m(APIServer pid=55535)[0;0m INFO:     Application startup complete.
[2026-01-18 17:25:03] [INFO] [0;36m(Worker_TP0 pid=56128)[0;0m [0;36m(Worker_TP1 pid=56129)[0;0m INFO 01-18 17:25:03 [multiproc_executor.py:707] Parent process exited, terminating worker
[2026-01-18 17:25:03] [INFO] INFO 01-18 17:25:03 [multiproc_executor.py:707] Parent process exited, terminating worker
[2026-01-18 17:25:03] [ERROR] [0;36m(APIServer pid=55535)[0;0m ERROR 01-18 17:25:03 [core_client.py:610] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.
[2026-01-18 17:25:03] [INFO] [0;36m(Worker_TP0 pid=56128)[0;0m INFO 01-18 17:25:03 [multiproc_executor.py:751] WorkerProc shutting down.
[2026-01-18 17:25:03] [INFO] [0;36m(Worker_TP1 pid=56129)[0;0m INFO 01-18 17:25:03 [multiproc_executor.py:751] WorkerProc shutting down.
[2026-01-18 17:25:03] [INFO] [0;36m(Worker_TP0 pid=56128)[0;0m /mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py:147: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.
[2026-01-18 17:25:03] [INFO] [0;36m(Worker_TP0 pid=56128)[0;0m   warnings.warn('resource_tracker: process died unexpectedly, '
[2026-01-18 17:25:03] [INFO] [0;36m(Worker_TP1 pid=56129)[0;0m /mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py:147: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.
[2026-01-18 17:25:03] [INFO] [0;36m(Worker_TP1 pid=56129)[0;0m   warnings.warn('resource_tracker: process died unexpectedly, '
[2026-01-18 17:25:03] [ERROR] Traceback (most recent call last):
[2026-01-18 17:25:03] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 17:25:03] [INFO] cache[rtype].remove(name)
[2026-01-18 17:25:03] [INFO] KeyError: '/psm_3b343ed9'
[2026-01-18 17:25:03] [ERROR] Traceback (most recent call last):
[2026-01-18 17:25:03] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 17:25:03] [INFO] cache[rtype].remove(name)
[2026-01-18 17:25:03] [INFO] KeyError: '/psm_c7474b15'
[2026-01-18 17:25:03] [ERROR] Traceback (most recent call last):
[2026-01-18 17:25:03] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 17:25:03] [INFO] cache[rtype].remove(name)
[2026-01-18 17:25:03] [INFO] KeyError: '/psm_83c389d0'
[2026-01-18 17:25:03] [INFO] 服务已停止
[2026-01-18 17:25:03] [ERROR] Traceback (most recent call last):
[2026-01-18 17:25:03] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 17:25:03] [INFO] cache[rtype].remove(name)
[2026-01-18 17:25:03] [INFO] KeyError: '/mp-a5tchz6r'
[2026-01-18 17:25:03] [ERROR] Traceback (most recent call last):
[2026-01-18 17:25:03] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 17:25:03] [INFO] cache[rtype].remove(name)
[2026-01-18 17:25:03] [INFO] KeyError: '/mp-3j264jdx'
[2026-01-18 17:25:04] [INFO] nvitop监控已启动
[2026-01-18 17:25:05] [INFO] nvitop监控已启动
[2026-01-18 17:25:06] [INFO] nvitop监控已启动
[2026-01-18 17:25:33] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 17:25:33] [INFO] nvitop监控已启动
[2026-01-18 17:26:01] [ERROR] Traceback (most recent call last):
[2026-01-18 17:26:01] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/bin/vllm", line 7, in <module>
[2026-01-18 17:26:01] [INFO] sys.exit(main())
[2026-01-18 17:26:01] [INFO] ^^^^^^
[2026-01-18 17:26:01] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 66, in main
[2026-01-18 17:26:01] [INFO] cmd.subparser_init(subparsers).set_defaults(dispatch_function=cmd.cmd)
[2026-01-18 17:26:01] [INFO] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:26:01] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 76, in subparser_init
[2026-01-18 17:26:01] [INFO] serve_parser = make_arg_parser(serve_parser)
[2026-01-18 17:26:01] [INFO] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:26:01] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/cli_args.py", line 296, in make_arg_parser
[2026-01-18 17:26:01] [INFO] parser = AsyncEngineArgs.add_cli_args(parser)
[2026-01-18 17:26:01] [INFO] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:26:01] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/engine/arg_utils.py", line 2044, in add_cli_args
[2026-01-18 17:26:01] [INFO] parser = EngineArgs.add_cli_args(parser)
[2026-01-18 17:26:01] [INFO] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:26:01] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/engine/arg_utils.py", line 1144, in add_cli_args
[2026-01-18 17:26:01] [INFO] vllm_kwargs = get_kwargs(VllmConfig)
[2026-01-18 17:26:01] [INFO] ^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:26:01] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/engine/arg_utils.py", line 345, in get_kwargs
[2026-01-18 17:26:01] [INFO] return copy.deepcopy(_compute_kwargs(cls))
[2026-01-18 17:26:01] [INFO] ^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:26:01] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/engine/arg_utils.py", line 257, in _compute_kwargs
[2026-01-18 17:26:01] [INFO] default = default.default_factory()
[2026-01-18 17:26:01] [INFO] ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:26:01] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/pydantic/_internal/_dataclasses.py", line 121, in __init__
[2026-01-18 17:26:01] [INFO] s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)
[2026-01-18 17:26:01] [INFO] pydantic_core._pydantic_core.ValidationError: 1 validation error for AttentionConfig
[2026-01-18 17:26:01] [INFO] Value error, Invalid value 'DUAL_CHUNK_FLASH_ATTN' for VLLM_ATTENTION_BACKEND. Valid options: ['FLASH_ATTN', 'FLASH_ATTN_DIFFKV', 'TRITON_ATTN', 'ROCM_ATTN', 'ROCM_AITER_MLA', 'ROCM_AITER_TRITON_MLA', 'ROCM_AITER_FA', 'ROCM_AITER_MLA_SPARSE', 'TORCH_SDPA', 'FLASHINFER', 'FLASHINFER_MLA', 'TRITON_MLA', 'CUTLASS_MLA', 'FLASHMLA', 'FLASHMLA_SPARSE', 'FLASH_ATTN_MLA', 'IPEX', 'NO_ATTENTION', 'FLEX_ATTENTION', 'TREE_ATTN', 'ROCM_AITER_UNIFIED_ATTN', 'CPU_ATTN', 'CUSTOM']. [type=value_error, input_value=ArgsKwargs(()), input_type=ArgsKwargs]
[2026-01-18 17:26:01] [INFO] For further information visit https://errors.pydantic.dev/2.12/v/value_error
[2026-01-18 17:26:03] [INFO] nvitop监控已启动
[2026-01-18 17:30:14] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 17:30:14] [INFO] nvitop监控已启动
[2026-01-18 17:30:14] [INFO] nvitop监控已停止
[2026-01-18 17:30:43] [ERROR] Traceback (most recent call last):
[2026-01-18 17:30:43] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/bin/vllm", line 7, in <module>
[2026-01-18 17:30:43] [INFO] sys.exit(main())
[2026-01-18 17:30:43] [INFO] ^^^^^^
[2026-01-18 17:30:43] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 66, in main
[2026-01-18 17:30:43] [INFO] cmd.subparser_init(subparsers).set_defaults(dispatch_function=cmd.cmd)
[2026-01-18 17:30:43] [INFO] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:30:43] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 76, in subparser_init
[2026-01-18 17:30:43] [INFO] serve_parser = make_arg_parser(serve_parser)
[2026-01-18 17:30:43] [INFO] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:30:43] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/cli_args.py", line 296, in make_arg_parser
[2026-01-18 17:30:43] [INFO] parser = AsyncEngineArgs.add_cli_args(parser)
[2026-01-18 17:30:43] [INFO] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:30:43] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/engine/arg_utils.py", line 2044, in add_cli_args
[2026-01-18 17:30:43] [INFO] parser = EngineArgs.add_cli_args(parser)
[2026-01-18 17:30:43] [INFO] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:30:43] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/engine/arg_utils.py", line 1144, in add_cli_args
[2026-01-18 17:30:43] [INFO] vllm_kwargs = get_kwargs(VllmConfig)
[2026-01-18 17:30:43] [INFO] ^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:30:43] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/engine/arg_utils.py", line 345, in get_kwargs
[2026-01-18 17:30:43] [INFO] return copy.deepcopy(_compute_kwargs(cls))
[2026-01-18 17:30:43] [INFO] ^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:30:43] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/engine/arg_utils.py", line 257, in _compute_kwargs
[2026-01-18 17:30:43] [INFO] default = default.default_factory()
[2026-01-18 17:30:43] [INFO] ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:30:43] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/pydantic/_internal/_dataclasses.py", line 121, in __init__
[2026-01-18 17:30:43] [INFO] s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)
[2026-01-18 17:30:43] [INFO] pydantic_core._pydantic_core.ValidationError: 1 validation error for AttentionConfig
[2026-01-18 17:30:43] [INFO] Value error, Invalid value 'DUAL_CHUNK_FLASH_ATTN' for VLLM_ATTENTION_BACKEND. Valid options: ['FLASH_ATTN', 'FLASH_ATTN_DIFFKV', 'TRITON_ATTN', 'ROCM_ATTN', 'ROCM_AITER_MLA', 'ROCM_AITER_TRITON_MLA', 'ROCM_AITER_FA', 'ROCM_AITER_MLA_SPARSE', 'TORCH_SDPA', 'FLASHINFER', 'FLASHINFER_MLA', 'TRITON_MLA', 'CUTLASS_MLA', 'FLASHMLA', 'FLASHMLA_SPARSE', 'FLASH_ATTN_MLA', 'IPEX', 'NO_ATTENTION', 'FLEX_ATTENTION', 'TREE_ATTN', 'ROCM_AITER_UNIFIED_ATTN', 'CPU_ATTN', 'CUSTOM']. [type=value_error, input_value=ArgsKwargs(()), input_type=ArgsKwargs]
[2026-01-18 17:30:43] [INFO] For further information visit https://errors.pydantic.dev/2.12/v/value_error
[2026-01-18 17:30:44] [INFO] nvitop监控已启动
[2026-01-18 17:32:06] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 17:32:06] [INFO] nvitop监控已启动
[2026-01-18 17:32:06] [INFO] nvitop监控已停止
[2026-01-18 17:32:36] [WARNING] WARNING 01-18 17:32:36 [attention.py:82] Using VLLM_ATTENTION_BACKEND environment variable is deprecated and will be removed in v0.14.0 or v1.0.0, whichever is soonest. Please use --attention-config.backend command line argument or AttentionConfig(backend=...) config field instead.
[2026-01-18 17:32:36] [WARNING] WARNING 01-18 17:32:36 [argparse_utils.py:342] Found duplicate keys --tokenizer-mode
[2026-01-18 17:32:36] [INFO] usage: vllm [-h] [-v] {chat,complete,serve,bench,collect-env,run-batch} ...
[2026-01-18 17:32:36] [INFO] vllm: error: unrecognized arguments: --enable-reasoning
[2026-01-18 17:32:37] [INFO] nvitop监控已启动
[2026-01-18 17:33:38] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 17:33:38] [INFO] nvitop监控已启动
[2026-01-18 17:34:07] [WARNING] WARNING 01-18 17:34:07 [attention.py:82] Using VLLM_ATTENTION_BACKEND environment variable is deprecated and will be removed in v0.14.0 or v1.0.0, whichever is soonest. Please use --attention-config.backend command line argument or AttentionConfig(backend=...) config field instead.
[2026-01-18 17:34:07] [WARNING] WARNING 01-18 17:34:07 [argparse_utils.py:342] Found duplicate keys --tokenizer-mode
[2026-01-18 17:34:07] [INFO] [0;36m(APIServer pid=61586)[0;0m INFO 01-18 17:34:07 [api_server.py:872] vLLM API server version 0.14.0rc2.dev117+g9e078d058
[2026-01-18 17:34:07] [INFO] [0;36m(APIServer pid=61586)[0;0m INFO 01-18 17:34:07 [utils.py:267] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-235B-A22B-Thinking-2507-AWQ/', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-235B-A22B-Thinking-2507-AWQ/chat_template.jinja', 'model': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-235B-A22B-Thinking-2507-AWQ/', 'trust_remote_code': True, 'max_model_len': 1010000, 'enforce_eager': True, 'served_model_name': ['VLLM-MODEL'], 'reasoning_parser': 'deepseek_r1', 'tensor_parallel_size': 2, 'kv_cache_dtype': 'fp8', 'max_num_seqs': 256, 'enable_chunked_prefill': False, 'async_scheduling': True, 'compilation_config': {'level': None, 'mode': None, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': [], 'splitting_ops': None, 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': None, 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.PIECEWISE: 1>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': None, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}}
[2026-01-18 17:34:07] [INFO] [0;36m(APIServer pid=61586)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 17:34:07] [INFO] [0;36m(APIServer pid=61586)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 17:34:31] [INFO] [0;36m(APIServer pid=61586)[0;0m INFO 01-18 17:34:31 [model.py:530] Resolved architecture: Qwen3MoeForCausalLM
[2026-01-18 17:34:31] [ERROR] [0;36m(APIServer pid=61586)[0;0m Traceback (most recent call last):
[2026-01-18 17:34:31] [INFO] [0;36m(APIServer pid=61586)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/bin/vllm", line 7, in <module>
[2026-01-18 17:34:31] [INFO] [0;36m(APIServer pid=61586)[0;0m     sys.exit(main())
[2026-01-18 17:34:31] [INFO] [0;36m(APIServer pid=61586)[0;0m              ^^^^^^
[2026-01-18 17:34:31] [INFO] [0;36m(APIServer pid=61586)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-18 17:34:31] [INFO] [0;36m(APIServer pid=61586)[0;0m     args.dispatch_function(args)
[2026-01-18 17:34:31] [INFO] [0;36m(APIServer pid=61586)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-18 17:34:31] [INFO] [0;36m(APIServer pid=61586)[0;0m     uvloop.run(run_server(args))
[2026-01-18 17:34:31] [INFO] [0;36m(APIServer pid=61586)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-18 17:34:31] [INFO] [0;36m(APIServer pid=61586)[0;0m     return __asyncio.run(
[2026-01-18 17:34:31] [INFO] [0;36m(APIServer pid=61586)[0;0m            ^^^^^^^^^^^^^^
[2026-01-18 17:34:31] [INFO] [0;36m(APIServer pid=61586)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-18 17:34:31] [INFO] [0;36m(APIServer pid=61586)[0;0m     return runner.run(main)
[2026-01-18 17:34:31] [INFO] [0;36m(APIServer pid=61586)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-18 17:34:31] [INFO] [0;36m(APIServer pid=61586)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-18 17:34:31] [INFO] [0;36m(APIServer pid=61586)[0;0m     return self._loop.run_until_complete(task)
[2026-01-18 17:34:31] [INFO] [0;36m(APIServer pid=61586)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:34:31] [INFO] [0;36m(APIServer pid=61586)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-18 17:34:31] [INFO] [0;36m(APIServer pid=61586)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-18 17:34:31] [INFO] [0;36m(APIServer pid=61586)[0;0m     return await main
[2026-01-18 17:34:31] [INFO] [0;36m(APIServer pid=61586)[0;0m            ^^^^^^^^^^
[2026-01-18 17:34:31] [INFO] [0;36m(APIServer pid=61586)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[2026-01-18 17:34:31] [INFO] [0;36m(APIServer pid=61586)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-18 17:34:31] [INFO] [0;36m(APIServer pid=61586)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[2026-01-18 17:34:31] [INFO] [0;36m(APIServer pid=61586)[0;0m     async with build_async_engine_client(
[2026-01-18 17:34:31] [INFO] [0;36m(APIServer pid=61586)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:34:31] [INFO] [0;36m(APIServer pid=61586)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 17:34:31] [INFO] [0;36m(APIServer pid=61586)[0;0m     return await anext(self.gen)
[2026-01-18 17:34:31] [INFO] [0;36m(APIServer pid=61586)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:34:31] [INFO] [0;36m(APIServer pid=61586)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 146, in build_async_engine_client
[2026-01-18 17:34:31] [INFO] [0;36m(APIServer pid=61586)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-18 17:34:31] [INFO] [0;36m(APIServer pid=61586)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:34:31] [INFO] [0;36m(APIServer pid=61586)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 17:34:31] [INFO] [0;36m(APIServer pid=61586)[0;0m     return await anext(self.gen)
[2026-01-18 17:34:31] [INFO] [0;36m(APIServer pid=61586)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:34:31] [INFO] [0;36m(APIServer pid=61586)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 172, in build_async_engine_client_from_engine_args
[2026-01-18 17:34:31] [INFO] [0;36m(APIServer pid=61586)[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)
[2026-01-18 17:34:31] [INFO] [0;36m(APIServer pid=61586)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:34:31] [INFO] [0;36m(APIServer pid=61586)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/engine/arg_utils.py", line 1364, in create_engine_config
[2026-01-18 17:34:31] [INFO] [0;36m(APIServer pid=61586)[0;0m     model_config = self.create_model_config()
[2026-01-18 17:34:31] [INFO] [0;36m(APIServer pid=61586)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:34:31] [INFO] [0;36m(APIServer pid=61586)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/engine/arg_utils.py", line 1219, in create_model_config
[2026-01-18 17:34:31] [INFO] [0;36m(APIServer pid=61586)[0;0m     return ModelConfig(
[2026-01-18 17:34:31] [INFO] [0;36m(APIServer pid=61586)[0;0m            ^^^^^^^^^^^^
[2026-01-18 17:34:32] [INFO] [0;36m(APIServer pid=61586)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/pydantic/_internal/_dataclasses.py", line 121, in __init__
[2026-01-18 17:34:32] [INFO] [0;36m(APIServer pid=61586)[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)
[2026-01-18 17:34:32] [INFO] [0;36m(APIServer pid=61586)[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig
[2026-01-18 17:34:32] [INFO] [0;36m(APIServer pid=61586)[0;0m   Value error, User-specified max_model_len (1010000) is greater than the derived max_model_len (max_position_embeddings=262144.0 or model_max_length=None in model's config.json). To allow overriding this maximum, set the env var VLLM_ALLOW_LONG_MAX_MODEL_LEN=1. VLLM_ALLOW_LONG_MAX_MODEL_LEN must be used with extreme caution. If the model uses relative position encoding (RoPE), positions exceeding derived_max_model_len lead to nan. If the model uses absolute position encoding, positions exceeding derived_max_model_len will cause a CUDA array out-of-bounds error. [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]
[2026-01-18 17:34:32] [INFO] [0;36m(APIServer pid=61586)[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error
[2026-01-18 17:34:36] [INFO] nvitop监控已启动
[2026-01-18 17:35:53] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 17:35:53] [INFO] nvitop监控已启动
[2026-01-18 17:35:53] [INFO] nvitop监控已停止
[2026-01-18 17:36:22] [WARNING] WARNING 01-18 17:36:22 [argparse_utils.py:342] Found duplicate keys --tokenizer-mode
[2026-01-18 17:36:22] [INFO] [0;36m(APIServer pid=62900)[0;0m INFO 01-18 17:36:22 [api_server.py:872] vLLM API server version 0.14.0rc2.dev117+g9e078d058
[2026-01-18 17:36:22] [INFO] [0;36m(APIServer pid=62900)[0;0m INFO 01-18 17:36:22 [utils.py:267] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-235B-A22B-Thinking-2507-AWQ/', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-235B-A22B-Thinking-2507-AWQ/chat_template.jinja', 'model': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-235B-A22B-Thinking-2507-AWQ/', 'trust_remote_code': True, 'max_model_len': 256000, 'enforce_eager': True, 'served_model_name': ['VLLM-MODEL'], 'reasoning_parser': 'deepseek_r1', 'tensor_parallel_size': 2, 'kv_cache_dtype': 'fp8', 'max_num_seqs': 256, 'enable_chunked_prefill': False, 'async_scheduling': True, 'compilation_config': {'level': None, 'mode': None, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': [], 'splitting_ops': None, 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': None, 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.PIECEWISE: 1>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': None, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}}
[2026-01-18 17:36:22] [INFO] [0;36m(APIServer pid=62900)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 17:36:22] [INFO] [0;36m(APIServer pid=62900)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 17:36:22] [INFO] [0;36m(APIServer pid=62900)[0;0m INFO 01-18 17:36:22 [model.py:530] Resolved architecture: Qwen3MoeForCausalLM
[2026-01-18 17:36:22] [INFO] [0;36m(APIServer pid=62900)[0;0m INFO 01-18 17:36:22 [model.py:1547] Using max model len 256000
[2026-01-18 17:36:22] [WARNING] [0;36m(APIServer pid=62900)[0;0m WARNING 01-18 17:36:22 [quark_ocp_mx.py:157] AITER is not found or QuarkOCP_MX is not supported on the current platform. QuarkOCP_MX quantization will not be available.
[2026-01-18 17:36:23] [INFO] [0;36m(APIServer pid=62900)[0;0m INFO 01-18 17:36:23 [awq_marlin.py:163] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2026-01-18 17:36:23] [WARNING] [0;36m(APIServer pid=62900)[0;0m WARNING 01-18 17:36:23 [arg_utils.py:1913] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[2026-01-18 17:36:23] [INFO] [0;36m(APIServer pid=62900)[0;0m INFO 01-18 17:36:23 [cache.py:206] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor.
[2026-01-18 17:36:23] [INFO] [0;36m(APIServer pid=62900)[0;0m INFO 01-18 17:36:23 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 17:36:23] [INFO] [0;36m(APIServer pid=62900)[0;0m INFO 01-18 17:36:23 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-18 17:36:23] [WARNING] [0;36m(APIServer pid=62900)[0;0m WARNING 01-18 17:36:23 [vllm.py:653] Enforce eager set, overriding optimization level to -O0
[2026-01-18 17:36:23] [INFO] [0;36m(APIServer pid=62900)[0;0m INFO 01-18 17:36:23 [vllm.py:705] Cudagraph mode PIECEWISE is not compatible with compilation mode 0.Overriding to NONE.
[2026-01-18 17:36:23] [INFO] [0;36m(APIServer pid=62900)[0;0m INFO 01-18 17:36:23 [vllm.py:753] Cudagraph is disabled under eager mode
[2026-01-18 17:36:53] [INFO] [0;36m(EngineCore_DP0 pid=63387)[0;0m INFO 01-18 17:36:53 [core.py:96] Initializing a V1 LLM engine (v0.14.0rc2.dev117+g9e078d058) with config: model='/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-235B-A22B-Thinking-2507-AWQ/', speculative_config=None, tokenizer='/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-235B-A22B-Thinking-2507-AWQ/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=256000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=fp8, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='deepseek_r1', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=True, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [256000], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-18 17:36:53] [WARNING] [0;36m(EngineCore_DP0 pid=63387)[0;0m WARNING 01-18 17:36:53 [multiproc_executor.py:897] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-18 17:37:23] [INFO] INFO 01-18 17:37:23 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:33587 backend=nccl
[2026-01-18 17:37:23] [INFO] INFO 01-18 17:37:23 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:33587 backend=nccl
[2026-01-18 17:37:24] [INFO] INFO 01-18 17:37:24 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-18 17:37:24] [WARNING] WARNING 01-18 17:37:24 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 17:37:24] [WARNING] WARNING 01-18 17:37:24 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 17:37:24] [INFO] INFO 01-18 17:37:24 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[2026-01-18 17:37:24] [INFO] INFO 01-18 17:37:24 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
[2026-01-18 17:37:26] [INFO] [0;36m(Worker_TP0 pid=63725)[0;0m INFO 01-18 17:37:26 [gpu_model_runner.py:3803] Starting to load model /mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-235B-A22B-Thinking-2507-AWQ/...
[2026-01-18 17:37:31] [INFO] [0;36m(Worker_TP0 pid=63725)[0;0m INFO 01-18 17:37:31 [cuda.py:351] Using FLASHINFER attention backend out of potential backends: ('FLASHINFER', 'TRITON_ATTN')
[2026-01-18 17:37:31] [INFO] [0;36m(Worker_TP0 pid=63725)[0;0m
[2026-01-18 17:37:31] [INFO] Loading safetensors checkpoint shards:   0% Completed | 0/25 [00:00<?, ?it/s]
[2026-01-18 17:37:37] [INFO] [0;36m(Worker_TP0 pid=63725)[0;0m
[2026-01-18 17:37:37] [INFO] Loading safetensors checkpoint shards:   4% Completed | 1/25 [00:05<02:12,  5.51s/it]
[2026-01-18 17:37:42] [INFO] [0;36m(Worker_TP0 pid=63725)[0;0m
[2026-01-18 17:37:42] [INFO] Loading safetensors checkpoint shards:   8% Completed | 2/25 [00:10<02:06,  5.49s/it]
[2026-01-18 17:37:48] [INFO] [0;36m(Worker_TP0 pid=63725)[0;0m
[2026-01-18 17:37:48] [INFO] Loading safetensors checkpoint shards:  12% Completed | 3/25 [00:16<02:04,  5.67s/it]
[2026-01-18 17:37:54] [INFO] [0;36m(Worker_TP0 pid=63725)[0;0m
[2026-01-18 17:37:54] [INFO] Loading safetensors checkpoint shards:  16% Completed | 4/25 [00:22<01:59,  5.69s/it]
[2026-01-18 17:37:59] [INFO] [0;36m(Worker_TP0 pid=63725)[0;0m
[2026-01-18 17:37:59] [INFO] Loading safetensors checkpoint shards:  20% Completed | 5/25 [00:28<01:51,  5.59s/it]
[2026-01-18 17:38:04] [INFO] [0;36m(Worker_TP0 pid=63725)[0;0m
[2026-01-18 17:38:04] [INFO] Loading safetensors checkpoint shards:  24% Completed | 6/25 [00:33<01:45,  5.55s/it]
[2026-01-18 17:38:10] [INFO] [0;36m(Worker_TP0 pid=63725)[0;0m
[2026-01-18 17:38:10] [INFO] Loading safetensors checkpoint shards:  28% Completed | 7/25 [00:38<01:39,  5.53s/it]
[2026-01-18 17:38:15] [INFO] [0;36m(Worker_TP0 pid=63725)[0;0m
[2026-01-18 17:38:15] [INFO] Loading safetensors checkpoint shards:  32% Completed | 8/25 [00:43<01:30,  5.34s/it]
[2026-01-18 17:38:20] [INFO] [0;36m(Worker_TP0 pid=63725)[0;0m
[2026-01-18 17:38:20] [INFO] Loading safetensors checkpoint shards:  36% Completed | 9/25 [00:49<01:25,  5.32s/it]
[2026-01-18 17:38:26] [INFO] [0;36m(Worker_TP0 pid=63725)[0;0m
[2026-01-18 17:38:26] [INFO] Loading safetensors checkpoint shards:  40% Completed | 10/25 [00:54<01:20,  5.38s/it]
[2026-01-18 17:38:31] [INFO] [0;36m(Worker_TP0 pid=63725)[0;0m
[2026-01-18 17:38:31] [INFO] Loading safetensors checkpoint shards:  44% Completed | 11/25 [01:00<01:16,  5.46s/it]
[2026-01-18 17:38:37] [INFO] [0;36m(Worker_TP0 pid=63725)[0;0m
[2026-01-18 17:38:37] [INFO] Loading safetensors checkpoint shards:  48% Completed | 12/25 [01:05<01:11,  5.46s/it]
[2026-01-18 17:38:43] [INFO] [0;36m(Worker_TP0 pid=63725)[0;0m
[2026-01-18 17:38:43] [INFO] Loading safetensors checkpoint shards:  52% Completed | 13/25 [01:11<01:07,  5.61s/it]
[2026-01-18 17:38:49] [INFO] [0;36m(Worker_TP0 pid=63725)[0;0m
[2026-01-18 17:38:49] [INFO] Loading safetensors checkpoint shards:  56% Completed | 14/25 [01:18<01:03,  5.80s/it]
[2026-01-18 17:38:55] [INFO] [0;36m(Worker_TP0 pid=63725)[0;0m
[2026-01-18 17:38:55] [INFO] Loading safetensors checkpoint shards:  60% Completed | 15/25 [01:24<00:58,  5.87s/it]
[2026-01-18 17:39:01] [INFO] [0;36m(Worker_TP0 pid=63725)[0;0m
[2026-01-18 17:39:01] [INFO] Loading safetensors checkpoint shards:  64% Completed | 16/25 [01:29<00:52,  5.83s/it]
[2026-01-18 17:39:07] [INFO] [0;36m(Worker_TP0 pid=63725)[0;0m
[2026-01-18 17:39:07] [INFO] Loading safetensors checkpoint shards:  68% Completed | 17/25 [01:35<00:46,  5.83s/it]
[2026-01-18 17:39:12] [INFO] [0;36m(Worker_TP0 pid=63725)[0;0m
[2026-01-18 17:39:12] [INFO] Loading safetensors checkpoint shards:  72% Completed | 18/25 [01:40<00:39,  5.64s/it]
[2026-01-18 17:39:18] [INFO] [0;36m(Worker_TP0 pid=63725)[0;0m
[2026-01-18 17:39:18] [INFO] Loading safetensors checkpoint shards:  76% Completed | 19/25 [01:46<00:34,  5.70s/it]
[2026-01-18 17:39:23] [INFO] [0;36m(Worker_TP0 pid=63725)[0;0m
[2026-01-18 17:39:23] [INFO] Loading safetensors checkpoint shards:  80% Completed | 20/25 [01:52<00:28,  5.68s/it]
[2026-01-18 17:39:29] [INFO] [0;36m(Worker_TP0 pid=63725)[0;0m
[2026-01-18 17:39:29] [INFO] Loading safetensors checkpoint shards:  84% Completed | 21/25 [01:58<00:23,  5.77s/it]
[2026-01-18 17:39:35] [INFO] [0;36m(Worker_TP0 pid=63725)[0;0m
[2026-01-18 17:39:35] [INFO] Loading safetensors checkpoint shards:  88% Completed | 22/25 [02:03<00:17,  5.69s/it]
[2026-01-18 17:39:40] [INFO] [0;36m(Worker_TP0 pid=63725)[0;0m
[2026-01-18 17:39:40] [INFO] Loading safetensors checkpoint shards:  92% Completed | 23/25 [02:09<00:11,  5.66s/it]
[2026-01-18 17:39:46] [INFO] [0;36m(Worker_TP0 pid=63725)[0;0m
[2026-01-18 17:39:46] [INFO] Loading safetensors checkpoint shards:  96% Completed | 24/25 [02:14<00:05,  5.56s/it]
[2026-01-18 17:39:50] [INFO] [0;36m(Worker_TP0 pid=63725)[0;0m
[2026-01-18 17:39:50] [INFO] Loading safetensors checkpoint shards: 100% Completed | 25/25 [02:19<00:00,  5.23s/it]
[2026-01-18 17:39:50] [INFO] [0;36m(Worker_TP0 pid=63725)[0;0m
[2026-01-18 17:39:50] [INFO] Loading safetensors checkpoint shards: 100% Completed | 25/25 [02:19<00:00,  5.57s/it]
[2026-01-18 17:39:50] [INFO] [0;36m(Worker_TP0 pid=63725)[0;0m
[2026-01-18 17:39:50] [INFO] [0;36m(Worker_TP0 pid=63725)[0;0m INFO 01-18 17:39:50 [default_loader.py:291] Loading weights took 139.23 seconds
[2026-01-18 17:40:00] [INFO] [0;36m(Worker_TP0 pid=63725)[0;0m INFO 01-18 17:40:00 [gpu_model_runner.py:3900] Model loading took 57.99 GiB memory and 153.182761 seconds
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839] WorkerProc hit an exception.
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839] Traceback (most recent call last):
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839] WorkerProc hit an exception.
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 834, in worker_busy_loop
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839] Traceback (most recent call last):
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     output = func(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 834, in worker_busy_loop
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]              ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     output = func(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return func(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]              ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 321, in determine_available_memory
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return func(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     self.model_runner.profile_run()
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4739, in profile_run
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 321, in determine_available_memory
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     hidden_states, last_hidden_states = self._dummy_run(
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]                                         ^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     self.model_runner.profile_run()
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4739, in profile_run
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return func(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     hidden_states, last_hidden_states = self._dummy_run(
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]                                         ^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4457, in _dummy_run
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     outputs = self.model(
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return func(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]               ^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4457, in _dummy_run
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     outputs = self.model(
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]               ^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 735, in forward
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     hidden_states = self.model(
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]                     ^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 389, in __call__
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 735, in forward
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return self.forward(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     hidden_states = self.model(
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]                     ^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 456, in forward
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 389, in __call__
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     hidden_states, residual = layer(positions, hidden_states, residual)
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return self.forward(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 456, in forward
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     hidden_states, residual = layer(positions, hidden_states, residual)
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 387, in forward
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     hidden_states = self.mlp(hidden_states)
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]                     ^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 387, in forward
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     hidden_states = self.mlp(hidden_states)
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]                     ^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 199, in forward
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     final_hidden_states = self.experts(
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]                           ^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 199, in forward
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     final_hidden_states = self.experts(
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]                           ^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/custom_op.py", line 47, in forward
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return self._forward_method(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1754, in forward_cuda
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/custom_op.py", line 47, in forward
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return self.forward_native(hidden_states, router_logits)
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return self._forward_method(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1722, in forward_native
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1754, in forward_cuda
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     fused_output = torch.ops.vllm.moe_forward(
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return self.forward_native(hidden_states, router_logits)
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_ops.py", line 1255, in __call__
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1722, in forward_native
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return self._op(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     fused_output = torch.ops.vllm.moe_forward(
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 2103, in moe_forward
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_ops.py", line 1255, in __call__
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return self.forward_impl(hidden_states, router_logits)
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return self._op(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1979, in forward_impl
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 2103, in moe_forward
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     final_hidden_states = self.quant_method.apply(
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return self.forward_impl(hidden_states, router_logits)
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]                           ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/awq_marlin.py", line 772, in apply
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1979, in forward_impl
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return fused_marlin_moe(
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     final_hidden_states = self.quant_method.apply(
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]                           ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/fused_marlin_moe.py", line 315, in fused_marlin_moe
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/awq_marlin.py", line 772, in apply
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     moe_output = _fused_marlin_moe(
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return fused_marlin_moe(
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]                  ^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/fused_marlin_moe.py", line 80, in _fused_marlin_moe
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/fused_marlin_moe.py", line 315, in fused_marlin_moe
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     intermediate_cache13 = torch.empty(
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     moe_output = _fused_marlin_moe(
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]                            ^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]                  ^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 15.62 GiB. GPU 1 has a total capacity of 94.97 GiB of which 11.97 GiB is free. Including non-PyTorch memory, this process has 82.97 GiB memory in use. Of the allocated memory 65.92 GiB is allocated by PyTorch, and 16.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/fused_marlin_moe.py", line 80, in _fused_marlin_moe
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839] Traceback (most recent call last):
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     intermediate_cache13 = torch.empty(
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 834, in worker_busy_loop
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]                            ^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     output = func(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 15.62 GiB. GPU 0 has a total capacity of 94.97 GiB of which 11.97 GiB is free. Including non-PyTorch memory, this process has 82.97 GiB memory in use. Of the allocated memory 65.92 GiB is allocated by PyTorch, and 16.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]              ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839] Traceback (most recent call last):
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 834, in worker_busy_loop
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return func(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     output = func(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]              ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 321, in determine_available_memory
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     self.model_runner.profile_run()
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return func(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4739, in profile_run
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     hidden_states, last_hidden_states = self._dummy_run(
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 321, in determine_available_memory
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]                                         ^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     self.model_runner.profile_run()
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4739, in profile_run
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return func(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     hidden_states, last_hidden_states = self._dummy_run(
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]                                         ^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4457, in _dummy_run
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     outputs = self.model(
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return func(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]               ^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4457, in _dummy_run
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     outputs = self.model(
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]               ^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m [0;36m(Worker_TP1 pid=63726)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 735, in forward
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     hidden_states = self.model(
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]                     ^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 389, in __call__
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 735, in forward
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return self.forward(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     hidden_states = self.model(
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]                     ^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 456, in forward
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 389, in __call__
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     hidden_states, residual = layer(positions, hidden_states, residual)
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return self.forward(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 456, in forward
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     hidden_states, residual = layer(positions, hidden_states, residual)
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 387, in forward
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     hidden_states = self.mlp(hidden_states)
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]                     ^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 387, in forward
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     hidden_states = self.mlp(hidden_states)
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]                     ^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 199, in forward
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     final_hidden_states = self.experts(
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]                           ^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 199, in forward
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     final_hidden_states = self.experts(
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]                           ^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/custom_op.py", line 47, in forward
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return self._forward_method(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1754, in forward_cuda
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/custom_op.py", line 47, in forward
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return self.forward_native(hidden_states, router_logits)
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return self._forward_method(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1722, in forward_native
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1754, in forward_cuda
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     fused_output = torch.ops.vllm.moe_forward(
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return self.forward_native(hidden_states, router_logits)
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_ops.py", line 1255, in __call__
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1722, in forward_native
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return self._op(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     fused_output = torch.ops.vllm.moe_forward(
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 2103, in moe_forward
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_ops.py", line 1255, in __call__
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return self.forward_impl(hidden_states, router_logits)
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return self._op(*args, **kwargs)
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1979, in forward_impl
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 2103, in moe_forward
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     final_hidden_states = self.quant_method.apply(
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return self.forward_impl(hidden_states, router_logits)
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]                           ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/awq_marlin.py", line 772, in apply
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1979, in forward_impl
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return fused_marlin_moe(
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     final_hidden_states = self.quant_method.apply(
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]                           ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/fused_marlin_moe.py", line 315, in fused_marlin_moe
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/awq_marlin.py", line 772, in apply
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     moe_output = _fused_marlin_moe(
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     return fused_marlin_moe(
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]                  ^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/fused_marlin_moe.py", line 80, in _fused_marlin_moe
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/fused_marlin_moe.py", line 315, in fused_marlin_moe
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]     intermediate_cache13 = torch.empty(
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     moe_output = _fused_marlin_moe(
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]                            ^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]                  ^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 15.62 GiB. GPU 1 has a total capacity of 94.97 GiB of which 11.97 GiB is free. Including non-PyTorch memory, this process has 82.97 GiB memory in use. Of the allocated memory 65.92 GiB is allocated by PyTorch, and 16.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/fused_marlin_moe.py", line 80, in _fused_marlin_moe
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP1 pid=63726)[0;0m [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]
[2026-01-18 17:40:53] [ERROR] ERROR 01-18 17:40:53 [multiproc_executor.py:839]     intermediate_cache13 = torch.empty(
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]                            ^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 15.62 GiB. GPU 0 has a total capacity of 94.97 GiB of which 11.97 GiB is free. Including non-PyTorch memory, this process has 82.97 GiB memory in use. Of the allocated memory 65.92 GiB is allocated by PyTorch, and 16.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2026-01-18 17:40:53] [ERROR] [0;36m(Worker_TP0 pid=63725)[0;0m ERROR 01-18 17:40:53 [multiproc_executor.py:839]
[2026-01-18 17:40:53] [ERROR] [0;36m(EngineCore_DP0 pid=63387)[0;0m ERROR 01-18 17:40:53 [core.py:935] EngineCore failed to start.
[2026-01-18 17:40:53] [ERROR] [0;36m(EngineCore_DP0 pid=63387)[0;0m ERROR 01-18 17:40:53 [core.py:935] Traceback (most recent call last):
[2026-01-18 17:40:53] [ERROR] [0;36m(EngineCore_DP0 pid=63387)[0;0m ERROR 01-18 17:40:53 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-18 17:40:53] [ERROR] [0;36m(EngineCore_DP0 pid=63387)[0;0m ERROR 01-18 17:40:53 [core.py:935]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 17:40:53] [ERROR] [0;36m(EngineCore_DP0 pid=63387)[0;0m ERROR 01-18 17:40:53 [core.py:935]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(EngineCore_DP0 pid=63387)[0;0m ERROR 01-18 17:40:53 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-18 17:40:53] [ERROR] [0;36m(EngineCore_DP0 pid=63387)[0;0m ERROR 01-18 17:40:53 [core.py:935]     super().__init__(
[2026-01-18 17:40:53] [ERROR] [0;36m(EngineCore_DP0 pid=63387)[0;0m ERROR 01-18 17:40:53 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 112, in __init__
[2026-01-18 17:40:53] [ERROR] [0;36m(EngineCore_DP0 pid=63387)[0;0m ERROR 01-18 17:40:53 [core.py:935]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[2026-01-18 17:40:53] [ERROR] [0;36m(EngineCore_DP0 pid=63387)[0;0m ERROR 01-18 17:40:53 [core.py:935]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(EngineCore_DP0 pid=63387)[0;0m ERROR 01-18 17:40:53 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 242, in _initialize_kv_caches
[2026-01-18 17:40:53] [ERROR] [0;36m(EngineCore_DP0 pid=63387)[0;0m ERROR 01-18 17:40:53 [core.py:935]     available_gpu_memory = self.model_executor.determine_available_memory()
[2026-01-18 17:40:53] [ERROR] [0;36m(EngineCore_DP0 pid=63387)[0;0m ERROR 01-18 17:40:53 [core.py:935]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(EngineCore_DP0 pid=63387)[0;0m ERROR 01-18 17:40:53 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[2026-01-18 17:40:53] [ERROR] [0;36m(EngineCore_DP0 pid=63387)[0;0m ERROR 01-18 17:40:53 [core.py:935]     return self.collective_rpc("determine_available_memory")
[2026-01-18 17:40:53] [ERROR] [0;36m(EngineCore_DP0 pid=63387)[0;0m ERROR 01-18 17:40:53 [core.py:935]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(EngineCore_DP0 pid=63387)[0;0m ERROR 01-18 17:40:53 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 374, in collective_rpc
[2026-01-18 17:40:53] [ERROR] [0;36m(EngineCore_DP0 pid=63387)[0;0m ERROR 01-18 17:40:53 [core.py:935]     return aggregate(get_response())
[2026-01-18 17:40:53] [ERROR] [0;36m(EngineCore_DP0 pid=63387)[0;0m ERROR 01-18 17:40:53 [core.py:935]                      ^^^^^^^^^^^^^^
[2026-01-18 17:40:53] [ERROR] [0;36m(EngineCore_DP0 pid=63387)[0;0m ERROR 01-18 17:40:53 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 357, in get_response
[2026-01-18 17:40:53] [ERROR] [0;36m(EngineCore_DP0 pid=63387)[0;0m ERROR 01-18 17:40:53 [core.py:935]     raise RuntimeError(
[2026-01-18 17:40:53] [ERROR] [0;36m(EngineCore_DP0 pid=63387)[0;0m ERROR 01-18 17:40:53 [core.py:935] RuntimeError: Worker failed with error 'CUDA out of memory. Tried to allocate 15.62 GiB. GPU 0 has a total capacity of 94.97 GiB of which 11.97 GiB is free. Including non-PyTorch memory, this process has 82.97 GiB memory in use. Of the allocated memory 65.92 GiB is allocated by PyTorch, and 16.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)', please check the stack trace above for the root cause
[2026-01-18 17:40:54] [ERROR] [0;36m(EngineCore_DP0 pid=63387)[0;0m ERROR 01-18 17:40:54 [multiproc_executor.py:246] Worker proc VllmWorker-1 died unexpectedly, shutting down executor.
[2026-01-18 17:40:56] [INFO] [0;36m(EngineCore_DP0 pid=63387)[0;0m Process EngineCore_DP0:
[2026-01-18 17:40:56] [ERROR] [0;36m(EngineCore_DP0 pid=63387)[0;0m Traceback (most recent call last):
[2026-01-18 17:40:56] [INFO] [0;36m(EngineCore_DP0 pid=63387)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[2026-01-18 17:40:56] [INFO] [0;36m(EngineCore_DP0 pid=63387)[0;0m     self.run()
[2026-01-18 17:40:56] [INFO] [0;36m(EngineCore_DP0 pid=63387)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/process.py", line 108, in run
[2026-01-18 17:40:56] [INFO] [0;36m(EngineCore_DP0 pid=63387)[0;0m     self._target(*self._args, **self._kwargs)
[2026-01-18 17:40:56] [INFO] [0;36m(EngineCore_DP0 pid=63387)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 939, in run_engine_core
[2026-01-18 17:40:56] [INFO] [0;36m(EngineCore_DP0 pid=63387)[0;0m     raise e
[2026-01-18 17:40:56] [INFO] [0;36m(EngineCore_DP0 pid=63387)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-18 17:40:56] [INFO] [0;36m(EngineCore_DP0 pid=63387)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 17:40:56] [INFO] [0;36m(EngineCore_DP0 pid=63387)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:56] [INFO] [0;36m(EngineCore_DP0 pid=63387)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-18 17:40:56] [INFO] [0;36m(EngineCore_DP0 pid=63387)[0;0m     super().__init__(
[2026-01-18 17:40:56] [INFO] [0;36m(EngineCore_DP0 pid=63387)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 112, in __init__
[2026-01-18 17:40:56] [INFO] [0;36m(EngineCore_DP0 pid=63387)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[2026-01-18 17:40:56] [INFO] [0;36m(EngineCore_DP0 pid=63387)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:56] [INFO] [0;36m(EngineCore_DP0 pid=63387)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 242, in _initialize_kv_caches
[2026-01-18 17:40:56] [INFO] [0;36m(EngineCore_DP0 pid=63387)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[2026-01-18 17:40:56] [INFO] [0;36m(EngineCore_DP0 pid=63387)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:56] [INFO] [0;36m(EngineCore_DP0 pid=63387)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[2026-01-18 17:40:56] [INFO] [0;36m(EngineCore_DP0 pid=63387)[0;0m     return self.collective_rpc("determine_available_memory")
[2026-01-18 17:40:56] [INFO] [0;36m(EngineCore_DP0 pid=63387)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:56] [INFO] [0;36m(EngineCore_DP0 pid=63387)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 374, in collective_rpc
[2026-01-18 17:40:56] [INFO] [0;36m(EngineCore_DP0 pid=63387)[0;0m     return aggregate(get_response())
[2026-01-18 17:40:56] [INFO] [0;36m(EngineCore_DP0 pid=63387)[0;0m                      ^^^^^^^^^^^^^^
[2026-01-18 17:40:56] [INFO] [0;36m(EngineCore_DP0 pid=63387)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 357, in get_response
[2026-01-18 17:40:56] [INFO] [0;36m(EngineCore_DP0 pid=63387)[0;0m     raise RuntimeError(
[2026-01-18 17:40:56] [INFO] [0;36m(EngineCore_DP0 pid=63387)[0;0m RuntimeError: Worker failed with error 'CUDA out of memory. Tried to allocate 15.62 GiB. GPU 0 has a total capacity of 94.97 GiB of which 11.97 GiB is free. Including non-PyTorch memory, this process has 82.97 GiB memory in use. Of the allocated memory 65.92 GiB is allocated by PyTorch, and 16.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)', please check the stack trace above for the root cause
[2026-01-18 17:40:57] [ERROR] [0;36m(APIServer pid=62900)[0;0m Traceback (most recent call last):
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/bin/vllm", line 7, in <module>
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m     sys.exit(main())
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m              ^^^^^^
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m     args.dispatch_function(args)
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m     uvloop.run(run_server(args))
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m     return __asyncio.run(
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m            ^^^^^^^^^^^^^^
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m     return runner.run(main)
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m     return self._loop.run_until_complete(task)
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m     return await main
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m            ^^^^^^^^^^
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m     async with build_async_engine_client(
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m     return await anext(self.gen)
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 146, in build_async_engine_client
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m     return await anext(self.gen)
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 187, in build_async_engine_client_from_engine_args
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 202, in from_vllm_config
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m     return cls(
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m            ^^^^
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 129, in __init__
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m     return AsyncMPClient(*client_args)
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m     super().__init__(
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 144, in __exit__
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m     next(self.gen)
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 918, in launch_core_engines
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m     wait_for_engine_startup(
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 977, in wait_for_engine_startup
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m     raise RuntimeError(
[2026-01-18 17:40:57] [INFO] [0;36m(APIServer pid=62900)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[2026-01-18 17:41:28] [INFO] nvitop监控已启动
[2026-01-18 17:42:18] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 17:42:18] [INFO] nvitop监控已启动
[2026-01-18 17:42:18] [INFO] nvitop监控已停止
[2026-01-18 17:42:48] [WARNING] WARNING 01-18 17:42:48 [argparse_utils.py:342] Found duplicate keys --tokenizer-mode
[2026-01-18 17:42:48] [INFO] [0;36m(APIServer pid=66010)[0;0m INFO 01-18 17:42:48 [api_server.py:872] vLLM API server version 0.14.0rc2.dev117+g9e078d058
[2026-01-18 17:42:48] [INFO] [0;36m(APIServer pid=66010)[0;0m INFO 01-18 17:42:48 [utils.py:267] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-235B-A22B-Thinking-2507-AWQ/', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-235B-A22B-Thinking-2507-AWQ/chat_template.jinja', 'model': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-235B-A22B-Thinking-2507-AWQ/', 'trust_remote_code': True, 'max_model_len': 128000, 'enforce_eager': True, 'served_model_name': ['VLLM-MODEL'], 'reasoning_parser': 'deepseek_r1', 'tensor_parallel_size': 2, 'gpu_memory_utilization': 0.95, 'kv_cache_dtype': 'fp8', 'max_num_seqs': 256, 'enable_chunked_prefill': False, 'async_scheduling': True, 'compilation_config': {'level': None, 'mode': None, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': [], 'splitting_ops': None, 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': None, 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.PIECEWISE: 1>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': None, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}}
[2026-01-18 17:42:48] [INFO] [0;36m(APIServer pid=66010)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 17:42:48] [INFO] [0;36m(APIServer pid=66010)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 17:42:48] [INFO] [0;36m(APIServer pid=66010)[0;0m INFO 01-18 17:42:48 [model.py:530] Resolved architecture: Qwen3MoeForCausalLM
[2026-01-18 17:42:48] [INFO] [0;36m(APIServer pid=66010)[0;0m INFO 01-18 17:42:48 [model.py:1547] Using max model len 128000
[2026-01-18 17:42:48] [WARNING] [0;36m(APIServer pid=66010)[0;0m WARNING 01-18 17:42:48 [quark_ocp_mx.py:157] AITER is not found or QuarkOCP_MX is not supported on the current platform. QuarkOCP_MX quantization will not be available.
[2026-01-18 17:42:48] [INFO] [0;36m(APIServer pid=66010)[0;0m INFO 01-18 17:42:48 [awq_marlin.py:163] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2026-01-18 17:42:48] [WARNING] [0;36m(APIServer pid=66010)[0;0m WARNING 01-18 17:42:48 [arg_utils.py:1913] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[2026-01-18 17:42:48] [INFO] [0;36m(APIServer pid=66010)[0;0m INFO 01-18 17:42:48 [cache.py:206] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor.
[2026-01-18 17:42:49] [INFO] [0;36m(APIServer pid=66010)[0;0m INFO 01-18 17:42:49 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 17:42:49] [INFO] [0;36m(APIServer pid=66010)[0;0m INFO 01-18 17:42:49 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-18 17:42:49] [WARNING] [0;36m(APIServer pid=66010)[0;0m WARNING 01-18 17:42:49 [vllm.py:653] Enforce eager set, overriding optimization level to -O0
[2026-01-18 17:42:49] [INFO] [0;36m(APIServer pid=66010)[0;0m INFO 01-18 17:42:49 [vllm.py:705] Cudagraph mode PIECEWISE is not compatible with compilation mode 0.Overriding to NONE.
[2026-01-18 17:42:49] [INFO] [0;36m(APIServer pid=66010)[0;0m INFO 01-18 17:42:49 [vllm.py:753] Cudagraph is disabled under eager mode
[2026-01-18 17:43:18] [INFO] [0;36m(EngineCore_DP0 pid=66508)[0;0m INFO 01-18 17:43:18 [core.py:96] Initializing a V1 LLM engine (v0.14.0rc2.dev117+g9e078d058) with config: model='/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-235B-A22B-Thinking-2507-AWQ/', speculative_config=None, tokenizer='/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-235B-A22B-Thinking-2507-AWQ/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=fp8, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='deepseek_r1', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=True, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [128000], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-18 17:43:18] [WARNING] [0;36m(EngineCore_DP0 pid=66508)[0;0m WARNING 01-18 17:43:18 [multiproc_executor.py:897] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-18 17:43:48] [INFO] INFO 01-18 17:43:48 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:56597 backend=nccl
[2026-01-18 17:43:48] [INFO] INFO 01-18 17:43:48 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:56597 backend=nccl
[2026-01-18 17:43:49] [INFO] INFO 01-18 17:43:49 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-18 17:43:49] [WARNING] WARNING 01-18 17:43:49 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 17:43:49] [WARNING] WARNING 01-18 17:43:49 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 17:43:49] [INFO] INFO 01-18 17:43:49 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[2026-01-18 17:43:49] [INFO] INFO 01-18 17:43:49 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
[2026-01-18 17:43:51] [INFO] [0;36m(Worker_TP0 pid=66838)[0;0m INFO 01-18 17:43:51 [gpu_model_runner.py:3803] Starting to load model /mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-235B-A22B-Thinking-2507-AWQ/...
[2026-01-18 17:43:55] [INFO] [0;36m(Worker_TP0 pid=66838)[0;0m INFO 01-18 17:43:55 [cuda.py:351] Using FLASHINFER attention backend out of potential backends: ('FLASHINFER', 'TRITON_ATTN')
[2026-01-18 17:43:56] [INFO] [0;36m(Worker_TP0 pid=66838)[0;0m
[2026-01-18 17:43:56] [INFO] Loading safetensors checkpoint shards:   0% Completed | 0/25 [00:00<?, ?it/s]
[2026-01-18 17:43:56] [INFO] [0;36m(Worker_TP0 pid=66838)[0;0m
[2026-01-18 17:43:56] [INFO] Loading safetensors checkpoint shards:   4% Completed | 1/25 [00:00<00:19,  1.23it/s]
[2026-01-18 17:43:57] [INFO] [0;36m(Worker_TP0 pid=66838)[0;0m
[2026-01-18 17:43:57] [INFO] Loading safetensors checkpoint shards:   8% Completed | 2/25 [00:01<00:18,  1.25it/s]
[2026-01-18 17:43:58] [INFO] [0;36m(Worker_TP0 pid=66838)[0;0m
[2026-01-18 17:43:58] [INFO] Loading safetensors checkpoint shards:  12% Completed | 3/25 [00:02<00:18,  1.17it/s]
[2026-01-18 17:43:59] [INFO] [0;36m(Worker_TP0 pid=66838)[0;0m
[2026-01-18 17:43:59] [INFO] Loading safetensors checkpoint shards:  16% Completed | 4/25 [00:03<00:18,  1.13it/s]
[2026-01-18 17:44:00] [INFO] [0;36m(Worker_TP0 pid=66838)[0;0m
[2026-01-18 17:44:00] [INFO] Loading safetensors checkpoint shards:  20% Completed | 5/25 [00:04<00:17,  1.12it/s]
[2026-01-18 17:44:01] [INFO] [0;36m(Worker_TP0 pid=66838)[0;0m
[2026-01-18 17:44:01] [INFO] Loading safetensors checkpoint shards:  24% Completed | 6/25 [00:05<00:17,  1.09it/s]
[2026-01-18 17:44:02] [INFO] [0;36m(Worker_TP0 pid=66838)[0;0m
[2026-01-18 17:44:02] [INFO] Loading safetensors checkpoint shards:  28% Completed | 7/25 [00:06<00:16,  1.07it/s]
[2026-01-18 17:44:03] [INFO] [0;36m(Worker_TP0 pid=66838)[0;0m
[2026-01-18 17:44:03] [INFO] Loading safetensors checkpoint shards:  32% Completed | 8/25 [00:07<00:16,  1.03it/s]
[2026-01-18 17:44:04] [INFO] [0;36m(Worker_TP0 pid=66838)[0;0m
[2026-01-18 17:44:04] [INFO] Loading safetensors checkpoint shards:  36% Completed | 9/25 [00:08<00:15,  1.02it/s]
[2026-01-18 17:44:05] [INFO] [0;36m(Worker_TP0 pid=66838)[0;0m
[2026-01-18 17:44:05] [INFO] Loading safetensors checkpoint shards:  40% Completed | 10/25 [00:09<00:14,  1.02it/s]
[2026-01-18 17:44:06] [INFO] [0;36m(Worker_TP0 pid=66838)[0;0m
[2026-01-18 17:44:06] [INFO] Loading safetensors checkpoint shards:  44% Completed | 11/25 [00:10<00:13,  1.03it/s]
[2026-01-18 17:44:07] [INFO] [0;36m(Worker_TP0 pid=66838)[0;0m
[2026-01-18 17:44:07] [INFO] Loading safetensors checkpoint shards:  48% Completed | 12/25 [00:11<00:12,  1.05it/s]
[2026-01-18 17:44:08] [INFO] [0;36m(Worker_TP0 pid=66838)[0;0m
[2026-01-18 17:44:08] [INFO] Loading safetensors checkpoint shards:  52% Completed | 13/25 [00:12<00:11,  1.06it/s]
[2026-01-18 17:44:09] [INFO] [0;36m(Worker_TP0 pid=66838)[0;0m
[2026-01-18 17:44:09] [INFO] Loading safetensors checkpoint shards:  56% Completed | 14/25 [00:13<00:10,  1.06it/s]
[2026-01-18 17:44:10] [INFO] [0;36m(Worker_TP0 pid=66838)[0;0m
[2026-01-18 17:44:10] [INFO] Loading safetensors checkpoint shards:  60% Completed | 15/25 [00:13<00:09,  1.07it/s]
[2026-01-18 17:44:10] [INFO] [0;36m(Worker_TP0 pid=66838)[0;0m
[2026-01-18 17:44:10] [INFO] Loading safetensors checkpoint shards:  64% Completed | 16/25 [00:14<00:08,  1.08it/s]
[2026-01-18 17:44:11] [INFO] [0;36m(Worker_TP0 pid=66838)[0;0m
[2026-01-18 17:44:11] [INFO] Loading safetensors checkpoint shards:  68% Completed | 17/25 [00:15<00:07,  1.07it/s]
[2026-01-18 17:44:12] [INFO] [0;36m(Worker_TP0 pid=66838)[0;0m
[2026-01-18 17:44:12] [INFO] Loading safetensors checkpoint shards:  72% Completed | 18/25 [00:16<00:06,  1.07it/s]
[2026-01-18 17:44:13] [INFO] [0;36m(Worker_TP0 pid=66838)[0;0m
[2026-01-18 17:44:13] [INFO] Loading safetensors checkpoint shards:  76% Completed | 19/25 [00:17<00:05,  1.07it/s]
[2026-01-18 17:44:14] [INFO] [0;36m(Worker_TP0 pid=66838)[0;0m
[2026-01-18 17:44:14] [INFO] Loading safetensors checkpoint shards:  80% Completed | 20/25 [00:18<00:04,  1.08it/s]
[2026-01-18 17:44:15] [INFO] [0;36m(Worker_TP0 pid=66838)[0;0m
[2026-01-18 17:44:15] [INFO] Loading safetensors checkpoint shards:  84% Completed | 21/25 [00:19<00:03,  1.08it/s]
[2026-01-18 17:44:16] [INFO] [0;36m(Worker_TP0 pid=66838)[0;0m
[2026-01-18 17:44:16] [INFO] Loading safetensors checkpoint shards:  88% Completed | 22/25 [00:20<00:02,  1.08it/s]
[2026-01-18 17:44:17] [INFO] [0;36m(Worker_TP0 pid=66838)[0;0m
[2026-01-18 17:44:17] [INFO] Loading safetensors checkpoint shards:  92% Completed | 23/25 [00:21<00:01,  1.08it/s]
[2026-01-18 17:44:18] [INFO] [0;36m(Worker_TP0 pid=66838)[0;0m
[2026-01-18 17:44:18] [INFO] Loading safetensors checkpoint shards:  96% Completed | 24/25 [00:22<00:00,  1.06it/s]
[2026-01-18 17:44:19] [INFO] [0;36m(Worker_TP0 pid=66838)[0;0m
[2026-01-18 17:44:19] [INFO] Loading safetensors checkpoint shards: 100% Completed | 25/25 [00:23<00:00,  1.14it/s]
[2026-01-18 17:44:19] [INFO] [0;36m(Worker_TP0 pid=66838)[0;0m
[2026-01-18 17:44:19] [INFO] Loading safetensors checkpoint shards: 100% Completed | 25/25 [00:23<00:00,  1.08it/s]
[2026-01-18 17:44:19] [INFO] [0;36m(Worker_TP0 pid=66838)[0;0m
[2026-01-18 17:44:19] [INFO] [0;36m(Worker_TP0 pid=66838)[0;0m INFO 01-18 17:44:19 [default_loader.py:291] Loading weights took 23.16 seconds
[2026-01-18 17:44:29] [INFO] [0;36m(Worker_TP0 pid=66838)[0;0m INFO 01-18 17:44:29 [gpu_model_runner.py:3900] Model loading took 57.93 GiB memory and 36.833714 seconds
[2026-01-18 17:44:48] [INFO] [0;36m(Worker_TP0 pid=66838)[0;0m INFO 01-18 17:44:48 [gpu_worker.py:355] Available KV cache memory: 18.49 GiB
[2026-01-18 17:44:48] [INFO] [0;36m(EngineCore_DP0 pid=66508)[0;0m INFO 01-18 17:44:48 [kv_cache_utils.py:1307] GPU KV cache size: 412,448 tokens
[2026-01-18 17:44:48] [INFO] [0;36m(EngineCore_DP0 pid=66508)[0;0m INFO 01-18 17:44:48 [kv_cache_utils.py:1312] Maximum concurrency for 128,000 tokens per request: 3.22x
[2026-01-18 17:44:48] [INFO] [0;36m(Worker_TP1 pid=66839)[0;0m [0;36m(Worker_TP0 pid=66838)[0;0m 2026-01-18 17:44:48,892 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[2026-01-18 17:44:48] [INFO] 2026-01-18 17:44:48,892 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[2026-01-18 17:44:49] [INFO] [0;36m(Worker_TP0 pid=66838)[0;0m 2026-01-18 17:44:49,161 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[2026-01-18 17:44:49] [INFO] [0;36m(Worker_TP1 pid=66839)[0;0m 2026-01-18 17:44:49,162 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839] WorkerProc hit an exception.
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839] Traceback (most recent call last):
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 834, in worker_busy_loop
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     output = func(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]              ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 447, in compile_or_warm_up_model
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     kernel_warmup(self)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/warmup/kernel_warmup.py", line 41, in kernel_warmup
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     flashinfer_autotune(worker.model_runner)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/warmup/kernel_warmup.py", line 93, in flashinfer_autotune
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     runner._dummy_run(
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return func(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4457, in _dummy_run
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     outputs = self.model(
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]               ^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 735, in forward
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     hidden_states = self.model(
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]                     ^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 389, in __call__
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return self.forward(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 456, in forward
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     hidden_states, residual = layer(positions, hidden_states, residual)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 387, in forward
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     hidden_states = self.mlp(hidden_states)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]                     ^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 199, in forward
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     final_hidden_states = self.experts(
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]                           ^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/custom_op.py", line 47, in forward
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return self._forward_method(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1754, in forward_cuda
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return self.forward_native(hidden_states, router_logits)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1722, in forward_native
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     fused_output = torch.ops.vllm.moe_forward(
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_ops.py", line 1255, in __call__
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return self._op(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 2103, in moe_forward
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return self.forward_impl(hidden_states, router_logits)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1979, in forward_impl
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     final_hidden_states = self.quant_method.apply(
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]                           ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/awq_marlin.py", line 772, in apply
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return fused_marlin_moe(
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/fused_marlin_moe.py", line 315, in fused_marlin_moe
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     moe_output = _fused_marlin_moe(
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]                  ^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/fused_marlin_moe.py", line 80, in _fused_marlin_moe
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     intermediate_cache13 = torch.empty(
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]                            ^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 7.81 GiB. GPU 0 has a total capacity of 94.97 GiB of which 5.10 GiB is free. Including non-PyTorch memory, this process has 89.84 GiB memory in use. Of the allocated memory 80.45 GiB is allocated by PyTorch, and 8.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839] Traceback (most recent call last):
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 834, in worker_busy_loop
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     output = func(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]              ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 447, in compile_or_warm_up_model
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     kernel_warmup(self)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/warmup/kernel_warmup.py", line 41, in kernel_warmup
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     flashinfer_autotune(worker.model_runner)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/warmup/kernel_warmup.py", line 93, in flashinfer_autotune
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     runner._dummy_run(
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return func(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4457, in _dummy_run
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     outputs = self.model(
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]               ^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 735, in forward
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     hidden_states = self.model(
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]                     ^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 389, in __call__
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return self.forward(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 456, in forward
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     hidden_states, residual = layer(positions, hidden_states, residual)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 387, in forward
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     hidden_states = self.mlp(hidden_states)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]                     ^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 199, in forward
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     final_hidden_states = self.experts(
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]                           ^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/custom_op.py", line 47, in forward
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return self._forward_method(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1754, in forward_cuda
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return self.forward_native(hidden_states, router_logits)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1722, in forward_native
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     fused_output = torch.ops.vllm.moe_forward(
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_ops.py", line 1255, in __call__
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return self._op(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 2103, in moe_forward
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return self.forward_impl(hidden_states, router_logits)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1979, in forward_impl
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     final_hidden_states = self.quant_method.apply(
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]                           ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/awq_marlin.py", line 772, in apply
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return fused_marlin_moe(
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/fused_marlin_moe.py", line 315, in fused_marlin_moe
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     moe_output = _fused_marlin_moe(
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]                  ^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/fused_marlin_moe.py", line 80, in _fused_marlin_moe
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     intermediate_cache13 = torch.empty(
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]                            ^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 7.81 GiB. GPU 0 has a total capacity of 94.97 GiB of which 5.10 GiB is free. Including non-PyTorch memory, this process has 89.84 GiB memory in use. Of the allocated memory 80.45 GiB is allocated by PyTorch, and 8.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP0 pid=66838)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839] WorkerProc hit an exception.
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839] Traceback (most recent call last):
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 834, in worker_busy_loop
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     output = func(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]              ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 447, in compile_or_warm_up_model
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     kernel_warmup(self)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/warmup/kernel_warmup.py", line 41, in kernel_warmup
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     flashinfer_autotune(worker.model_runner)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/warmup/kernel_warmup.py", line 93, in flashinfer_autotune
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     runner._dummy_run(
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return func(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4457, in _dummy_run
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     outputs = self.model(
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]               ^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 735, in forward
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     hidden_states = self.model(
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]                     ^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 389, in __call__
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return self.forward(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 456, in forward
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     hidden_states, residual = layer(positions, hidden_states, residual)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 387, in forward
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     hidden_states = self.mlp(hidden_states)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]                     ^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 199, in forward
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     final_hidden_states = self.experts(
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]                           ^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/custom_op.py", line 47, in forward
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return self._forward_method(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1754, in forward_cuda
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return self.forward_native(hidden_states, router_logits)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1722, in forward_native
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     fused_output = torch.ops.vllm.moe_forward(
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_ops.py", line 1255, in __call__
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return self._op(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 2103, in moe_forward
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return self.forward_impl(hidden_states, router_logits)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1979, in forward_impl
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     final_hidden_states = self.quant_method.apply(
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]                           ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/awq_marlin.py", line 772, in apply
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return fused_marlin_moe(
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/fused_marlin_moe.py", line 315, in fused_marlin_moe
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     moe_output = _fused_marlin_moe(
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]                  ^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/fused_marlin_moe.py", line 80, in _fused_marlin_moe
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     intermediate_cache13 = torch.empty(
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]                            ^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 7.81 GiB. GPU 1 has a total capacity of 94.97 GiB of which 5.10 GiB is free. Including non-PyTorch memory, this process has 89.84 GiB memory in use. Of the allocated memory 80.45 GiB is allocated by PyTorch, and 8.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839] Traceback (most recent call last):
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 834, in worker_busy_loop
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     output = func(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]              ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 447, in compile_or_warm_up_model
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     kernel_warmup(self)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/warmup/kernel_warmup.py", line 41, in kernel_warmup
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     flashinfer_autotune(worker.model_runner)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/warmup/kernel_warmup.py", line 93, in flashinfer_autotune
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     runner._dummy_run(
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return func(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4457, in _dummy_run
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     outputs = self.model(
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]               ^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 735, in forward
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     hidden_states = self.model(
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]                     ^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 389, in __call__
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return self.forward(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 456, in forward
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     hidden_states, residual = layer(positions, hidden_states, residual)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 387, in forward
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     hidden_states = self.mlp(hidden_states)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]                     ^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 199, in forward
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     final_hidden_states = self.experts(
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]                           ^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/custom_op.py", line 47, in forward
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return self._forward_method(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1754, in forward_cuda
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return self.forward_native(hidden_states, router_logits)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1722, in forward_native
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     fused_output = torch.ops.vllm.moe_forward(
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_ops.py", line 1255, in __call__
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return self._op(*args, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 2103, in moe_forward
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return self.forward_impl(hidden_states, router_logits)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1979, in forward_impl
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     final_hidden_states = self.quant_method.apply(
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]                           ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/awq_marlin.py", line 772, in apply
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     return fused_marlin_moe(
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/fused_marlin_moe.py", line 315, in fused_marlin_moe
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     moe_output = _fused_marlin_moe(
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]                  ^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/fused_marlin_moe.py", line 80, in _fused_marlin_moe
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]     intermediate_cache13 = torch.empty(
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]                            ^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 7.81 GiB. GPU 1 has a total capacity of 94.97 GiB of which 5.10 GiB is free. Including non-PyTorch memory, this process has 89.84 GiB memory in use. Of the allocated memory 80.45 GiB is allocated by PyTorch, and 8.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2026-01-18 17:44:49] [ERROR] [0;36m(Worker_TP1 pid=66839)[0;0m ERROR 01-18 17:44:49 [multiproc_executor.py:839]
[2026-01-18 17:44:49] [ERROR] [0;36m(EngineCore_DP0 pid=66508)[0;0m ERROR 01-18 17:44:49 [core.py:935] EngineCore failed to start.
[2026-01-18 17:44:49] [ERROR] [0;36m(EngineCore_DP0 pid=66508)[0;0m ERROR 01-18 17:44:49 [core.py:935] Traceback (most recent call last):
[2026-01-18 17:44:49] [ERROR] [0;36m(EngineCore_DP0 pid=66508)[0;0m ERROR 01-18 17:44:49 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-18 17:44:49] [ERROR] [0;36m(EngineCore_DP0 pid=66508)[0;0m ERROR 01-18 17:44:49 [core.py:935]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 17:44:49] [ERROR] [0;36m(EngineCore_DP0 pid=66508)[0;0m ERROR 01-18 17:44:49 [core.py:935]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(EngineCore_DP0 pid=66508)[0;0m ERROR 01-18 17:44:49 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-18 17:44:49] [ERROR] [0;36m(EngineCore_DP0 pid=66508)[0;0m ERROR 01-18 17:44:49 [core.py:935]     super().__init__(
[2026-01-18 17:44:49] [ERROR] [0;36m(EngineCore_DP0 pid=66508)[0;0m ERROR 01-18 17:44:49 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 112, in __init__
[2026-01-18 17:44:49] [ERROR] [0;36m(EngineCore_DP0 pid=66508)[0;0m ERROR 01-18 17:44:49 [core.py:935]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[2026-01-18 17:44:49] [ERROR] [0;36m(EngineCore_DP0 pid=66508)[0;0m ERROR 01-18 17:44:49 [core.py:935]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(EngineCore_DP0 pid=66508)[0;0m ERROR 01-18 17:44:49 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 269, in _initialize_kv_caches
[2026-01-18 17:44:49] [ERROR] [0;36m(EngineCore_DP0 pid=66508)[0;0m ERROR 01-18 17:44:49 [core.py:935]     self.model_executor.initialize_from_config(kv_cache_configs)
[2026-01-18 17:44:49] [ERROR] [0;36m(EngineCore_DP0 pid=66508)[0;0m ERROR 01-18 17:44:49 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[2026-01-18 17:44:49] [ERROR] [0;36m(EngineCore_DP0 pid=66508)[0;0m ERROR 01-18 17:44:49 [core.py:935]     self.collective_rpc("compile_or_warm_up_model")
[2026-01-18 17:44:49] [ERROR] [0;36m(EngineCore_DP0 pid=66508)[0;0m ERROR 01-18 17:44:49 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 374, in collective_rpc
[2026-01-18 17:44:49] [ERROR] [0;36m(EngineCore_DP0 pid=66508)[0;0m ERROR 01-18 17:44:49 [core.py:935]     return aggregate(get_response())
[2026-01-18 17:44:49] [ERROR] [0;36m(EngineCore_DP0 pid=66508)[0;0m ERROR 01-18 17:44:49 [core.py:935]                      ^^^^^^^^^^^^^^
[2026-01-18 17:44:49] [ERROR] [0;36m(EngineCore_DP0 pid=66508)[0;0m ERROR 01-18 17:44:49 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 357, in get_response
[2026-01-18 17:44:49] [ERROR] [0;36m(EngineCore_DP0 pid=66508)[0;0m ERROR 01-18 17:44:49 [core.py:935]     raise RuntimeError(
[2026-01-18 17:44:49] [ERROR] [0;36m(EngineCore_DP0 pid=66508)[0;0m ERROR 01-18 17:44:49 [core.py:935] RuntimeError: Worker failed with error 'CUDA out of memory. Tried to allocate 7.81 GiB. GPU 0 has a total capacity of 94.97 GiB of which 5.10 GiB is free. Including non-PyTorch memory, this process has 89.84 GiB memory in use. Of the allocated memory 80.45 GiB is allocated by PyTorch, and 8.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)', please check the stack trace above for the root cause
[2026-01-18 17:44:50] [ERROR] [0;36m(EngineCore_DP0 pid=66508)[0;0m ERROR 01-18 17:44:50 [multiproc_executor.py:246] Worker proc VllmWorker-0 died unexpectedly, shutting down executor.
[2026-01-18 17:44:51] [INFO] [0;36m(EngineCore_DP0 pid=66508)[0;0m Process EngineCore_DP0:
[2026-01-18 17:44:51] [ERROR] [0;36m(EngineCore_DP0 pid=66508)[0;0m Traceback (most recent call last):
[2026-01-18 17:44:51] [INFO] [0;36m(EngineCore_DP0 pid=66508)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[2026-01-18 17:44:51] [INFO] [0;36m(EngineCore_DP0 pid=66508)[0;0m     self.run()
[2026-01-18 17:44:51] [INFO] [0;36m(EngineCore_DP0 pid=66508)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/process.py", line 108, in run
[2026-01-18 17:44:51] [INFO] [0;36m(EngineCore_DP0 pid=66508)[0;0m     self._target(*self._args, **self._kwargs)
[2026-01-18 17:44:51] [INFO] [0;36m(EngineCore_DP0 pid=66508)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 939, in run_engine_core
[2026-01-18 17:44:51] [INFO] [0;36m(EngineCore_DP0 pid=66508)[0;0m     raise e
[2026-01-18 17:44:51] [INFO] [0;36m(EngineCore_DP0 pid=66508)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-18 17:44:51] [INFO] [0;36m(EngineCore_DP0 pid=66508)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 17:44:51] [INFO] [0;36m(EngineCore_DP0 pid=66508)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:51] [INFO] [0;36m(EngineCore_DP0 pid=66508)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-18 17:44:51] [INFO] [0;36m(EngineCore_DP0 pid=66508)[0;0m     super().__init__(
[2026-01-18 17:44:51] [INFO] [0;36m(EngineCore_DP0 pid=66508)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 112, in __init__
[2026-01-18 17:44:51] [INFO] [0;36m(EngineCore_DP0 pid=66508)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[2026-01-18 17:44:51] [INFO] [0;36m(EngineCore_DP0 pid=66508)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:51] [INFO] [0;36m(EngineCore_DP0 pid=66508)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 269, in _initialize_kv_caches
[2026-01-18 17:44:51] [INFO] [0;36m(EngineCore_DP0 pid=66508)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[2026-01-18 17:44:51] [INFO] [0;36m(EngineCore_DP0 pid=66508)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[2026-01-18 17:44:51] [INFO] [0;36m(EngineCore_DP0 pid=66508)[0;0m     self.collective_rpc("compile_or_warm_up_model")
[2026-01-18 17:44:51] [INFO] [0;36m(EngineCore_DP0 pid=66508)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 374, in collective_rpc
[2026-01-18 17:44:51] [INFO] [0;36m(EngineCore_DP0 pid=66508)[0;0m     return aggregate(get_response())
[2026-01-18 17:44:51] [INFO] [0;36m(EngineCore_DP0 pid=66508)[0;0m                      ^^^^^^^^^^^^^^
[2026-01-18 17:44:51] [INFO] [0;36m(EngineCore_DP0 pid=66508)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 357, in get_response
[2026-01-18 17:44:51] [INFO] [0;36m(EngineCore_DP0 pid=66508)[0;0m     raise RuntimeError(
[2026-01-18 17:44:51] [INFO] [0;36m(EngineCore_DP0 pid=66508)[0;0m RuntimeError: Worker failed with error 'CUDA out of memory. Tried to allocate 7.81 GiB. GPU 0 has a total capacity of 94.97 GiB of which 5.10 GiB is free. Including non-PyTorch memory, this process has 89.84 GiB memory in use. Of the allocated memory 80.45 GiB is allocated by PyTorch, and 8.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)', please check the stack trace above for the root cause
[2026-01-18 17:44:52] [ERROR] [0;36m(APIServer pid=66010)[0;0m Traceback (most recent call last):
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/bin/vllm", line 7, in <module>
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m     sys.exit(main())
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m              ^^^^^^
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m     args.dispatch_function(args)
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m     uvloop.run(run_server(args))
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m     return __asyncio.run(
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m            ^^^^^^^^^^^^^^
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m     return runner.run(main)
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m     return self._loop.run_until_complete(task)
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m     return await main
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m            ^^^^^^^^^^
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m     async with build_async_engine_client(
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m     return await anext(self.gen)
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 146, in build_async_engine_client
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m     return await anext(self.gen)
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 187, in build_async_engine_client_from_engine_args
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 202, in from_vllm_config
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m     return cls(
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m            ^^^^
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 129, in __init__
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m     return AsyncMPClient(*client_args)
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m     super().__init__(
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 144, in __exit__
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m     next(self.gen)
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 918, in launch_core_engines
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m     wait_for_engine_startup(
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 977, in wait_for_engine_startup
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m     raise RuntimeError(
[2026-01-18 17:44:52] [INFO] [0;36m(APIServer pid=66010)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[2026-01-18 17:45:11] [INFO] nvitop监控已启动
[2026-01-18 18:08:39] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 18:08:39] [INFO] nvitop监控已启动
[2026-01-18 18:09:07] [WARNING] WARNING 01-18 18:09:07 [argparse_utils.py:342] Found duplicate keys --tokenizer-mode
[2026-01-18 18:09:07] [INFO] [0;36m(APIServer pid=78997)[0;0m INFO 01-18 18:09:07 [api_server.py:872] vLLM API server version 0.14.0rc2.dev117+g9e078d058
[2026-01-18 18:09:07] [INFO] [0;36m(APIServer pid=78997)[0;0m INFO 01-18 18:09:07 [utils.py:267] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-235B-A22B-Thinking-2507-AWQ/', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-235B-A22B-Thinking-2507-AWQ/chat_template.jinja', 'model': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-235B-A22B-Thinking-2507-AWQ/', 'trust_remote_code': True, 'max_model_len': 81000, 'enforce_eager': True, 'served_model_name': ['VLLM-MODEL'], 'reasoning_parser': 'deepseek_r1', 'tensor_parallel_size': 2, 'gpu_memory_utilization': 0.95, 'kv_cache_dtype': 'fp8', 'max_num_seqs': 256, 'enable_chunked_prefill': False, 'async_scheduling': True, 'compilation_config': {'level': None, 'mode': None, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': [], 'splitting_ops': None, 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': None, 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.PIECEWISE: 1>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': None, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}}
[2026-01-18 18:09:07] [INFO] [0;36m(APIServer pid=78997)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 18:09:07] [INFO] [0;36m(APIServer pid=78997)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 18:09:07] [INFO] [0;36m(APIServer pid=78997)[0;0m INFO 01-18 18:09:07 [model.py:530] Resolved architecture: Qwen3MoeForCausalLM
[2026-01-18 18:09:07] [INFO] [0;36m(APIServer pid=78997)[0;0m INFO 01-18 18:09:07 [model.py:1547] Using max model len 81000
[2026-01-18 18:09:07] [WARNING] [0;36m(APIServer pid=78997)[0;0m WARNING 01-18 18:09:07 [quark_ocp_mx.py:157] AITER is not found or QuarkOCP_MX is not supported on the current platform. QuarkOCP_MX quantization will not be available.
[2026-01-18 18:09:07] [INFO] [0;36m(APIServer pid=78997)[0;0m INFO 01-18 18:09:07 [awq_marlin.py:163] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2026-01-18 18:09:07] [WARNING] [0;36m(APIServer pid=78997)[0;0m WARNING 01-18 18:09:07 [arg_utils.py:1913] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[2026-01-18 18:09:07] [INFO] [0;36m(APIServer pid=78997)[0;0m INFO 01-18 18:09:07 [cache.py:206] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor.
[2026-01-18 18:09:08] [INFO] [0;36m(APIServer pid=78997)[0;0m INFO 01-18 18:09:08 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 18:09:08] [INFO] [0;36m(APIServer pid=78997)[0;0m INFO 01-18 18:09:08 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-18 18:09:08] [WARNING] [0;36m(APIServer pid=78997)[0;0m WARNING 01-18 18:09:08 [vllm.py:653] Enforce eager set, overriding optimization level to -O0
[2026-01-18 18:09:08] [INFO] [0;36m(APIServer pid=78997)[0;0m INFO 01-18 18:09:08 [vllm.py:705] Cudagraph mode PIECEWISE is not compatible with compilation mode 0.Overriding to NONE.
[2026-01-18 18:09:08] [INFO] [0;36m(APIServer pid=78997)[0;0m INFO 01-18 18:09:08 [vllm.py:753] Cudagraph is disabled under eager mode
[2026-01-18 18:09:36] [INFO] [0;36m(EngineCore_DP0 pid=79336)[0;0m INFO 01-18 18:09:36 [core.py:96] Initializing a V1 LLM engine (v0.14.0rc2.dev117+g9e078d058) with config: model='/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-235B-A22B-Thinking-2507-AWQ/', speculative_config=None, tokenizer='/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-235B-A22B-Thinking-2507-AWQ/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=81000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=fp8, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='deepseek_r1', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=True, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [81000], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-18 18:09:36] [WARNING] [0;36m(EngineCore_DP0 pid=79336)[0;0m WARNING 01-18 18:09:36 [multiproc_executor.py:897] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-18 18:10:06] [INFO] INFO 01-18 18:10:06 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:34425 backend=nccl
[2026-01-18 18:10:06] [INFO] INFO 01-18 18:10:06 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:34425 backend=nccl
[2026-01-18 18:10:06] [INFO] INFO 01-18 18:10:06 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-18 18:10:06] [WARNING] WARNING 01-18 18:10:06 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 18:10:06] [WARNING] WARNING 01-18 18:10:06 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 18:10:06] [INFO] INFO 01-18 18:10:06 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[2026-01-18 18:10:06] [INFO] INFO 01-18 18:10:06 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
[2026-01-18 18:10:07] [INFO] [0;36m(Worker_TP0 pid=79649)[0;0m INFO 01-18 18:10:07 [gpu_model_runner.py:3803] Starting to load model /mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-235B-A22B-Thinking-2507-AWQ/...
[2026-01-18 18:10:12] [INFO] [0;36m(Worker_TP0 pid=79649)[0;0m INFO 01-18 18:10:12 [cuda.py:351] Using FLASHINFER attention backend out of potential backends: ('FLASHINFER', 'TRITON_ATTN')
[2026-01-18 18:10:12] [INFO] [0;36m(Worker_TP0 pid=79649)[0;0m
[2026-01-18 18:10:12] [INFO] Loading safetensors checkpoint shards:   0% Completed | 0/25 [00:00<?, ?it/s]
[2026-01-18 18:10:13] [INFO] [0;36m(Worker_TP0 pid=79649)[0;0m
[2026-01-18 18:10:13] [INFO] Loading safetensors checkpoint shards:   4% Completed | 1/25 [00:00<00:20,  1.17it/s]
[2026-01-18 18:10:14] [INFO] [0;36m(Worker_TP0 pid=79649)[0;0m
[2026-01-18 18:10:14] [INFO] Loading safetensors checkpoint shards:   8% Completed | 2/25 [00:01<00:18,  1.24it/s]
[2026-01-18 18:10:15] [INFO] [0;36m(Worker_TP0 pid=79649)[0;0m
[2026-01-18 18:10:15] [INFO] Loading safetensors checkpoint shards:  12% Completed | 3/25 [00:02<00:18,  1.18it/s]
[2026-01-18 18:10:16] [INFO] [0;36m(Worker_TP0 pid=79649)[0;0m
[2026-01-18 18:10:16] [INFO] Loading safetensors checkpoint shards:  16% Completed | 4/25 [00:03<00:18,  1.14it/s]
[2026-01-18 18:10:17] [INFO] [0;36m(Worker_TP0 pid=79649)[0;0m
[2026-01-18 18:10:17] [INFO] Loading safetensors checkpoint shards:  20% Completed | 5/25 [00:04<00:17,  1.12it/s]
[2026-01-18 18:10:18] [INFO] [0;36m(Worker_TP0 pid=79649)[0;0m
[2026-01-18 18:10:18] [INFO] Loading safetensors checkpoint shards:  24% Completed | 6/25 [00:05<00:17,  1.10it/s]
[2026-01-18 18:10:19] [INFO] [0;36m(Worker_TP0 pid=79649)[0;0m
[2026-01-18 18:10:19] [INFO] Loading safetensors checkpoint shards:  28% Completed | 7/25 [00:06<00:16,  1.08it/s]
[2026-01-18 18:10:20] [INFO] [0;36m(Worker_TP0 pid=79649)[0;0m
[2026-01-18 18:10:20] [INFO] Loading safetensors checkpoint shards:  32% Completed | 8/25 [00:07<00:16,  1.04it/s]
[2026-01-18 18:10:21] [INFO] [0;36m(Worker_TP0 pid=79649)[0;0m
[2026-01-18 18:10:21] [INFO] Loading safetensors checkpoint shards:  36% Completed | 9/25 [00:08<00:15,  1.03it/s]
[2026-01-18 18:10:22] [INFO] [0;36m(Worker_TP0 pid=79649)[0;0m
[2026-01-18 18:10:22] [INFO] Loading safetensors checkpoint shards:  40% Completed | 10/25 [00:09<00:14,  1.04it/s]
[2026-01-18 18:10:22] [INFO] [0;36m(Worker_TP0 pid=79649)[0;0m
[2026-01-18 18:10:22] [INFO] Loading safetensors checkpoint shards:  44% Completed | 11/25 [00:10<00:13,  1.05it/s]
[2026-01-18 18:10:23] [INFO] [0;36m(Worker_TP0 pid=79649)[0;0m
[2026-01-18 18:10:23] [INFO] Loading safetensors checkpoint shards:  48% Completed | 12/25 [00:11<00:12,  1.06it/s]
[2026-01-18 18:10:24] [INFO] [0;36m(Worker_TP0 pid=79649)[0;0m
[2026-01-18 18:10:24] [INFO] Loading safetensors checkpoint shards:  52% Completed | 13/25 [00:11<00:11,  1.08it/s]
[2026-01-18 18:10:25] [INFO] [0;36m(Worker_TP0 pid=79649)[0;0m
[2026-01-18 18:10:25] [INFO] Loading safetensors checkpoint shards:  56% Completed | 14/25 [00:12<00:10,  1.09it/s]
[2026-01-18 18:10:26] [INFO] [0;36m(Worker_TP0 pid=79649)[0;0m
[2026-01-18 18:10:26] [INFO] Loading safetensors checkpoint shards:  60% Completed | 15/25 [00:13<00:09,  1.09it/s]
[2026-01-18 18:10:27] [INFO] [0;36m(Worker_TP0 pid=79649)[0;0m
[2026-01-18 18:10:27] [INFO] Loading safetensors checkpoint shards:  64% Completed | 16/25 [00:14<00:08,  1.10it/s]
[2026-01-18 18:10:28] [INFO] [0;36m(Worker_TP0 pid=79649)[0;0m
[2026-01-18 18:10:28] [INFO] Loading safetensors checkpoint shards:  68% Completed | 17/25 [00:15<00:07,  1.10it/s]
[2026-01-18 18:10:29] [INFO] [0;36m(Worker_TP0 pid=79649)[0;0m
[2026-01-18 18:10:29] [INFO] Loading safetensors checkpoint shards:  72% Completed | 18/25 [00:16<00:06,  1.10it/s]
[2026-01-18 18:10:30] [INFO] [0;36m(Worker_TP0 pid=79649)[0;0m
[2026-01-18 18:10:30] [INFO] Loading safetensors checkpoint shards:  76% Completed | 19/25 [00:17<00:05,  1.10it/s]
[2026-01-18 18:10:31] [INFO] [0;36m(Worker_TP0 pid=79649)[0;0m
[2026-01-18 18:10:31] [INFO] Loading safetensors checkpoint shards:  80% Completed | 20/25 [00:18<00:04,  1.10it/s]
[2026-01-18 18:10:32] [INFO] [0;36m(Worker_TP0 pid=79649)[0;0m
[2026-01-18 18:10:32] [INFO] Loading safetensors checkpoint shards:  84% Completed | 21/25 [00:19<00:03,  1.10it/s]
[2026-01-18 18:10:32] [INFO] [0;36m(Worker_TP0 pid=79649)[0;0m
[2026-01-18 18:10:32] [INFO] Loading safetensors checkpoint shards:  88% Completed | 22/25 [00:20<00:02,  1.11it/s]
[2026-01-18 18:10:33] [INFO] [0;36m(Worker_TP0 pid=79649)[0;0m
[2026-01-18 18:10:33] [INFO] Loading safetensors checkpoint shards:  92% Completed | 23/25 [00:21<00:01,  1.10it/s]
[2026-01-18 18:10:34] [INFO] [0;36m(Worker_TP0 pid=79649)[0;0m
[2026-01-18 18:10:34] [INFO] Loading safetensors checkpoint shards:  96% Completed | 24/25 [00:21<00:00,  1.08it/s]
[2026-01-18 18:10:35] [INFO] [0;36m(Worker_TP0 pid=79649)[0;0m
[2026-01-18 18:10:35] [INFO] Loading safetensors checkpoint shards: 100% Completed | 25/25 [00:22<00:00,  1.18it/s]
[2026-01-18 18:10:35] [INFO] [0;36m(Worker_TP0 pid=79649)[0;0m
[2026-01-18 18:10:35] [INFO] Loading safetensors checkpoint shards: 100% Completed | 25/25 [00:22<00:00,  1.10it/s]
[2026-01-18 18:10:35] [INFO] [0;36m(Worker_TP0 pid=79649)[0;0m
[2026-01-18 18:10:35] [INFO] [0;36m(Worker_TP0 pid=79649)[0;0m INFO 01-18 18:10:35 [default_loader.py:291] Loading weights took 22.73 seconds
[2026-01-18 18:10:45] [INFO] [0;36m(Worker_TP0 pid=79649)[0;0m INFO 01-18 18:10:45 [gpu_model_runner.py:3900] Model loading took 57.94 GiB memory and 36.523807 seconds
[2026-01-18 18:10:59] [INFO] [0;36m(Worker_TP0 pid=79649)[0;0m INFO 01-18 18:10:59 [gpu_worker.py:355] Available KV cache memory: 23.51 GiB
[2026-01-18 18:10:59] [INFO] [0;36m(EngineCore_DP0 pid=79336)[0;0m INFO 01-18 18:10:59 [kv_cache_utils.py:1307] GPU KV cache size: 524,496 tokens
[2026-01-18 18:10:59] [INFO] [0;36m(EngineCore_DP0 pid=79336)[0;0m INFO 01-18 18:10:59 [kv_cache_utils.py:1312] Maximum concurrency for 81,000 tokens per request: 6.47x
[2026-01-18 18:10:59] [INFO] [0;36m(Worker_TP0 pid=79649)[0;0m [0;36m(Worker_TP1 pid=79650)[0;0m 2026-01-18 18:10:59,358 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[2026-01-18 18:10:59] [INFO] 2026-01-18 18:10:59,358 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[2026-01-18 18:10:59] [INFO] [0;36m(Worker_TP0 pid=79649)[0;0m 2026-01-18 18:10:59,526 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[2026-01-18 18:10:59] [INFO] [0;36m(Worker_TP1 pid=79650)[0;0m 2026-01-18 18:10:59,526 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP0 pid=79649)[0;0m [0;36m(Worker_TP1 pid=79650)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839] WorkerProc hit an exception.
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839] WorkerProc hit an exception.
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP0 pid=79649)[0;0m [0;36m(Worker_TP1 pid=79650)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839] Traceback (most recent call last):
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839] Traceback (most recent call last):
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP0 pid=79649)[0;0m [0;36m(Worker_TP1 pid=79650)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 834, in worker_busy_loop
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 834, in worker_busy_loop
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP0 pid=79649)[0;0m [0;36m(Worker_TP1 pid=79650)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     output = func(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     output = func(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP0 pid=79649)[0;0m [0;36m(Worker_TP1 pid=79650)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]              ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]              ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP0 pid=79649)[0;0m [0;36m(Worker_TP1 pid=79650)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 447, in compile_or_warm_up_model
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 447, in compile_or_warm_up_model
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP0 pid=79649)[0;0m [0;36m(Worker_TP1 pid=79650)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     kernel_warmup(self)
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     kernel_warmup(self)
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP0 pid=79649)[0;0m [0;36m(Worker_TP1 pid=79650)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/warmup/kernel_warmup.py", line 41, in kernel_warmup
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/warmup/kernel_warmup.py", line 41, in kernel_warmup
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP0 pid=79649)[0;0m [0;36m(Worker_TP1 pid=79650)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     flashinfer_autotune(worker.model_runner)
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     flashinfer_autotune(worker.model_runner)
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP0 pid=79649)[0;0m [0;36m(Worker_TP1 pid=79650)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/warmup/kernel_warmup.py", line 93, in flashinfer_autotune
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/warmup/kernel_warmup.py", line 93, in flashinfer_autotune
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP0 pid=79649)[0;0m [0;36m(Worker_TP1 pid=79650)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     runner._dummy_run(
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     runner._dummy_run(
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP0 pid=79649)[0;0m [0;36m(Worker_TP1 pid=79650)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP0 pid=79649)[0;0m [0;36m(Worker_TP1 pid=79650)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return func(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return func(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP0 pid=79649)[0;0m [0;36m(Worker_TP1 pid=79650)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP0 pid=79649)[0;0m [0;36m(Worker_TP1 pid=79650)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4457, in _dummy_run
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4457, in _dummy_run
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP0 pid=79649)[0;0m [0;36m(Worker_TP1 pid=79650)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     outputs = self.model(
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     outputs = self.model(
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP0 pid=79649)[0;0m [0;36m(Worker_TP1 pid=79650)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]               ^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]               ^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP0 pid=79649)[0;0m [0;36m(Worker_TP1 pid=79650)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP0 pid=79649)[0;0m [0;36m(Worker_TP1 pid=79650)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP0 pid=79649)[0;0m [0;36m(Worker_TP1 pid=79650)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP0 pid=79649)[0;0m [0;36m(Worker_TP1 pid=79650)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 735, in forward
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 735, in forward
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     hidden_states = self.model(
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     hidden_states = self.model(
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]                     ^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]                     ^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 389, in __call__
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 389, in __call__
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return self.forward(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return self.forward(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 456, in forward
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 456, in forward
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     hidden_states, residual = layer(positions, hidden_states, residual)
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     hidden_states, residual = layer(positions, hidden_states, residual)
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 387, in forward
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 387, in forward
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     hidden_states = self.mlp(hidden_states)
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     hidden_states = self.mlp(hidden_states)
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]                     ^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]                     ^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 199, in forward
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 199, in forward
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     final_hidden_states = self.experts(
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     final_hidden_states = self.experts(
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]                           ^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]                           ^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/custom_op.py", line 47, in forward
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/custom_op.py", line 47, in forward
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return self._forward_method(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return self._forward_method(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1754, in forward_cuda
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1754, in forward_cuda
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return self.forward_native(hidden_states, router_logits)
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return self.forward_native(hidden_states, router_logits)
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1722, in forward_native
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1722, in forward_native
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     fused_output = torch.ops.vllm.moe_forward(
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     fused_output = torch.ops.vllm.moe_forward(
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_ops.py", line 1255, in __call__
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_ops.py", line 1255, in __call__
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return self._op(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return self._op(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 2103, in moe_forward
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 2103, in moe_forward
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return self.forward_impl(hidden_states, router_logits)
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return self.forward_impl(hidden_states, router_logits)
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1979, in forward_impl
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1979, in forward_impl
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     final_hidden_states = self.quant_method.apply(
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     final_hidden_states = self.quant_method.apply(
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]                           ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]                           ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/awq_marlin.py", line 772, in apply
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/awq_marlin.py", line 772, in apply
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return fused_marlin_moe(
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return fused_marlin_moe(
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/fused_marlin_moe.py", line 315, in fused_marlin_moe
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/fused_marlin_moe.py", line 315, in fused_marlin_moe
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     moe_output = _fused_marlin_moe(
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     moe_output = _fused_marlin_moe(
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]                  ^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]                  ^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/fused_marlin_moe.py", line 80, in _fused_marlin_moe
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/fused_marlin_moe.py", line 80, in _fused_marlin_moe
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     intermediate_cache13 = torch.empty(
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     intermediate_cache13 = torch.empty(
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]                            ^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]                            ^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.95 GiB. GPU 1 has a total capacity of 94.97 GiB of which 4.24 GiB is free. Including non-PyTorch memory, this process has 90.70 GiB memory in use. Of the allocated memory 83.97 GiB is allocated by PyTorch, and 5.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.95 GiB. GPU 0 has a total capacity of 94.97 GiB of which 4.24 GiB is free. Including non-PyTorch memory, this process has 90.70 GiB memory in use. Of the allocated memory 83.97 GiB is allocated by PyTorch, and 5.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839] Traceback (most recent call last):
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839] Traceback (most recent call last):
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 834, in worker_busy_loop
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 834, in worker_busy_loop
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     output = func(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     output = func(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]              ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]              ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 447, in compile_or_warm_up_model
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 447, in compile_or_warm_up_model
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     kernel_warmup(self)
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     kernel_warmup(self)
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/warmup/kernel_warmup.py", line 41, in kernel_warmup
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/warmup/kernel_warmup.py", line 41, in kernel_warmup
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     flashinfer_autotune(worker.model_runner)
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     flashinfer_autotune(worker.model_runner)
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/warmup/kernel_warmup.py", line 93, in flashinfer_autotune
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/warmup/kernel_warmup.py", line 93, in flashinfer_autotune
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     runner._dummy_run(
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     runner._dummy_run(
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return func(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return func(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4457, in _dummy_run
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4457, in _dummy_run
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     outputs = self.model(
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     outputs = self.model(
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]               ^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]               ^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 735, in forward
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 735, in forward
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     hidden_states = self.model(
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     hidden_states = self.model(
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]                     ^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]                     ^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 389, in __call__
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 389, in __call__
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return self.forward(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return self.forward(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 456, in forward
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 456, in forward
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     hidden_states, residual = layer(positions, hidden_states, residual)
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     hidden_states, residual = layer(positions, hidden_states, residual)
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 387, in forward
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 387, in forward
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     hidden_states = self.mlp(hidden_states)
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     hidden_states = self.mlp(hidden_states)
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]                     ^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]                     ^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 199, in forward
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 199, in forward
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     final_hidden_states = self.experts(
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     final_hidden_states = self.experts(
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]                           ^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]                           ^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/custom_op.py", line 47, in forward
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/custom_op.py", line 47, in forward
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return self._forward_method(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return self._forward_method(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1754, in forward_cuda
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1754, in forward_cuda
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return self.forward_native(hidden_states, router_logits)
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return self.forward_native(hidden_states, router_logits)
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1722, in forward_native
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1722, in forward_native
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     fused_output = torch.ops.vllm.moe_forward(
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     fused_output = torch.ops.vllm.moe_forward(
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_ops.py", line 1255, in __call__
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_ops.py", line 1255, in __call__
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return self._op(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return self._op(*args, **kwargs)
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 2103, in moe_forward
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 2103, in moe_forward
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return self.forward_impl(hidden_states, router_logits)
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return self.forward_impl(hidden_states, router_logits)
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1979, in forward_impl
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1979, in forward_impl
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     final_hidden_states = self.quant_method.apply(
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     final_hidden_states = self.quant_method.apply(
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]                           ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]                           ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/awq_marlin.py", line 772, in apply
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/awq_marlin.py", line 772, in apply
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return fused_marlin_moe(
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     return fused_marlin_moe(
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/fused_marlin_moe.py", line 315, in fused_marlin_moe
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/fused_marlin_moe.py", line 315, in fused_marlin_moe
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     moe_output = _fused_marlin_moe(
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     moe_output = _fused_marlin_moe(
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]                  ^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]                  ^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/fused_marlin_moe.py", line 80, in _fused_marlin_moe
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/fused_marlin_moe.py", line 80, in _fused_marlin_moe
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]     intermediate_cache13 = torch.empty(
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]     intermediate_cache13 = torch.empty(
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]                            ^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]                            ^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.95 GiB. GPU 1 has a total capacity of 94.97 GiB of which 4.24 GiB is free. Including non-PyTorch memory, this process has 90.70 GiB memory in use. Of the allocated memory 83.97 GiB is allocated by PyTorch, and 5.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.95 GiB. GPU 0 has a total capacity of 94.97 GiB of which 4.24 GiB is free. Including non-PyTorch memory, this process has 90.70 GiB memory in use. Of the allocated memory 83.97 GiB is allocated by PyTorch, and 5.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2026-01-18 18:10:59] [ERROR] [0;36m(Worker_TP1 pid=79650)[0;0m [0;36m(Worker_TP0 pid=79649)[0;0m ERROR 01-18 18:10:59 [multiproc_executor.py:839]
[2026-01-18 18:10:59] [ERROR] ERROR 01-18 18:10:59 [multiproc_executor.py:839]
[2026-01-18 18:10:59] [ERROR] [0;36m(EngineCore_DP0 pid=79336)[0;0m ERROR 01-18 18:10:59 [core.py:935] EngineCore failed to start.
[2026-01-18 18:10:59] [ERROR] [0;36m(EngineCore_DP0 pid=79336)[0;0m ERROR 01-18 18:10:59 [core.py:935] Traceback (most recent call last):
[2026-01-18 18:10:59] [ERROR] [0;36m(EngineCore_DP0 pid=79336)[0;0m ERROR 01-18 18:10:59 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-18 18:10:59] [ERROR] [0;36m(EngineCore_DP0 pid=79336)[0;0m ERROR 01-18 18:10:59 [core.py:935]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 18:10:59] [ERROR] [0;36m(EngineCore_DP0 pid=79336)[0;0m ERROR 01-18 18:10:59 [core.py:935]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(EngineCore_DP0 pid=79336)[0;0m ERROR 01-18 18:10:59 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-18 18:10:59] [ERROR] [0;36m(EngineCore_DP0 pid=79336)[0;0m ERROR 01-18 18:10:59 [core.py:935]     super().__init__(
[2026-01-18 18:10:59] [ERROR] [0;36m(EngineCore_DP0 pid=79336)[0;0m ERROR 01-18 18:10:59 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 112, in __init__
[2026-01-18 18:10:59] [ERROR] [0;36m(EngineCore_DP0 pid=79336)[0;0m ERROR 01-18 18:10:59 [core.py:935]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[2026-01-18 18:10:59] [ERROR] [0;36m(EngineCore_DP0 pid=79336)[0;0m ERROR 01-18 18:10:59 [core.py:935]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(EngineCore_DP0 pid=79336)[0;0m ERROR 01-18 18:10:59 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 269, in _initialize_kv_caches
[2026-01-18 18:10:59] [ERROR] [0;36m(EngineCore_DP0 pid=79336)[0;0m ERROR 01-18 18:10:59 [core.py:935]     self.model_executor.initialize_from_config(kv_cache_configs)
[2026-01-18 18:10:59] [ERROR] [0;36m(EngineCore_DP0 pid=79336)[0;0m ERROR 01-18 18:10:59 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[2026-01-18 18:10:59] [ERROR] [0;36m(EngineCore_DP0 pid=79336)[0;0m ERROR 01-18 18:10:59 [core.py:935]     self.collective_rpc("compile_or_warm_up_model")
[2026-01-18 18:10:59] [ERROR] [0;36m(EngineCore_DP0 pid=79336)[0;0m ERROR 01-18 18:10:59 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 374, in collective_rpc
[2026-01-18 18:10:59] [ERROR] [0;36m(EngineCore_DP0 pid=79336)[0;0m ERROR 01-18 18:10:59 [core.py:935]     return aggregate(get_response())
[2026-01-18 18:10:59] [ERROR] [0;36m(EngineCore_DP0 pid=79336)[0;0m ERROR 01-18 18:10:59 [core.py:935]                      ^^^^^^^^^^^^^^
[2026-01-18 18:10:59] [ERROR] [0;36m(EngineCore_DP0 pid=79336)[0;0m ERROR 01-18 18:10:59 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 357, in get_response
[2026-01-18 18:10:59] [ERROR] [0;36m(EngineCore_DP0 pid=79336)[0;0m ERROR 01-18 18:10:59 [core.py:935]     raise RuntimeError(
[2026-01-18 18:10:59] [ERROR] [0;36m(EngineCore_DP0 pid=79336)[0;0m ERROR 01-18 18:10:59 [core.py:935] RuntimeError: Worker failed with error 'CUDA out of memory. Tried to allocate 4.95 GiB. GPU 0 has a total capacity of 94.97 GiB of which 4.24 GiB is free. Including non-PyTorch memory, this process has 90.70 GiB memory in use. Of the allocated memory 83.97 GiB is allocated by PyTorch, and 5.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)', please check the stack trace above for the root cause
[2026-01-18 18:11:00] [ERROR] [0;36m(EngineCore_DP0 pid=79336)[0;0m ERROR 01-18 18:11:00 [multiproc_executor.py:246] Worker proc VllmWorker-1 died unexpectedly, shutting down executor.
[2026-01-18 18:11:01] [INFO] [0;36m(EngineCore_DP0 pid=79336)[0;0m Process EngineCore_DP0:
[2026-01-18 18:11:01] [ERROR] [0;36m(EngineCore_DP0 pid=79336)[0;0m Traceback (most recent call last):
[2026-01-18 18:11:01] [INFO] [0;36m(EngineCore_DP0 pid=79336)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[2026-01-18 18:11:01] [INFO] [0;36m(EngineCore_DP0 pid=79336)[0;0m     self.run()
[2026-01-18 18:11:01] [INFO] [0;36m(EngineCore_DP0 pid=79336)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/process.py", line 108, in run
[2026-01-18 18:11:01] [INFO] [0;36m(EngineCore_DP0 pid=79336)[0;0m     self._target(*self._args, **self._kwargs)
[2026-01-18 18:11:01] [INFO] [0;36m(EngineCore_DP0 pid=79336)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 939, in run_engine_core
[2026-01-18 18:11:01] [INFO] [0;36m(EngineCore_DP0 pid=79336)[0;0m     raise e
[2026-01-18 18:11:01] [INFO] [0;36m(EngineCore_DP0 pid=79336)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-18 18:11:01] [INFO] [0;36m(EngineCore_DP0 pid=79336)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 18:11:01] [INFO] [0;36m(EngineCore_DP0 pid=79336)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:11:01] [INFO] [0;36m(EngineCore_DP0 pid=79336)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-18 18:11:01] [INFO] [0;36m(EngineCore_DP0 pid=79336)[0;0m     super().__init__(
[2026-01-18 18:11:01] [INFO] [0;36m(EngineCore_DP0 pid=79336)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 112, in __init__
[2026-01-18 18:11:01] [INFO] [0;36m(EngineCore_DP0 pid=79336)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[2026-01-18 18:11:01] [INFO] [0;36m(EngineCore_DP0 pid=79336)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:11:01] [INFO] [0;36m(EngineCore_DP0 pid=79336)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 269, in _initialize_kv_caches
[2026-01-18 18:11:01] [INFO] [0;36m(EngineCore_DP0 pid=79336)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[2026-01-18 18:11:01] [INFO] [0;36m(EngineCore_DP0 pid=79336)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
[2026-01-18 18:11:01] [INFO] [0;36m(EngineCore_DP0 pid=79336)[0;0m     self.collective_rpc("compile_or_warm_up_model")
[2026-01-18 18:11:01] [INFO] [0;36m(EngineCore_DP0 pid=79336)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 374, in collective_rpc
[2026-01-18 18:11:01] [INFO] [0;36m(EngineCore_DP0 pid=79336)[0;0m     return aggregate(get_response())
[2026-01-18 18:11:01] [INFO] [0;36m(EngineCore_DP0 pid=79336)[0;0m                      ^^^^^^^^^^^^^^
[2026-01-18 18:11:01] [INFO] [0;36m(EngineCore_DP0 pid=79336)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 357, in get_response
[2026-01-18 18:11:01] [INFO] [0;36m(EngineCore_DP0 pid=79336)[0;0m     raise RuntimeError(
[2026-01-18 18:11:01] [INFO] [0;36m(EngineCore_DP0 pid=79336)[0;0m RuntimeError: Worker failed with error 'CUDA out of memory. Tried to allocate 4.95 GiB. GPU 0 has a total capacity of 94.97 GiB of which 4.24 GiB is free. Including non-PyTorch memory, this process has 90.70 GiB memory in use. Of the allocated memory 83.97 GiB is allocated by PyTorch, and 5.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)', please check the stack trace above for the root cause
[2026-01-18 18:11:03] [ERROR] [0;36m(APIServer pid=78997)[0;0m Traceback (most recent call last):
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/bin/vllm", line 7, in <module>
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m     sys.exit(main())
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m              ^^^^^^
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m     args.dispatch_function(args)
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m     uvloop.run(run_server(args))
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m     return __asyncio.run(
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m            ^^^^^^^^^^^^^^
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m     return runner.run(main)
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m     return self._loop.run_until_complete(task)
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m     return await main
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m            ^^^^^^^^^^
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m     async with build_async_engine_client(
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m     return await anext(self.gen)
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 146, in build_async_engine_client
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m     return await anext(self.gen)
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 187, in build_async_engine_client_from_engine_args
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 202, in from_vllm_config
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m     return cls(
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m            ^^^^
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 129, in __init__
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m     return AsyncMPClient(*client_args)
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m     super().__init__(
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 144, in __exit__
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m     next(self.gen)
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 918, in launch_core_engines
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m     wait_for_engine_startup(
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 977, in wait_for_engine_startup
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m     raise RuntimeError(
[2026-01-18 18:11:03] [INFO] [0;36m(APIServer pid=78997)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[2026-01-18 18:11:33] [INFO] nvitop监控已启动
[2026-01-18 18:13:07] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 18:13:07] [INFO] nvitop监控已启动
[2026-01-18 18:13:35] [WARNING] WARNING 01-18 18:13:35 [argparse_utils.py:342] Found duplicate keys --tokenizer-mode
[2026-01-18 18:13:35] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:13:35 [api_server.py:872] vLLM API server version 0.14.0rc2.dev117+g9e078d058
[2026-01-18 18:13:35] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:13:35 [utils.py:267] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-235B-A22B-Thinking-2507-AWQ/', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-235B-A22B-Thinking-2507-AWQ/chat_template.jinja', 'model': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-235B-A22B-Thinking-2507-AWQ/', 'trust_remote_code': True, 'max_model_len': 81000, 'enforce_eager': True, 'served_model_name': ['VLLM-MODEL'], 'reasoning_parser': 'deepseek_r1', 'tensor_parallel_size': 2, 'gpu_memory_utilization': 0.92, 'kv_cache_dtype': 'fp8', 'max_num_seqs': 256, 'enable_chunked_prefill': False, 'async_scheduling': True, 'compilation_config': {'level': None, 'mode': None, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': [], 'splitting_ops': None, 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': None, 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.PIECEWISE: 1>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': None, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}}
[2026-01-18 18:13:35] [INFO] [0;36m(APIServer pid=81400)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 18:13:35] [INFO] [0;36m(APIServer pid=81400)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 18:13:35] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:13:35 [model.py:530] Resolved architecture: Qwen3MoeForCausalLM
[2026-01-18 18:13:35] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:13:35 [model.py:1547] Using max model len 81000
[2026-01-18 18:13:35] [WARNING] [0;36m(APIServer pid=81400)[0;0m WARNING 01-18 18:13:35 [quark_ocp_mx.py:157] AITER is not found or QuarkOCP_MX is not supported on the current platform. QuarkOCP_MX quantization will not be available.
[2026-01-18 18:13:35] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:13:35 [awq_marlin.py:163] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2026-01-18 18:13:35] [WARNING] [0;36m(APIServer pid=81400)[0;0m WARNING 01-18 18:13:35 [arg_utils.py:1913] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[2026-01-18 18:13:35] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:13:35 [cache.py:206] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor.
[2026-01-18 18:13:36] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:13:36 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 18:13:36] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:13:36 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-18 18:13:36] [WARNING] [0;36m(APIServer pid=81400)[0;0m WARNING 01-18 18:13:36 [vllm.py:653] Enforce eager set, overriding optimization level to -O0
[2026-01-18 18:13:36] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:13:36 [vllm.py:705] Cudagraph mode PIECEWISE is not compatible with compilation mode 0.Overriding to NONE.
[2026-01-18 18:13:36] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:13:36 [vllm.py:753] Cudagraph is disabled under eager mode
[2026-01-18 18:14:05] [INFO] [0;36m(EngineCore_DP0 pid=81755)[0;0m INFO 01-18 18:14:05 [core.py:96] Initializing a V1 LLM engine (v0.14.0rc2.dev117+g9e078d058) with config: model='/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-235B-A22B-Thinking-2507-AWQ/', speculative_config=None, tokenizer='/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-235B-A22B-Thinking-2507-AWQ/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=81000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=fp8, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='deepseek_r1', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=True, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [81000], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-18 18:14:05] [WARNING] [0;36m(EngineCore_DP0 pid=81755)[0;0m WARNING 01-18 18:14:05 [multiproc_executor.py:897] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-18 18:14:34] [INFO] INFO 01-18 18:14:34 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:45337 backend=nccl
[2026-01-18 18:14:34] [INFO] INFO 01-18 18:14:34 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:45337 backend=nccl
[2026-01-18 18:14:34] [INFO] INFO 01-18 18:14:34 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-18 18:14:34] [WARNING] WARNING 01-18 18:14:34 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 18:14:34] [WARNING] WARNING 01-18 18:14:34 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 18:14:34] [INFO] INFO 01-18 18:14:34 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[2026-01-18 18:14:34] [INFO] INFO 01-18 18:14:34 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
[2026-01-18 18:14:36] [INFO] [0;36m(Worker_TP0 pid=82073)[0;0m INFO 01-18 18:14:36 [gpu_model_runner.py:3803] Starting to load model /mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-235B-A22B-Thinking-2507-AWQ/...
[2026-01-18 18:14:39] [INFO] [0;36m(Worker_TP0 pid=82073)[0;0m INFO 01-18 18:14:39 [cuda.py:351] Using FLASHINFER attention backend out of potential backends: ('FLASHINFER', 'TRITON_ATTN')
[2026-01-18 18:14:40] [INFO] [0;36m(Worker_TP0 pid=82073)[0;0m
[2026-01-18 18:14:40] [INFO] Loading safetensors checkpoint shards:   0% Completed | 0/25 [00:00<?, ?it/s]
[2026-01-18 18:14:40] [INFO] [0;36m(Worker_TP0 pid=82073)[0;0m
[2026-01-18 18:14:40] [INFO] Loading safetensors checkpoint shards:   4% Completed | 1/25 [00:00<00:19,  1.21it/s]
[2026-01-18 18:14:41] [INFO] [0;36m(Worker_TP0 pid=82073)[0;0m
[2026-01-18 18:14:41] [INFO] Loading safetensors checkpoint shards:   8% Completed | 2/25 [00:01<00:17,  1.28it/s]
[2026-01-18 18:14:42] [INFO] [0;36m(Worker_TP0 pid=82073)[0;0m
[2026-01-18 18:14:42] [INFO] Loading safetensors checkpoint shards:  12% Completed | 3/25 [00:02<00:18,  1.20it/s]
[2026-01-18 18:14:43] [INFO] [0;36m(Worker_TP0 pid=82073)[0;0m
[2026-01-18 18:14:43] [INFO] Loading safetensors checkpoint shards:  16% Completed | 4/25 [00:03<00:18,  1.16it/s]
[2026-01-18 18:14:44] [INFO] [0;36m(Worker_TP0 pid=82073)[0;0m
[2026-01-18 18:14:44] [INFO] Loading safetensors checkpoint shards:  20% Completed | 5/25 [00:04<00:17,  1.15it/s]
[2026-01-18 18:14:45] [INFO] [0;36m(Worker_TP0 pid=82073)[0;0m
[2026-01-18 18:14:45] [INFO] Loading safetensors checkpoint shards:  24% Completed | 6/25 [00:05<00:16,  1.12it/s]
[2026-01-18 18:14:46] [INFO] [0;36m(Worker_TP0 pid=82073)[0;0m
[2026-01-18 18:14:46] [INFO] Loading safetensors checkpoint shards:  28% Completed | 7/25 [00:06<00:16,  1.10it/s]
[2026-01-18 18:14:47] [INFO] [0;36m(Worker_TP0 pid=82073)[0;0m
[2026-01-18 18:14:47] [INFO] Loading safetensors checkpoint shards:  32% Completed | 8/25 [00:07<00:15,  1.07it/s]
[2026-01-18 18:14:48] [INFO] [0;36m(Worker_TP0 pid=82073)[0;0m
[2026-01-18 18:14:48] [INFO] Loading safetensors checkpoint shards:  36% Completed | 9/25 [00:08<00:15,  1.06it/s]
[2026-01-18 18:14:49] [INFO] [0;36m(Worker_TP0 pid=82073)[0;0m
[2026-01-18 18:14:49] [INFO] Loading safetensors checkpoint shards:  40% Completed | 10/25 [00:09<00:14,  1.06it/s]
[2026-01-18 18:14:50] [INFO] [0;36m(Worker_TP0 pid=82073)[0;0m
[2026-01-18 18:14:50] [INFO] Loading safetensors checkpoint shards:  44% Completed | 11/25 [00:09<00:13,  1.07it/s]
[2026-01-18 18:14:50] [INFO] [0;36m(Worker_TP0 pid=82073)[0;0m
[2026-01-18 18:14:50] [INFO] Loading safetensors checkpoint shards:  48% Completed | 12/25 [00:10<00:12,  1.07it/s]
[2026-01-18 18:14:51] [INFO] [0;36m(Worker_TP0 pid=82073)[0;0m
[2026-01-18 18:14:51] [INFO] Loading safetensors checkpoint shards:  52% Completed | 13/25 [00:11<00:11,  1.09it/s]
[2026-01-18 18:14:52] [INFO] [0;36m(Worker_TP0 pid=82073)[0;0m
[2026-01-18 18:14:52] [INFO] Loading safetensors checkpoint shards:  56% Completed | 14/25 [00:12<00:10,  1.09it/s]
[2026-01-18 18:14:53] [INFO] [0;36m(Worker_TP0 pid=82073)[0;0m
[2026-01-18 18:14:53] [INFO] Loading safetensors checkpoint shards:  60% Completed | 15/25 [00:13<00:09,  1.09it/s]
[2026-01-18 18:14:54] [INFO] [0;36m(Worker_TP0 pid=82073)[0;0m
[2026-01-18 18:14:54] [INFO] Loading safetensors checkpoint shards:  64% Completed | 16/25 [00:14<00:08,  1.10it/s]
[2026-01-18 18:14:55] [INFO] [0;36m(Worker_TP0 pid=82073)[0;0m
[2026-01-18 18:14:55] [INFO] Loading safetensors checkpoint shards:  68% Completed | 17/25 [00:15<00:07,  1.10it/s]
[2026-01-18 18:14:56] [INFO] [0;36m(Worker_TP0 pid=82073)[0;0m
[2026-01-18 18:14:56] [INFO] Loading safetensors checkpoint shards:  72% Completed | 18/25 [00:16<00:06,  1.11it/s]
[2026-01-18 18:14:57] [INFO] [0;36m(Worker_TP0 pid=82073)[0;0m
[2026-01-18 18:14:57] [INFO] Loading safetensors checkpoint shards:  76% Completed | 19/25 [00:17<00:05,  1.11it/s]
[2026-01-18 18:14:58] [INFO] [0;36m(Worker_TP0 pid=82073)[0;0m
[2026-01-18 18:14:58] [INFO] Loading safetensors checkpoint shards:  80% Completed | 20/25 [00:18<00:04,  1.11it/s]
[2026-01-18 18:14:59] [INFO] [0;36m(Worker_TP0 pid=82073)[0;0m
[2026-01-18 18:14:59] [INFO] Loading safetensors checkpoint shards:  84% Completed | 21/25 [00:18<00:03,  1.12it/s]
[2026-01-18 18:14:59] [INFO] [0;36m(Worker_TP0 pid=82073)[0;0m
[2026-01-18 18:14:59] [INFO] Loading safetensors checkpoint shards:  88% Completed | 22/25 [00:19<00:02,  1.12it/s]
[2026-01-18 18:15:00] [INFO] [0;36m(Worker_TP0 pid=82073)[0;0m
[2026-01-18 18:15:00] [INFO] Loading safetensors checkpoint shards:  92% Completed | 23/25 [00:20<00:01,  1.11it/s]
[2026-01-18 18:15:01] [INFO] [0;36m(Worker_TP0 pid=82073)[0;0m
[2026-01-18 18:15:01] [INFO] Loading safetensors checkpoint shards:  96% Completed | 24/25 [00:21<00:00,  1.08it/s]
[2026-01-18 18:15:02] [INFO] [0;36m(Worker_TP0 pid=82073)[0;0m
[2026-01-18 18:15:02] [INFO] Loading safetensors checkpoint shards: 100% Completed | 25/25 [00:22<00:00,  1.18it/s]
[2026-01-18 18:15:02] [INFO] [0;36m(Worker_TP0 pid=82073)[0;0m
[2026-01-18 18:15:02] [INFO] Loading safetensors checkpoint shards: 100% Completed | 25/25 [00:22<00:00,  1.12it/s]
[2026-01-18 18:15:02] [INFO] [0;36m(Worker_TP0 pid=82073)[0;0m
[2026-01-18 18:15:02] [INFO] [0;36m(Worker_TP0 pid=82073)[0;0m INFO 01-18 18:15:02 [default_loader.py:291] Loading weights took 22.49 seconds
[2026-01-18 18:15:12] [INFO] [0;36m(Worker_TP0 pid=82073)[0;0m INFO 01-18 18:15:12 [gpu_model_runner.py:3900] Model loading took 57.94 GiB memory and 35.556709 seconds
[2026-01-18 18:15:26] [INFO] [0;36m(Worker_TP0 pid=82073)[0;0m INFO 01-18 18:15:26 [gpu_worker.py:355] Available KV cache memory: 20.66 GiB
[2026-01-18 18:15:26] [INFO] [0;36m(EngineCore_DP0 pid=81755)[0;0m INFO 01-18 18:15:26 [kv_cache_utils.py:1307] GPU KV cache size: 460,928 tokens
[2026-01-18 18:15:26] [INFO] [0;36m(EngineCore_DP0 pid=81755)[0;0m INFO 01-18 18:15:26 [kv_cache_utils.py:1312] Maximum concurrency for 81,000 tokens per request: 5.69x
[2026-01-18 18:15:26] [INFO] [0;36m(Worker_TP1 pid=82074)[0;0m 2026-01-18 18:15:26,938 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[2026-01-18 18:15:26] [INFO] [0;36m(Worker_TP0 pid=82073)[0;0m 2026-01-18 18:15:26,938 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[2026-01-18 18:15:35] [INFO] [0;36m(Worker_TP0 pid=82073)[0;0m 2026-01-18 18:15:35,157 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[2026-01-18 18:15:35] [INFO] [0;36m(Worker_TP1 pid=82074)[0;0m 2026-01-18 18:15:35,157 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[2026-01-18 18:15:35] [INFO] [0;36m(Worker_TP0 pid=82073)[0;0m INFO 01-18 18:15:35 [kernel_warmup.py:64] Warming up FlashInfer attention.
[2026-01-18 18:15:35] [INFO] [0;36m(Worker_TP1 pid=82074)[0;0m INFO 01-18 18:15:35 [kernel_warmup.py:64] Warming up FlashInfer attention.
[2026-01-18 18:16:02] [INFO] [0;36m(EngineCore_DP0 pid=81755)[0;0m INFO 01-18 18:16:02 [core.py:272] init engine (profile, create kv cache, warmup model) took 49.45 seconds
[2026-01-18 18:16:03] [INFO] [0;36m(EngineCore_DP0 pid=81755)[0;0m INFO 01-18 18:16:03 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 18:16:03] [WARNING] [0;36m(EngineCore_DP0 pid=81755)[0;0m WARNING 01-18 18:16:03 [vllm.py:660] Inductor compilation was disabled by user settings, optimizations settings that are only active during inductor compilation will be ignored.
[2026-01-18 18:16:03] [INFO] [0;36m(EngineCore_DP0 pid=81755)[0;0m INFO 01-18 18:16:03 [vllm.py:753] Cudagraph is disabled under eager mode
[2026-01-18 18:16:03] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:03 [api_server.py:663] Supported tasks: ['generate']
[2026-01-18 18:16:04] [WARNING] [0;36m(APIServer pid=81400)[0;0m WARNING 01-18 18:16:04 [model.py:1360] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [serving.py:185] Warming up chat template processing...
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [serving.py:221] Chat template warmup completed in 36.2ms
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:8005
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [launcher.py:38] Available routes are:
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [launcher.py:46] Route: /docs, Methods: HEAD, GET
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [launcher.py:46] Route: /tokenize, Methods: POST
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [launcher.py:46] Route: /detokenize, Methods: POST
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [launcher.py:46] Route: /pause, Methods: POST
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [launcher.py:46] Route: /resume, Methods: POST
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [launcher.py:46] Route: /is_paused, Methods: GET
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [launcher.py:46] Route: /metrics, Methods: GET
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [launcher.py:46] Route: /health, Methods: GET
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [launcher.py:46] Route: /v1/responses, Methods: POST
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [launcher.py:46] Route: /v1/completions, Methods: POST
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [launcher.py:46] Route: /v1/messages, Methods: POST
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [launcher.py:46] Route: /v1/models, Methods: GET
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [launcher.py:46] Route: /load, Methods: GET
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [launcher.py:46] Route: /version, Methods: GET
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [launcher.py:46] Route: /ping, Methods: GET
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [launcher.py:46] Route: /ping, Methods: POST
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [launcher.py:46] Route: /invocations, Methods: POST
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [launcher.py:46] Route: /classify, Methods: POST
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [launcher.py:46] Route: /score, Methods: POST
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [launcher.py:46] Route: /v1/score, Methods: POST
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [launcher.py:46] Route: /rerank, Methods: POST
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [launcher.py:46] Route: /v1/rerank, Methods: POST
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [launcher.py:46] Route: /v2/rerank, Methods: POST
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:04 [launcher.py:46] Route: /pooling, Methods: POST
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO:     Started server process [81400]
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO:     Waiting for application startup.
[2026-01-18 18:16:04] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO:     Application startup complete.
[2026-01-18 18:16:17] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO:     127.0.0.1:36732 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-18 18:16:24] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:24 [loggers.py:257] Engine 000: Avg prompt throughput: 1.1 tokens/s, Avg generation throughput: 11.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[2026-01-18 18:16:34] [INFO] [0;36m(APIServer pid=81400)[0;0m INFO 01-18 18:16:34 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 11.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[2026-01-18 18:16:35] [INFO] [0;36m(Worker_TP1 pid=82074)[0;0m INFO 01-18 18:16:35 [multiproc_executor.py:724] Parent process exited, terminating worker
[2026-01-18 18:16:35] [INFO] [0;36m(Worker_TP0 pid=82073)[0;0m INFO 01-18 18:16:35 [multiproc_executor.py:724] Parent process exited, terminating worker
[2026-01-18 18:16:35] [ERROR] [0;36m(APIServer pid=81400)[0;0m ERROR 01-18 18:16:35 [core_client.py:605] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.
[2026-01-18 18:16:35] [INFO] [0;36m(Worker_TP1 pid=82074)[0;0m INFO 01-18 18:16:35 [multiproc_executor.py:768] WorkerProc shutting down.
[2026-01-18 18:16:35] [INFO] [0;36m(Worker_TP0 pid=82073)[0;0m INFO 01-18 18:16:35 [multiproc_executor.py:768] WorkerProc shutting down.
[2026-01-18 18:16:35] [INFO] [0;36m(Worker_TP1 pid=82074)[0;0m /mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/resource_tracker.py:147: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.
[2026-01-18 18:16:35] [INFO] [0;36m(Worker_TP1 pid=82074)[0;0m   warnings.warn('resource_tracker: process died unexpectedly, '
[2026-01-18 18:16:35] [INFO] [0;36m(Worker_TP0 pid=82073)[0;0m /mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/resource_tracker.py:147: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.
[2026-01-18 18:16:35] [INFO] [0;36m(Worker_TP0 pid=82073)[0;0m   warnings.warn('resource_tracker: process died unexpectedly, '
[2026-01-18 18:16:35] [ERROR] Traceback (most recent call last):
[2026-01-18 18:16:35] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 18:16:35] [INFO] cache[rtype].remove(name)
[2026-01-18 18:16:35] [INFO] KeyError: '/psm_a9e24a97'
[2026-01-18 18:16:35] [ERROR] Traceback (most recent call last):
[2026-01-18 18:16:35] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 18:16:35] [INFO] cache[rtype].remove(name)
[2026-01-18 18:16:35] [INFO] KeyError: '/psm_32c3dc54'
[2026-01-18 18:16:35] [ERROR] Traceback (most recent call last):
[2026-01-18 18:16:35] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 18:16:35] [INFO] cache[rtype].remove(name)
[2026-01-18 18:16:35] [INFO] KeyError: '/psm_e72b8e7c'
[2026-01-18 18:16:35] [INFO] 服务已停止
[2026-01-18 18:16:35] [ERROR] Traceback (most recent call last):
[2026-01-18 18:16:35] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 18:16:35] [INFO] cache[rtype].remove(name)
[2026-01-18 18:16:35] [INFO] KeyError: '/mp-_46uvw1x'
[2026-01-18 18:16:35] [ERROR] Traceback (most recent call last):
[2026-01-18 18:16:35] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 18:16:35] [INFO] cache[rtype].remove(name)
[2026-01-18 18:16:35] [INFO] KeyError: '/mp-34m4f7hm'
[2026-01-18 18:16:36] [INFO] nvitop监控已启动
[2026-01-18 18:16:37] [INFO] nvitop监控已启动
[2026-01-18 18:16:38] [INFO] nvitop监控已启动
[2026-01-18 18:16:50] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 18:16:50] [INFO] nvitop监控已启动
[2026-01-18 18:17:20] [WARNING] WARNING 01-18 18:17:20 [argparse_utils.py:342] Found duplicate keys --tokenizer-mode
[2026-01-18 18:17:20] [INFO] [0;36m(APIServer pid=84082)[0;0m INFO 01-18 18:17:20 [api_server.py:872] vLLM API server version 0.14.0rc2.dev117+g9e078d058
[2026-01-18 18:17:20] [INFO] [0;36m(APIServer pid=84082)[0;0m INFO 01-18 18:17:20 [utils.py:267] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-235B-A22B-Thinking-2507-AWQ/', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-235B-A22B-Thinking-2507-AWQ/chat_template.jinja', 'model': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-235B-A22B-Thinking-2507-AWQ/', 'trust_remote_code': True, 'max_model_len': 200000, 'enforce_eager': True, 'served_model_name': ['VLLM-MODEL'], 'reasoning_parser': 'deepseek_r1', 'tensor_parallel_size': 2, 'gpu_memory_utilization': 0.92, 'kv_cache_dtype': 'fp8', 'max_num_seqs': 256, 'enable_chunked_prefill': False, 'async_scheduling': True, 'compilation_config': {'level': None, 'mode': None, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': [], 'splitting_ops': None, 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': None, 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.PIECEWISE: 1>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': None, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}}
[2026-01-18 18:17:20] [INFO] [0;36m(APIServer pid=84082)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 18:17:20] [INFO] [0;36m(APIServer pid=84082)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 18:17:20] [INFO] [0;36m(APIServer pid=84082)[0;0m INFO 01-18 18:17:20 [model.py:530] Resolved architecture: Qwen3MoeForCausalLM
[2026-01-18 18:17:20] [INFO] [0;36m(APIServer pid=84082)[0;0m INFO 01-18 18:17:20 [model.py:1547] Using max model len 200000
[2026-01-18 18:17:20] [WARNING] [0;36m(APIServer pid=84082)[0;0m WARNING 01-18 18:17:20 [quark_ocp_mx.py:157] AITER is not found or QuarkOCP_MX is not supported on the current platform. QuarkOCP_MX quantization will not be available.
[2026-01-18 18:17:21] [INFO] [0;36m(APIServer pid=84082)[0;0m INFO 01-18 18:17:21 [awq_marlin.py:163] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2026-01-18 18:17:21] [WARNING] [0;36m(APIServer pid=84082)[0;0m WARNING 01-18 18:17:21 [arg_utils.py:1913] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[2026-01-18 18:17:21] [INFO] [0;36m(APIServer pid=84082)[0;0m INFO 01-18 18:17:21 [cache.py:206] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor.
[2026-01-18 18:17:21] [INFO] [0;36m(APIServer pid=84082)[0;0m INFO 01-18 18:17:21 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 18:17:21] [INFO] [0;36m(APIServer pid=84082)[0;0m INFO 01-18 18:17:21 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-18 18:17:21] [WARNING] [0;36m(APIServer pid=84082)[0;0m WARNING 01-18 18:17:21 [vllm.py:653] Enforce eager set, overriding optimization level to -O0
[2026-01-18 18:17:21] [INFO] [0;36m(APIServer pid=84082)[0;0m INFO 01-18 18:17:21 [vllm.py:705] Cudagraph mode PIECEWISE is not compatible with compilation mode 0.Overriding to NONE.
[2026-01-18 18:17:21] [INFO] [0;36m(APIServer pid=84082)[0;0m INFO 01-18 18:17:21 [vllm.py:753] Cudagraph is disabled under eager mode
[2026-01-18 18:17:47] [INFO] [0;36m(EngineCore_DP0 pid=84550)[0;0m INFO 01-18 18:17:47 [core.py:96] Initializing a V1 LLM engine (v0.14.0rc2.dev117+g9e078d058) with config: model='/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-235B-A22B-Thinking-2507-AWQ/', speculative_config=None, tokenizer='/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-235B-A22B-Thinking-2507-AWQ/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=200000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=fp8, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='deepseek_r1', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=True, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [200000], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-18 18:17:47] [WARNING] [0;36m(EngineCore_DP0 pid=84550)[0;0m WARNING 01-18 18:17:47 [multiproc_executor.py:897] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-18 18:18:16] [INFO] INFO 01-18 18:18:16 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:45651 backend=nccl
[2026-01-18 18:18:17] [INFO] INFO 01-18 18:18:17 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:45651 backend=nccl
[2026-01-18 18:18:17] [INFO] INFO 01-18 18:18:17 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-18 18:18:17] [WARNING] WARNING 01-18 18:18:17 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 18:18:17] [WARNING] WARNING 01-18 18:18:17 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 18:18:18] [INFO] INFO 01-18 18:18:18 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
[2026-01-18 18:18:18] [INFO] INFO 01-18 18:18:18 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[2026-01-18 18:18:20] [INFO] [0;36m(Worker_TP0 pid=84907)[0;0m INFO 01-18 18:18:20 [gpu_model_runner.py:3803] Starting to load model /mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-235B-A22B-Thinking-2507-AWQ/...
[2026-01-18 18:18:24] [INFO] [0;36m(Worker_TP0 pid=84907)[0;0m INFO 01-18 18:18:24 [cuda.py:351] Using FLASHINFER attention backend out of potential backends: ('FLASHINFER', 'TRITON_ATTN')
[2026-01-18 18:18:25] [INFO] [0;36m(Worker_TP0 pid=84907)[0;0m
[2026-01-18 18:18:25] [INFO] Loading safetensors checkpoint shards:   0% Completed | 0/25 [00:00<?, ?it/s]
[2026-01-18 18:18:25] [INFO] [0;36m(Worker_TP0 pid=84907)[0;0m
[2026-01-18 18:18:25] [INFO] Loading safetensors checkpoint shards:   4% Completed | 1/25 [00:00<00:19,  1.21it/s]
[2026-01-18 18:18:26] [INFO] [0;36m(Worker_TP0 pid=84907)[0;0m
[2026-01-18 18:18:26] [INFO] Loading safetensors checkpoint shards:   8% Completed | 2/25 [00:01<00:18,  1.23it/s]
[2026-01-18 18:18:27] [INFO] [0;36m(Worker_TP0 pid=84907)[0;0m
[2026-01-18 18:18:27] [INFO] Loading safetensors checkpoint shards:  12% Completed | 3/25 [00:02<00:18,  1.17it/s]
[2026-01-18 18:18:28] [INFO] [0;36m(Worker_TP0 pid=84907)[0;0m
[2026-01-18 18:18:28] [INFO] Loading safetensors checkpoint shards:  16% Completed | 4/25 [00:03<00:18,  1.14it/s]
[2026-01-18 18:18:29] [INFO] [0;36m(Worker_TP0 pid=84907)[0;0m
[2026-01-18 18:18:29] [INFO] Loading safetensors checkpoint shards:  20% Completed | 5/25 [00:04<00:17,  1.13it/s]
[2026-01-18 18:18:30] [INFO] [0;36m(Worker_TP0 pid=84907)[0;0m
[2026-01-18 18:18:30] [INFO] Loading safetensors checkpoint shards:  24% Completed | 6/25 [00:05<00:17,  1.10it/s]
[2026-01-18 18:18:31] [INFO] [0;36m(Worker_TP0 pid=84907)[0;0m
[2026-01-18 18:18:31] [INFO] Loading safetensors checkpoint shards:  28% Completed | 7/25 [00:06<00:16,  1.07it/s]
[2026-01-18 18:18:32] [INFO] [0;36m(Worker_TP0 pid=84907)[0;0m
[2026-01-18 18:18:32] [INFO] Loading safetensors checkpoint shards:  32% Completed | 8/25 [00:07<00:16,  1.05it/s]
[2026-01-18 18:18:33] [INFO] [0;36m(Worker_TP0 pid=84907)[0;0m
[2026-01-18 18:18:33] [INFO] Loading safetensors checkpoint shards:  36% Completed | 9/25 [00:08<00:15,  1.04it/s]
[2026-01-18 18:18:34] [INFO] [0;36m(Worker_TP0 pid=84907)[0;0m
[2026-01-18 18:18:34] [INFO] Loading safetensors checkpoint shards:  40% Completed | 10/25 [00:09<00:14,  1.04it/s]
[2026-01-18 18:18:35] [INFO] [0;36m(Worker_TP0 pid=84907)[0;0m
[2026-01-18 18:18:35] [INFO] Loading safetensors checkpoint shards:  44% Completed | 11/25 [00:10<00:13,  1.06it/s]
[2026-01-18 18:18:36] [INFO] [0;36m(Worker_TP0 pid=84907)[0;0m
[2026-01-18 18:18:36] [INFO] Loading safetensors checkpoint shards:  48% Completed | 12/25 [00:11<00:12,  1.08it/s]
[2026-01-18 18:18:37] [INFO] [0;36m(Worker_TP0 pid=84907)[0;0m
[2026-01-18 18:18:37] [INFO] Loading safetensors checkpoint shards:  52% Completed | 13/25 [00:11<00:10,  1.09it/s]
[2026-01-18 18:18:37] [INFO] [0;36m(Worker_TP0 pid=84907)[0;0m
[2026-01-18 18:18:37] [INFO] Loading safetensors checkpoint shards:  56% Completed | 14/25 [00:12<00:10,  1.10it/s]
[2026-01-18 18:18:38] [INFO] [0;36m(Worker_TP0 pid=84907)[0;0m
[2026-01-18 18:18:38] [INFO] Loading safetensors checkpoint shards:  60% Completed | 15/25 [00:13<00:09,  1.10it/s]
[2026-01-18 18:18:39] [INFO] [0;36m(Worker_TP0 pid=84907)[0;0m
[2026-01-18 18:18:39] [INFO] Loading safetensors checkpoint shards:  64% Completed | 16/25 [00:14<00:08,  1.10it/s]
[2026-01-18 18:18:40] [INFO] [0;36m(Worker_TP0 pid=84907)[0;0m
[2026-01-18 18:18:40] [INFO] Loading safetensors checkpoint shards:  68% Completed | 17/25 [00:15<00:07,  1.10it/s]
[2026-01-18 18:18:41] [INFO] [0;36m(Worker_TP0 pid=84907)[0;0m
[2026-01-18 18:18:41] [INFO] Loading safetensors checkpoint shards:  72% Completed | 18/25 [00:16<00:06,  1.10it/s]
[2026-01-18 18:18:42] [INFO] [0;36m(Worker_TP0 pid=84907)[0;0m
[2026-01-18 18:18:42] [INFO] Loading safetensors checkpoint shards:  76% Completed | 19/25 [00:17<00:05,  1.10it/s]
[2026-01-18 18:18:43] [INFO] [0;36m(Worker_TP0 pid=84907)[0;0m
[2026-01-18 18:18:43] [INFO] Loading safetensors checkpoint shards:  80% Completed | 20/25 [00:18<00:04,  1.10it/s]
[2026-01-18 18:18:44] [INFO] [0;36m(Worker_TP0 pid=84907)[0;0m
[2026-01-18 18:18:44] [INFO] Loading safetensors checkpoint shards:  84% Completed | 21/25 [00:19<00:03,  1.10it/s]
[2026-01-18 18:18:45] [INFO] [0;36m(Worker_TP0 pid=84907)[0;0m
[2026-01-18 18:18:45] [INFO] Loading safetensors checkpoint shards:  88% Completed | 22/25 [00:20<00:02,  1.11it/s]
[2026-01-18 18:18:46] [INFO] [0;36m(Worker_TP0 pid=84907)[0;0m
[2026-01-18 18:18:46] [INFO] Loading safetensors checkpoint shards:  92% Completed | 23/25 [00:20<00:01,  1.11it/s]
[2026-01-18 18:18:47] [INFO] [0;36m(Worker_TP0 pid=84907)[0;0m
[2026-01-18 18:18:47] [INFO] Loading safetensors checkpoint shards:  96% Completed | 24/25 [00:21<00:00,  1.09it/s]
[2026-01-18 18:18:47] [INFO] [0;36m(Worker_TP0 pid=84907)[0;0m
[2026-01-18 18:18:47] [INFO] Loading safetensors checkpoint shards: 100% Completed | 25/25 [00:22<00:00,  1.19it/s]
[2026-01-18 18:18:47] [INFO] [0;36m(Worker_TP0 pid=84907)[0;0m
[2026-01-18 18:18:47] [INFO] Loading safetensors checkpoint shards: 100% Completed | 25/25 [00:22<00:00,  1.11it/s]
[2026-01-18 18:18:47] [INFO] [0;36m(Worker_TP0 pid=84907)[0;0m
[2026-01-18 18:18:47] [INFO] [0;36m(Worker_TP0 pid=84907)[0;0m INFO 01-18 18:18:47 [default_loader.py:291] Loading weights took 22.68 seconds
[2026-01-18 18:18:57] [INFO] [0;36m(Worker_TP0 pid=84907)[0;0m INFO 01-18 18:18:57 [gpu_model_runner.py:3900] Model loading took 57.92 GiB memory and 36.517805 seconds
[2026-01-18 18:19:25] [INFO] [0;36m(Worker_TP0 pid=84907)[0;0m INFO 01-18 18:19:25 [gpu_worker.py:355] Available KV cache memory: 7.92 GiB
[2026-01-18 18:19:25] [ERROR] [0;36m(EngineCore_DP0 pid=84550)[0;0m ERROR 01-18 18:19:25 [core.py:935] EngineCore failed to start.
[2026-01-18 18:19:25] [ERROR] [0;36m(EngineCore_DP0 pid=84550)[0;0m ERROR 01-18 18:19:25 [core.py:935] Traceback (most recent call last):
[2026-01-18 18:19:25] [ERROR] [0;36m(EngineCore_DP0 pid=84550)[0;0m ERROR 01-18 18:19:25 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-18 18:19:25] [ERROR] [0;36m(EngineCore_DP0 pid=84550)[0;0m ERROR 01-18 18:19:25 [core.py:935]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 18:19:25] [ERROR] [0;36m(EngineCore_DP0 pid=84550)[0;0m ERROR 01-18 18:19:25 [core.py:935]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:19:25] [ERROR] [0;36m(EngineCore_DP0 pid=84550)[0;0m ERROR 01-18 18:19:25 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-18 18:19:25] [ERROR] [0;36m(EngineCore_DP0 pid=84550)[0;0m ERROR 01-18 18:19:25 [core.py:935]     super().__init__(
[2026-01-18 18:19:25] [ERROR] [0;36m(EngineCore_DP0 pid=84550)[0;0m ERROR 01-18 18:19:25 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 112, in __init__
[2026-01-18 18:19:25] [ERROR] [0;36m(EngineCore_DP0 pid=84550)[0;0m ERROR 01-18 18:19:25 [core.py:935]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[2026-01-18 18:19:25] [ERROR] [0;36m(EngineCore_DP0 pid=84550)[0;0m ERROR 01-18 18:19:25 [core.py:935]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:19:25] [ERROR] [0;36m(EngineCore_DP0 pid=84550)[0;0m ERROR 01-18 18:19:25 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 253, in _initialize_kv_caches
[2026-01-18 18:19:25] [ERROR] [0;36m(EngineCore_DP0 pid=84550)[0;0m ERROR 01-18 18:19:25 [core.py:935]     kv_cache_configs = get_kv_cache_configs(
[2026-01-18 18:19:25] [ERROR] [0;36m(EngineCore_DP0 pid=84550)[0;0m ERROR 01-18 18:19:25 [core.py:935]                        ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:19:25] [ERROR] [0;36m(EngineCore_DP0 pid=84550)[0;0m ERROR 01-18 18:19:25 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/core/kv_cache_utils.py", line 1516, in get_kv_cache_configs
[2026-01-18 18:19:25] [ERROR] [0;36m(EngineCore_DP0 pid=84550)[0;0m ERROR 01-18 18:19:25 [core.py:935]     _check_enough_kv_cache_memory(
[2026-01-18 18:19:25] [ERROR] [0;36m(EngineCore_DP0 pid=84550)[0;0m ERROR 01-18 18:19:25 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/core/kv_cache_utils.py", line 634, in _check_enough_kv_cache_memory
[2026-01-18 18:19:25] [ERROR] [0;36m(EngineCore_DP0 pid=84550)[0;0m ERROR 01-18 18:19:25 [core.py:935]     raise ValueError(
[2026-01-18 18:19:25] [ERROR] [0;36m(EngineCore_DP0 pid=84550)[0;0m ERROR 01-18 18:19:25 [core.py:935] ValueError: To serve at least one request with the models's max seq len (200000), (8.96 GiB KV cache is needed, which is larger than the available KV cache memory (7.92 GiB). Based on the available memory, the estimated maximum model length is 176736. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[2026-01-18 18:19:26] [ERROR] [0;36m(EngineCore_DP0 pid=84550)[0;0m ERROR 01-18 18:19:26 [multiproc_executor.py:246] Worker proc VllmWorker-1 died unexpectedly, shutting down executor.
[2026-01-18 18:19:28] [INFO] [0;36m(EngineCore_DP0 pid=84550)[0;0m Process EngineCore_DP0:
[2026-01-18 18:19:28] [ERROR] [0;36m(EngineCore_DP0 pid=84550)[0;0m Traceback (most recent call last):
[2026-01-18 18:19:28] [INFO] [0;36m(EngineCore_DP0 pid=84550)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[2026-01-18 18:19:28] [INFO] [0;36m(EngineCore_DP0 pid=84550)[0;0m     self.run()
[2026-01-18 18:19:28] [INFO] [0;36m(EngineCore_DP0 pid=84550)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/process.py", line 108, in run
[2026-01-18 18:19:28] [INFO] [0;36m(EngineCore_DP0 pid=84550)[0;0m     self._target(*self._args, **self._kwargs)
[2026-01-18 18:19:28] [INFO] [0;36m(EngineCore_DP0 pid=84550)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 939, in run_engine_core
[2026-01-18 18:19:28] [INFO] [0;36m(EngineCore_DP0 pid=84550)[0;0m     raise e
[2026-01-18 18:19:28] [INFO] [0;36m(EngineCore_DP0 pid=84550)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-18 18:19:28] [INFO] [0;36m(EngineCore_DP0 pid=84550)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 18:19:28] [INFO] [0;36m(EngineCore_DP0 pid=84550)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:19:28] [INFO] [0;36m(EngineCore_DP0 pid=84550)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-18 18:19:28] [INFO] [0;36m(EngineCore_DP0 pid=84550)[0;0m     super().__init__(
[2026-01-18 18:19:28] [INFO] [0;36m(EngineCore_DP0 pid=84550)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 112, in __init__
[2026-01-18 18:19:28] [INFO] [0;36m(EngineCore_DP0 pid=84550)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[2026-01-18 18:19:28] [INFO] [0;36m(EngineCore_DP0 pid=84550)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:19:28] [INFO] [0;36m(EngineCore_DP0 pid=84550)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 253, in _initialize_kv_caches
[2026-01-18 18:19:28] [INFO] [0;36m(EngineCore_DP0 pid=84550)[0;0m     kv_cache_configs = get_kv_cache_configs(
[2026-01-18 18:19:28] [INFO] [0;36m(EngineCore_DP0 pid=84550)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:19:28] [INFO] [0;36m(EngineCore_DP0 pid=84550)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/core/kv_cache_utils.py", line 1516, in get_kv_cache_configs
[2026-01-18 18:19:28] [INFO] [0;36m(EngineCore_DP0 pid=84550)[0;0m     _check_enough_kv_cache_memory(
[2026-01-18 18:19:28] [INFO] [0;36m(EngineCore_DP0 pid=84550)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/core/kv_cache_utils.py", line 634, in _check_enough_kv_cache_memory
[2026-01-18 18:19:28] [INFO] [0;36m(EngineCore_DP0 pid=84550)[0;0m     raise ValueError(
[2026-01-18 18:19:28] [INFO] [0;36m(EngineCore_DP0 pid=84550)[0;0m ValueError: To serve at least one request with the models's max seq len (200000), (8.96 GiB KV cache is needed, which is larger than the available KV cache memory (7.92 GiB). Based on the available memory, the estimated maximum model length is 176736. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[2026-01-18 18:19:29] [ERROR] [0;36m(APIServer pid=84082)[0;0m Traceback (most recent call last):
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/bin/vllm", line 7, in <module>
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m     sys.exit(main())
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m              ^^^^^^
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m     args.dispatch_function(args)
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m     uvloop.run(run_server(args))
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m     return __asyncio.run(
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m            ^^^^^^^^^^^^^^
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m     return runner.run(main)
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m     return self._loop.run_until_complete(task)
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m     return await main
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m            ^^^^^^^^^^
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m     async with build_async_engine_client(
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m     return await anext(self.gen)
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 146, in build_async_engine_client
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m     return await anext(self.gen)
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 187, in build_async_engine_client_from_engine_args
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 202, in from_vllm_config
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m     return cls(
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m            ^^^^
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 129, in __init__
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m     return AsyncMPClient(*client_args)
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m     super().__init__(
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 144, in __exit__
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m     next(self.gen)
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 918, in launch_core_engines
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m     wait_for_engine_startup(
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 977, in wait_for_engine_startup
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m     raise RuntimeError(
[2026-01-18 18:19:29] [INFO] [0;36m(APIServer pid=84082)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[2026-01-18 18:19:36] [INFO] nvitop监控已启动
[2026-01-18 18:19:44] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 18:19:44] [INFO] nvitop监控已启动
[2026-01-18 18:20:11] [WARNING] WARNING 01-18 18:20:11 [argparse_utils.py:342] Found duplicate keys --tokenizer-mode
[2026-01-18 18:20:11] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:20:11 [api_server.py:872] vLLM API server version 0.14.0rc2.dev117+g9e078d058
[2026-01-18 18:20:11] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:20:11 [utils.py:267] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-235B-A22B-Thinking-2507-AWQ/', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-235B-A22B-Thinking-2507-AWQ/chat_template.jinja', 'model': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-235B-A22B-Thinking-2507-AWQ/', 'trust_remote_code': True, 'max_model_len': 128000, 'enforce_eager': True, 'served_model_name': ['VLLM-MODEL'], 'reasoning_parser': 'deepseek_r1', 'tensor_parallel_size': 2, 'gpu_memory_utilization': 0.92, 'kv_cache_dtype': 'fp8', 'max_num_seqs': 256, 'enable_chunked_prefill': False, 'async_scheduling': True, 'compilation_config': {'level': None, 'mode': None, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': [], 'splitting_ops': None, 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': None, 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.PIECEWISE: 1>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': None, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}}
[2026-01-18 18:20:11] [INFO] [0;36m(APIServer pid=86443)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 18:20:11] [INFO] [0;36m(APIServer pid=86443)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 18:20:11] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:20:11 [model.py:530] Resolved architecture: Qwen3MoeForCausalLM
[2026-01-18 18:20:11] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:20:11 [model.py:1547] Using max model len 128000
[2026-01-18 18:20:11] [WARNING] [0;36m(APIServer pid=86443)[0;0m WARNING 01-18 18:20:11 [quark_ocp_mx.py:157] AITER is not found or QuarkOCP_MX is not supported on the current platform. QuarkOCP_MX quantization will not be available.
[2026-01-18 18:20:12] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:20:12 [awq_marlin.py:163] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2026-01-18 18:20:12] [WARNING] [0;36m(APIServer pid=86443)[0;0m WARNING 01-18 18:20:12 [arg_utils.py:1913] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
[2026-01-18 18:20:12] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:20:12 [cache.py:206] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor.
[2026-01-18 18:20:12] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:20:12 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 18:20:12] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:20:12 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-18 18:20:12] [WARNING] [0;36m(APIServer pid=86443)[0;0m WARNING 01-18 18:20:12 [vllm.py:653] Enforce eager set, overriding optimization level to -O0
[2026-01-18 18:20:12] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:20:12 [vllm.py:705] Cudagraph mode PIECEWISE is not compatible with compilation mode 0.Overriding to NONE.
[2026-01-18 18:20:12] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:20:12 [vllm.py:753] Cudagraph is disabled under eager mode
[2026-01-18 18:20:40] [INFO] [0;36m(EngineCore_DP0 pid=86943)[0;0m INFO 01-18 18:20:40 [core.py:96] Initializing a V1 LLM engine (v0.14.0rc2.dev117+g9e078d058) with config: model='/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-235B-A22B-Thinking-2507-AWQ/', speculative_config=None, tokenizer='/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-235B-A22B-Thinking-2507-AWQ/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=True, enable_return_routed_experts=False, kv_cache_dtype=fp8, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='deepseek_r1', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=True, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [128000], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-18 18:20:40] [WARNING] [0;36m(EngineCore_DP0 pid=86943)[0;0m WARNING 01-18 18:20:40 [multiproc_executor.py:897] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-18 18:21:08] [INFO] INFO 01-18 18:21:08 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:41691 backend=nccl
[2026-01-18 18:21:08] [INFO] INFO 01-18 18:21:08 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:41691 backend=nccl
[2026-01-18 18:21:09] [INFO] INFO 01-18 18:21:09 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-18 18:21:09] [WARNING] WARNING 01-18 18:21:09 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 18:21:09] [WARNING] WARNING 01-18 18:21:09 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 18:21:09] [INFO] INFO 01-18 18:21:09 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
[2026-01-18 18:21:09] [INFO] INFO 01-18 18:21:09 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[2026-01-18 18:21:11] [INFO] [0;36m(Worker_TP0 pid=87421)[0;0m INFO 01-18 18:21:11 [gpu_model_runner.py:3803] Starting to load model /mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-235B-A22B-Thinking-2507-AWQ/...
[2026-01-18 18:21:15] [INFO] [0;36m(Worker_TP0 pid=87421)[0;0m INFO 01-18 18:21:15 [cuda.py:351] Using FLASHINFER attention backend out of potential backends: ('FLASHINFER', 'TRITON_ATTN')
[2026-01-18 18:21:15] [INFO] [0;36m(Worker_TP0 pid=87421)[0;0m
[2026-01-18 18:21:15] [INFO] Loading safetensors checkpoint shards:   0% Completed | 0/25 [00:00<?, ?it/s]
[2026-01-18 18:21:16] [INFO] [0;36m(Worker_TP0 pid=87421)[0;0m
[2026-01-18 18:21:16] [INFO] Loading safetensors checkpoint shards:   4% Completed | 1/25 [00:00<00:19,  1.24it/s]
[2026-01-18 18:21:17] [INFO] [0;36m(Worker_TP0 pid=87421)[0;0m
[2026-01-18 18:21:17] [INFO] Loading safetensors checkpoint shards:   8% Completed | 2/25 [00:01<00:18,  1.23it/s]
[2026-01-18 18:21:18] [INFO] [0;36m(Worker_TP0 pid=87421)[0;0m
[2026-01-18 18:21:18] [INFO] Loading safetensors checkpoint shards:  12% Completed | 3/25 [00:02<00:19,  1.15it/s]
[2026-01-18 18:21:19] [INFO] [0;36m(Worker_TP0 pid=87421)[0;0m
[2026-01-18 18:21:19] [INFO] Loading safetensors checkpoint shards:  16% Completed | 4/25 [00:03<00:19,  1.10it/s]
[2026-01-18 18:21:20] [INFO] [0;36m(Worker_TP0 pid=87421)[0;0m
[2026-01-18 18:21:20] [INFO] Loading safetensors checkpoint shards:  20% Completed | 5/25 [00:04<00:18,  1.08it/s]
[2026-01-18 18:21:21] [INFO] [0;36m(Worker_TP0 pid=87421)[0;0m
[2026-01-18 18:21:21] [INFO] Loading safetensors checkpoint shards:  24% Completed | 6/25 [00:05<00:18,  1.05it/s]
[2026-01-18 18:21:22] [INFO] [0;36m(Worker_TP0 pid=87421)[0;0m
[2026-01-18 18:21:22] [INFO] Loading safetensors checkpoint shards:  28% Completed | 7/25 [00:06<00:17,  1.04it/s]
[2026-01-18 18:21:23] [INFO] [0;36m(Worker_TP0 pid=87421)[0;0m
[2026-01-18 18:21:23] [INFO] Loading safetensors checkpoint shards:  32% Completed | 8/25 [00:07<00:16,  1.01it/s]
[2026-01-18 18:21:24] [INFO] [0;36m(Worker_TP0 pid=87421)[0;0m
[2026-01-18 18:21:24] [INFO] Loading safetensors checkpoint shards:  36% Completed | 9/25 [00:08<00:16,  1.01s/it]
[2026-01-18 18:21:25] [INFO] [0;36m(Worker_TP0 pid=87421)[0;0m
[2026-01-18 18:21:25] [INFO] Loading safetensors checkpoint shards:  40% Completed | 10/25 [00:09<00:15,  1.00s/it]
[2026-01-18 18:21:26] [INFO] [0;36m(Worker_TP0 pid=87421)[0;0m
[2026-01-18 18:21:26] [INFO] Loading safetensors checkpoint shards:  44% Completed | 11/25 [00:10<00:13,  1.02it/s]
[2026-01-18 18:21:27] [INFO] [0;36m(Worker_TP0 pid=87421)[0;0m
[2026-01-18 18:21:27] [INFO] Loading safetensors checkpoint shards:  48% Completed | 12/25 [00:11<00:12,  1.03it/s]
[2026-01-18 18:21:28] [INFO] [0;36m(Worker_TP0 pid=87421)[0;0m
[2026-01-18 18:21:28] [INFO] Loading safetensors checkpoint shards:  52% Completed | 13/25 [00:12<00:11,  1.04it/s]
[2026-01-18 18:21:29] [INFO] [0;36m(Worker_TP0 pid=87421)[0;0m
[2026-01-18 18:21:29] [INFO] Loading safetensors checkpoint shards:  56% Completed | 14/25 [00:13<00:10,  1.06it/s]
[2026-01-18 18:21:30] [INFO] [0;36m(Worker_TP0 pid=87421)[0;0m
[2026-01-18 18:21:30] [INFO] Loading safetensors checkpoint shards:  60% Completed | 15/25 [00:14<00:09,  1.06it/s]
[2026-01-18 18:21:31] [INFO] [0;36m(Worker_TP0 pid=87421)[0;0m
[2026-01-18 18:21:31] [INFO] Loading safetensors checkpoint shards:  64% Completed | 16/25 [00:15<00:08,  1.07it/s]
[2026-01-18 18:21:31] [INFO] [0;36m(Worker_TP0 pid=87421)[0;0m
[2026-01-18 18:21:31] [INFO] Loading safetensors checkpoint shards:  68% Completed | 17/25 [00:16<00:07,  1.07it/s]
[2026-01-18 18:21:32] [INFO] [0;36m(Worker_TP0 pid=87421)[0;0m
[2026-01-18 18:21:32] [INFO] Loading safetensors checkpoint shards:  72% Completed | 18/25 [00:16<00:06,  1.07it/s]
[2026-01-18 18:21:33] [INFO] [0;36m(Worker_TP0 pid=87421)[0;0m
[2026-01-18 18:21:33] [INFO] Loading safetensors checkpoint shards:  76% Completed | 19/25 [00:17<00:05,  1.07it/s]
[2026-01-18 18:21:34] [INFO] [0;36m(Worker_TP0 pid=87421)[0;0m
[2026-01-18 18:21:34] [INFO] Loading safetensors checkpoint shards:  80% Completed | 20/25 [00:18<00:04,  1.07it/s]
[2026-01-18 18:21:35] [INFO] [0;36m(Worker_TP0 pid=87421)[0;0m
[2026-01-18 18:21:35] [INFO] Loading safetensors checkpoint shards:  84% Completed | 21/25 [00:19<00:03,  1.08it/s]
[2026-01-18 18:21:36] [INFO] [0;36m(Worker_TP0 pid=87421)[0;0m
[2026-01-18 18:21:36] [INFO] Loading safetensors checkpoint shards:  88% Completed | 22/25 [00:20<00:02,  1.08it/s]
[2026-01-18 18:21:37] [INFO] [0;36m(Worker_TP0 pid=87421)[0;0m
[2026-01-18 18:21:37] [INFO] Loading safetensors checkpoint shards:  92% Completed | 23/25 [00:21<00:01,  1.08it/s]
[2026-01-18 18:21:38] [INFO] [0;36m(Worker_TP0 pid=87421)[0;0m
[2026-01-18 18:21:38] [INFO] Loading safetensors checkpoint shards:  96% Completed | 24/25 [00:22<00:00,  1.06it/s]
[2026-01-18 18:21:39] [INFO] [0;36m(Worker_TP0 pid=87421)[0;0m
[2026-01-18 18:21:39] [INFO] Loading safetensors checkpoint shards: 100% Completed | 25/25 [00:23<00:00,  1.15it/s]
[2026-01-18 18:21:39] [INFO] [0;36m(Worker_TP0 pid=87421)[0;0m
[2026-01-18 18:21:39] [INFO] Loading safetensors checkpoint shards: 100% Completed | 25/25 [00:23<00:00,  1.07it/s]
[2026-01-18 18:21:39] [INFO] [0;36m(Worker_TP0 pid=87421)[0;0m
[2026-01-18 18:21:39] [INFO] [0;36m(Worker_TP0 pid=87421)[0;0m INFO 01-18 18:21:39 [default_loader.py:291] Loading weights took 23.42 seconds
[2026-01-18 18:21:49] [INFO] [0;36m(Worker_TP0 pid=87421)[0;0m INFO 01-18 18:21:49 [gpu_model_runner.py:3900] Model loading took 57.93 GiB memory and 37.205803 seconds
[2026-01-18 18:22:07] [INFO] [0;36m(Worker_TP0 pid=87421)[0;0m INFO 01-18 18:22:07 [gpu_worker.py:355] Available KV cache memory: 15.64 GiB
[2026-01-18 18:22:07] [INFO] [0;36m(EngineCore_DP0 pid=86943)[0;0m INFO 01-18 18:22:07 [kv_cache_utils.py:1307] GPU KV cache size: 348,880 tokens
[2026-01-18 18:22:07] [INFO] [0;36m(EngineCore_DP0 pid=86943)[0;0m INFO 01-18 18:22:07 [kv_cache_utils.py:1312] Maximum concurrency for 128,000 tokens per request: 2.73x
[2026-01-18 18:22:07] [INFO] [0;36m(Worker_TP0 pid=87421)[0;0m 2026-01-18 18:22:07,737 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[2026-01-18 18:22:07] [INFO] [0;36m(Worker_TP1 pid=87422)[0;0m 2026-01-18 18:22:07,737 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[2026-01-18 18:22:21] [INFO] [0;36m(Worker_TP1 pid=87422)[0;0m 2026-01-18 18:22:21,260 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[2026-01-18 18:22:21] [INFO] [0;36m(Worker_TP0 pid=87421)[0;0m 2026-01-18 18:22:21,260 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[2026-01-18 18:22:21] [INFO] [0;36m(Worker_TP1 pid=87422)[0;0m INFO 01-18 18:22:21 [kernel_warmup.py:64] Warming up FlashInfer attention.
[2026-01-18 18:22:21] [INFO] [0;36m(Worker_TP0 pid=87421)[0;0m INFO 01-18 18:22:21 [kernel_warmup.py:64] Warming up FlashInfer attention.
[2026-01-18 18:22:22] [INFO] [0;36m(EngineCore_DP0 pid=86943)[0;0m INFO 01-18 18:22:22 [core.py:272] init engine (profile, create kv cache, warmup model) took 33.69 seconds
[2026-01-18 18:22:23] [INFO] [0;36m(EngineCore_DP0 pid=86943)[0;0m INFO 01-18 18:22:23 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 18:22:23] [WARNING] [0;36m(EngineCore_DP0 pid=86943)[0;0m WARNING 01-18 18:22:23 [vllm.py:660] Inductor compilation was disabled by user settings, optimizations settings that are only active during inductor compilation will be ignored.
[2026-01-18 18:22:23] [INFO] [0;36m(EngineCore_DP0 pid=86943)[0;0m INFO 01-18 18:22:23 [vllm.py:753] Cudagraph is disabled under eager mode
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [api_server.py:663] Supported tasks: ['generate']
[2026-01-18 18:22:24] [WARNING] [0;36m(APIServer pid=86443)[0;0m WARNING 01-18 18:22:24 [model.py:1360] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [serving.py:227] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [serving.py:185] Warming up chat template processing...
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [serving.py:221] Chat template warmup completed in 34.7ms
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [serving.py:80] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [serving.py:149] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:8005
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [launcher.py:38] Available routes are:
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [launcher.py:46] Route: /docs, Methods: HEAD, GET
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [launcher.py:46] Route: /tokenize, Methods: POST
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [launcher.py:46] Route: /detokenize, Methods: POST
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [launcher.py:46] Route: /pause, Methods: POST
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [launcher.py:46] Route: /resume, Methods: POST
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [launcher.py:46] Route: /is_paused, Methods: GET
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [launcher.py:46] Route: /metrics, Methods: GET
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [launcher.py:46] Route: /health, Methods: GET
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [launcher.py:46] Route: /v1/responses, Methods: POST
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [launcher.py:46] Route: /v1/completions, Methods: POST
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [launcher.py:46] Route: /v1/messages, Methods: POST
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [launcher.py:46] Route: /v1/models, Methods: GET
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [launcher.py:46] Route: /load, Methods: GET
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [launcher.py:46] Route: /version, Methods: GET
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [launcher.py:46] Route: /ping, Methods: GET
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [launcher.py:46] Route: /ping, Methods: POST
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [launcher.py:46] Route: /invocations, Methods: POST
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [launcher.py:46] Route: /classify, Methods: POST
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [launcher.py:46] Route: /score, Methods: POST
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [launcher.py:46] Route: /v1/score, Methods: POST
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [launcher.py:46] Route: /rerank, Methods: POST
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [launcher.py:46] Route: /v1/rerank, Methods: POST
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [launcher.py:46] Route: /v2/rerank, Methods: POST
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:24 [launcher.py:46] Route: /pooling, Methods: POST
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO:     Started server process [86443]
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO:     Waiting for application startup.
[2026-01-18 18:22:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO:     Application startup complete.
[2026-01-18 18:22:54] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO:     127.0.0.1:34654 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-18 18:22:54] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:22:54 [loggers.py:257] Engine 000: Avg prompt throughput: 1.1 tokens/s, Avg generation throughput: 1.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[2026-01-18 18:23:04] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:23:04 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
[2026-01-18 18:23:14] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:23:14 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
[2026-01-18 18:23:24] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:23:24 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[2026-01-18 18:23:34] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:23:34 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[2026-01-18 18:27:55] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO:     127.0.0.1:46228 - "GET /v1/models HTTP/1.1" 200 OK
[2026-01-18 18:30:51] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO:     127.0.0.1:50952 - "GET /v1/models HTTP/1.1" 200 OK
[2026-01-18 18:30:56] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO:     127.0.0.1:50952 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-18 18:31:05] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:31:05 [loggers.py:257] Engine 000: Avg prompt throughput: 424.4 tokens/s, Avg generation throughput: 15.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[2026-01-18 18:31:15] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:31:15 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 0.0%
[2026-01-18 18:31:25] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:31:25 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[2026-01-18 18:31:35] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:31:35 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 0.0%
[2026-01-18 18:31:45] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:31:45 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[2026-01-18 18:31:55] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:31:55 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 0.0%
[2026-01-18 18:32:05] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:32:05 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[2026-01-18 18:32:15] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:32:15 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 0.0%
[2026-01-18 18:32:19] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO:     127.0.0.1:50952 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-18 18:32:25] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:32:25 [loggers.py:257] Engine 000: Avg prompt throughput: 577.5 tokens/s, Avg generation throughput: 10.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 32.2%
[2026-01-18 18:32:35] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:32:35 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 32.2%
[2026-01-18 18:32:45] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:32:45 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 32.2%
[2026-01-18 18:32:55] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:32:55 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 32.2%
[2026-01-18 18:33:05] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:33:05 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 32.2%
[2026-01-18 18:33:15] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:33:15 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 32.2%
[2026-01-18 18:33:25] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:33:25 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 8.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 32.2%
[2026-01-18 18:33:35] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:33:35 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 32.2%
[2026-01-18 18:33:53] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO:     127.0.0.1:58998 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-18 18:33:55] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:33:55 [loggers.py:257] Engine 000: Avg prompt throughput: 328.3 tokens/s, Avg generation throughput: 1.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 24.8%
[2026-01-18 18:34:05] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:34:05 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 24.8%
[2026-01-18 18:34:15] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:34:15 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 24.8%
[2026-01-18 18:34:25] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:34:25 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 24.8%
[2026-01-18 18:34:35] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:34:35 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 24.8%
[2026-01-18 18:34:45] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:34:45 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 24.8%
[2026-01-18 18:34:55] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:34:55 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 24.8%
[2026-01-18 18:35:05] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:35:05 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 24.8%
[2026-01-18 18:35:15] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:35:15 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 24.8%
[2026-01-18 18:35:18] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO:     127.0.0.1:58998 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-18 18:35:25] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:35:25 [loggers.py:257] Engine 000: Avg prompt throughput: 519.0 tokens/s, Avg generation throughput: 12.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 31.0%
[2026-01-18 18:35:35] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:35:35 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 31.0%
[2026-01-18 18:35:45] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:35:45 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.6%, Prefix cache hit rate: 31.0%
[2026-01-18 18:35:55] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:35:55 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 31.0%
[2026-01-18 18:36:05] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:36:05 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.7%, Prefix cache hit rate: 31.0%
[2026-01-18 18:36:15] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:36:15 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 31.0%
[2026-01-18 18:36:25] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:36:25 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.8%, Prefix cache hit rate: 31.0%
[2026-01-18 18:36:35] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:36:35 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 31.0%
[2026-01-18 18:36:45] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:36:45 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.9%, Prefix cache hit rate: 31.0%
[2026-01-18 18:36:55] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:36:55 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 31.0%
[2026-01-18 18:37:05] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:37:05 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.0%, Prefix cache hit rate: 31.0%
[2026-01-18 18:37:15] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:37:15 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 16.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 31.0%
[2026-01-18 18:37:25] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:37:25 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 31.0%
[2026-01-18 18:40:33] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO:     127.0.0.1:42928 - "GET /v1/models HTTP/1.1" 200 OK
[2026-01-18 18:40:46] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO:     127.0.0.1:33076 - "GET /v1/models HTTP/1.1" 200 OK
[2026-01-18 18:40:46] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO:     127.0.0.1:33076 - "GET /v1/models HTTP/1.1" 200 OK
[2026-01-18 18:40:57] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO:     127.0.0.1:39180 - "GET /v1/models HTTP/1.1" 200 OK
[2026-01-18 18:40:57] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO:     127.0.0.1:39180 - "GET /v1/models HTTP/1.1" 200 OK
[2026-01-18 18:41:07] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO:     127.0.0.1:39194 - "GET /v1/models HTTP/1.1" 200 OK
[2026-01-18 18:41:07] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO:     127.0.0.1:39194 - "GET /v1/models HTTP/1.1" 200 OK
[2026-01-18 18:41:17] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO:     127.0.0.1:52076 - "GET /v1/models HTTP/1.1" 200 OK
[2026-01-18 18:41:17] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO:     127.0.0.1:52076 - "GET /v1/models HTTP/1.1" 200 OK
[2026-01-18 18:47:55] [WARNING] [0;36m(APIServer pid=86443)[0;0m WARNING:  Invalid HTTP request received.
[2026-01-18 18:48:03] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO:     127.0.0.1:54418 - "GET /v1/models HTTP/1.1" 200 OK
[2026-01-18 18:48:03] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO:     127.0.0.1:54418 - "GET /v1/models HTTP/1.1" 200 OK
[2026-01-18 18:48:16] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO:     127.0.0.1:56778 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-18 18:48:25] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:48:25 [loggers.py:257] Engine 000: Avg prompt throughput: 1609.0 tokens/s, Avg generation throughput: 9.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.6%, Prefix cache hit rate: 16.6%
[2026-01-18 18:48:35] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:48:35 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.7%, Prefix cache hit rate: 16.6%
[2026-01-18 18:48:45] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:48:45 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 18.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.7%, Prefix cache hit rate: 16.6%
[2026-01-18 18:48:55] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:48:55 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 18.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.8%, Prefix cache hit rate: 16.6%
[2026-01-18 18:49:05] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:49:05 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 18.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.8%, Prefix cache hit rate: 16.6%
[2026-01-18 18:49:15] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:49:15 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 18.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 16.6%
[2026-01-18 18:49:25] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:49:25 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 16.6%
[2026-01-18 18:50:41] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO:     127.0.0.1:53562 - "GET /v1/models HTTP/1.1" 200 OK
[2026-01-18 18:50:43] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO:     127.0.0.1:53562 - "GET /v1/models HTTP/1.1" 200 OK
[2026-01-18 18:50:44] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO:     127.0.0.1:53562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-18 18:50:55] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:50:55 [loggers.py:257] Engine 000: Avg prompt throughput: 1710.2 tokens/s, Avg generation throughput: 16.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.9%, Prefix cache hit rate: 31.4%
[2026-01-18 18:51:05] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:51:05 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.0%, Prefix cache hit rate: 31.4%
[2026-01-18 18:51:15] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:51:15 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.0%, Prefix cache hit rate: 31.4%
[2026-01-18 18:51:25] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:51:25 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.1%, Prefix cache hit rate: 31.4%
[2026-01-18 18:51:35] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:51:35 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.2%, Prefix cache hit rate: 31.4%
[2026-01-18 18:51:45] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:51:45 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 31.4%
[2026-01-18 18:51:55] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 18:51:55 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 31.4%
[2026-01-18 19:09:52] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO:     127.0.0.1:42280 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-18 19:09:55] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 19:09:55 [loggers.py:257] Engine 000: Avg prompt throughput: 1814.0 tokens/s, Avg generation throughput: 3.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.2%, Prefix cache hit rate: 42.7%
[2026-01-18 19:10:05] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 19:10:05 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.3%, Prefix cache hit rate: 42.7%
[2026-01-18 19:10:15] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 19:10:15 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.3%, Prefix cache hit rate: 42.7%
[2026-01-18 19:10:25] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 19:10:25 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.4%, Prefix cache hit rate: 42.7%
[2026-01-18 19:10:35] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 19:10:35 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 18.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.4%, Prefix cache hit rate: 42.7%
[2026-01-18 19:10:45] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 19:10:45 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 18.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.5%, Prefix cache hit rate: 42.7%
[2026-01-18 19:10:55] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 19:10:55 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 42.7%
[2026-01-18 19:11:05] [INFO] [0;36m(APIServer pid=86443)[0;0m INFO 01-18 19:11:05 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 42.7%
[2026-01-18 19:21:02] [INFO] [0;36m(Worker_TP1 pid=87422)[0;0m [0;36m(APIServer pid=86443)[0;0m [0;36m(Worker_TP0 pid=87421)[0;0m INFO 01-18 19:21:02 [multiproc_executor.py:724] Parent process exited, terminating worker
[2026-01-18 19:21:02] [INFO] INFO 01-18 19:21:02 [multiproc_executor.py:724] Parent process exited, terminating worker
[2026-01-18 19:21:02] [ERROR] ERROR 01-18 19:21:02 [core_client.py:605] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.
[2026-01-18 19:21:02] [INFO] [0;36m(Worker_TP0 pid=87421)[0;0m INFO 01-18 19:21:02 [multiproc_executor.py:768] WorkerProc shutting down.
[2026-01-18 19:21:02] [INFO] [0;36m(Worker_TP1 pid=87422)[0;0m INFO 01-18 19:21:02 [multiproc_executor.py:768] WorkerProc shutting down.
[2026-01-18 19:21:02] [INFO] [0;36m(Worker_TP1 pid=87422)[0;0m /mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/resource_tracker.py:147: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.
[2026-01-18 19:21:02] [INFO] [0;36m(Worker_TP1 pid=87422)[0;0m   warnings.warn('resource_tracker: process died unexpectedly, '
[2026-01-18 19:21:02] [INFO] [0;36m(Worker_TP0 pid=87421)[0;0m /mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/resource_tracker.py:147: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.
[2026-01-18 19:21:02] [INFO] [0;36m(Worker_TP0 pid=87421)[0;0m   warnings.warn('resource_tracker: process died unexpectedly, '
[2026-01-18 19:21:02] [ERROR] Traceback (most recent call last):
[2026-01-18 19:21:02] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 19:21:02] [ERROR] Traceback (most recent call last):
[2026-01-18 19:21:02] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 19:21:02] [INFO] cache[rtype].remove(name)
[2026-01-18 19:21:02] [INFO] cache[rtype].remove(name)
[2026-01-18 19:21:02] [INFO] KeyError: '/psm_7ce5dcc4'
[2026-01-18 19:21:02] [INFO] KeyError: '/psm_fc066e15'
[2026-01-18 19:21:03] [INFO] 服务已停止
[2026-01-18 19:21:03] [ERROR] Traceback (most recent call last):
[2026-01-18 19:21:03] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 19:21:03] [INFO] cache[rtype].remove(name)
[2026-01-18 19:21:03] [INFO] KeyError: '/psm_c28f2651'
[2026-01-18 19:21:03] [ERROR] Traceback (most recent call last):
[2026-01-18 19:21:03] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 19:21:03] [ERROR] Traceback (most recent call last):
[2026-01-18 19:21:03] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 19:21:03] [INFO] cache[rtype].remove(name)
[2026-01-18 19:21:03] [INFO] KeyError: '/mp-zu3r_fgb'
[2026-01-18 19:21:03] [INFO] cache[rtype].remove(name)
[2026-01-18 19:21:03] [INFO] KeyError: '/mp-7mu67ptq'
[2026-01-18 19:21:04] [INFO] nvitop监控已启动
[2026-01-18 19:21:05] [INFO] nvitop监控已启动
[2026-01-18 19:21:06] [INFO] nvitop监控已启动
[2026-01-18 19:21:12] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 19:21:12] [INFO] nvitop监控已启动
[2026-01-18 19:21:39] [INFO] [0;36m(APIServer pid=117286)[0;0m INFO 01-18 19:21:39 [api_server.py:1278] vLLM API server version 0.14.0rc1.dev492+g19504ac07
[2026-01-18 19:21:39] [INFO] [0;36m(APIServer pid=117286)[0;0m INFO 01-18 19:21:39 [utils.py:254] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.7-REAP-40-W4A16', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.7-REAP-40-W4A16/chat_template.jinja', 'enable_auto_tool_choice': True, 'tool_call_parser': 'glm47', 'model': '/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.7-REAP-40-W4A16', 'trust_remote_code': True, 'max_model_len': 128000, 'served_model_name': ['VLLM-MODEL'], 'reasoning_parser': 'glm45', 'tensor_parallel_size': 2, 'kv_cache_dtype': 'fp8', 'max_num_seqs': 256, 'async_scheduling': True}
[2026-01-18 19:21:39] [INFO] [0;36m(APIServer pid=117286)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 19:21:39] [INFO] [0;36m(APIServer pid=117286)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m INFO 01-18 19:21:59 [model.py:528] Resolved architecture: Glm4MoeForCausalLM
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m INFO 01-18 19:21:59 [model.py:1543] Using max model len 128000
[2026-01-18 19:21:59] [ERROR] [0;36m(APIServer pid=117286)[0;0m Traceback (most recent call last):
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/bin/vllm", line 7, in <module>
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m     sys.exit(main())
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m              ^^^^^^
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m     args.dispatch_function(args)
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m     uvloop.run(run_server(args))
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m     return __asyncio.run(
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m            ^^^^^^^^^^^^^^
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m     return runner.run(main)
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m     return self._loop.run_until_complete(task)
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m     return await main
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m            ^^^^^^^^^^
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 1325, in run_server
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 1344, in run_server_worker
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m     async with build_async_engine_client(
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m     return await anext(self.gen)
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 173, in build_async_engine_client
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m     return await anext(self.gen)
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 199, in build_async_engine_client_from_engine_args
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/engine/arg_utils.py", line 1365, in create_engine_config
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m     model_config = self.create_model_config()
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/engine/arg_utils.py", line 1220, in create_model_config
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m     return ModelConfig(
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m            ^^^^^^^^^^^^
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/pydantic/_internal/_dataclasses.py", line 121, in __init__
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m   Value error, ('The quantization method %s is deprecated and will be removed in future versions of vLLM. To bypass, set `--allow-deprecated-quantization`.', 'auto-round') [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]
[2026-01-18 19:21:59] [INFO] [0;36m(APIServer pid=117286)[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error
[2026-01-18 19:22:05] [INFO] nvitop监控已启动
[2026-01-18 19:23:04] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 19:23:04] [INFO] nvitop监控已启动
[2026-01-18 19:23:28] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:23:28 [api_server.py:1278] vLLM API server version 0.14.0rc1.dev492+g19504ac07
[2026-01-18 19:23:28] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:23:28 [utils.py:254] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.7-REAP-40-W4A16', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.7-REAP-40-W4A16/chat_template.jinja', 'enable_auto_tool_choice': True, 'tool_call_parser': 'glm47', 'model': '/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.7-REAP-40-W4A16', 'trust_remote_code': True, 'max_model_len': 128000, 'allow_deprecated_quantization': True, 'served_model_name': ['VLLM-MODEL'], 'reasoning_parser': 'glm45', 'tensor_parallel_size': 2, 'kv_cache_dtype': 'fp8', 'max_num_seqs': 256, 'async_scheduling': True}
[2026-01-18 19:23:28] [INFO] [0;36m(APIServer pid=119166)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 19:23:28] [INFO] [0;36m(APIServer pid=119166)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 19:23:28] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:23:28 [model.py:528] Resolved architecture: Glm4MoeForCausalLM
[2026-01-18 19:23:28] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:23:28 [model.py:1543] Using max model len 128000
[2026-01-18 19:23:29] [WARNING] [0;36m(APIServer pid=119166)[0;0m WARNING 01-18 19:23:29 [model.py:950] The quantization method auto-round is deprecated and will be removed in future versions of vLLM.
[2026-01-18 19:23:29] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:23:29 [cache.py:206] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor.
[2026-01-18 19:23:29] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:23:29 [scheduler.py:231] Chunked prefill is enabled with max_num_batched_tokens=8192.
[2026-01-18 19:23:29] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:23:29 [vllm.py:640] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-18 19:23:29] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:23:29 [vllm.py:645] Asynchronous scheduling is enabled.
[2026-01-18 19:23:30] [INFO] [0;36m(APIServer pid=119166)[0;0m The tokenizer you are loading from '/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.7-REAP-40-W4A16' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[2026-01-18 19:23:53] [INFO] [0;36m(EngineCore_DP0 pid=119659)[0;0m INFO 01-18 19:23:53 [core.py:97] Initializing a V1 LLM engine (v0.14.0rc1.dev492+g19504ac07) with config: model='/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.7-REAP-40-W4A16', speculative_config=None, tokenizer='/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.7-REAP-40-W4A16', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=auto-round, enforce_eager=False, kv_cache_dtype=fp8, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='glm45', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-18 19:23:53] [WARNING] [0;36m(EngineCore_DP0 pid=119659)[0;0m WARNING 01-18 19:23:53 [multiproc_executor.py:880] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-18 19:24:17] [INFO] INFO 01-18 19:24:17 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:52633 backend=nccl
[2026-01-18 19:24:17] [INFO] INFO 01-18 19:24:17 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:52633 backend=nccl
[2026-01-18 19:24:18] [INFO] INFO 01-18 19:24:18 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-18 19:24:18] [WARNING] WARNING 01-18 19:24:18 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 19:24:18] [WARNING] WARNING 01-18 19:24:18 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 19:24:18] [INFO] INFO 01-18 19:24:18 [parallel_state.py:1425] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[2026-01-18 19:24:18] [INFO] INFO 01-18 19:24:18 [parallel_state.py:1425] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
[2026-01-18 19:24:19] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m INFO 01-18 19:24:19 [gpu_model_runner.py:3789] Starting to load model /mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.7-REAP-40-W4A16...
[2026-01-18 19:24:19] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m INFO 01-18 19:24:19 [gptq_marlin.py:377] Using MarlinLinearKernel for GPTQMarlinLinearMethod
[2026-01-18 19:24:19] [INFO] [0;36m(Worker_TP1 pid=120125)[0;0m INFO 01-18 19:24:19 [gptq_marlin.py:377] Using MarlinLinearKernel for GPTQMarlinLinearMethod
[2026-01-18 19:24:23] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m INFO 01-18 19:24:23 [cuda.py:351] Using FLASHINFER attention backend out of potential backends: ('FLASHINFER', 'TRITON_ATTN')
[2026-01-18 19:24:24] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m
[2026-01-18 19:24:24] [INFO] Loading safetensors checkpoint shards:   0% Completed | 0/24 [00:00<?, ?it/s]
[2026-01-18 19:24:29] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m
[2026-01-18 19:24:29] [INFO] Loading safetensors checkpoint shards:   4% Completed | 1/24 [00:04<01:52,  4.89s/it]
[2026-01-18 19:24:34] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m
[2026-01-18 19:24:34] [INFO] Loading safetensors checkpoint shards:   8% Completed | 2/24 [00:10<01:51,  5.07s/it]
[2026-01-18 19:24:38] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m
[2026-01-18 19:24:38] [INFO] Loading safetensors checkpoint shards:  12% Completed | 3/24 [00:14<01:40,  4.78s/it]
[2026-01-18 19:24:42] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m
[2026-01-18 19:24:42] [INFO] Loading safetensors checkpoint shards:  17% Completed | 4/24 [00:18<01:31,  4.56s/it]
[2026-01-18 19:24:47] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m
[2026-01-18 19:24:47] [INFO] Loading safetensors checkpoint shards:  21% Completed | 5/24 [00:23<01:29,  4.71s/it]
[2026-01-18 19:24:53] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m
[2026-01-18 19:24:53] [INFO] Loading safetensors checkpoint shards:  25% Completed | 6/24 [00:29<01:29,  4.97s/it]
[2026-01-18 19:24:59] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m
[2026-01-18 19:24:59] [INFO] Loading safetensors checkpoint shards:  29% Completed | 7/24 [00:35<01:32,  5.45s/it]
[2026-01-18 19:25:04] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m
[2026-01-18 19:25:04] [INFO] Loading safetensors checkpoint shards:  33% Completed | 8/24 [00:39<01:21,  5.08s/it]
[2026-01-18 19:25:08] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m
[2026-01-18 19:25:08] [INFO] Loading safetensors checkpoint shards:  38% Completed | 9/24 [00:44<01:12,  4.84s/it]
[2026-01-18 19:25:13] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m
[2026-01-18 19:25:13] [INFO] Loading safetensors checkpoint shards:  42% Completed | 10/24 [00:48<01:06,  4.78s/it]
[2026-01-18 19:25:18] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m
[2026-01-18 19:25:18] [INFO] Loading safetensors checkpoint shards:  46% Completed | 11/24 [00:54<01:03,  4.91s/it]
[2026-01-18 19:25:24] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m
[2026-01-18 19:25:24] [INFO] Loading safetensors checkpoint shards:  50% Completed | 12/24 [01:00<01:04,  5.36s/it]
[2026-01-18 19:25:30] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m
[2026-01-18 19:25:30] [INFO] Loading safetensors checkpoint shards:  54% Completed | 13/24 [01:05<00:59,  5.40s/it]
[2026-01-18 19:25:35] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m
[2026-01-18 19:25:35] [INFO] Loading safetensors checkpoint shards:  58% Completed | 14/24 [01:11<00:54,  5.43s/it]
[2026-01-18 19:25:40] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m
[2026-01-18 19:25:40] [INFO] Loading safetensors checkpoint shards:  62% Completed | 15/24 [01:16<00:47,  5.30s/it]
[2026-01-18 19:25:45] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m
[2026-01-18 19:25:45] [INFO] Loading safetensors checkpoint shards:  67% Completed | 16/24 [01:21<00:41,  5.23s/it]
[2026-01-18 19:25:50] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m
[2026-01-18 19:25:50] [INFO] Loading safetensors checkpoint shards:  71% Completed | 17/24 [01:26<00:36,  5.20s/it]
[2026-01-18 19:25:56] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m
[2026-01-18 19:25:56] [INFO] Loading safetensors checkpoint shards:  75% Completed | 18/24 [01:31<00:31,  5.19s/it]
[2026-01-18 19:26:01] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m
[2026-01-18 19:26:01] [INFO] Loading safetensors checkpoint shards:  79% Completed | 19/24 [01:37<00:26,  5.28s/it]
[2026-01-18 19:26:06] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m
[2026-01-18 19:26:06] [INFO] Loading safetensors checkpoint shards:  83% Completed | 20/24 [01:42<00:20,  5.22s/it]
[2026-01-18 19:26:12] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m
[2026-01-18 19:26:12] [INFO] Loading safetensors checkpoint shards:  88% Completed | 21/24 [01:47<00:15,  5.28s/it]
[2026-01-18 19:26:17] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m
[2026-01-18 19:26:17] [INFO] Loading safetensors checkpoint shards:  92% Completed | 22/24 [01:53<00:10,  5.33s/it]
[2026-01-18 19:26:22] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m
[2026-01-18 19:26:22] [INFO] Loading safetensors checkpoint shards:  96% Completed | 23/24 [01:58<00:05,  5.27s/it]
[2026-01-18 19:26:24] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m
[2026-01-18 19:26:24] [INFO] Loading safetensors checkpoint shards: 100% Completed | 24/24 [01:59<00:00,  4.16s/it]
[2026-01-18 19:26:24] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m
[2026-01-18 19:26:24] [INFO] Loading safetensors checkpoint shards: 100% Completed | 24/24 [01:59<00:00,  5.00s/it]
[2026-01-18 19:26:24] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m
[2026-01-18 19:26:24] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m INFO 01-18 19:26:24 [default_loader.py:291] Loading weights took 119.99 seconds
[2026-01-18 19:26:25] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m INFO 01-18 19:26:25 [gpu_model_runner.py:3886] Model loading took 54.07 GiB memory and 125.473790 seconds
[2026-01-18 19:26:46] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m INFO 01-18 19:26:46 [backends.py:644] Using cache directory: /home/wuwen/.cache/vllm/torch_compile_cache/9962bf4278/rank_0_0/backbone for vLLM's torch.compile
[2026-01-18 19:26:46] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m INFO 01-18 19:26:46 [backends.py:704] Dynamo bytecode transform time: 20.04 s
[2026-01-18 19:27:15] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m INFO 01-18 19:27:15 [backends.py:261] Cache the graph of compile range (1, 8192) for later use
[2026-01-18 19:27:15] [INFO] [0;36m(Worker_TP1 pid=120125)[0;0m INFO 01-18 19:27:15 [backends.py:261] Cache the graph of compile range (1, 8192) for later use
[2026-01-18 19:27:25] [INFO] [0;36m(EngineCore_DP0 pid=119659)[0;0m INFO 01-18 19:27:25 [shm_broadcast.py:542] No available shared memory broadcast block found in 60 seconds. This typically happens when some processes are hanging or doing some time-consuming work (e.g. compilation, weight/kv cache quantization).
[2026-01-18 19:27:40] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m /mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:312: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
[2026-01-18 19:27:40] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m   warnings.warn(
[2026-01-18 19:27:40] [INFO] [0;36m(Worker_TP1 pid=120125)[0;0m /mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:312: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
[2026-01-18 19:27:40] [INFO] [0;36m(Worker_TP1 pid=120125)[0;0m   warnings.warn(
[2026-01-18 19:27:42] [INFO] [0;36m(Worker_TP1 pid=120125)[0;0m INFO 01-18 19:27:42 [marlin_utils.py:458] Marlin kernel can achieve better performance for small size_n with experimental use_atomic_add feature. You can consider set environment variable VLLM_MARLIN_USE_ATOMIC_ADD to 1 if possible.
[2026-01-18 19:27:42] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m INFO 01-18 19:27:42 [marlin_utils.py:458] Marlin kernel can achieve better performance for small size_n with experimental use_atomic_add feature. You can consider set environment variable VLLM_MARLIN_USE_ATOMIC_ADD to 1 if possible.
[2026-01-18 19:28:25] [INFO] [0;36m(EngineCore_DP0 pid=119659)[0;0m INFO 01-18 19:28:25 [shm_broadcast.py:542] No available shared memory broadcast block found in 60 seconds. This typically happens when some processes are hanging or doing some time-consuming work (e.g. compilation, weight/kv cache quantization).
[2026-01-18 19:29:25] [INFO] [0;36m(EngineCore_DP0 pid=119659)[0;0m INFO 01-18 19:29:25 [shm_broadcast.py:542] No available shared memory broadcast block found in 60 seconds. This typically happens when some processes are hanging or doing some time-consuming work (e.g. compilation, weight/kv cache quantization).
[2026-01-18 19:30:22] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m INFO 01-18 19:30:22 [backends.py:278] Compiling a graph for compile range (1, 8192) takes 194.89 s
[2026-01-18 19:30:22] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m INFO 01-18 19:30:22 [monitor.py:34] torch.compile takes 214.93 s in total
[2026-01-18 19:30:24] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m INFO 01-18 19:30:24 [gpu_worker.py:358] Available KV cache memory: 29.85 GiB
[2026-01-18 19:30:25] [INFO] [0;36m(EngineCore_DP0 pid=119659)[0;0m INFO 01-18 19:30:25 [kv_cache_utils.py:1305] GPU KV cache size: 332,656 tokens
[2026-01-18 19:30:25] [INFO] [0;36m(EngineCore_DP0 pid=119659)[0;0m INFO 01-18 19:30:25 [kv_cache_utils.py:1310] Maximum concurrency for 128,000 tokens per request: 2.60x
[2026-01-18 19:30:25] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m 2026-01-18 19:30:25,129 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[2026-01-18 19:30:25] [INFO] [0;36m(Worker_TP1 pid=120125)[0;0m 2026-01-18 19:30:25,130 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[2026-01-18 19:30:26] [INFO] [0;36m(Worker_TP1 pid=120125)[0;0m 2026-01-18 19:30:26,132 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[2026-01-18 19:30:26] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m 2026-01-18 19:30:26,132 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[2026-01-18 19:30:26] [INFO] [0;36m(Worker_TP1 pid=120125)[0;0m INFO 01-18 19:30:26 [kernel_warmup.py:64] Warming up FlashInfer attention.
[2026-01-18 19:30:26] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m INFO 01-18 19:30:26 [kernel_warmup.py:64] Warming up FlashInfer attention.
[2026-01-18 19:30:27] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m
[2026-01-18 19:30:27] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
[2026-01-18 19:30:27] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:10,  4.78it/s]
[2026-01-18 19:30:28] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:12,  4.04it/s]
[2026-01-18 19:30:28] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:10,  4.54it/s]
[2026-01-18 19:30:28] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:00<00:09,  4.77it/s]
[2026-01-18 19:30:28] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:01<00:09,  4.95it/s]
[2026-01-18 19:30:28] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:01<00:08,  5.02it/s]
[2026-01-18 19:30:29] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▎        | 7/51 [00:01<00:08,  5.10it/s]
[2026-01-18 19:30:29] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:01<00:08,  5.12it/s]
[2026-01-18 19:30:29] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:01<00:08,  5.20it/s]
[2026-01-18 19:30:29] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|█▉        | 10/51 [00:02<00:07,  5.23it/s]
[2026-01-18 19:30:29] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:02<00:07,  5.23it/s]
[2026-01-18 19:30:30] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:02<00:07,  5.22it/s]
[2026-01-18 19:30:30] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:02<00:07,  5.27it/s]
[2026-01-18 19:30:30] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:02<00:07,  5.22it/s]
[2026-01-18 19:30:30] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:02<00:06,  5.22it/s]
[2026-01-18 19:30:30] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 16/51 [00:03<00:06,  5.16it/s]
[2026-01-18 19:30:31] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:03<00:06,  5.06it/s]
[2026-01-18 19:30:31] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:03<00:06,  4.97it/s]
[2026-01-18 19:30:31] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 19/51 [00:03<00:06,  4.88it/s]
[2026-01-18 19:30:31] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:04<00:06,  4.79it/s]
[2026-01-18 19:30:31] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:04<00:06,  4.78it/s]
[2026-01-18 19:30:32] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 22/51 [00:04<00:06,  4.66it/s]
[2026-01-18 19:30:32] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:04<00:06,  4.57it/s]
[2026-01-18 19:30:32] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:04<00:05,  4.56it/s]
[2026-01-18 19:30:32] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 25/51 [00:05<00:05,  4.63it/s]
[2026-01-18 19:30:33] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:05<00:05,  4.59it/s]
[2026-01-18 19:30:33] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:05<00:05,  4.63it/s]
[2026-01-18 19:30:33] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:05<00:04,  4.64it/s]
[2026-01-18 19:30:33] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:05<00:04,  4.61it/s]
[2026-01-18 19:30:33] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:06<00:04,  4.65it/s]
[2026-01-18 19:30:34] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 31/51 [00:06<00:04,  4.66it/s]
[2026-01-18 19:30:34] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:06<00:04,  4.65it/s]
[2026-01-18 19:30:34] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:06<00:03,  4.70it/s]
[2026-01-18 19:30:34] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:07<00:03,  4.68it/s]
[2026-01-18 19:30:34] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:07<00:03,  4.68it/s]
[2026-01-18 19:30:35] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:07<00:03,  4.68it/s]
[2026-01-18 19:30:35] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 37/51 [00:07<00:03,  4.60it/s]
[2026-01-18 19:30:35] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:07<00:02,  4.62it/s]
[2026-01-18 19:30:35] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▋  | 39/51 [00:08<00:02,  4.53it/s]
[2026-01-18 19:30:36] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:08<00:02,  4.55it/s]
[2026-01-18 19:30:36] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:08<00:02,  4.56it/s]
[2026-01-18 19:30:36] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:08<00:01,  4.54it/s]
[2026-01-18 19:30:36] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 43/51 [00:09<00:01,  4.58it/s]
[2026-01-18 19:30:36] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:09<00:01,  4.50it/s]
[2026-01-18 19:30:37] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 45/51 [00:09<00:01,  4.56it/s]
[2026-01-18 19:30:37] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|█████████ | 46/51 [00:09<00:01,  4.57it/s]
[2026-01-18 19:30:37] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:09<00:00,  4.56it/s]
[2026-01-18 19:30:37] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 48/51 [00:10<00:00,  4.58it/s]
[2026-01-18 19:30:38] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▌| 49/51 [00:10<00:00,  4.55it/s]
[2026-01-18 19:30:38] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:10<00:00,  4.57it/s]
[2026-01-18 19:30:38] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:11<00:00,  3.12it/s]
[2026-01-18 19:30:38] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:11<00:00,  4.59it/s]
[2026-01-18 19:30:38] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m
[2026-01-18 19:30:38] [INFO] Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
[2026-01-18 19:30:38] [INFO] Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:07,  4.81it/s]
[2026-01-18 19:30:39] [INFO] Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:00<00:06,  4.81it/s]
[2026-01-18 19:30:39] [INFO] Capturing CUDA graphs (decode, FULL):   9%|▊         | 3/35 [00:00<00:07,  4.39it/s]
[2026-01-18 19:30:39] [INFO] Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:06,  4.48it/s]
[2026-01-18 19:30:39] [INFO] Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:01<00:06,  4.57it/s]
[2026-01-18 19:30:40] [INFO] Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:01<00:06,  4.62it/s]
[2026-01-18 19:30:40] [INFO] Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:01<00:06,  4.66it/s]
[2026-01-18 19:30:40] [INFO] Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:01<00:05,  4.64it/s]
[2026-01-18 19:30:40] [INFO] Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:01<00:05,  4.71it/s]
[2026-01-18 19:30:40] [INFO] Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:02<00:05,  4.75it/s]
[2026-01-18 19:30:41] [INFO] Capturing CUDA graphs (decode, FULL):  31%|███▏      | 11/35 [00:02<00:05,  4.76it/s]
[2026-01-18 19:30:41] [INFO] Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:02<00:04,  4.74it/s]
[2026-01-18 19:30:41] [INFO] Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:02<00:04,  4.75it/s]
[2026-01-18 19:30:41] [INFO] Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:02<00:04,  4.77it/s]
[2026-01-18 19:30:41] [INFO] Capturing CUDA graphs (decode, FULL):  43%|████▎     | 15/35 [00:03<00:04,  4.78it/s]
[2026-01-18 19:30:42] [INFO] Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:03<00:03,  4.76it/s]
[2026-01-18 19:30:42] [INFO] Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:03<00:03,  4.77it/s]
[2026-01-18 19:30:42] [INFO] Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:03<00:03,  4.78it/s]
[2026-01-18 19:30:42] [INFO] Capturing CUDA graphs (decode, FULL):  54%|█████▍    | 19/35 [00:04<00:03,  4.81it/s]
[2026-01-18 19:30:43] [INFO] Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:04<00:03,  4.84it/s]
[2026-01-18 19:30:43] [INFO] Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:04<00:02,  4.81it/s]
[2026-01-18 19:30:43] [INFO] Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:04<00:02,  4.72it/s]
[2026-01-18 19:30:43] [INFO] Capturing CUDA graphs (decode, FULL):  66%|██████▌   | 23/35 [00:04<00:02,  4.60it/s]
[2026-01-18 19:30:43] [INFO] Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:05<00:02,  4.64it/s]
[2026-01-18 19:30:44] [INFO] Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:05<00:02,  4.69it/s]
[2026-01-18 19:30:44] [INFO] Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:05<00:01,  4.71it/s]
[2026-01-18 19:30:44] [INFO] Capturing CUDA graphs (decode, FULL):  77%|███████▋  | 27/35 [00:05<00:01,  4.75it/s]
[2026-01-18 19:30:44] [INFO] Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:05<00:01,  4.82it/s]
[2026-01-18 19:30:44] [INFO] Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:06<00:01,  4.86it/s]
[2026-01-18 19:30:45] [INFO] Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:06<00:01,  4.87it/s]
[2026-01-18 19:30:45] [INFO] Capturing CUDA graphs (decode, FULL):  89%|████████▊ | 31/35 [00:06<00:00,  4.85it/s]
[2026-01-18 19:30:45] [INFO] Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:06<00:00,  4.89it/s]
[2026-01-18 19:30:45] [INFO] Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:06<00:00,  4.95it/s]
[2026-01-18 19:30:45] [INFO] Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:07<00:00,  4.97it/s][0;36m(Worker_TP1 pid=120125)[0;0m INFO 01-18 19:30:45 [custom_all_reduce.py:216] Registering 15910 cuda graph addresses
[2026-01-18 19:30:45] [INFO] 
[2026-01-18 19:30:45] [INFO] Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:07<00:00,  5.00it/s]
[2026-01-18 19:30:45] [INFO] Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:07<00:00,  4.77it/s]
[2026-01-18 19:30:45] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m INFO 01-18 19:30:45 [custom_all_reduce.py:216] Registering 15910 cuda graph addresses
[2026-01-18 19:30:46] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m INFO 01-18 19:30:46 [gpu_model_runner.py:4837] Graph capturing finished in 20 secs, took 1.63 GiB
[2026-01-18 19:30:46] [INFO] [0;36m(EngineCore_DP0 pid=119659)[0;0m INFO 01-18 19:30:46 [core.py:273] init engine (profile, create kv cache, warmup model) took 261.04 seconds
[2026-01-18 19:30:47] [INFO] [0;36m(EngineCore_DP0 pid=119659)[0;0m The tokenizer you are loading from '/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.7-REAP-40-W4A16' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[2026-01-18 19:30:47] [INFO] [0;36m(EngineCore_DP0 pid=119659)[0;0m INFO 01-18 19:30:47 [core.py:186] Batch queue is enabled with size 2
[2026-01-18 19:30:47] [INFO] [0;36m(EngineCore_DP0 pid=119659)[0;0m INFO 01-18 19:30:47 [vllm.py:645] Asynchronous scheduling is enabled.
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [api_server.py:1020] Supported tasks: ['generate']
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m The tokenizer you are loading from '/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.7-REAP-40-W4A16' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [serving_engine.py:271] "auto" tool choice has been enabled.
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [serving_engine.py:271] "auto" tool choice has been enabled.
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [serving_chat.py:182] Warming up chat template processing...
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [chat_utils.py:599] Detected the chat template content format to be 'openai'. You can set `--chat-template-content-format` to override this.
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [serving_chat.py:218] Chat template warmup completed in 33.4ms
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [serving_engine.py:271] "auto" tool choice has been enabled.
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [api_server.py:1352] Starting vLLM API server 0 on http://0.0.0.0:8005
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [launcher.py:38] Available routes are:
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [launcher.py:46] Route: /docs, Methods: HEAD, GET
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [launcher.py:46] Route: /tokenize, Methods: POST
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [launcher.py:46] Route: /detokenize, Methods: POST
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [launcher.py:46] Route: /pause, Methods: POST
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [launcher.py:46] Route: /resume, Methods: POST
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [launcher.py:46] Route: /is_paused, Methods: GET
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [launcher.py:46] Route: /metrics, Methods: GET
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [launcher.py:46] Route: /health, Methods: GET
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [launcher.py:46] Route: /load, Methods: GET
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [launcher.py:46] Route: /v1/models, Methods: GET
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [launcher.py:46] Route: /version, Methods: GET
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [launcher.py:46] Route: /v1/responses, Methods: POST
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [launcher.py:46] Route: /v1/messages, Methods: POST
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [launcher.py:46] Route: /v1/completions, Methods: POST
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [launcher.py:46] Route: /ping, Methods: GET
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [launcher.py:46] Route: /ping, Methods: POST
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [launcher.py:46] Route: /invocations, Methods: POST
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [launcher.py:46] Route: /classify, Methods: POST
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [launcher.py:46] Route: /score, Methods: POST
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [launcher.py:46] Route: /v1/score, Methods: POST
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [launcher.py:46] Route: /rerank, Methods: POST
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [launcher.py:46] Route: /v1/rerank, Methods: POST
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [launcher.py:46] Route: /v2/rerank, Methods: POST
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:30:48 [launcher.py:46] Route: /pooling, Methods: POST
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO:     Started server process [119166]
[2026-01-18 19:30:48] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO:     Waiting for application startup.
[2026-01-18 19:30:49] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO:     Application startup complete.
[2026-01-18 19:50:15] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO:     127.0.0.1:58446 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-18 19:50:19] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:50:19 [loggers.py:257] Engine 000: Avg prompt throughput: 0.6 tokens/s, Avg generation throughput: 15.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[2026-01-18 19:50:29] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:50:29 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[2026-01-18 19:50:34] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO:     127.0.0.1:50940 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-18 19:50:39] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:50:39 [loggers.py:257] Engine 000: Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 36.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 0.0%
[2026-01-18 19:50:49] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:50:49 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 38.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[2026-01-18 19:50:59] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:50:59 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[2026-01-18 19:51:00] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO:     127.0.0.1:50464 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-18 19:51:09] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:51:09 [loggers.py:257] Engine 000: Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 77.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 11.3%
[2026-01-18 19:51:19] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:51:19 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 86.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 11.3%
[2026-01-18 19:51:29] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:51:29 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 50.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 11.3%
[2026-01-18 19:51:32] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO:     127.0.0.1:57248 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-18 19:51:39] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:51:39 [loggers.py:257] Engine 000: Avg prompt throughput: 107.4 tokens/s, Avg generation throughput: 55.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 10.5%
[2026-01-18 19:51:49] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:51:49 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 86.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.8%, Prefix cache hit rate: 10.5%
[2026-01-18 19:51:59] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:51:59 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 85.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 10.5%
[2026-01-18 19:52:09] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:52:09 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 85.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.3%, Prefix cache hit rate: 10.5%
[2026-01-18 19:52:19] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:52:19 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 84.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 10.5%
[2026-01-18 19:52:29] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:52:29 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 10.5%
[2026-01-18 19:52:30] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO:     127.0.0.1:42146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-18 19:52:39] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:52:39 [loggers.py:257] Engine 000: Avg prompt throughput: 12.1 tokens/s, Avg generation throughput: 76.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 16.8%
[2026-01-18 19:52:46] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO:     127.0.0.1:53456 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-18 19:52:49] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:52:49 [loggers.py:257] Engine 000: Avg prompt throughput: 16.8 tokens/s, Avg generation throughput: 24.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 22.3%
[2026-01-18 19:52:59] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:52:59 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 86.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 22.3%
[2026-01-18 19:53:09] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:53:09 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 31.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 22.3%
[2026-01-18 19:53:19] [INFO] [0;36m(APIServer pid=119166)[0;0m INFO 01-18 19:53:19 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 22.3%
[2026-01-18 19:53:36] [INFO] [0;36m(Worker_TP1 pid=120125)[0;0m INFO 01-18 19:53:36 [multiproc_executor.py:707] Parent process exited, terminating worker
[2026-01-18 19:53:36] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m INFO 01-18 19:53:36 [multiproc_executor.py:707] Parent process exited, terminating worker
[2026-01-18 19:53:36] [ERROR] [0;36m(APIServer pid=119166)[0;0m ERROR 01-18 19:53:36 [core_client.py:610] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.
[2026-01-18 19:53:36] [INFO] [0;36m(Worker_TP1 pid=120125)[0;0m INFO 01-18 19:53:36 [multiproc_executor.py:751] WorkerProc shutting down.
[2026-01-18 19:53:36] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m INFO 01-18 19:53:36 [multiproc_executor.py:751] WorkerProc shutting down.
[2026-01-18 19:53:36] [INFO] [0;36m(Worker_TP1 pid=120125)[0;0m Exception ignored in: <function ShmRingBuffer.__del__ at 0x763762feef20>
[2026-01-18 19:53:36] [ERROR] [0;36m(Worker_TP1 pid=120125)[0;0m Traceback (most recent call last):
[2026-01-18 19:53:36] [INFO] [0;36m(Worker_TP1 pid=120125)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/distributed/device_communicators/shm_broadcast.py", line 245, in __del__
[2026-01-18 19:53:36] [INFO] [0;36m(Worker_TP1 pid=120125)[0;0m     self.shared_memory.unlink()
[2026-01-18 19:53:36] [INFO] [0;36m(Worker_TP1 pid=120125)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/shared_memory.py", line 244, in unlink
[2026-01-18 19:53:36] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m /mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py:147: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.
[2026-01-18 19:53:36] [INFO] [0;36m(Worker_TP0 pid=120124)[0;0m   warnings.warn('resource_tracker: process died unexpectedly, '
[2026-01-18 19:53:36] [INFO] [0;36m(Worker_TP1 pid=120125)[0;0m     resource_tracker.unregister(self._name, "shared_memory")
[2026-01-18 19:53:36] [INFO] [0;36m(Worker_TP1 pid=120125)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 203, in unregister
[2026-01-18 19:53:36] [INFO] [0;36m(Worker_TP1 pid=120125)[0;0m     self._send('UNREGISTER', name, rtype)
[2026-01-18 19:53:36] [INFO] [0;36m(Worker_TP1 pid=120125)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 222, in _send
[2026-01-18 19:53:36] [INFO] [0;36m(Worker_TP1 pid=120125)[0;0m     nbytes = os.write(self._fd, msg)
[2026-01-18 19:53:36] [INFO] [0;36m(Worker_TP1 pid=120125)[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:53:36] [INFO] [0;36m(Worker_TP1 pid=120125)[0;0m BrokenPipeError: [Errno 32] Broken pipe
[2026-01-18 19:53:36] [ERROR] Traceback (most recent call last):
[2026-01-18 19:53:36] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 19:53:36] [INFO] cache[rtype].remove(name)
[2026-01-18 19:53:36] [INFO] KeyError: '/psm_68779e3a'
[2026-01-18 19:53:36] [INFO] 服务已停止
[2026-01-18 19:53:36] [ERROR] Traceback (most recent call last):
[2026-01-18 19:53:36] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 19:53:36] [INFO] cache[rtype].remove(name)
[2026-01-18 19:53:36] [INFO] KeyError: '/psm_81f96ccf'
[2026-01-18 19:53:36] [ERROR] Traceback (most recent call last):
[2026-01-18 19:53:36] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 19:53:36] [INFO] cache[rtype].remove(name)
[2026-01-18 19:53:36] [INFO] KeyError: '/mp-htg4h8o8'
[2026-01-18 19:53:36] [INFO] [0;36m(Worker_TP1 pid=120125)[0;0m /mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py:147: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.
[2026-01-18 19:53:36] [INFO] [0;36m(Worker_TP1 pid=120125)[0;0m   warnings.warn('resource_tracker: process died unexpectedly, '
[2026-01-18 19:53:36] [ERROR] Traceback (most recent call last):
[2026-01-18 19:53:36] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 19:53:36] [INFO] cache[rtype].remove(name)
[2026-01-18 19:53:36] [INFO] KeyError: '/mp-_qeaxn8z'
[2026-01-18 19:53:37] [INFO] nvitop监控已启动
[2026-01-18 19:53:39] [INFO] nvitop监控已启动
[2026-01-18 19:53:42] [INFO] nvitop监控已启动
[2026-01-18 19:53:46] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 19:53:46] [INFO] nvitop监控已启动
[2026-01-18 19:54:13] [INFO] [0;36m(APIServer pid=150658)[0;0m INFO 01-18 19:54:13 [api_server.py:1278] vLLM API server version 0.14.0rc1.dev492+g19504ac07
[2026-01-18 19:54:13] [INFO] [0;36m(APIServer pid=150658)[0;0m INFO 01-18 19:54:13 [utils.py:254] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8/chat_template.jinja', 'enable_auto_tool_choice': True, 'tool_call_parser': 'glm45', 'model': '/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8', 'trust_remote_code': True, 'allowed_local_media_path': '/', 'max_model_len': 128000, 'served_model_name': ['VLLM-MODEL'], 'reasoning_parser': 'glm45', 'tensor_parallel_size': 2, 'kv_cache_dtype': 'fp8', 'mm_processor_cache_type': 'shm', 'mm_encoder_tp_mode': 'data', 'max_num_seqs': 256, 'async_scheduling': True}
[2026-01-18 19:54:13] [INFO] [0;36m(APIServer pid=150658)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 19:54:13] [INFO] [0;36m(APIServer pid=150658)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 19:54:35] [INFO] [0;36m(APIServer pid=150658)[0;0m INFO 01-18 19:54:35 [model.py:528] Resolved architecture: Glm4vMoeForConditionalGeneration
[2026-01-18 19:54:35] [INFO] [0;36m(APIServer pid=150658)[0;0m INFO 01-18 19:54:35 [model.py:1543] Using max model len 128000
[2026-01-18 19:54:35] [INFO] [0;36m(APIServer pid=150658)[0;0m INFO 01-18 19:54:35 [cache.py:206] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor.
[2026-01-18 19:54:36] [INFO] [0;36m(APIServer pid=150658)[0;0m INFO 01-18 19:54:36 [scheduler.py:231] Chunked prefill is enabled with max_num_batched_tokens=8192.
[2026-01-18 19:54:36] [INFO] [0;36m(APIServer pid=150658)[0;0m INFO 01-18 19:54:36 [vllm.py:640] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-18 19:54:36] [INFO] [0;36m(APIServer pid=150658)[0;0m INFO 01-18 19:54:36 [vllm.py:645] Asynchronous scheduling is enabled.
[2026-01-18 19:54:36] [INFO] [0;36m(APIServer pid=150658)[0;0m The tokenizer you are loading from '/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[2026-01-18 19:54:59] [INFO] [0;36m(EngineCore_DP0 pid=151734)[0;0m INFO 01-18 19:54:59 [core.py:97] Initializing a V1 LLM engine (v0.14.0rc1.dev492+g19504ac07) with config: model='/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8', speculative_config=None, tokenizer='/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=fp8, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='glm45', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-18 19:54:59] [WARNING] [0;36m(EngineCore_DP0 pid=151734)[0;0m WARNING 01-18 19:54:59 [multiproc_executor.py:880] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-18 19:55:22] [INFO] The tokenizer you are loading from '/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[2026-01-18 19:55:23] [INFO] INFO 01-18 19:55:23 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:43877 backend=nccl
[2026-01-18 19:55:23] [INFO] The tokenizer you are loading from '/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[2026-01-18 19:55:23] [INFO] INFO 01-18 19:55:23 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:43877 backend=nccl
[2026-01-18 19:55:23] [INFO] INFO 01-18 19:55:23 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-18 19:55:24] [WARNING] WARNING 01-18 19:55:24 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 19:55:24] [WARNING] WARNING 01-18 19:55:24 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 19:55:24] [INFO] INFO 01-18 19:55:24 [parallel_state.py:1425] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[2026-01-18 19:55:24] [INFO] INFO 01-18 19:55:24 [parallel_state.py:1425] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
[2026-01-18 19:55:25] [INFO] The tokenizer you are loading from '/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[2026-01-18 19:55:25] [INFO] The tokenizer you are loading from '/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749] WorkerProc failed to start.
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749] Traceback (most recent call last):
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 720, in worker_main
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]     worker = WorkerProc(*args, **kwargs)
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 551, in __init__
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]     self.worker.init_device()
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/worker/worker_base.py", line 326, in init_device
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]     self.worker.init_device()  # type: ignore
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/worker/gpu_worker.py", line 261, in init_device
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]     self.model_runner = GPUModelRunnerV1(self.vllm_config, self.device)
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/worker/gpu_model_runner.py", line 624, in __init__
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]     MultiModalBudget(
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/worker/utils.py", line 48, in __init__
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]     max_tokens_by_modality = mm_registry.get_max_tokens_per_item_by_modality(
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/multimodal/registry.py", line 171, in get_max_tokens_per_item_by_modality
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]     return profiler.get_mm_max_tokens(
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/multimodal/profiling.py", line 350, in get_mm_max_tokens
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]     mm_inputs = self._get_dummy_mm_inputs(seq_len, mm_counts)
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/multimodal/profiling.py", line 263, in _get_dummy_mm_inputs
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]     processor_inputs = factory.get_dummy_processor_inputs(
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/multimodal/profiling.py", line 120, in get_dummy_processor_inputs
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]     dummy_text = self.get_dummy_text(mm_counts)
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/glm4_1v.py", line 1145, in get_dummy_text
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]     hf_processor = self.info.get_hf_processor()
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/multimodal/processing.py", line 1397, in get_hf_processor
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]     return self.ctx.get_hf_processor(**kwargs)
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/multimodal/processing.py", line 1195, in get_hf_processor
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]     return cached_processor_from_config(
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/transformers_utils/processor.py", line 251, in cached_processor_from_config
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]     return cached_get_processor_without_dynamic_kwargs(
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/transformers_utils/processor.py", line 210, in cached_get_processor_without_dynamic_kwargs
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]     processor = cached_get_processor(
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]                 ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/transformers_utils/processor.py", line 155, in get_processor
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]     raise TypeError(
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749] TypeError: Invalid type of HuggingFace processor. Expected type: <class 'transformers.processing_utils.ProcessorMixin'>, but found type: <class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749] WorkerProc failed to start.
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749] Traceback (most recent call last):
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 720, in worker_main
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]     worker = WorkerProc(*args, **kwargs)
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 551, in __init__
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]     self.worker.init_device()
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/worker/worker_base.py", line 326, in init_device
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]     self.worker.init_device()  # type: ignore
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/worker/gpu_worker.py", line 261, in init_device
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]     self.model_runner = GPUModelRunnerV1(self.vllm_config, self.device)
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/worker/gpu_model_runner.py", line 624, in __init__
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]     MultiModalBudget(
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/worker/utils.py", line 48, in __init__
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]     max_tokens_by_modality = mm_registry.get_max_tokens_per_item_by_modality(
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/multimodal/registry.py", line 171, in get_max_tokens_per_item_by_modality
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]     return profiler.get_mm_max_tokens(
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/multimodal/profiling.py", line 350, in get_mm_max_tokens
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]     mm_inputs = self._get_dummy_mm_inputs(seq_len, mm_counts)
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/multimodal/profiling.py", line 263, in _get_dummy_mm_inputs
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]     processor_inputs = factory.get_dummy_processor_inputs(
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/multimodal/profiling.py", line 120, in get_dummy_processor_inputs
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]     dummy_text = self.get_dummy_text(mm_counts)
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/glm4_1v.py", line 1145, in get_dummy_text
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]     hf_processor = self.info.get_hf_processor()
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/multimodal/processing.py", line 1397, in get_hf_processor
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]     return self.ctx.get_hf_processor(**kwargs)
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/multimodal/processing.py", line 1195, in get_hf_processor
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]     return cached_processor_from_config(
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/transformers_utils/processor.py", line 251, in cached_processor_from_config
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]     return cached_get_processor_without_dynamic_kwargs(
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/transformers_utils/processor.py", line 210, in cached_get_processor_without_dynamic_kwargs
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]     processor = cached_get_processor(
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]                 ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/transformers_utils/processor.py", line 155, in get_processor
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749]     raise TypeError(
[2026-01-18 19:55:25] [ERROR] ERROR 01-18 19:55:25 [multiproc_executor.py:749] TypeError: Invalid type of HuggingFace processor. Expected type: <class 'transformers.processing_utils.ProcessorMixin'>, but found type: <class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>
[2026-01-18 19:55:25] [INFO] INFO 01-18 19:55:25 [multiproc_executor.py:707] Parent process exited, terminating worker
[2026-01-18 19:55:25] [INFO] INFO 01-18 19:55:25 [multiproc_executor.py:707] Parent process exited, terminating worker
[2026-01-18 19:55:26] [WARNING] [rank0]:[W118 19:55:26.866052150 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2026-01-18 19:55:27] [ERROR] [0;36m(EngineCore_DP0 pid=151734)[0;0m ERROR 01-18 19:55:27 [core.py:936] EngineCore failed to start.
[2026-01-18 19:55:27] [ERROR] [0;36m(EngineCore_DP0 pid=151734)[0;0m ERROR 01-18 19:55:27 [core.py:936] Traceback (most recent call last):
[2026-01-18 19:55:27] [ERROR] [0;36m(EngineCore_DP0 pid=151734)[0;0m ERROR 01-18 19:55:27 [core.py:936]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core.py", line 927, in run_engine_core
[2026-01-18 19:55:27] [ERROR] [0;36m(EngineCore_DP0 pid=151734)[0;0m ERROR 01-18 19:55:27 [core.py:936]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 19:55:27] [ERROR] [0;36m(EngineCore_DP0 pid=151734)[0;0m ERROR 01-18 19:55:27 [core.py:936]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:55:27] [ERROR] [0;36m(EngineCore_DP0 pid=151734)[0;0m ERROR 01-18 19:55:27 [core.py:936]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core.py", line 692, in __init__
[2026-01-18 19:55:27] [ERROR] [0;36m(EngineCore_DP0 pid=151734)[0;0m ERROR 01-18 19:55:27 [core.py:936]     super().__init__(
[2026-01-18 19:55:27] [ERROR] [0;36m(EngineCore_DP0 pid=151734)[0;0m ERROR 01-18 19:55:27 [core.py:936]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core.py", line 106, in __init__
[2026-01-18 19:55:27] [ERROR] [0;36m(EngineCore_DP0 pid=151734)[0;0m ERROR 01-18 19:55:27 [core.py:936]     self.model_executor = executor_class(vllm_config)
[2026-01-18 19:55:27] [ERROR] [0;36m(EngineCore_DP0 pid=151734)[0;0m ERROR 01-18 19:55:27 [core.py:936]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:55:27] [ERROR] [0;36m(EngineCore_DP0 pid=151734)[0;0m ERROR 01-18 19:55:27 [core.py:936]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[2026-01-18 19:55:27] [ERROR] [0;36m(EngineCore_DP0 pid=151734)[0;0m ERROR 01-18 19:55:27 [core.py:936]     super().__init__(vllm_config)
[2026-01-18 19:55:27] [ERROR] [0;36m(EngineCore_DP0 pid=151734)[0;0m ERROR 01-18 19:55:27 [core.py:936]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/abstract.py", line 101, in __init__
[2026-01-18 19:55:27] [ERROR] [0;36m(EngineCore_DP0 pid=151734)[0;0m ERROR 01-18 19:55:27 [core.py:936]     self._init_executor()
[2026-01-18 19:55:27] [ERROR] [0;36m(EngineCore_DP0 pid=151734)[0;0m ERROR 01-18 19:55:27 [core.py:936]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 172, in _init_executor
[2026-01-18 19:55:27] [ERROR] [0;36m(EngineCore_DP0 pid=151734)[0;0m ERROR 01-18 19:55:27 [core.py:936]     self.workers = WorkerProc.wait_for_ready(unready_workers)
[2026-01-18 19:55:27] [ERROR] [0;36m(EngineCore_DP0 pid=151734)[0;0m ERROR 01-18 19:55:27 [core.py:936]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:55:27] [ERROR] [0;36m(EngineCore_DP0 pid=151734)[0;0m ERROR 01-18 19:55:27 [core.py:936]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 658, in wait_for_ready
[2026-01-18 19:55:27] [ERROR] [0;36m(EngineCore_DP0 pid=151734)[0;0m ERROR 01-18 19:55:27 [core.py:936]     raise e from None
[2026-01-18 19:55:27] [ERROR] [0;36m(EngineCore_DP0 pid=151734)[0;0m ERROR 01-18 19:55:27 [core.py:936] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[2026-01-18 19:55:27] [INFO] [0;36m(EngineCore_DP0 pid=151734)[0;0m Process EngineCore_DP0:
[2026-01-18 19:55:27] [ERROR] [0;36m(EngineCore_DP0 pid=151734)[0;0m Traceback (most recent call last):
[2026-01-18 19:55:27] [INFO] [0;36m(EngineCore_DP0 pid=151734)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[2026-01-18 19:55:27] [INFO] [0;36m(EngineCore_DP0 pid=151734)[0;0m     self.run()
[2026-01-18 19:55:27] [INFO] [0;36m(EngineCore_DP0 pid=151734)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/process.py", line 108, in run
[2026-01-18 19:55:27] [INFO] [0;36m(EngineCore_DP0 pid=151734)[0;0m     self._target(*self._args, **self._kwargs)
[2026-01-18 19:55:27] [INFO] [0;36m(EngineCore_DP0 pid=151734)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core.py", line 940, in run_engine_core
[2026-01-18 19:55:27] [INFO] [0;36m(EngineCore_DP0 pid=151734)[0;0m     raise e
[2026-01-18 19:55:27] [INFO] [0;36m(EngineCore_DP0 pid=151734)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core.py", line 927, in run_engine_core
[2026-01-18 19:55:27] [INFO] [0;36m(EngineCore_DP0 pid=151734)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 19:55:27] [INFO] [0;36m(EngineCore_DP0 pid=151734)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:55:27] [INFO] [0;36m(EngineCore_DP0 pid=151734)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core.py", line 692, in __init__
[2026-01-18 19:55:27] [INFO] [0;36m(EngineCore_DP0 pid=151734)[0;0m     super().__init__(
[2026-01-18 19:55:27] [INFO] [0;36m(EngineCore_DP0 pid=151734)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core.py", line 106, in __init__
[2026-01-18 19:55:27] [INFO] [0;36m(EngineCore_DP0 pid=151734)[0;0m     self.model_executor = executor_class(vllm_config)
[2026-01-18 19:55:27] [INFO] [0;36m(EngineCore_DP0 pid=151734)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:55:27] [INFO] [0;36m(EngineCore_DP0 pid=151734)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[2026-01-18 19:55:27] [INFO] [0;36m(EngineCore_DP0 pid=151734)[0;0m     super().__init__(vllm_config)
[2026-01-18 19:55:27] [INFO] [0;36m(EngineCore_DP0 pid=151734)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/abstract.py", line 101, in __init__
[2026-01-18 19:55:27] [INFO] [0;36m(EngineCore_DP0 pid=151734)[0;0m     self._init_executor()
[2026-01-18 19:55:27] [INFO] [0;36m(EngineCore_DP0 pid=151734)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 172, in _init_executor
[2026-01-18 19:55:27] [INFO] [0;36m(EngineCore_DP0 pid=151734)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
[2026-01-18 19:55:27] [INFO] [0;36m(EngineCore_DP0 pid=151734)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:55:27] [INFO] [0;36m(EngineCore_DP0 pid=151734)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 658, in wait_for_ready
[2026-01-18 19:55:27] [INFO] [0;36m(EngineCore_DP0 pid=151734)[0;0m     raise e from None
[2026-01-18 19:55:27] [INFO] [0;36m(EngineCore_DP0 pid=151734)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[2026-01-18 19:55:28] [ERROR] [0;36m(APIServer pid=150658)[0;0m Traceback (most recent call last):
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/bin/vllm", line 7, in <module>
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m     sys.exit(main())
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m              ^^^^^^
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m     args.dispatch_function(args)
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m     uvloop.run(run_server(args))
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m     return __asyncio.run(
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m            ^^^^^^^^^^^^^^
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m     return runner.run(main)
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m     return self._loop.run_until_complete(task)
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m     return await main
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m            ^^^^^^^^^^
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 1325, in run_server
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 1344, in run_server_worker
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m     async with build_async_engine_client(
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m     return await anext(self.gen)
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 173, in build_async_engine_client
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m     return await anext(self.gen)
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 214, in build_async_engine_client_from_engine_args
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/async_llm.py", line 205, in from_vllm_config
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m     return cls(
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m            ^^^^
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/async_llm.py", line 132, in __init__
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m     return AsyncMPClient(*client_args)
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core_client.py", line 824, in __init__
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m     super().__init__(
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core_client.py", line 479, in __init__
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/contextlib.py", line 144, in __exit__
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m     next(self.gen)
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/utils.py", line 921, in launch_core_engines
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m     wait_for_engine_startup(
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/utils.py", line 980, in wait_for_engine_startup
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m     raise RuntimeError(
[2026-01-18 19:55:28] [INFO] [0;36m(APIServer pid=150658)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[2026-01-18 19:55:29] [INFO] /mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py:279: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
[2026-01-18 19:55:29] [INFO] warnings.warn('resource_tracker: There appear to be %d '
[2026-01-18 19:55:54] [INFO] nvitop监控已启动
[2026-01-18 19:56:05] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 19:56:05] [INFO] nvitop监控已启动
[2026-01-18 19:56:05] [INFO] nvitop监控已停止
[2026-01-18 19:56:34] [INFO] [0;36m(APIServer pid=153183)[0;0m INFO 01-18 19:56:34 [api_server.py:872] vLLM API server version 0.14.0rc2.dev117+g9e078d058
[2026-01-18 19:56:34] [INFO] [0;36m(APIServer pid=153183)[0;0m INFO 01-18 19:56:34 [utils.py:267] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8/chat_template.jinja', 'enable_auto_tool_choice': True, 'tool_call_parser': 'glm45', 'model': '/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8', 'trust_remote_code': True, 'allowed_local_media_path': '/', 'max_model_len': 128000, 'served_model_name': ['VLLM-MODEL'], 'reasoning_parser': 'glm45', 'tensor_parallel_size': 2, 'kv_cache_dtype': 'fp8', 'mm_processor_cache_type': 'shm', 'mm_encoder_tp_mode': 'data', 'max_num_seqs': 256, 'async_scheduling': True}
[2026-01-18 19:56:34] [INFO] [0;36m(APIServer pid=153183)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 19:56:34] [INFO] [0;36m(APIServer pid=153183)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 19:56:34] [INFO] [0;36m(APIServer pid=153183)[0;0m Unrecognized keys in `rope_parameters` for 'rope_type'='default': {'mrope_section'}
[2026-01-18 19:56:34] [INFO] [0;36m(APIServer pid=153183)[0;0m Unrecognized keys in `rope_parameters` for 'rope_type'='default': {'mrope_section', 'partial_rotary_factor'}
[2026-01-18 19:56:34] [INFO] [0;36m(APIServer pid=153183)[0;0m Unrecognized keys in `rope_parameters` for 'rope_type'='default': {'mrope_section', 'partial_rotary_factor'}
[2026-01-18 19:57:00] [INFO] [0;36m(APIServer pid=153183)[0;0m INFO 01-18 19:57:00 [model.py:530] Resolved architecture: Glm4vMoeForConditionalGeneration
[2026-01-18 19:57:00] [INFO] [0;36m(APIServer pid=153183)[0;0m INFO 01-18 19:57:00 [model.py:1547] Using max model len 128000
[2026-01-18 19:57:00] [WARNING] [0;36m(APIServer pid=153183)[0;0m WARNING 01-18 19:57:00 [quark_ocp_mx.py:157] AITER is not found or QuarkOCP_MX is not supported on the current platform. QuarkOCP_MX quantization will not be available.
[2026-01-18 19:57:00] [INFO] [0;36m(APIServer pid=153183)[0;0m INFO 01-18 19:57:00 [cache.py:206] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor.
[2026-01-18 19:57:01] [INFO] [0;36m(APIServer pid=153183)[0;0m INFO 01-18 19:57:01 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[2026-01-18 19:57:01] [INFO] [0;36m(APIServer pid=153183)[0;0m INFO 01-18 19:57:01 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 19:57:01] [INFO] [0;36m(APIServer pid=153183)[0;0m INFO 01-18 19:57:01 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-18 19:57:31] [INFO] [0;36m(EngineCore_DP0 pid=154466)[0;0m INFO 01-18 19:57:31 [core.py:96] Initializing a V1 LLM engine (v0.14.0rc2.dev117+g9e078d058) with config: model='/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8', speculative_config=None, tokenizer='/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=fp8, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='glm45', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-18 19:57:31] [WARNING] [0;36m(EngineCore_DP0 pid=154466)[0;0m WARNING 01-18 19:57:31 [multiproc_executor.py:897] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-18 19:58:02] [INFO] INFO 01-18 19:58:02 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:43355 backend=nccl
[2026-01-18 19:58:02] [INFO] INFO 01-18 19:58:02 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:43355 backend=nccl
[2026-01-18 19:58:03] [INFO] INFO 01-18 19:58:03 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-18 19:58:03] [WARNING] WARNING 01-18 19:58:03 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 19:58:03] [WARNING] WARNING 01-18 19:58:03 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 19:58:03] [INFO] INFO 01-18 19:58:03 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[2026-01-18 19:58:03] [INFO] INFO 01-18 19:58:03 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
[2026-01-18 19:58:04] [INFO] Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[2026-01-18 19:58:04] [INFO] Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[2026-01-18 19:58:24] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m INFO 01-18 19:58:24 [gpu_model_runner.py:3803] Starting to load model /mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8...
[2026-01-18 19:58:25] [WARNING] [0;36m(Worker_TP1 pid=155026)[0;0m WARNING 01-18 19:58:25 [compressed_tensors.py:738] Acceleration for non-quantized schemes is not supported by Compressed Tensors. Falling back to UnquantizedLinearMethod
[2026-01-18 19:58:25] [WARNING] [0;36m(Worker_TP0 pid=155025)[0;0m WARNING 01-18 19:58:25 [compressed_tensors.py:738] Acceleration for non-quantized schemes is not supported by Compressed Tensors. Falling back to UnquantizedLinearMethod
[2026-01-18 19:58:25] [INFO] [0;36m(Worker_TP1 pid=155026)[0;0m [0;36m(Worker_TP0 pid=155025)[0;0m INFO 01-18 19:58:25 [mm_encoder_attention.py:86] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[2026-01-18 19:58:25] [INFO] INFO 01-18 19:58:25 [mm_encoder_attention.py:86] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[2026-01-18 19:58:25] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m [0;36m(Worker_TP1 pid=155026)[0;0m INFO 01-18 19:58:25 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 19:58:25] [INFO] INFO 01-18 19:58:25 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 19:58:27] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m INFO 01-18 19:58:27 [cuda.py:351] Using FLASHINFER attention backend out of potential backends: ('FLASHINFER', 'TRITON_ATTN')
[2026-01-18 19:58:27] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m INFO 01-18 19:58:27 [fp8.py:126] DeepGEMM is disabled because the platform does not support it.
[2026-01-18 19:58:27] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m INFO 01-18 19:58:27 [fp8.py:150] Using vLLM CUTLASS backend for FP8 MoE
[2026-01-18 19:58:28] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m
[2026-01-18 19:58:28] [INFO] Loading safetensors checkpoint shards:   0% Completed | 0/41 [00:00<?, ?it/s]
[2026-01-18 19:58:30] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m
[2026-01-18 19:58:30] [INFO] Loading safetensors checkpoint shards:   2% Completed | 1/41 [00:01<01:15,  1.90s/it]
[2026-01-18 19:58:31] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m
[2026-01-18 19:58:31] [INFO] Loading safetensors checkpoint shards:   5% Completed | 2/41 [00:03<01:14,  1.90s/it]
[2026-01-18 19:58:34] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m
[2026-01-18 19:58:34] [INFO] Loading safetensors checkpoint shards:   7% Completed | 3/41 [00:06<01:28,  2.32s/it]
[2026-01-18 19:58:38] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m
[2026-01-18 19:58:38] [INFO] Loading safetensors checkpoint shards:  10% Completed | 4/41 [00:10<01:47,  2.90s/it]
[2026-01-18 19:58:42] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m
[2026-01-18 19:58:42] [INFO] Loading safetensors checkpoint shards:  12% Completed | 5/41 [00:13<01:51,  3.09s/it]
[2026-01-18 19:58:45] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m
[2026-01-18 19:58:45] [INFO] Loading safetensors checkpoint shards:  15% Completed | 6/41 [00:16<01:48,  3.11s/it]
[2026-01-18 19:58:48] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m
[2026-01-18 19:58:48] [INFO] Loading safetensors checkpoint shards:  17% Completed | 7/41 [00:20<01:44,  3.09s/it]
[2026-01-18 19:58:51] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m
[2026-01-18 19:58:51] [INFO] Loading safetensors checkpoint shards:  20% Completed | 8/41 [00:23<01:41,  3.07s/it]
[2026-01-18 19:58:54] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m
[2026-01-18 19:58:54] [INFO] Loading safetensors checkpoint shards:  22% Completed | 9/41 [00:26<01:37,  3.06s/it]
[2026-01-18 19:58:56] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m
[2026-01-18 19:58:56] [INFO] Loading safetensors checkpoint shards:  24% Completed | 10/41 [00:28<01:31,  2.94s/it]
[2026-01-18 19:58:59] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m
[2026-01-18 19:58:59] [INFO] Loading safetensors checkpoint shards:  27% Completed | 11/41 [00:31<01:25,  2.85s/it]
[2026-01-18 19:59:02] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m
[2026-01-18 19:59:02] [INFO] Loading safetensors checkpoint shards:  29% Completed | 12/41 [00:34<01:24,  2.92s/it]
[2026-01-18 19:59:05] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m
[2026-01-18 19:59:05] [INFO] Loading safetensors checkpoint shards:  32% Completed | 13/41 [00:37<01:23,  2.97s/it]
[2026-01-18 19:59:08] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m
[2026-01-18 19:59:08] [INFO] Loading safetensors checkpoint shards:  34% Completed | 14/41 [00:40<01:21,  3.01s/it]
[2026-01-18 19:59:11] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m
[2026-01-18 19:59:11] [INFO] Loading safetensors checkpoint shards:  37% Completed | 15/41 [00:43<01:16,  2.96s/it]
[2026-01-18 19:59:14] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m
[2026-01-18 19:59:14] [INFO] Loading safetensors checkpoint shards:  39% Completed | 16/41 [00:46<01:11,  2.86s/it]
[2026-01-18 19:59:16] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m
[2026-01-18 19:59:16] [INFO] Loading safetensors checkpoint shards:  41% Completed | 17/41 [00:48<01:05,  2.75s/it]
[2026-01-18 19:59:19] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m
[2026-01-18 19:59:19] [INFO] Loading safetensors checkpoint shards:  44% Completed | 18/41 [00:51<01:05,  2.85s/it]
[2026-01-18 19:59:22] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m
[2026-01-18 19:59:22] [INFO] Loading safetensors checkpoint shards:  46% Completed | 19/41 [00:54<01:02,  2.85s/it]
[2026-01-18 19:59:25] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m
[2026-01-18 19:59:25] [INFO] Loading safetensors checkpoint shards:  49% Completed | 20/41 [00:57<01:00,  2.88s/it]
[2026-01-18 19:59:28] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m
[2026-01-18 19:59:28] [INFO] Loading safetensors checkpoint shards:  51% Completed | 21/41 [01:00<00:55,  2.78s/it]
[2026-01-18 19:59:30] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m
[2026-01-18 19:59:30] [INFO] Loading safetensors checkpoint shards:  54% Completed | 22/41 [01:02<00:50,  2.66s/it]
[2026-01-18 19:59:32] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m
[2026-01-18 19:59:32] [INFO] Loading safetensors checkpoint shards:  56% Completed | 23/41 [01:04<00:46,  2.56s/it]
[2026-01-18 19:59:35] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m
[2026-01-18 19:59:35] [INFO] Loading safetensors checkpoint shards:  59% Completed | 24/41 [01:07<00:43,  2.53s/it]
[2026-01-18 19:59:37] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m
[2026-01-18 19:59:37] [INFO] Loading safetensors checkpoint shards:  61% Completed | 25/41 [01:09<00:39,  2.49s/it]
[2026-01-18 19:59:40] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m
[2026-01-18 19:59:40] [INFO] Loading safetensors checkpoint shards:  63% Completed | 26/41 [01:12<00:37,  2.47s/it]
[2026-01-18 19:59:42] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m
[2026-01-18 19:59:42] [INFO] Loading safetensors checkpoint shards:  66% Completed | 27/41 [01:14<00:34,  2.49s/it]
[2026-01-18 19:59:45] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m
[2026-01-18 19:59:45] [INFO] Loading safetensors checkpoint shards:  68% Completed | 28/41 [01:17<00:32,  2.50s/it]
[2026-01-18 19:59:47] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m
[2026-01-18 19:59:47] [INFO] Loading safetensors checkpoint shards:  71% Completed | 29/41 [01:19<00:30,  2.55s/it]
[2026-01-18 19:59:50] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m
[2026-01-18 19:59:50] [INFO] Loading safetensors checkpoint shards:  73% Completed | 30/41 [01:22<00:28,  2.55s/it]
[2026-01-18 19:59:53] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m
[2026-01-18 19:59:53] [INFO] Loading safetensors checkpoint shards:  76% Completed | 31/41 [01:24<00:25,  2.56s/it]
[2026-01-18 19:59:55] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m
[2026-01-18 19:59:55] [INFO] Loading safetensors checkpoint shards:  78% Completed | 32/41 [01:27<00:23,  2.58s/it]
[2026-01-18 19:59:58] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m
[2026-01-18 19:59:58] [INFO] Loading safetensors checkpoint shards:  80% Completed | 33/41 [01:30<00:21,  2.67s/it]
[2026-01-18 20:00:01] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m
[2026-01-18 20:00:01] [INFO] Loading safetensors checkpoint shards:  83% Completed | 34/41 [01:33<00:19,  2.79s/it]
[2026-01-18 20:00:04] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m
[2026-01-18 20:00:04] [INFO] Loading safetensors checkpoint shards:  85% Completed | 35/41 [01:36<00:17,  2.91s/it]
[2026-01-18 20:00:07] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m
[2026-01-18 20:00:07] [INFO] Loading safetensors checkpoint shards:  88% Completed | 36/41 [01:39<00:14,  2.83s/it]
[2026-01-18 20:00:09] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m
[2026-01-18 20:00:09] [INFO] Loading safetensors checkpoint shards:  90% Completed | 37/41 [01:41<00:10,  2.72s/it]
[2026-01-18 20:00:12] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m
[2026-01-18 20:00:12] [INFO] Loading safetensors checkpoint shards:  93% Completed | 38/41 [01:44<00:07,  2.66s/it]
[2026-01-18 20:00:15] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m
[2026-01-18 20:00:15] [INFO] Loading safetensors checkpoint shards:  95% Completed | 39/41 [01:46<00:05,  2.63s/it]
[2026-01-18 20:00:18] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m
[2026-01-18 20:00:18] [INFO] Loading safetensors checkpoint shards:  98% Completed | 40/41 [01:50<00:02,  2.79s/it]
[2026-01-18 20:00:19] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m
[2026-01-18 20:00:19] [INFO] Loading safetensors checkpoint shards: 100% Completed | 41/41 [01:50<00:00,  2.19s/it]
[2026-01-18 20:00:19] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m
[2026-01-18 20:00:19] [INFO] Loading safetensors checkpoint shards: 100% Completed | 41/41 [01:50<00:00,  2.70s/it]
[2026-01-18 20:00:19] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m
[2026-01-18 20:00:19] [WARNING] [0;36m(Worker_TP1 pid=155026)[0;0m WARNING 01-18 20:00:19 [kv_cache.py:90] Checkpoint does not provide a q scaling factor. Setting it to k_scale. This only matters for FP8 Attention backends (flash-attn or flashinfer).
[2026-01-18 20:00:19] [WARNING] [0;36m(Worker_TP1 pid=155026)[0;0m WARNING 01-18 20:00:19 [kv_cache.py:104] Using KV cache scaling factor 1.0 for fp8_e4m3. If this is unintended, verify that k/v_scale scaling factors are properly set in the checkpoint.
[2026-01-18 20:00:19] [WARNING] [0;36m(Worker_TP1 pid=155026)[0;0m WARNING 01-18 20:00:19 [kv_cache.py:143] Using uncalibrated q_scale 1.0 and/or prob_scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure q/prob scaling factors are available in the fp8 checkpoint.
[2026-01-18 20:00:19] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m INFO 01-18 20:00:19 [default_loader.py:291] Loading weights took 110.87 seconds
[2026-01-18 20:00:19] [WARNING] [0;36m(Worker_TP0 pid=155025)[0;0m WARNING 01-18 20:00:19 [kv_cache.py:90] Checkpoint does not provide a q scaling factor. Setting it to k_scale. This only matters for FP8 Attention backends (flash-attn or flashinfer).
[2026-01-18 20:00:19] [WARNING] [0;36m(Worker_TP0 pid=155025)[0;0m WARNING 01-18 20:00:19 [kv_cache.py:104] Using KV cache scaling factor 1.0 for fp8_e4m3. If this is unintended, verify that k/v_scale scaling factors are properly set in the checkpoint.
[2026-01-18 20:00:19] [WARNING] [0;36m(Worker_TP0 pid=155025)[0;0m WARNING 01-18 20:00:19 [kv_cache.py:143] Using uncalibrated q_scale 1.0 and/or prob_scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure q/prob scaling factors are available in the fp8 checkpoint.
[2026-01-18 20:00:19] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m INFO 01-18 20:00:19 [gpu_model_runner.py:3900] Model loading took 52.22 GiB memory and 113.833366 seconds
[2026-01-18 20:00:19] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m INFO 01-18 20:00:19 [gpu_model_runner.py:4711] Encoder cache will be initialized with a budget of 30000 tokens, and profiled with 1 video items of the maximum feature size.
[2026-01-18 20:00:19] [INFO] [0;36m(Worker_TP1 pid=155026)[0;0m INFO 01-18 20:00:19 [gpu_model_runner.py:4711] Encoder cache will be initialized with a budget of 30000 tokens, and profiled with 1 video items of the maximum feature size.
[2026-01-18 20:00:43] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m INFO 01-18 20:00:43 [backends.py:644] Using cache directory: /home/wuwen/.cache/vllm/torch_compile_cache/24560f5dbf/rank_0_0/backbone for vLLM's torch.compile
[2026-01-18 20:00:43] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m INFO 01-18 20:00:43 [backends.py:704] Dynamo bytecode transform time: 12.84 s
[2026-01-18 20:01:00] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m INFO 01-18 20:01:00 [backends.py:261] Cache the graph of compile range (1, 8192) for later use
[2026-01-18 20:01:00] [INFO] [0;36m(Worker_TP1 pid=155026)[0;0m INFO 01-18 20:01:00 [backends.py:261] Cache the graph of compile range (1, 8192) for later use
[2026-01-18 20:01:04] [INFO] [0;36m(Worker_TP1 pid=155026)[0;0m /mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:312: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
[2026-01-18 20:01:04] [INFO] [0;36m(Worker_TP1 pid=155026)[0;0m   warnings.warn(
[2026-01-18 20:01:05] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m /mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:312: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
[2026-01-18 20:01:05] [INFO] [0;36m(Worker_TP0 pid=155025)[0;0m   warnings.warn(
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839] WorkerProc hit an exception.
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839] Traceback (most recent call last):
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 834, in worker_busy_loop
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     output = func(*args, **kwargs)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]              ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return func(*args, **kwargs)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 321, in determine_available_memory
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     self.model_runner.profile_run()
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4739, in profile_run
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     hidden_states, last_hidden_states = self._dummy_run(
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]                                         ^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return func(*args, **kwargs)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4457, in _dummy_run
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     outputs = self.model(
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]               ^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 222, in __call__
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return self.runnable(*args, **kwargs)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/glm4_1v.py", line 1764, in forward
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     hidden_states = self.language_model.model(
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 557, in __call__
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)  # type: ignore[arg-type]
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 228, in __call__
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return self._call_with_optional_nvtx_range(
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 119, in _call_with_optional_nvtx_range
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return callable_fn(*args, **kwargs)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return fn(*args, **kwargs)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/glm4_moe.py", line 452, in forward
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     def forward(
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return fn(*args, **kwargs)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/caching.py", line 64, in __call__
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return self.optimized_call(*args, **kwargs)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return self._wrapped_call(self, *args, **kwargs)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/fx/graph_module.py", line 413, in __call__
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     raise e
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/fx/graph_module.py", line 400, in __call__
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "<eval_with_key>.94", line 393, in forward
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     submod_4 = self.submod_4(getitem_7, s59, s18, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_8, l_self_modules_layers_modules_1_modules_mlp_modules_gate_parameters_weight_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_, l_positions_, s7);  getitem_7 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_8 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_parameters_weight_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 222, in __call__
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return self.runnable(*args, **kwargs)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/piecewise_backend.py", line 191, in __call__
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return range_entry.runnable(*args)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return self._compiled_fn(*args)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return fn(*args, **kwargs)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return compiled_fn(full_args)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     all_outs = call_func_at_runtime_with_args(
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     out = normalize_as_list(f(args))
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]                             ^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     outs = compiled_fn(args)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return compiled_fn(runtime_args)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_inductor/output_code.py", line 613, in __call__
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return self.current_callable(inputs)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_inductor/utils.py", line 3017, in run
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     out = model(new_inputs)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]           ^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/tmp/torchinductor_wuwen/ct/cctdwzzfufdzd76q46mh5ef2a3dt4kas62lnlxc54qvfsslr3v7f.py", line 1504, in call
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     buf13 = torch.ops.vllm.moe_forward_shared.default(buf12, buf11, 'language_model.model.layers.1.mlp.experts')
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_ops.py", line 841, in __call__
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return self._op(*args, **kwargs)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 2131, in moe_forward_shared
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return self.forward_impl(hidden_states, router_logits)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1979, in forward_impl
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     final_hidden_states = self.quant_method.apply(
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]                           ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py", line 1122, in apply
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     result = self.kernel(
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]              ^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/modular_kernel.py", line 1228, in forward
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     fused_out = self._fused_experts(
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]                 ^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/modular_kernel.py", line 1082, in _fused_experts
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     self.fused_experts.apply(
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/fallback.py", line 111, in apply
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     experts.apply(
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/cutlass_moe.py", line 298, in apply
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     run_cutlass_moe_fp8(
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/cutlass_moe.py", line 188, in run_cutlass_moe_fp8
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     ops.cutlass_moe_mm(
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/_custom_ops.py", line 1175, in cutlass_moe_mm
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return torch.ops._C.cutlass_moe_mm(
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_ops.py", line 1255, in __call__
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return self._op(*args, **kwargs)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839] NotImplementedError: No compiled cutlass_scaled_mm for CUDA device capability: 120. Required capability: 90 or 100
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839] Traceback (most recent call last):
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 834, in worker_busy_loop
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     output = func(*args, **kwargs)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]              ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return func(*args, **kwargs)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 321, in determine_available_memory
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     self.model_runner.profile_run()
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4739, in profile_run
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     hidden_states, last_hidden_states = self._dummy_run(
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]                                         ^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return func(*args, **kwargs)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4457, in _dummy_run
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     outputs = self.model(
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]               ^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 222, in __call__
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return self.runnable(*args, **kwargs)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/glm4_1v.py", line 1764, in forward
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     hidden_states = self.language_model.model(
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 557, in __call__
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)  # type: ignore[arg-type]
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 228, in __call__
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return self._call_with_optional_nvtx_range(
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 119, in _call_with_optional_nvtx_range
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return callable_fn(*args, **kwargs)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return fn(*args, **kwargs)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/glm4_moe.py", line 452, in forward
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     def forward(
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return fn(*args, **kwargs)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/caching.py", line 64, in __call__
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return self.optimized_call(*args, **kwargs)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return self._wrapped_call(self, *args, **kwargs)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/fx/graph_module.py", line 413, in __call__
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     raise e
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/fx/graph_module.py", line 400, in __call__
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "<eval_with_key>.94", line 393, in forward
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     submod_4 = self.submod_4(getitem_7, s59, s18, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_8, l_self_modules_layers_modules_1_modules_mlp_modules_gate_parameters_weight_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_, l_positions_, s7);  getitem_7 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_8 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_parameters_weight_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 222, in __call__
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return self.runnable(*args, **kwargs)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/piecewise_backend.py", line 191, in __call__
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return range_entry.runnable(*args)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return self._compiled_fn(*args)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return fn(*args, **kwargs)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return compiled_fn(full_args)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     all_outs = call_func_at_runtime_with_args(
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     out = normalize_as_list(f(args))
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]                             ^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     outs = compiled_fn(args)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return compiled_fn(runtime_args)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_inductor/output_code.py", line 613, in __call__
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return self.current_callable(inputs)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_inductor/utils.py", line 3017, in run
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     out = model(new_inputs)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]           ^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/tmp/torchinductor_wuwen/ct/cctdwzzfufdzd76q46mh5ef2a3dt4kas62lnlxc54qvfsslr3v7f.py", line 1504, in call
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     buf13 = torch.ops.vllm.moe_forward_shared.default(buf12, buf11, 'language_model.model.layers.1.mlp.experts')
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_ops.py", line 841, in __call__
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return self._op(*args, **kwargs)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 2131, in moe_forward_shared
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return self.forward_impl(hidden_states, router_logits)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1979, in forward_impl
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     final_hidden_states = self.quant_method.apply(
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]                           ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py", line 1122, in apply
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     result = self.kernel(
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]              ^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/modular_kernel.py", line 1228, in forward
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     fused_out = self._fused_experts(
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]                 ^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/modular_kernel.py", line 1082, in _fused_experts
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     self.fused_experts.apply(
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/fallback.py", line 111, in apply
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     experts.apply(
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/cutlass_moe.py", line 298, in apply
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     run_cutlass_moe_fp8(
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/cutlass_moe.py", line 188, in run_cutlass_moe_fp8
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     ops.cutlass_moe_mm(
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/_custom_ops.py", line 1175, in cutlass_moe_mm
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return torch.ops._C.cutlass_moe_mm(
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_ops.py", line 1255, in __call__
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]     return self._op(*args, **kwargs)
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839] NotImplementedError: No compiled cutlass_scaled_mm for CUDA device capability: 120. Required capability: 90 or 100
[2026-01-18 20:01:06] [ERROR] [0;36m(Worker_TP0 pid=155025)[0;0m ERROR 01-18 20:01:06 [multiproc_executor.py:839]
[2026-01-18 20:01:06] [ERROR] [0;36m(EngineCore_DP0 pid=154466)[0;0m ERROR 01-18 20:01:06 [core.py:935] EngineCore failed to start.
[2026-01-18 20:01:06] [ERROR] [0;36m(EngineCore_DP0 pid=154466)[0;0m ERROR 01-18 20:01:06 [core.py:935] Traceback (most recent call last):
[2026-01-18 20:01:06] [ERROR] [0;36m(EngineCore_DP0 pid=154466)[0;0m ERROR 01-18 20:01:06 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-18 20:01:06] [ERROR] [0;36m(EngineCore_DP0 pid=154466)[0;0m ERROR 01-18 20:01:06 [core.py:935]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 20:01:06] [ERROR] [0;36m(EngineCore_DP0 pid=154466)[0;0m ERROR 01-18 20:01:06 [core.py:935]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(EngineCore_DP0 pid=154466)[0;0m ERROR 01-18 20:01:06 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-18 20:01:06] [ERROR] [0;36m(EngineCore_DP0 pid=154466)[0;0m ERROR 01-18 20:01:06 [core.py:935]     super().__init__(
[2026-01-18 20:01:06] [ERROR] [0;36m(EngineCore_DP0 pid=154466)[0;0m ERROR 01-18 20:01:06 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 112, in __init__
[2026-01-18 20:01:06] [ERROR] [0;36m(EngineCore_DP0 pid=154466)[0;0m ERROR 01-18 20:01:06 [core.py:935]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[2026-01-18 20:01:06] [ERROR] [0;36m(EngineCore_DP0 pid=154466)[0;0m ERROR 01-18 20:01:06 [core.py:935]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(EngineCore_DP0 pid=154466)[0;0m ERROR 01-18 20:01:06 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 242, in _initialize_kv_caches
[2026-01-18 20:01:06] [ERROR] [0;36m(EngineCore_DP0 pid=154466)[0;0m ERROR 01-18 20:01:06 [core.py:935]     available_gpu_memory = self.model_executor.determine_available_memory()
[2026-01-18 20:01:06] [ERROR] [0;36m(EngineCore_DP0 pid=154466)[0;0m ERROR 01-18 20:01:06 [core.py:935]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(EngineCore_DP0 pid=154466)[0;0m ERROR 01-18 20:01:06 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[2026-01-18 20:01:06] [ERROR] [0;36m(EngineCore_DP0 pid=154466)[0;0m ERROR 01-18 20:01:06 [core.py:935]     return self.collective_rpc("determine_available_memory")
[2026-01-18 20:01:06] [ERROR] [0;36m(EngineCore_DP0 pid=154466)[0;0m ERROR 01-18 20:01:06 [core.py:935]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(EngineCore_DP0 pid=154466)[0;0m ERROR 01-18 20:01:06 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 374, in collective_rpc
[2026-01-18 20:01:06] [ERROR] [0;36m(EngineCore_DP0 pid=154466)[0;0m ERROR 01-18 20:01:06 [core.py:935]     return aggregate(get_response())
[2026-01-18 20:01:06] [ERROR] [0;36m(EngineCore_DP0 pid=154466)[0;0m ERROR 01-18 20:01:06 [core.py:935]                      ^^^^^^^^^^^^^^
[2026-01-18 20:01:06] [ERROR] [0;36m(EngineCore_DP0 pid=154466)[0;0m ERROR 01-18 20:01:06 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 357, in get_response
[2026-01-18 20:01:06] [ERROR] [0;36m(EngineCore_DP0 pid=154466)[0;0m ERROR 01-18 20:01:06 [core.py:935]     raise RuntimeError(
[2026-01-18 20:01:06] [ERROR] [0;36m(EngineCore_DP0 pid=154466)[0;0m ERROR 01-18 20:01:06 [core.py:935] RuntimeError: Worker failed with error 'No compiled cutlass_scaled_mm for CUDA device capability: 120. Required capability: 90 or 100', please check the stack trace above for the root cause
[2026-01-18 20:01:09] [ERROR] [0;36m(EngineCore_DP0 pid=154466)[0;0m ERROR 01-18 20:01:09 [multiproc_executor.py:246] Worker proc VllmWorker-1 died unexpectedly, shutting down executor.
[2026-01-18 20:01:09] [INFO] [0;36m(EngineCore_DP0 pid=154466)[0;0m Process EngineCore_DP0:
[2026-01-18 20:01:09] [ERROR] [0;36m(EngineCore_DP0 pid=154466)[0;0m Traceback (most recent call last):
[2026-01-18 20:01:09] [INFO] [0;36m(EngineCore_DP0 pid=154466)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[2026-01-18 20:01:09] [INFO] [0;36m(EngineCore_DP0 pid=154466)[0;0m     self.run()
[2026-01-18 20:01:09] [INFO] [0;36m(EngineCore_DP0 pid=154466)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/process.py", line 108, in run
[2026-01-18 20:01:09] [INFO] [0;36m(EngineCore_DP0 pid=154466)[0;0m     self._target(*self._args, **self._kwargs)
[2026-01-18 20:01:09] [INFO] [0;36m(EngineCore_DP0 pid=154466)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 939, in run_engine_core
[2026-01-18 20:01:09] [INFO] [0;36m(EngineCore_DP0 pid=154466)[0;0m     raise e
[2026-01-18 20:01:09] [INFO] [0;36m(EngineCore_DP0 pid=154466)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-18 20:01:09] [INFO] [0;36m(EngineCore_DP0 pid=154466)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 20:01:09] [INFO] [0;36m(EngineCore_DP0 pid=154466)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:09] [INFO] [0;36m(EngineCore_DP0 pid=154466)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-18 20:01:09] [INFO] [0;36m(EngineCore_DP0 pid=154466)[0;0m     super().__init__(
[2026-01-18 20:01:09] [INFO] [0;36m(EngineCore_DP0 pid=154466)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 112, in __init__
[2026-01-18 20:01:09] [INFO] [0;36m(EngineCore_DP0 pid=154466)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[2026-01-18 20:01:09] [INFO] [0;36m(EngineCore_DP0 pid=154466)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:09] [INFO] [0;36m(EngineCore_DP0 pid=154466)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 242, in _initialize_kv_caches
[2026-01-18 20:01:09] [INFO] [0;36m(EngineCore_DP0 pid=154466)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[2026-01-18 20:01:09] [INFO] [0;36m(EngineCore_DP0 pid=154466)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:09] [INFO] [0;36m(EngineCore_DP0 pid=154466)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[2026-01-18 20:01:09] [INFO] [0;36m(EngineCore_DP0 pid=154466)[0;0m     return self.collective_rpc("determine_available_memory")
[2026-01-18 20:01:09] [INFO] [0;36m(EngineCore_DP0 pid=154466)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:09] [INFO] [0;36m(EngineCore_DP0 pid=154466)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 374, in collective_rpc
[2026-01-18 20:01:09] [INFO] [0;36m(EngineCore_DP0 pid=154466)[0;0m     return aggregate(get_response())
[2026-01-18 20:01:09] [INFO] [0;36m(EngineCore_DP0 pid=154466)[0;0m                      ^^^^^^^^^^^^^^
[2026-01-18 20:01:09] [INFO] [0;36m(EngineCore_DP0 pid=154466)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 357, in get_response
[2026-01-18 20:01:09] [INFO] [0;36m(EngineCore_DP0 pid=154466)[0;0m     raise RuntimeError(
[2026-01-18 20:01:09] [INFO] [0;36m(EngineCore_DP0 pid=154466)[0;0m RuntimeError: Worker failed with error 'No compiled cutlass_scaled_mm for CUDA device capability: 120. Required capability: 90 or 100', please check the stack trace above for the root cause
[2026-01-18 20:01:11] [ERROR] [0;36m(APIServer pid=153183)[0;0m Traceback (most recent call last):
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/bin/vllm", line 7, in <module>
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m     sys.exit(main())
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m              ^^^^^^
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m     args.dispatch_function(args)
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m     uvloop.run(run_server(args))
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m     return __asyncio.run(
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m            ^^^^^^^^^^^^^^
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m     return runner.run(main)
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m     return self._loop.run_until_complete(task)
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m     return await main
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m            ^^^^^^^^^^
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m     async with build_async_engine_client(
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m     return await anext(self.gen)
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 146, in build_async_engine_client
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m     return await anext(self.gen)
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 187, in build_async_engine_client_from_engine_args
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 202, in from_vllm_config
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m     return cls(
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m            ^^^^
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 129, in __init__
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m     return AsyncMPClient(*client_args)
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m     super().__init__(
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 144, in __exit__
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m     next(self.gen)
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 918, in launch_core_engines
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m     wait_for_engine_startup(
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 977, in wait_for_engine_startup
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m     raise RuntimeError(
[2026-01-18 20:01:11] [INFO] [0;36m(APIServer pid=153183)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[2026-01-18 20:01:12] [INFO] nvitop监控已启动
[2026-01-18 20:03:09] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 20:03:09] [INFO] nvitop监控已启动
[2026-01-18 20:03:09] [INFO] nvitop监控已停止
[2026-01-18 20:03:37] [INFO] [0;36m(APIServer pid=160461)[0;0m INFO 01-18 20:03:37 [api_server.py:872] vLLM API server version 0.14.0rc2.dev117+g9e078d058
[2026-01-18 20:03:37] [INFO] [0;36m(APIServer pid=160461)[0;0m INFO 01-18 20:03:37 [utils.py:267] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8/chat_template.jinja', 'enable_auto_tool_choice': True, 'tool_call_parser': 'glm45', 'model': '/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8', 'trust_remote_code': True, 'allowed_local_media_path': '/', 'max_model_len': 128000, 'served_model_name': ['VLLM-MODEL'], 'reasoning_parser': 'glm45', 'tensor_parallel_size': 2, 'mm_processor_cache_type': 'shm', 'mm_encoder_tp_mode': 'data', 'max_num_seqs': 256, 'async_scheduling': True}
[2026-01-18 20:03:37] [INFO] [0;36m(APIServer pid=160461)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 20:03:37] [INFO] [0;36m(APIServer pid=160461)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 20:03:37] [INFO] [0;36m(APIServer pid=160461)[0;0m Unrecognized keys in `rope_parameters` for 'rope_type'='default': {'mrope_section'}
[2026-01-18 20:03:37] [INFO] [0;36m(APIServer pid=160461)[0;0m Unrecognized keys in `rope_parameters` for 'rope_type'='default': {'mrope_section', 'partial_rotary_factor'}
[2026-01-18 20:03:37] [INFO] [0;36m(APIServer pid=160461)[0;0m Unrecognized keys in `rope_parameters` for 'rope_type'='default': {'mrope_section', 'partial_rotary_factor'}
[2026-01-18 20:03:37] [INFO] [0;36m(APIServer pid=160461)[0;0m INFO 01-18 20:03:37 [model.py:530] Resolved architecture: Glm4vMoeForConditionalGeneration
[2026-01-18 20:03:37] [INFO] [0;36m(APIServer pid=160461)[0;0m INFO 01-18 20:03:37 [model.py:1547] Using max model len 128000
[2026-01-18 20:03:37] [WARNING] [0;36m(APIServer pid=160461)[0;0m WARNING 01-18 20:03:37 [quark_ocp_mx.py:157] AITER is not found or QuarkOCP_MX is not supported on the current platform. QuarkOCP_MX quantization will not be available.
[2026-01-18 20:03:39] [INFO] [0;36m(APIServer pid=160461)[0;0m INFO 01-18 20:03:39 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[2026-01-18 20:03:39] [INFO] [0;36m(APIServer pid=160461)[0;0m INFO 01-18 20:03:39 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 20:03:39] [INFO] [0;36m(APIServer pid=160461)[0;0m INFO 01-18 20:03:39 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-18 20:04:10] [INFO] [0;36m(EngineCore_DP0 pid=161122)[0;0m INFO 01-18 20:04:10 [core.py:96] Initializing a V1 LLM engine (v0.14.0rc2.dev117+g9e078d058) with config: model='/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8', speculative_config=None, tokenizer='/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='glm45', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-18 20:04:10] [WARNING] [0;36m(EngineCore_DP0 pid=161122)[0;0m WARNING 01-18 20:04:10 [multiproc_executor.py:897] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-18 20:04:41] [INFO] INFO 01-18 20:04:41 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:49547 backend=nccl
[2026-01-18 20:04:42] [INFO] INFO 01-18 20:04:42 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:49547 backend=nccl
[2026-01-18 20:04:42] [INFO] INFO 01-18 20:04:42 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-18 20:04:42] [WARNING] WARNING 01-18 20:04:42 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 20:04:42] [WARNING] WARNING 01-18 20:04:42 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 20:04:42] [INFO] INFO 01-18 20:04:42 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[2026-01-18 20:04:42] [INFO] INFO 01-18 20:04:42 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
[2026-01-18 20:04:43] [INFO] Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[2026-01-18 20:04:43] [INFO] Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[2026-01-18 20:05:04] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m INFO 01-18 20:05:04 [gpu_model_runner.py:3803] Starting to load model /mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8...
[2026-01-18 20:05:04] [WARNING] [0;36m(Worker_TP1 pid=161744)[0;0m WARNING 01-18 20:05:04 [compressed_tensors.py:738] Acceleration for non-quantized schemes is not supported by Compressed Tensors. Falling back to UnquantizedLinearMethod
[2026-01-18 20:05:04] [INFO] [0;36m(Worker_TP1 pid=161744)[0;0m INFO 01-18 20:05:04 [mm_encoder_attention.py:86] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[2026-01-18 20:05:04] [INFO] [0;36m(Worker_TP1 pid=161744)[0;0m INFO 01-18 20:05:04 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 20:05:04] [WARNING] [0;36m(Worker_TP0 pid=161743)[0;0m WARNING 01-18 20:05:04 [compressed_tensors.py:738] Acceleration for non-quantized schemes is not supported by Compressed Tensors. Falling back to UnquantizedLinearMethod
[2026-01-18 20:05:04] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m INFO 01-18 20:05:04 [mm_encoder_attention.py:86] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[2026-01-18 20:05:04] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m INFO 01-18 20:05:04 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 20:05:09] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m INFO 01-18 20:05:09 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[2026-01-18 20:05:09] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m INFO 01-18 20:05:09 [fp8.py:126] DeepGEMM is disabled because the platform does not support it.
[2026-01-18 20:05:09] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m INFO 01-18 20:05:09 [fp8.py:150] Using vLLM CUTLASS backend for FP8 MoE
[2026-01-18 20:05:09] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m
[2026-01-18 20:05:09] [INFO] Loading safetensors checkpoint shards:   0% Completed | 0/41 [00:00<?, ?it/s]
[2026-01-18 20:05:10] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m
[2026-01-18 20:05:10] [INFO] Loading safetensors checkpoint shards:   2% Completed | 1/41 [00:00<00:13,  2.97it/s]
[2026-01-18 20:05:10] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m
[2026-01-18 20:05:10] [INFO] Loading safetensors checkpoint shards:   5% Completed | 2/41 [00:00<00:14,  2.74it/s]
[2026-01-18 20:05:10] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m
[2026-01-18 20:05:10] [INFO] Loading safetensors checkpoint shards:   7% Completed | 3/41 [00:01<00:13,  2.83it/s]
[2026-01-18 20:05:11] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m
[2026-01-18 20:05:11] [INFO] Loading safetensors checkpoint shards:  10% Completed | 4/41 [00:01<00:13,  2.67it/s]
[2026-01-18 20:05:11] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m
[2026-01-18 20:05:11] [INFO] Loading safetensors checkpoint shards:  12% Completed | 5/41 [00:01<00:13,  2.67it/s]
[2026-01-18 20:05:11] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m
[2026-01-18 20:05:11] [INFO] Loading safetensors checkpoint shards:  15% Completed | 6/41 [00:02<00:13,  2.65it/s]
[2026-01-18 20:05:12] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m
[2026-01-18 20:05:12] [INFO] Loading safetensors checkpoint shards:  17% Completed | 7/41 [00:02<00:12,  2.70it/s]
[2026-01-18 20:05:12] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m
[2026-01-18 20:05:12] [INFO] Loading safetensors checkpoint shards:  20% Completed | 8/41 [00:02<00:11,  2.75it/s]
[2026-01-18 20:05:13] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m
[2026-01-18 20:05:13] [INFO] Loading safetensors checkpoint shards:  22% Completed | 9/41 [00:03<00:11,  2.69it/s]
[2026-01-18 20:05:13] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m
[2026-01-18 20:05:13] [INFO] Loading safetensors checkpoint shards:  24% Completed | 10/41 [00:03<00:11,  2.65it/s]
[2026-01-18 20:05:13] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m
[2026-01-18 20:05:13] [INFO] Loading safetensors checkpoint shards:  27% Completed | 11/41 [00:04<00:11,  2.61it/s]
[2026-01-18 20:05:14] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m
[2026-01-18 20:05:14] [INFO] Loading safetensors checkpoint shards:  29% Completed | 12/41 [00:04<00:10,  2.65it/s]
[2026-01-18 20:05:14] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m
[2026-01-18 20:05:14] [INFO] Loading safetensors checkpoint shards:  32% Completed | 13/41 [00:04<00:10,  2.71it/s]
[2026-01-18 20:05:14] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m
[2026-01-18 20:05:14] [INFO] Loading safetensors checkpoint shards:  34% Completed | 14/41 [00:05<00:09,  2.75it/s]
[2026-01-18 20:05:15] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m
[2026-01-18 20:05:15] [INFO] Loading safetensors checkpoint shards:  37% Completed | 15/41 [00:05<00:09,  2.76it/s]
[2026-01-18 20:05:15] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m
[2026-01-18 20:05:15] [INFO] Loading safetensors checkpoint shards:  39% Completed | 16/41 [00:05<00:09,  2.71it/s]
[2026-01-18 20:05:16] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m
[2026-01-18 20:05:16] [INFO] Loading safetensors checkpoint shards:  41% Completed | 17/41 [00:06<00:09,  2.61it/s]
[2026-01-18 20:05:16] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m
[2026-01-18 20:05:16] [INFO] Loading safetensors checkpoint shards:  44% Completed | 18/41 [00:06<00:08,  2.67it/s]
[2026-01-18 20:05:16] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m
[2026-01-18 20:05:16] [INFO] Loading safetensors checkpoint shards:  46% Completed | 19/41 [00:07<00:08,  2.74it/s]
[2026-01-18 20:05:17] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m
[2026-01-18 20:05:17] [INFO] Loading safetensors checkpoint shards:  49% Completed | 20/41 [00:07<00:07,  2.77it/s]
[2026-01-18 20:05:17] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m
[2026-01-18 20:05:17] [INFO] Loading safetensors checkpoint shards:  51% Completed | 21/41 [00:07<00:07,  2.80it/s]
[2026-01-18 20:05:17] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m
[2026-01-18 20:05:17] [INFO] Loading safetensors checkpoint shards:  54% Completed | 22/41 [00:08<00:06,  2.80it/s]
[2026-01-18 20:05:18] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m
[2026-01-18 20:05:18] [INFO] Loading safetensors checkpoint shards:  56% Completed | 23/41 [00:08<00:06,  2.74it/s]
[2026-01-18 20:05:18] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m
[2026-01-18 20:05:18] [INFO] Loading safetensors checkpoint shards:  59% Completed | 24/41 [00:08<00:06,  2.71it/s]
[2026-01-18 20:05:18] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m
[2026-01-18 20:05:18] [INFO] Loading safetensors checkpoint shards:  61% Completed | 25/41 [00:09<00:05,  2.75it/s]
[2026-01-18 20:05:19] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m
[2026-01-18 20:05:19] [INFO] Loading safetensors checkpoint shards:  63% Completed | 26/41 [00:09<00:05,  2.79it/s]
[2026-01-18 20:05:19] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m
[2026-01-18 20:05:19] [INFO] Loading safetensors checkpoint shards:  66% Completed | 27/41 [00:09<00:05,  2.77it/s]
[2026-01-18 20:05:20] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m
[2026-01-18 20:05:20] [INFO] Loading safetensors checkpoint shards:  68% Completed | 28/41 [00:10<00:04,  2.77it/s]
[2026-01-18 20:05:20] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m
[2026-01-18 20:05:20] [INFO] Loading safetensors checkpoint shards:  71% Completed | 29/41 [00:10<00:04,  2.73it/s]
[2026-01-18 20:05:20] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m
[2026-01-18 20:05:20] [INFO] Loading safetensors checkpoint shards:  73% Completed | 30/41 [00:11<00:04,  2.69it/s]
[2026-01-18 20:05:21] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m
[2026-01-18 20:05:21] [INFO] Loading safetensors checkpoint shards:  76% Completed | 31/41 [00:11<00:03,  2.72it/s]
[2026-01-18 20:05:21] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m
[2026-01-18 20:05:21] [INFO] Loading safetensors checkpoint shards:  78% Completed | 32/41 [00:11<00:03,  2.77it/s]
[2026-01-18 20:05:21] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m
[2026-01-18 20:05:21] [INFO] Loading safetensors checkpoint shards:  80% Completed | 33/41 [00:12<00:02,  2.79it/s]
[2026-01-18 20:05:22] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m
[2026-01-18 20:05:22] [INFO] Loading safetensors checkpoint shards:  83% Completed | 34/41 [00:12<00:02,  2.82it/s]
[2026-01-18 20:05:22] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m
[2026-01-18 20:05:22] [INFO] Loading safetensors checkpoint shards:  85% Completed | 35/41 [00:12<00:02,  2.80it/s]
[2026-01-18 20:05:22] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m
[2026-01-18 20:05:22] [INFO] Loading safetensors checkpoint shards:  88% Completed | 36/41 [00:13<00:01,  2.74it/s]
[2026-01-18 20:05:23] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m
[2026-01-18 20:05:23] [INFO] Loading safetensors checkpoint shards:  90% Completed | 37/41 [00:13<00:01,  2.71it/s]
[2026-01-18 20:05:23] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m
[2026-01-18 20:05:23] [INFO] Loading safetensors checkpoint shards:  93% Completed | 38/41 [00:13<00:01,  2.75it/s]
[2026-01-18 20:05:24] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m
[2026-01-18 20:05:24] [INFO] Loading safetensors checkpoint shards:  95% Completed | 39/41 [00:14<00:00,  2.78it/s]
[2026-01-18 20:05:24] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m
[2026-01-18 20:05:24] [INFO] Loading safetensors checkpoint shards:  98% Completed | 40/41 [00:14<00:00,  2.65it/s]
[2026-01-18 20:05:24] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m
[2026-01-18 20:05:24] [INFO] Loading safetensors checkpoint shards: 100% Completed | 41/41 [00:14<00:00,  3.10it/s]
[2026-01-18 20:05:24] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m
[2026-01-18 20:05:24] [INFO] Loading safetensors checkpoint shards: 100% Completed | 41/41 [00:14<00:00,  2.76it/s]
[2026-01-18 20:05:24] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m
[2026-01-18 20:05:24] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m INFO 01-18 20:05:24 [default_loader.py:291] Loading weights took 14.91 seconds
[2026-01-18 20:05:25] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m INFO 01-18 20:05:25 [gpu_model_runner.py:3900] Model loading took 52.22 GiB memory and 19.931516 seconds
[2026-01-18 20:05:25] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m INFO 01-18 20:05:25 [gpu_model_runner.py:4711] Encoder cache will be initialized with a budget of 30000 tokens, and profiled with 1 video items of the maximum feature size.
[2026-01-18 20:05:25] [INFO] [0;36m(Worker_TP1 pid=161744)[0;0m INFO 01-18 20:05:25 [gpu_model_runner.py:4711] Encoder cache will be initialized with a budget of 30000 tokens, and profiled with 1 video items of the maximum feature size.
[2026-01-18 20:05:42] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m INFO 01-18 20:05:42 [backends.py:644] Using cache directory: /home/wuwen/.cache/vllm/torch_compile_cache/0ad3b19624/rank_0_0/backbone for vLLM's torch.compile
[2026-01-18 20:05:42] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m INFO 01-18 20:05:42 [backends.py:704] Dynamo bytecode transform time: 12.69 s
[2026-01-18 20:05:54] [INFO] [0;36m(Worker_TP0 pid=161743)[0;0m INFO 01-18 20:05:54 [backends.py:261] Cache the graph of compile range (1, 8192) for later use
[2026-01-18 20:05:55] [INFO] [0;36m(Worker_TP1 pid=161744)[0;0m INFO 01-18 20:05:55 [backends.py:261] Cache the graph of compile range (1, 8192) for later use
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839] WorkerProc hit an exception.
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839] Traceback (most recent call last):
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 834, in worker_busy_loop
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     output = func(*args, **kwargs)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]              ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return func(*args, **kwargs)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 321, in determine_available_memory
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     self.model_runner.profile_run()
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4739, in profile_run
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     hidden_states, last_hidden_states = self._dummy_run(
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]                                         ^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return func(*args, **kwargs)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4457, in _dummy_run
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     outputs = self.model(
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]               ^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 222, in __call__
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return self.runnable(*args, **kwargs)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/glm4_1v.py", line 1764, in forward
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     hidden_states = self.language_model.model(
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 557, in __call__
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)  # type: ignore[arg-type]
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 228, in __call__
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return self._call_with_optional_nvtx_range(
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 119, in _call_with_optional_nvtx_range
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return callable_fn(*args, **kwargs)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return fn(*args, **kwargs)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/glm4_moe.py", line 452, in forward
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     def forward(
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return fn(*args, **kwargs)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/caching.py", line 64, in __call__
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return self.optimized_call(*args, **kwargs)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return self._wrapped_call(self, *args, **kwargs)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/fx/graph_module.py", line 413, in __call__
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     raise e
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/fx/graph_module.py", line 400, in __call__
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "<eval_with_key>.94", line 393, in forward
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     submod_4 = self.submod_4(getitem_7, s59, s18, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_8, l_self_modules_layers_modules_1_modules_mlp_modules_gate_parameters_weight_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_, l_positions_, s7);  getitem_7 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_8 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_parameters_weight_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 222, in __call__
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return self.runnable(*args, **kwargs)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/piecewise_backend.py", line 191, in __call__
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return range_entry.runnable(*args)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return self._compiled_fn(*args)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return fn(*args, **kwargs)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return compiled_fn(full_args)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     all_outs = call_func_at_runtime_with_args(
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     out = normalize_as_list(f(args))
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]                             ^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return compiled_fn(runtime_args)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_inductor/output_code.py", line 613, in __call__
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return self.current_callable(inputs)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_inductor/utils.py", line 3017, in run
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     out = model(new_inputs)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]           ^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/tmp/torchinductor_wuwen/ct/cctdwzzfufdzd76q46mh5ef2a3dt4kas62lnlxc54qvfsslr3v7f.py", line 1504, in call
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     buf13 = torch.ops.vllm.moe_forward_shared.default(buf12, buf11, 'language_model.model.layers.1.mlp.experts')
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_ops.py", line 841, in __call__
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return self._op(*args, **kwargs)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 2131, in moe_forward_shared
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return self.forward_impl(hidden_states, router_logits)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1979, in forward_impl
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     final_hidden_states = self.quant_method.apply(
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]                           ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py", line 1122, in apply
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     result = self.kernel(
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]              ^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/modular_kernel.py", line 1228, in forward
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     fused_out = self._fused_experts(
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]                 ^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/modular_kernel.py", line 1082, in _fused_experts
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     self.fused_experts.apply(
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/fallback.py", line 111, in apply
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     experts.apply(
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/cutlass_moe.py", line 298, in apply
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     run_cutlass_moe_fp8(
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/cutlass_moe.py", line 188, in run_cutlass_moe_fp8
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     ops.cutlass_moe_mm(
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/_custom_ops.py", line 1175, in cutlass_moe_mm
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return torch.ops._C.cutlass_moe_mm(
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_ops.py", line 1255, in __call__
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return self._op(*args, **kwargs)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839] NotImplementedError: No compiled cutlass_scaled_mm for CUDA device capability: 120. Required capability: 90 or 100
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839] Traceback (most recent call last):
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 834, in worker_busy_loop
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     output = func(*args, **kwargs)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]              ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return func(*args, **kwargs)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 321, in determine_available_memory
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     self.model_runner.profile_run()
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4739, in profile_run
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     hidden_states, last_hidden_states = self._dummy_run(
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]                                         ^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return func(*args, **kwargs)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4457, in _dummy_run
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     outputs = self.model(
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]               ^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 222, in __call__
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return self.runnable(*args, **kwargs)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/glm4_1v.py", line 1764, in forward
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     hidden_states = self.language_model.model(
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 557, in __call__
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)  # type: ignore[arg-type]
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 228, in __call__
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return self._call_with_optional_nvtx_range(
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 119, in _call_with_optional_nvtx_range
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return callable_fn(*args, **kwargs)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return fn(*args, **kwargs)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/glm4_moe.py", line 452, in forward
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     def forward(
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return fn(*args, **kwargs)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/caching.py", line 64, in __call__
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return self.optimized_call(*args, **kwargs)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return self._wrapped_call(self, *args, **kwargs)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/fx/graph_module.py", line 413, in __call__
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     raise e
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/fx/graph_module.py", line 400, in __call__
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "<eval_with_key>.94", line 393, in forward
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     submod_4 = self.submod_4(getitem_7, s59, s18, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_8, l_self_modules_layers_modules_1_modules_mlp_modules_gate_parameters_weight_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_, l_positions_, s7);  getitem_7 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_8 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_parameters_weight_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 222, in __call__
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return self.runnable(*args, **kwargs)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/piecewise_backend.py", line 191, in __call__
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return range_entry.runnable(*args)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return self._compiled_fn(*args)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return fn(*args, **kwargs)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return compiled_fn(full_args)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     all_outs = call_func_at_runtime_with_args(
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     out = normalize_as_list(f(args))
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]                             ^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return compiled_fn(runtime_args)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_inductor/output_code.py", line 613, in __call__
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return self.current_callable(inputs)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_inductor/utils.py", line 3017, in run
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     out = model(new_inputs)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]           ^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/tmp/torchinductor_wuwen/ct/cctdwzzfufdzd76q46mh5ef2a3dt4kas62lnlxc54qvfsslr3v7f.py", line 1504, in call
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     buf13 = torch.ops.vllm.moe_forward_shared.default(buf12, buf11, 'language_model.model.layers.1.mlp.experts')
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_ops.py", line 841, in __call__
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return self._op(*args, **kwargs)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 2131, in moe_forward_shared
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return self.forward_impl(hidden_states, router_logits)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1979, in forward_impl
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     final_hidden_states = self.quant_method.apply(
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]                           ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py", line 1122, in apply
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     result = self.kernel(
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]              ^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/modular_kernel.py", line 1228, in forward
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     fused_out = self._fused_experts(
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]                 ^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/modular_kernel.py", line 1082, in _fused_experts
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     self.fused_experts.apply(
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/fallback.py", line 111, in apply
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     experts.apply(
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/cutlass_moe.py", line 298, in apply
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     run_cutlass_moe_fp8(
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/cutlass_moe.py", line 188, in run_cutlass_moe_fp8
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     ops.cutlass_moe_mm(
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/_custom_ops.py", line 1175, in cutlass_moe_mm
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return torch.ops._C.cutlass_moe_mm(
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_ops.py", line 1255, in __call__
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]     return self._op(*args, **kwargs)
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839] NotImplementedError: No compiled cutlass_scaled_mm for CUDA device capability: 120. Required capability: 90 or 100
[2026-01-18 20:05:56] [ERROR] [0;36m(Worker_TP0 pid=161743)[0;0m ERROR 01-18 20:05:56 [multiproc_executor.py:839]
[2026-01-18 20:05:56] [ERROR] [0;36m(EngineCore_DP0 pid=161122)[0;0m ERROR 01-18 20:05:56 [core.py:935] EngineCore failed to start.
[2026-01-18 20:05:56] [ERROR] [0;36m(EngineCore_DP0 pid=161122)[0;0m ERROR 01-18 20:05:56 [core.py:935] Traceback (most recent call last):
[2026-01-18 20:05:56] [ERROR] [0;36m(EngineCore_DP0 pid=161122)[0;0m ERROR 01-18 20:05:56 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-18 20:05:56] [ERROR] [0;36m(EngineCore_DP0 pid=161122)[0;0m ERROR 01-18 20:05:56 [core.py:935]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 20:05:56] [ERROR] [0;36m(EngineCore_DP0 pid=161122)[0;0m ERROR 01-18 20:05:56 [core.py:935]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(EngineCore_DP0 pid=161122)[0;0m ERROR 01-18 20:05:56 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-18 20:05:56] [ERROR] [0;36m(EngineCore_DP0 pid=161122)[0;0m ERROR 01-18 20:05:56 [core.py:935]     super().__init__(
[2026-01-18 20:05:56] [ERROR] [0;36m(EngineCore_DP0 pid=161122)[0;0m ERROR 01-18 20:05:56 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 112, in __init__
[2026-01-18 20:05:56] [ERROR] [0;36m(EngineCore_DP0 pid=161122)[0;0m ERROR 01-18 20:05:56 [core.py:935]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[2026-01-18 20:05:56] [ERROR] [0;36m(EngineCore_DP0 pid=161122)[0;0m ERROR 01-18 20:05:56 [core.py:935]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(EngineCore_DP0 pid=161122)[0;0m ERROR 01-18 20:05:56 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 242, in _initialize_kv_caches
[2026-01-18 20:05:56] [ERROR] [0;36m(EngineCore_DP0 pid=161122)[0;0m ERROR 01-18 20:05:56 [core.py:935]     available_gpu_memory = self.model_executor.determine_available_memory()
[2026-01-18 20:05:56] [ERROR] [0;36m(EngineCore_DP0 pid=161122)[0;0m ERROR 01-18 20:05:56 [core.py:935]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(EngineCore_DP0 pid=161122)[0;0m ERROR 01-18 20:05:56 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[2026-01-18 20:05:56] [ERROR] [0;36m(EngineCore_DP0 pid=161122)[0;0m ERROR 01-18 20:05:56 [core.py:935]     return self.collective_rpc("determine_available_memory")
[2026-01-18 20:05:56] [ERROR] [0;36m(EngineCore_DP0 pid=161122)[0;0m ERROR 01-18 20:05:56 [core.py:935]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(EngineCore_DP0 pid=161122)[0;0m ERROR 01-18 20:05:56 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 374, in collective_rpc
[2026-01-18 20:05:56] [ERROR] [0;36m(EngineCore_DP0 pid=161122)[0;0m ERROR 01-18 20:05:56 [core.py:935]     return aggregate(get_response())
[2026-01-18 20:05:56] [ERROR] [0;36m(EngineCore_DP0 pid=161122)[0;0m ERROR 01-18 20:05:56 [core.py:935]                      ^^^^^^^^^^^^^^
[2026-01-18 20:05:56] [ERROR] [0;36m(EngineCore_DP0 pid=161122)[0;0m ERROR 01-18 20:05:56 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 357, in get_response
[2026-01-18 20:05:56] [ERROR] [0;36m(EngineCore_DP0 pid=161122)[0;0m ERROR 01-18 20:05:56 [core.py:935]     raise RuntimeError(
[2026-01-18 20:05:56] [ERROR] [0;36m(EngineCore_DP0 pid=161122)[0;0m ERROR 01-18 20:05:56 [core.py:935] RuntimeError: Worker failed with error 'No compiled cutlass_scaled_mm for CUDA device capability: 120. Required capability: 90 or 100', please check the stack trace above for the root cause
[2026-01-18 20:05:58] [ERROR] [0;36m(EngineCore_DP0 pid=161122)[0;0m ERROR 01-18 20:05:58 [multiproc_executor.py:246] Worker proc VllmWorker-1 died unexpectedly, shutting down executor.
[2026-01-18 20:05:59] [INFO] [0;36m(EngineCore_DP0 pid=161122)[0;0m Process EngineCore_DP0:
[2026-01-18 20:05:59] [ERROR] [0;36m(EngineCore_DP0 pid=161122)[0;0m Traceback (most recent call last):
[2026-01-18 20:05:59] [INFO] [0;36m(EngineCore_DP0 pid=161122)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[2026-01-18 20:05:59] [INFO] [0;36m(EngineCore_DP0 pid=161122)[0;0m     self.run()
[2026-01-18 20:05:59] [INFO] [0;36m(EngineCore_DP0 pid=161122)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/process.py", line 108, in run
[2026-01-18 20:05:59] [INFO] [0;36m(EngineCore_DP0 pid=161122)[0;0m     self._target(*self._args, **self._kwargs)
[2026-01-18 20:05:59] [INFO] [0;36m(EngineCore_DP0 pid=161122)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 939, in run_engine_core
[2026-01-18 20:05:59] [INFO] [0;36m(EngineCore_DP0 pid=161122)[0;0m     raise e
[2026-01-18 20:05:59] [INFO] [0;36m(EngineCore_DP0 pid=161122)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-18 20:05:59] [INFO] [0;36m(EngineCore_DP0 pid=161122)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 20:05:59] [INFO] [0;36m(EngineCore_DP0 pid=161122)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:59] [INFO] [0;36m(EngineCore_DP0 pid=161122)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-18 20:05:59] [INFO] [0;36m(EngineCore_DP0 pid=161122)[0;0m     super().__init__(
[2026-01-18 20:05:59] [INFO] [0;36m(EngineCore_DP0 pid=161122)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 112, in __init__
[2026-01-18 20:05:59] [INFO] [0;36m(EngineCore_DP0 pid=161122)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[2026-01-18 20:05:59] [INFO] [0;36m(EngineCore_DP0 pid=161122)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:59] [INFO] [0;36m(EngineCore_DP0 pid=161122)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 242, in _initialize_kv_caches
[2026-01-18 20:05:59] [INFO] [0;36m(EngineCore_DP0 pid=161122)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[2026-01-18 20:05:59] [INFO] [0;36m(EngineCore_DP0 pid=161122)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:59] [INFO] [0;36m(EngineCore_DP0 pid=161122)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[2026-01-18 20:05:59] [INFO] [0;36m(EngineCore_DP0 pid=161122)[0;0m     return self.collective_rpc("determine_available_memory")
[2026-01-18 20:05:59] [INFO] [0;36m(EngineCore_DP0 pid=161122)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:05:59] [INFO] [0;36m(EngineCore_DP0 pid=161122)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 374, in collective_rpc
[2026-01-18 20:05:59] [INFO] [0;36m(EngineCore_DP0 pid=161122)[0;0m     return aggregate(get_response())
[2026-01-18 20:05:59] [INFO] [0;36m(EngineCore_DP0 pid=161122)[0;0m                      ^^^^^^^^^^^^^^
[2026-01-18 20:05:59] [INFO] [0;36m(EngineCore_DP0 pid=161122)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 357, in get_response
[2026-01-18 20:05:59] [INFO] [0;36m(EngineCore_DP0 pid=161122)[0;0m     raise RuntimeError(
[2026-01-18 20:05:59] [INFO] [0;36m(EngineCore_DP0 pid=161122)[0;0m RuntimeError: Worker failed with error 'No compiled cutlass_scaled_mm for CUDA device capability: 120. Required capability: 90 or 100', please check the stack trace above for the root cause
[2026-01-18 20:06:00] [ERROR] [0;36m(APIServer pid=160461)[0;0m Traceback (most recent call last):
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/bin/vllm", line 7, in <module>
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m     sys.exit(main())
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m              ^^^^^^
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m     args.dispatch_function(args)
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m     uvloop.run(run_server(args))
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m     return __asyncio.run(
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m            ^^^^^^^^^^^^^^
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m     return runner.run(main)
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m     return self._loop.run_until_complete(task)
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m     return await main
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m            ^^^^^^^^^^
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m     async with build_async_engine_client(
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m     return await anext(self.gen)
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 146, in build_async_engine_client
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m     return await anext(self.gen)
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 187, in build_async_engine_client_from_engine_args
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 202, in from_vllm_config
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m     return cls(
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m            ^^^^
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 129, in __init__
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m     return AsyncMPClient(*client_args)
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m     super().__init__(
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 144, in __exit__
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m     next(self.gen)
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 918, in launch_core_engines
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m     wait_for_engine_startup(
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 977, in wait_for_engine_startup
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m     raise RuntimeError(
[2026-01-18 20:06:00] [INFO] [0;36m(APIServer pid=160461)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[2026-01-18 20:06:04] [INFO] nvitop监控已启动
[2026-01-18 20:10:09] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 20:10:09] [INFO] nvitop监控已启动
[2026-01-18 20:10:09] [INFO] nvitop监控已停止
[2026-01-18 20:10:38] [INFO] usage: vllm [-h] [-v] {chat,complete,serve,bench,collect-env,run-batch} ...
[2026-01-18 20:10:38] [INFO] vllm: error: unrecognized arguments: --torch-backend auto
[2026-01-18 20:10:39] [INFO] nvitop监控已启动
[2026-01-18 20:11:59] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 20:11:59] [INFO] nvitop监控已启动
[2026-01-18 20:11:59] [INFO] nvitop监控已停止
[2026-01-18 20:12:28] [INFO] [0;36m(APIServer pid=169705)[0;0m INFO 01-18 20:12:28 [api_server.py:872] vLLM API server version 0.14.0rc2.dev117+g9e078d058
[2026-01-18 20:12:28] [INFO] [0;36m(APIServer pid=169705)[0;0m INFO 01-18 20:12:28 [utils.py:267] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8/chat_template.jinja', 'enable_auto_tool_choice': True, 'tool_call_parser': 'glm45', 'model': '/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8', 'trust_remote_code': True, 'max_model_len': 128000, 'served_model_name': ['VLLM-MODEL'], 'reasoning_parser': 'glm45', 'tensor_parallel_size': 2, 'max_num_seqs': 256, 'async_scheduling': True}
[2026-01-18 20:12:28] [INFO] [0;36m(APIServer pid=169705)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 20:12:28] [INFO] [0;36m(APIServer pid=169705)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 20:12:28] [INFO] [0;36m(APIServer pid=169705)[0;0m Unrecognized keys in `rope_parameters` for 'rope_type'='default': {'mrope_section'}
[2026-01-18 20:12:28] [INFO] [0;36m(APIServer pid=169705)[0;0m Unrecognized keys in `rope_parameters` for 'rope_type'='default': {'mrope_section', 'partial_rotary_factor'}
[2026-01-18 20:12:28] [INFO] [0;36m(APIServer pid=169705)[0;0m Unrecognized keys in `rope_parameters` for 'rope_type'='default': {'mrope_section', 'partial_rotary_factor'}
[2026-01-18 20:12:28] [INFO] [0;36m(APIServer pid=169705)[0;0m INFO 01-18 20:12:28 [model.py:530] Resolved architecture: Glm4vMoeForConditionalGeneration
[2026-01-18 20:12:28] [INFO] [0;36m(APIServer pid=169705)[0;0m INFO 01-18 20:12:28 [model.py:1547] Using max model len 128000
[2026-01-18 20:12:28] [WARNING] [0;36m(APIServer pid=169705)[0;0m WARNING 01-18 20:12:28 [quark_ocp_mx.py:157] AITER is not found or QuarkOCP_MX is not supported on the current platform. QuarkOCP_MX quantization will not be available.
[2026-01-18 20:12:29] [INFO] [0;36m(APIServer pid=169705)[0;0m INFO 01-18 20:12:29 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[2026-01-18 20:12:29] [INFO] [0;36m(APIServer pid=169705)[0;0m INFO 01-18 20:12:29 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 20:12:29] [INFO] [0;36m(APIServer pid=169705)[0;0m INFO 01-18 20:12:29 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-18 20:12:59] [INFO] [0;36m(EngineCore_DP0 pid=170331)[0;0m INFO 01-18 20:12:59 [core.py:96] Initializing a V1 LLM engine (v0.14.0rc2.dev117+g9e078d058) with config: model='/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8', speculative_config=None, tokenizer='/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='glm45', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-18 20:12:59] [WARNING] [0;36m(EngineCore_DP0 pid=170331)[0;0m WARNING 01-18 20:12:59 [multiproc_executor.py:897] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-18 20:13:30] [INFO] INFO 01-18 20:13:30 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:58225 backend=nccl
[2026-01-18 20:13:30] [INFO] INFO 01-18 20:13:30 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:58225 backend=nccl
[2026-01-18 20:13:30] [INFO] INFO 01-18 20:13:30 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-18 20:13:31] [WARNING] WARNING 01-18 20:13:31 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 20:13:31] [WARNING] WARNING 01-18 20:13:31 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 20:13:31] [INFO] INFO 01-18 20:13:31 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
[2026-01-18 20:13:31] [INFO] INFO 01-18 20:13:31 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[2026-01-18 20:13:32] [INFO] Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[2026-01-18 20:13:32] [INFO] Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[2026-01-18 20:13:52] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m INFO 01-18 20:13:52 [gpu_model_runner.py:3803] Starting to load model /mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8...
[2026-01-18 20:13:52] [WARNING] [0;36m(Worker_TP0 pid=170909)[0;0m WARNING 01-18 20:13:52 [compressed_tensors.py:738] Acceleration for non-quantized schemes is not supported by Compressed Tensors. Falling back to UnquantizedLinearMethod
[2026-01-18 20:13:52] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m INFO 01-18 20:13:52 [mm_encoder_attention.py:86] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[2026-01-18 20:13:52] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m INFO 01-18 20:13:52 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 20:13:52] [WARNING] [0;36m(Worker_TP1 pid=170910)[0;0m WARNING 01-18 20:13:52 [compressed_tensors.py:738] Acceleration for non-quantized schemes is not supported by Compressed Tensors. Falling back to UnquantizedLinearMethod
[2026-01-18 20:13:52] [INFO] [0;36m(Worker_TP1 pid=170910)[0;0m INFO 01-18 20:13:52 [mm_encoder_attention.py:86] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[2026-01-18 20:13:52] [INFO] [0;36m(Worker_TP1 pid=170910)[0;0m INFO 01-18 20:13:52 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 20:13:56] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m INFO 01-18 20:13:56 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[2026-01-18 20:13:56] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m INFO 01-18 20:13:56 [fp8.py:126] DeepGEMM is disabled because the platform does not support it.
[2026-01-18 20:13:56] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m INFO 01-18 20:13:56 [fp8.py:150] Using vLLM CUTLASS backend for FP8 MoE
[2026-01-18 20:13:57] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m
[2026-01-18 20:13:57] [INFO] Loading safetensors checkpoint shards:   0% Completed | 0/41 [00:00<?, ?it/s]
[2026-01-18 20:13:57] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m
[2026-01-18 20:13:57] [INFO] Loading safetensors checkpoint shards:   2% Completed | 1/41 [00:00<00:13,  2.91it/s]
[2026-01-18 20:13:57] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m
[2026-01-18 20:13:57] [INFO] Loading safetensors checkpoint shards:   5% Completed | 2/41 [00:00<00:14,  2.73it/s]
[2026-01-18 20:13:58] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m
[2026-01-18 20:13:58] [INFO] Loading safetensors checkpoint shards:   7% Completed | 3/41 [00:01<00:13,  2.84it/s]
[2026-01-18 20:13:58] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m
[2026-01-18 20:13:58] [INFO] Loading safetensors checkpoint shards:  10% Completed | 4/41 [00:01<00:13,  2.67it/s]
[2026-01-18 20:13:58] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m
[2026-01-18 20:13:58] [INFO] Loading safetensors checkpoint shards:  12% Completed | 5/41 [00:01<00:13,  2.70it/s]
[2026-01-18 20:13:59] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m
[2026-01-18 20:13:59] [INFO] Loading safetensors checkpoint shards:  15% Completed | 6/41 [00:02<00:13,  2.69it/s]
[2026-01-18 20:13:59] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m
[2026-01-18 20:13:59] [INFO] Loading safetensors checkpoint shards:  17% Completed | 7/41 [00:02<00:12,  2.75it/s]
[2026-01-18 20:14:00] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m
[2026-01-18 20:14:00] [INFO] Loading safetensors checkpoint shards:  20% Completed | 8/41 [00:02<00:11,  2.81it/s]
[2026-01-18 20:14:00] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m
[2026-01-18 20:14:00] [INFO] Loading safetensors checkpoint shards:  22% Completed | 9/41 [00:03<00:11,  2.75it/s]
[2026-01-18 20:14:00] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m
[2026-01-18 20:14:00] [INFO] Loading safetensors checkpoint shards:  24% Completed | 10/41 [00:03<00:11,  2.70it/s]
[2026-01-18 20:14:01] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m
[2026-01-18 20:14:01] [INFO] Loading safetensors checkpoint shards:  27% Completed | 11/41 [00:04<00:11,  2.67it/s]
[2026-01-18 20:14:01] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m
[2026-01-18 20:14:01] [INFO] Loading safetensors checkpoint shards:  29% Completed | 12/41 [00:04<00:10,  2.73it/s]
[2026-01-18 20:14:01] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m
[2026-01-18 20:14:01] [INFO] Loading safetensors checkpoint shards:  32% Completed | 13/41 [00:04<00:10,  2.77it/s]
[2026-01-18 20:14:02] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m
[2026-01-18 20:14:02] [INFO] Loading safetensors checkpoint shards:  34% Completed | 14/41 [00:05<00:09,  2.80it/s]
[2026-01-18 20:14:02] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m
[2026-01-18 20:14:02] [INFO] Loading safetensors checkpoint shards:  37% Completed | 15/41 [00:05<00:09,  2.81it/s]
[2026-01-18 20:14:02] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m
[2026-01-18 20:14:02] [INFO] Loading safetensors checkpoint shards:  39% Completed | 16/41 [00:05<00:09,  2.75it/s]
[2026-01-18 20:14:03] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m
[2026-01-18 20:14:03] [INFO] Loading safetensors checkpoint shards:  41% Completed | 17/41 [00:06<00:09,  2.65it/s]
[2026-01-18 20:14:03] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m
[2026-01-18 20:14:03] [INFO] Loading safetensors checkpoint shards:  44% Completed | 18/41 [00:06<00:08,  2.71it/s]
[2026-01-18 20:14:04] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m
[2026-01-18 20:14:04] [INFO] Loading safetensors checkpoint shards:  46% Completed | 19/41 [00:06<00:07,  2.77it/s]
[2026-01-18 20:14:04] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m
[2026-01-18 20:14:04] [INFO] Loading safetensors checkpoint shards:  49% Completed | 20/41 [00:07<00:07,  2.79it/s]
[2026-01-18 20:14:04] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m
[2026-01-18 20:14:04] [INFO] Loading safetensors checkpoint shards:  51% Completed | 21/41 [00:07<00:07,  2.83it/s]
[2026-01-18 20:14:05] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m
[2026-01-18 20:14:05] [INFO] Loading safetensors checkpoint shards:  54% Completed | 22/41 [00:07<00:06,  2.83it/s]
[2026-01-18 20:14:05] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m
[2026-01-18 20:14:05] [INFO] Loading safetensors checkpoint shards:  56% Completed | 23/41 [00:08<00:06,  2.77it/s]
[2026-01-18 20:14:05] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m
[2026-01-18 20:14:05] [INFO] Loading safetensors checkpoint shards:  59% Completed | 24/41 [00:08<00:06,  2.73it/s]
[2026-01-18 20:14:06] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m
[2026-01-18 20:14:06] [INFO] Loading safetensors checkpoint shards:  61% Completed | 25/41 [00:09<00:05,  2.77it/s]
[2026-01-18 20:14:06] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m
[2026-01-18 20:14:06] [INFO] Loading safetensors checkpoint shards:  63% Completed | 26/41 [00:09<00:05,  2.82it/s]
[2026-01-18 20:14:06] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m
[2026-01-18 20:14:06] [INFO] Loading safetensors checkpoint shards:  66% Completed | 27/41 [00:09<00:04,  2.80it/s]
[2026-01-18 20:14:07] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m
[2026-01-18 20:14:07] [INFO] Loading safetensors checkpoint shards:  68% Completed | 28/41 [00:10<00:04,  2.80it/s]
[2026-01-18 20:14:07] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m
[2026-01-18 20:14:07] [INFO] Loading safetensors checkpoint shards:  71% Completed | 29/41 [00:10<00:04,  2.74it/s]
[2026-01-18 20:14:08] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m
[2026-01-18 20:14:08] [INFO] Loading safetensors checkpoint shards:  73% Completed | 30/41 [00:10<00:04,  2.71it/s]
[2026-01-18 20:14:08] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m
[2026-01-18 20:14:08] [INFO] Loading safetensors checkpoint shards:  76% Completed | 31/41 [00:11<00:03,  2.76it/s]
[2026-01-18 20:14:08] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m
[2026-01-18 20:14:08] [INFO] Loading safetensors checkpoint shards:  78% Completed | 32/41 [00:11<00:03,  2.81it/s]
[2026-01-18 20:14:09] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m
[2026-01-18 20:14:09] [INFO] Loading safetensors checkpoint shards:  80% Completed | 33/41 [00:11<00:02,  2.83it/s]
[2026-01-18 20:14:09] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m
[2026-01-18 20:14:09] [INFO] Loading safetensors checkpoint shards:  83% Completed | 34/41 [00:12<00:02,  2.85it/s]
[2026-01-18 20:14:09] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m
[2026-01-18 20:14:09] [INFO] Loading safetensors checkpoint shards:  85% Completed | 35/41 [00:12<00:02,  2.82it/s]
[2026-01-18 20:14:10] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m
[2026-01-18 20:14:10] [INFO] Loading safetensors checkpoint shards:  88% Completed | 36/41 [00:13<00:01,  2.76it/s]
[2026-01-18 20:14:10] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m
[2026-01-18 20:14:10] [INFO] Loading safetensors checkpoint shards:  90% Completed | 37/41 [00:13<00:01,  2.75it/s]
[2026-01-18 20:14:10] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m
[2026-01-18 20:14:10] [INFO] Loading safetensors checkpoint shards:  93% Completed | 38/41 [00:13<00:01,  2.80it/s]
[2026-01-18 20:14:11] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m
[2026-01-18 20:14:11] [INFO] Loading safetensors checkpoint shards:  95% Completed | 39/41 [00:14<00:00,  2.83it/s]
[2026-01-18 20:14:11] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m
[2026-01-18 20:14:11] [INFO] Loading safetensors checkpoint shards:  98% Completed | 40/41 [00:14<00:00,  2.77it/s]
[2026-01-18 20:14:11] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m
[2026-01-18 20:14:11] [INFO] Loading safetensors checkpoint shards: 100% Completed | 41/41 [00:14<00:00,  3.28it/s]
[2026-01-18 20:14:11] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m
[2026-01-18 20:14:11] [INFO] Loading safetensors checkpoint shards: 100% Completed | 41/41 [00:14<00:00,  2.80it/s]
[2026-01-18 20:14:11] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m
[2026-01-18 20:14:11] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m INFO 01-18 20:14:11 [default_loader.py:291] Loading weights took 14.65 seconds
[2026-01-18 20:14:12] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m INFO 01-18 20:14:12 [gpu_model_runner.py:3900] Model loading took 51.44 GiB memory and 19.224032 seconds
[2026-01-18 20:14:12] [INFO] [0;36m(Worker_TP1 pid=170910)[0;0m INFO 01-18 20:14:12 [gpu_model_runner.py:4711] Encoder cache will be initialized with a budget of 30000 tokens, and profiled with 1 video items of the maximum feature size.
[2026-01-18 20:14:12] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m INFO 01-18 20:14:12 [gpu_model_runner.py:4711] Encoder cache will be initialized with a budget of 30000 tokens, and profiled with 1 video items of the maximum feature size.
[2026-01-18 20:14:30] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m INFO 01-18 20:14:30 [backends.py:644] Using cache directory: /home/wuwen/.cache/vllm/torch_compile_cache/0ad3b19624/rank_0_0/backbone for vLLM's torch.compile
[2026-01-18 20:14:30] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m INFO 01-18 20:14:30 [backends.py:704] Dynamo bytecode transform time: 13.03 s
[2026-01-18 20:14:42] [INFO] [0;36m(Worker_TP0 pid=170909)[0;0m INFO 01-18 20:14:42 [backends.py:261] Cache the graph of compile range (1, 8192) for later use
[2026-01-18 20:14:42] [INFO] [0;36m(Worker_TP1 pid=170910)[0;0m INFO 01-18 20:14:42 [backends.py:261] Cache the graph of compile range (1, 8192) for later use
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839] WorkerProc hit an exception.
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839] Traceback (most recent call last):
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 834, in worker_busy_loop
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     output = func(*args, **kwargs)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]              ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return func(*args, **kwargs)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 321, in determine_available_memory
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     self.model_runner.profile_run()
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4739, in profile_run
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     hidden_states, last_hidden_states = self._dummy_run(
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]                                         ^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return func(*args, **kwargs)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4457, in _dummy_run
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     outputs = self.model(
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]               ^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 222, in __call__
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return self.runnable(*args, **kwargs)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/glm4_1v.py", line 1764, in forward
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     hidden_states = self.language_model.model(
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 557, in __call__
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)  # type: ignore[arg-type]
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 228, in __call__
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return self._call_with_optional_nvtx_range(
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 119, in _call_with_optional_nvtx_range
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return callable_fn(*args, **kwargs)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return fn(*args, **kwargs)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/glm4_moe.py", line 452, in forward
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     def forward(
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return fn(*args, **kwargs)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/caching.py", line 64, in __call__
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return self.optimized_call(*args, **kwargs)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return self._wrapped_call(self, *args, **kwargs)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/fx/graph_module.py", line 413, in __call__
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     raise e
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/fx/graph_module.py", line 400, in __call__
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "<eval_with_key>.94", line 393, in forward
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     submod_4 = self.submod_4(getitem_7, s59, s18, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_8, l_self_modules_layers_modules_1_modules_mlp_modules_gate_parameters_weight_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_, l_positions_, s7);  getitem_7 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_8 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_parameters_weight_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 222, in __call__
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return self.runnable(*args, **kwargs)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/piecewise_backend.py", line 191, in __call__
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return range_entry.runnable(*args)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return self._compiled_fn(*args)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return fn(*args, **kwargs)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return compiled_fn(full_args)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     all_outs = call_func_at_runtime_with_args(
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     out = normalize_as_list(f(args))
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]                             ^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return compiled_fn(runtime_args)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_inductor/output_code.py", line 613, in __call__
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return self.current_callable(inputs)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_inductor/utils.py", line 3017, in run
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     out = model(new_inputs)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]           ^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/tmp/torchinductor_wuwen/ct/cctdwzzfufdzd76q46mh5ef2a3dt4kas62lnlxc54qvfsslr3v7f.py", line 1504, in call
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     buf13 = torch.ops.vllm.moe_forward_shared.default(buf12, buf11, 'language_model.model.layers.1.mlp.experts')
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_ops.py", line 841, in __call__
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return self._op(*args, **kwargs)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 2131, in moe_forward_shared
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return self.forward_impl(hidden_states, router_logits)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1979, in forward_impl
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     final_hidden_states = self.quant_method.apply(
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]                           ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py", line 1122, in apply
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     result = self.kernel(
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]              ^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/modular_kernel.py", line 1228, in forward
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     fused_out = self._fused_experts(
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]                 ^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/modular_kernel.py", line 1082, in _fused_experts
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     self.fused_experts.apply(
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/fallback.py", line 111, in apply
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     experts.apply(
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/cutlass_moe.py", line 298, in apply
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     run_cutlass_moe_fp8(
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/cutlass_moe.py", line 188, in run_cutlass_moe_fp8
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     ops.cutlass_moe_mm(
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/_custom_ops.py", line 1175, in cutlass_moe_mm
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return torch.ops._C.cutlass_moe_mm(
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_ops.py", line 1255, in __call__
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return self._op(*args, **kwargs)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839] NotImplementedError: No compiled cutlass_scaled_mm for CUDA device capability: 120. Required capability: 90 or 100
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839] Traceback (most recent call last):
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 834, in worker_busy_loop
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     output = func(*args, **kwargs)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]              ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return func(*args, **kwargs)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 321, in determine_available_memory
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     self.model_runner.profile_run()
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4739, in profile_run
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     hidden_states, last_hidden_states = self._dummy_run(
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]                                         ^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return func(*args, **kwargs)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4457, in _dummy_run
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     outputs = self.model(
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]               ^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 222, in __call__
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return self.runnable(*args, **kwargs)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/glm4_1v.py", line 1764, in forward
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     hidden_states = self.language_model.model(
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 557, in __call__
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)  # type: ignore[arg-type]
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 228, in __call__
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return self._call_with_optional_nvtx_range(
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 119, in _call_with_optional_nvtx_range
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return callable_fn(*args, **kwargs)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return fn(*args, **kwargs)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/glm4_moe.py", line 452, in forward
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     def forward(
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return fn(*args, **kwargs)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/caching.py", line 64, in __call__
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return self.optimized_call(*args, **kwargs)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return self._wrapped_call(self, *args, **kwargs)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/fx/graph_module.py", line 413, in __call__
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     raise e
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/fx/graph_module.py", line 400, in __call__
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "<eval_with_key>.94", line 393, in forward
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     submod_4 = self.submod_4(getitem_7, s59, s18, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_8, l_self_modules_layers_modules_1_modules_mlp_modules_gate_parameters_weight_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_, l_positions_, s7);  getitem_7 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_8 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_parameters_weight_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 222, in __call__
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return self.runnable(*args, **kwargs)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/piecewise_backend.py", line 191, in __call__
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return range_entry.runnable(*args)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return self._compiled_fn(*args)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return fn(*args, **kwargs)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return compiled_fn(full_args)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     all_outs = call_func_at_runtime_with_args(
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     out = normalize_as_list(f(args))
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]                             ^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return compiled_fn(runtime_args)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_inductor/output_code.py", line 613, in __call__
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return self.current_callable(inputs)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_inductor/utils.py", line 3017, in run
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     out = model(new_inputs)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]           ^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/tmp/torchinductor_wuwen/ct/cctdwzzfufdzd76q46mh5ef2a3dt4kas62lnlxc54qvfsslr3v7f.py", line 1504, in call
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     buf13 = torch.ops.vllm.moe_forward_shared.default(buf12, buf11, 'language_model.model.layers.1.mlp.experts')
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_ops.py", line 841, in __call__
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return self._op(*args, **kwargs)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 2131, in moe_forward_shared
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return self.forward_impl(hidden_states, router_logits)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1979, in forward_impl
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     final_hidden_states = self.quant_method.apply(
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]                           ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py", line 1122, in apply
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     result = self.kernel(
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]              ^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/modular_kernel.py", line 1228, in forward
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     fused_out = self._fused_experts(
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]                 ^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/modular_kernel.py", line 1082, in _fused_experts
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     self.fused_experts.apply(
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/fallback.py", line 111, in apply
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     experts.apply(
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/cutlass_moe.py", line 298, in apply
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     run_cutlass_moe_fp8(
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/cutlass_moe.py", line 188, in run_cutlass_moe_fp8
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     ops.cutlass_moe_mm(
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/_custom_ops.py", line 1175, in cutlass_moe_mm
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return torch.ops._C.cutlass_moe_mm(
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_ops.py", line 1255, in __call__
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]     return self._op(*args, **kwargs)
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839] NotImplementedError: No compiled cutlass_scaled_mm for CUDA device capability: 120. Required capability: 90 or 100
[2026-01-18 20:14:43] [ERROR] [0;36m(Worker_TP0 pid=170909)[0;0m ERROR 01-18 20:14:43 [multiproc_executor.py:839]
[2026-01-18 20:14:43] [ERROR] [0;36m(EngineCore_DP0 pid=170331)[0;0m ERROR 01-18 20:14:43 [core.py:935] EngineCore failed to start.
[2026-01-18 20:14:43] [ERROR] [0;36m(EngineCore_DP0 pid=170331)[0;0m ERROR 01-18 20:14:43 [core.py:935] Traceback (most recent call last):
[2026-01-18 20:14:43] [ERROR] [0;36m(EngineCore_DP0 pid=170331)[0;0m ERROR 01-18 20:14:43 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-18 20:14:43] [ERROR] [0;36m(EngineCore_DP0 pid=170331)[0;0m ERROR 01-18 20:14:43 [core.py:935]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 20:14:43] [ERROR] [0;36m(EngineCore_DP0 pid=170331)[0;0m ERROR 01-18 20:14:43 [core.py:935]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(EngineCore_DP0 pid=170331)[0;0m ERROR 01-18 20:14:43 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-18 20:14:43] [ERROR] [0;36m(EngineCore_DP0 pid=170331)[0;0m ERROR 01-18 20:14:43 [core.py:935]     super().__init__(
[2026-01-18 20:14:43] [ERROR] [0;36m(EngineCore_DP0 pid=170331)[0;0m ERROR 01-18 20:14:43 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 112, in __init__
[2026-01-18 20:14:43] [ERROR] [0;36m(EngineCore_DP0 pid=170331)[0;0m ERROR 01-18 20:14:43 [core.py:935]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[2026-01-18 20:14:43] [ERROR] [0;36m(EngineCore_DP0 pid=170331)[0;0m ERROR 01-18 20:14:43 [core.py:935]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(EngineCore_DP0 pid=170331)[0;0m ERROR 01-18 20:14:43 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 242, in _initialize_kv_caches
[2026-01-18 20:14:43] [ERROR] [0;36m(EngineCore_DP0 pid=170331)[0;0m ERROR 01-18 20:14:43 [core.py:935]     available_gpu_memory = self.model_executor.determine_available_memory()
[2026-01-18 20:14:43] [ERROR] [0;36m(EngineCore_DP0 pid=170331)[0;0m ERROR 01-18 20:14:43 [core.py:935]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(EngineCore_DP0 pid=170331)[0;0m ERROR 01-18 20:14:43 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[2026-01-18 20:14:43] [ERROR] [0;36m(EngineCore_DP0 pid=170331)[0;0m ERROR 01-18 20:14:43 [core.py:935]     return self.collective_rpc("determine_available_memory")
[2026-01-18 20:14:43] [ERROR] [0;36m(EngineCore_DP0 pid=170331)[0;0m ERROR 01-18 20:14:43 [core.py:935]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(EngineCore_DP0 pid=170331)[0;0m ERROR 01-18 20:14:43 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 374, in collective_rpc
[2026-01-18 20:14:43] [ERROR] [0;36m(EngineCore_DP0 pid=170331)[0;0m ERROR 01-18 20:14:43 [core.py:935]     return aggregate(get_response())
[2026-01-18 20:14:43] [ERROR] [0;36m(EngineCore_DP0 pid=170331)[0;0m ERROR 01-18 20:14:43 [core.py:935]                      ^^^^^^^^^^^^^^
[2026-01-18 20:14:43] [ERROR] [0;36m(EngineCore_DP0 pid=170331)[0;0m ERROR 01-18 20:14:43 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 357, in get_response
[2026-01-18 20:14:43] [ERROR] [0;36m(EngineCore_DP0 pid=170331)[0;0m ERROR 01-18 20:14:43 [core.py:935]     raise RuntimeError(
[2026-01-18 20:14:43] [ERROR] [0;36m(EngineCore_DP0 pid=170331)[0;0m ERROR 01-18 20:14:43 [core.py:935] RuntimeError: Worker failed with error 'No compiled cutlass_scaled_mm for CUDA device capability: 120. Required capability: 90 or 100', please check the stack trace above for the root cause
[2026-01-18 20:14:45] [ERROR] [0;36m(EngineCore_DP0 pid=170331)[0;0m ERROR 01-18 20:14:45 [multiproc_executor.py:246] Worker proc VllmWorker-0 died unexpectedly, shutting down executor.
[2026-01-18 20:14:46] [INFO] [0;36m(EngineCore_DP0 pid=170331)[0;0m Process EngineCore_DP0:
[2026-01-18 20:14:46] [ERROR] [0;36m(EngineCore_DP0 pid=170331)[0;0m Traceback (most recent call last):
[2026-01-18 20:14:46] [INFO] [0;36m(EngineCore_DP0 pid=170331)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[2026-01-18 20:14:46] [INFO] [0;36m(EngineCore_DP0 pid=170331)[0;0m     self.run()
[2026-01-18 20:14:46] [INFO] [0;36m(EngineCore_DP0 pid=170331)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/process.py", line 108, in run
[2026-01-18 20:14:46] [INFO] [0;36m(EngineCore_DP0 pid=170331)[0;0m     self._target(*self._args, **self._kwargs)
[2026-01-18 20:14:46] [INFO] [0;36m(EngineCore_DP0 pid=170331)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 939, in run_engine_core
[2026-01-18 20:14:46] [INFO] [0;36m(EngineCore_DP0 pid=170331)[0;0m     raise e
[2026-01-18 20:14:46] [INFO] [0;36m(EngineCore_DP0 pid=170331)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-18 20:14:46] [INFO] [0;36m(EngineCore_DP0 pid=170331)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 20:14:46] [INFO] [0;36m(EngineCore_DP0 pid=170331)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:46] [INFO] [0;36m(EngineCore_DP0 pid=170331)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-18 20:14:46] [INFO] [0;36m(EngineCore_DP0 pid=170331)[0;0m     super().__init__(
[2026-01-18 20:14:46] [INFO] [0;36m(EngineCore_DP0 pid=170331)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 112, in __init__
[2026-01-18 20:14:46] [INFO] [0;36m(EngineCore_DP0 pid=170331)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[2026-01-18 20:14:46] [INFO] [0;36m(EngineCore_DP0 pid=170331)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:46] [INFO] [0;36m(EngineCore_DP0 pid=170331)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 242, in _initialize_kv_caches
[2026-01-18 20:14:46] [INFO] [0;36m(EngineCore_DP0 pid=170331)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[2026-01-18 20:14:46] [INFO] [0;36m(EngineCore_DP0 pid=170331)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:46] [INFO] [0;36m(EngineCore_DP0 pid=170331)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[2026-01-18 20:14:46] [INFO] [0;36m(EngineCore_DP0 pid=170331)[0;0m     return self.collective_rpc("determine_available_memory")
[2026-01-18 20:14:46] [INFO] [0;36m(EngineCore_DP0 pid=170331)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:46] [INFO] [0;36m(EngineCore_DP0 pid=170331)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 374, in collective_rpc
[2026-01-18 20:14:46] [INFO] [0;36m(EngineCore_DP0 pid=170331)[0;0m     return aggregate(get_response())
[2026-01-18 20:14:46] [INFO] [0;36m(EngineCore_DP0 pid=170331)[0;0m                      ^^^^^^^^^^^^^^
[2026-01-18 20:14:46] [INFO] [0;36m(EngineCore_DP0 pid=170331)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 357, in get_response
[2026-01-18 20:14:46] [INFO] [0;36m(EngineCore_DP0 pid=170331)[0;0m     raise RuntimeError(
[2026-01-18 20:14:46] [INFO] [0;36m(EngineCore_DP0 pid=170331)[0;0m RuntimeError: Worker failed with error 'No compiled cutlass_scaled_mm for CUDA device capability: 120. Required capability: 90 or 100', please check the stack trace above for the root cause
[2026-01-18 20:14:47] [ERROR] [0;36m(APIServer pid=169705)[0;0m Traceback (most recent call last):
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/bin/vllm", line 7, in <module>
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m     sys.exit(main())
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m              ^^^^^^
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m     args.dispatch_function(args)
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m     uvloop.run(run_server(args))
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m     return __asyncio.run(
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m            ^^^^^^^^^^^^^^
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m     return runner.run(main)
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m     return self._loop.run_until_complete(task)
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m     return await main
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m            ^^^^^^^^^^
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m     async with build_async_engine_client(
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m     return await anext(self.gen)
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 146, in build_async_engine_client
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m     return await anext(self.gen)
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 187, in build_async_engine_client_from_engine_args
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 202, in from_vllm_config
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m     return cls(
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m            ^^^^
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 129, in __init__
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m     return AsyncMPClient(*client_args)
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m     super().__init__(
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 144, in __exit__
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m     next(self.gen)
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 918, in launch_core_engines
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m     wait_for_engine_startup(
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 977, in wait_for_engine_startup
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m     raise RuntimeError(
[2026-01-18 20:14:47] [INFO] [0;36m(APIServer pid=169705)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[2026-01-18 20:14:50] [INFO] nvitop监控已启动
[2026-01-18 20:16:07] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 20:16:07] [INFO] nvitop监控已启动
[2026-01-18 20:16:07] [INFO] nvitop监控已停止
[2026-01-18 20:16:32] [INFO] [0;36m(APIServer pid=174451)[0;0m INFO 01-18 20:16:32 [api_server.py:1278] vLLM API server version 0.14.0rc1.dev492+g19504ac07
[2026-01-18 20:16:32] [INFO] [0;36m(APIServer pid=174451)[0;0m INFO 01-18 20:16:32 [utils.py:254] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8/chat_template.jinja', 'enable_auto_tool_choice': True, 'tool_call_parser': 'glm45', 'model': '/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8', 'trust_remote_code': True, 'allowed_local_media_path': '/', 'max_model_len': 128000, 'quantization': 'fp8', 'served_model_name': ['VLLM-MODEL'], 'reasoning_parser': 'glm45', 'tensor_parallel_size': 2, 'kv_cache_dtype': 'fp8', 'mm_processor_cache_type': 'shm', 'mm_encoder_tp_mode': 'data', 'max_num_seqs': 256, 'async_scheduling': True}
[2026-01-18 20:16:32] [INFO] [0;36m(APIServer pid=174451)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 20:16:32] [INFO] [0;36m(APIServer pid=174451)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m INFO 01-18 20:16:52 [model.py:528] Resolved architecture: Glm4vMoeForConditionalGeneration
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m INFO 01-18 20:16:52 [model.py:1543] Using max model len 128000
[2026-01-18 20:16:52] [ERROR] [0;36m(APIServer pid=174451)[0;0m Traceback (most recent call last):
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/bin/vllm", line 7, in <module>
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m     sys.exit(main())
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m              ^^^^^^
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m     args.dispatch_function(args)
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m     uvloop.run(run_server(args))
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m     return __asyncio.run(
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m            ^^^^^^^^^^^^^^
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m     return runner.run(main)
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m     return self._loop.run_until_complete(task)
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m     return await main
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m            ^^^^^^^^^^
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 1325, in run_server
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 1344, in run_server_worker
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m     async with build_async_engine_client(
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m     return await anext(self.gen)
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 173, in build_async_engine_client
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m     return await anext(self.gen)
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 199, in build_async_engine_client_from_engine_args
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/engine/arg_utils.py", line 1365, in create_engine_config
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m     model_config = self.create_model_config()
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/engine/arg_utils.py", line 1220, in create_model_config
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m     return ModelConfig(
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m            ^^^^^^^^^^^^
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/pydantic/_internal/_dataclasses.py", line 121, in __init__
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m   Value error, Quantization method specified in the model config (compressed-tensors) does not match the quantization method specified in the `quantization` argument (fp8). [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]
[2026-01-18 20:16:52] [INFO] [0;36m(APIServer pid=174451)[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error
[2026-01-18 20:16:54] [INFO] nvitop监控已启动
[2026-01-18 20:18:12] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 20:18:12] [INFO] nvitop监控已启动
[2026-01-18 20:18:12] [INFO] nvitop监控已停止
[2026-01-18 20:18:21] [INFO] nvitop监控已启动
[2026-01-18 20:18:21] [INFO] 服务已停止
[2026-01-18 20:18:22] [INFO] nvitop监控已启动
[2026-01-18 20:18:56] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 20:18:56] [INFO] nvitop监控已启动
[2026-01-18 20:18:56] [INFO] nvitop监控已停止
[2026-01-18 20:19:22] [INFO] [0;36m(APIServer pid=177641)[0;0m INFO 01-18 20:19:22 [api_server.py:1278] vLLM API server version 0.14.0rc1.dev492+g19504ac07
[2026-01-18 20:19:22] [INFO] [0;36m(APIServer pid=177641)[0;0m INFO 01-18 20:19:22 [utils.py:254] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8/chat_template.jinja', 'enable_auto_tool_choice': True, 'tool_call_parser': 'glm45', 'model': '/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8', 'allowed_local_media_path': '/', 'max_model_len': 128000, 'served_model_name': ['VLLM-MODEL'], 'reasoning_parser': 'glm45', 'tensor_parallel_size': 2, 'kv_cache_dtype': 'fp8', 'mm_processor_cache_type': 'shm', 'mm_encoder_tp_mode': 'data', 'max_num_seqs': 256}
[2026-01-18 20:19:22] [INFO] [0;36m(APIServer pid=177641)[0;0m INFO 01-18 20:19:22 [model.py:528] Resolved architecture: Glm4vMoeForConditionalGeneration
[2026-01-18 20:19:22] [INFO] [0;36m(APIServer pid=177641)[0;0m INFO 01-18 20:19:22 [model.py:1543] Using max model len 128000
[2026-01-18 20:19:22] [INFO] [0;36m(APIServer pid=177641)[0;0m INFO 01-18 20:19:22 [cache.py:206] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor.
[2026-01-18 20:19:22] [INFO] [0;36m(APIServer pid=177641)[0;0m INFO 01-18 20:19:22 [scheduler.py:231] Chunked prefill is enabled with max_num_batched_tokens=8192.
[2026-01-18 20:19:22] [INFO] [0;36m(APIServer pid=177641)[0;0m INFO 01-18 20:19:22 [vllm.py:640] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-18 20:19:22] [INFO] [0;36m(APIServer pid=177641)[0;0m INFO 01-18 20:19:22 [vllm.py:645] Asynchronous scheduling is enabled.
[2026-01-18 20:19:23] [INFO] [0;36m(APIServer pid=177641)[0;0m The tokenizer you are loading from '/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[2026-01-18 20:19:46] [INFO] [0;36m(EngineCore_DP0 pid=178234)[0;0m INFO 01-18 20:19:46 [core.py:97] Initializing a V1 LLM engine (v0.14.0rc1.dev492+g19504ac07) with config: model='/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8', speculative_config=None, tokenizer='/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=fp8, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='glm45', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-18 20:19:46] [WARNING] [0;36m(EngineCore_DP0 pid=178234)[0;0m WARNING 01-18 20:19:46 [multiproc_executor.py:880] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-18 20:20:09] [INFO] The tokenizer you are loading from '/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[2026-01-18 20:20:09] [INFO] The tokenizer you are loading from '/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[2026-01-18 20:20:09] [INFO] INFO 01-18 20:20:09 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:41201 backend=nccl
[2026-01-18 20:20:09] [INFO] INFO 01-18 20:20:09 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:41201 backend=nccl
[2026-01-18 20:20:10] [INFO] INFO 01-18 20:20:10 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-18 20:20:10] [WARNING] WARNING 01-18 20:20:10 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 20:20:10] [WARNING] WARNING 01-18 20:20:10 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 20:20:10] [INFO] INFO 01-18 20:20:10 [parallel_state.py:1425] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[2026-01-18 20:20:10] [INFO] INFO 01-18 20:20:10 [parallel_state.py:1425] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
[2026-01-18 20:20:11] [INFO] The tokenizer you are loading from '/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749] WorkerProc failed to start.
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749] Traceback (most recent call last):
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 720, in worker_main
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]     worker = WorkerProc(*args, **kwargs)
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 551, in __init__
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]     self.worker.init_device()
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/worker/worker_base.py", line 326, in init_device
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]     self.worker.init_device()  # type: ignore
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/worker/gpu_worker.py", line 261, in init_device
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]     self.model_runner = GPUModelRunnerV1(self.vllm_config, self.device)
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/worker/gpu_model_runner.py", line 624, in __init__
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]     MultiModalBudget(
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/worker/utils.py", line 48, in __init__
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]     max_tokens_by_modality = mm_registry.get_max_tokens_per_item_by_modality(
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/multimodal/registry.py", line 171, in get_max_tokens_per_item_by_modality
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]     return profiler.get_mm_max_tokens(
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/multimodal/profiling.py", line 350, in get_mm_max_tokens
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]     mm_inputs = self._get_dummy_mm_inputs(seq_len, mm_counts)
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/multimodal/profiling.py", line 263, in _get_dummy_mm_inputs
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]     processor_inputs = factory.get_dummy_processor_inputs(
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/multimodal/profiling.py", line 120, in get_dummy_processor_inputs
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]     dummy_text = self.get_dummy_text(mm_counts)
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/glm4_1v.py", line 1145, in get_dummy_text
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]     hf_processor = self.info.get_hf_processor()
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/multimodal/processing.py", line 1397, in get_hf_processor
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]     return self.ctx.get_hf_processor(**kwargs)
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/multimodal/processing.py", line 1195, in get_hf_processor
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]     return cached_processor_from_config(
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/transformers_utils/processor.py", line 251, in cached_processor_from_config
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]     return cached_get_processor_without_dynamic_kwargs(
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/transformers_utils/processor.py", line 210, in cached_get_processor_without_dynamic_kwargs
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]     processor = cached_get_processor(
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]                 ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/transformers_utils/processor.py", line 155, in get_processor
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749]     raise TypeError(
[2026-01-18 20:20:11] [ERROR] ERROR 01-18 20:20:11 [multiproc_executor.py:749] TypeError: Invalid type of HuggingFace processor. Expected type: <class 'transformers.processing_utils.ProcessorMixin'>, but found type: <class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>
[2026-01-18 20:20:12] [INFO] INFO 01-18 20:20:12 [multiproc_executor.py:707] Parent process exited, terminating worker
[2026-01-18 20:20:12] [INFO] INFO 01-18 20:20:12 [multiproc_executor.py:707] Parent process exited, terminating worker
[2026-01-18 20:20:12] [WARNING] [rank0]:[W118 20:20:12.196319425 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2026-01-18 20:20:13] [ERROR] [0;36m(EngineCore_DP0 pid=178234)[0;0m ERROR 01-18 20:20:13 [core.py:936] EngineCore failed to start.
[2026-01-18 20:20:13] [ERROR] [0;36m(EngineCore_DP0 pid=178234)[0;0m ERROR 01-18 20:20:13 [core.py:936] Traceback (most recent call last):
[2026-01-18 20:20:13] [ERROR] [0;36m(EngineCore_DP0 pid=178234)[0;0m ERROR 01-18 20:20:13 [core.py:936]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core.py", line 927, in run_engine_core
[2026-01-18 20:20:13] [ERROR] [0;36m(EngineCore_DP0 pid=178234)[0;0m ERROR 01-18 20:20:13 [core.py:936]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 20:20:13] [ERROR] [0;36m(EngineCore_DP0 pid=178234)[0;0m ERROR 01-18 20:20:13 [core.py:936]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:20:13] [ERROR] [0;36m(EngineCore_DP0 pid=178234)[0;0m ERROR 01-18 20:20:13 [core.py:936]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core.py", line 692, in __init__
[2026-01-18 20:20:13] [ERROR] [0;36m(EngineCore_DP0 pid=178234)[0;0m ERROR 01-18 20:20:13 [core.py:936]     super().__init__(
[2026-01-18 20:20:13] [ERROR] [0;36m(EngineCore_DP0 pid=178234)[0;0m ERROR 01-18 20:20:13 [core.py:936]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core.py", line 106, in __init__
[2026-01-18 20:20:13] [ERROR] [0;36m(EngineCore_DP0 pid=178234)[0;0m ERROR 01-18 20:20:13 [core.py:936]     self.model_executor = executor_class(vllm_config)
[2026-01-18 20:20:13] [ERROR] [0;36m(EngineCore_DP0 pid=178234)[0;0m ERROR 01-18 20:20:13 [core.py:936]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:20:13] [ERROR] [0;36m(EngineCore_DP0 pid=178234)[0;0m ERROR 01-18 20:20:13 [core.py:936]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[2026-01-18 20:20:13] [ERROR] [0;36m(EngineCore_DP0 pid=178234)[0;0m ERROR 01-18 20:20:13 [core.py:936]     super().__init__(vllm_config)
[2026-01-18 20:20:13] [ERROR] [0;36m(EngineCore_DP0 pid=178234)[0;0m ERROR 01-18 20:20:13 [core.py:936]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/abstract.py", line 101, in __init__
[2026-01-18 20:20:13] [ERROR] [0;36m(EngineCore_DP0 pid=178234)[0;0m ERROR 01-18 20:20:13 [core.py:936]     self._init_executor()
[2026-01-18 20:20:13] [ERROR] [0;36m(EngineCore_DP0 pid=178234)[0;0m ERROR 01-18 20:20:13 [core.py:936]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 172, in _init_executor
[2026-01-18 20:20:13] [ERROR] [0;36m(EngineCore_DP0 pid=178234)[0;0m ERROR 01-18 20:20:13 [core.py:936]     self.workers = WorkerProc.wait_for_ready(unready_workers)
[2026-01-18 20:20:13] [ERROR] [0;36m(EngineCore_DP0 pid=178234)[0;0m ERROR 01-18 20:20:13 [core.py:936]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:20:13] [ERROR] [0;36m(EngineCore_DP0 pid=178234)[0;0m ERROR 01-18 20:20:13 [core.py:936]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 658, in wait_for_ready
[2026-01-18 20:20:13] [ERROR] [0;36m(EngineCore_DP0 pid=178234)[0;0m ERROR 01-18 20:20:13 [core.py:936]     raise e from None
[2026-01-18 20:20:13] [ERROR] [0;36m(EngineCore_DP0 pid=178234)[0;0m ERROR 01-18 20:20:13 [core.py:936] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[2026-01-18 20:20:13] [INFO] [0;36m(EngineCore_DP0 pid=178234)[0;0m Process EngineCore_DP0:
[2026-01-18 20:20:13] [ERROR] [0;36m(EngineCore_DP0 pid=178234)[0;0m Traceback (most recent call last):
[2026-01-18 20:20:13] [INFO] [0;36m(EngineCore_DP0 pid=178234)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[2026-01-18 20:20:13] [INFO] [0;36m(EngineCore_DP0 pid=178234)[0;0m     self.run()
[2026-01-18 20:20:13] [INFO] [0;36m(EngineCore_DP0 pid=178234)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/process.py", line 108, in run
[2026-01-18 20:20:13] [INFO] [0;36m(EngineCore_DP0 pid=178234)[0;0m     self._target(*self._args, **self._kwargs)
[2026-01-18 20:20:13] [INFO] [0;36m(EngineCore_DP0 pid=178234)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core.py", line 940, in run_engine_core
[2026-01-18 20:20:13] [INFO] [0;36m(EngineCore_DP0 pid=178234)[0;0m     raise e
[2026-01-18 20:20:13] [INFO] [0;36m(EngineCore_DP0 pid=178234)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core.py", line 927, in run_engine_core
[2026-01-18 20:20:13] [INFO] [0;36m(EngineCore_DP0 pid=178234)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 20:20:13] [INFO] [0;36m(EngineCore_DP0 pid=178234)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:20:13] [INFO] [0;36m(EngineCore_DP0 pid=178234)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core.py", line 692, in __init__
[2026-01-18 20:20:13] [INFO] [0;36m(EngineCore_DP0 pid=178234)[0;0m     super().__init__(
[2026-01-18 20:20:13] [INFO] [0;36m(EngineCore_DP0 pid=178234)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core.py", line 106, in __init__
[2026-01-18 20:20:13] [INFO] [0;36m(EngineCore_DP0 pid=178234)[0;0m     self.model_executor = executor_class(vllm_config)
[2026-01-18 20:20:13] [INFO] [0;36m(EngineCore_DP0 pid=178234)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:20:13] [INFO] [0;36m(EngineCore_DP0 pid=178234)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[2026-01-18 20:20:13] [INFO] [0;36m(EngineCore_DP0 pid=178234)[0;0m     super().__init__(vllm_config)
[2026-01-18 20:20:13] [INFO] [0;36m(EngineCore_DP0 pid=178234)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/abstract.py", line 101, in __init__
[2026-01-18 20:20:13] [INFO] [0;36m(EngineCore_DP0 pid=178234)[0;0m     self._init_executor()
[2026-01-18 20:20:13] [INFO] [0;36m(EngineCore_DP0 pid=178234)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 172, in _init_executor
[2026-01-18 20:20:13] [INFO] [0;36m(EngineCore_DP0 pid=178234)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
[2026-01-18 20:20:13] [INFO] [0;36m(EngineCore_DP0 pid=178234)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:20:13] [INFO] [0;36m(EngineCore_DP0 pid=178234)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 658, in wait_for_ready
[2026-01-18 20:20:13] [INFO] [0;36m(EngineCore_DP0 pid=178234)[0;0m     raise e from None
[2026-01-18 20:20:13] [INFO] [0;36m(EngineCore_DP0 pid=178234)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[2026-01-18 20:20:14] [ERROR] [0;36m(APIServer pid=177641)[0;0m Traceback (most recent call last):
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/bin/vllm", line 7, in <module>
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m     sys.exit(main())
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m              ^^^^^^
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m     args.dispatch_function(args)
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m     uvloop.run(run_server(args))
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m     return __asyncio.run(
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m            ^^^^^^^^^^^^^^
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m     return runner.run(main)
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m     return self._loop.run_until_complete(task)
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m     return await main
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m            ^^^^^^^^^^
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 1325, in run_server
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 1344, in run_server_worker
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m     async with build_async_engine_client(
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m     return await anext(self.gen)
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 173, in build_async_engine_client
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m     return await anext(self.gen)
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 214, in build_async_engine_client_from_engine_args
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/async_llm.py", line 205, in from_vllm_config
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m     return cls(
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m            ^^^^
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/async_llm.py", line 132, in __init__
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m     return AsyncMPClient(*client_args)
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core_client.py", line 824, in __init__
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m     super().__init__(
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core_client.py", line 479, in __init__
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/contextlib.py", line 144, in __exit__
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m     next(self.gen)
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/utils.py", line 921, in launch_core_engines
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m     wait_for_engine_startup(
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/utils.py", line 980, in wait_for_engine_startup
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m     raise RuntimeError(
[2026-01-18 20:20:14] [INFO] [0;36m(APIServer pid=177641)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[2026-01-18 20:20:15] [INFO] /mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py:279: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown
[2026-01-18 20:20:15] [INFO] warnings.warn('resource_tracker: There appear to be %d '
[2026-01-18 20:20:16] [INFO] nvitop监控已启动
[2026-01-18 20:20:34] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 20:20:34] [INFO] nvitop监控已启动
[2026-01-18 20:20:34] [INFO] nvitop监控已停止
[2026-01-18 20:21:04] [INFO] [0;36m(APIServer pid=179800)[0;0m INFO 01-18 20:21:04 [api_server.py:872] vLLM API server version 0.14.0rc2.dev117+g9e078d058
[2026-01-18 20:21:04] [INFO] [0;36m(APIServer pid=179800)[0;0m INFO 01-18 20:21:04 [utils.py:267] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8/chat_template.jinja', 'enable_auto_tool_choice': True, 'tool_call_parser': 'glm45', 'model': '/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8', 'allowed_local_media_path': '/', 'max_model_len': 128000, 'served_model_name': ['VLLM-MODEL'], 'reasoning_parser': 'glm45', 'tensor_parallel_size': 2, 'kv_cache_dtype': 'fp8', 'mm_processor_cache_type': 'shm', 'mm_encoder_tp_mode': 'data', 'max_num_seqs': 256}
[2026-01-18 20:21:04] [INFO] [0;36m(APIServer pid=179800)[0;0m Unrecognized keys in `rope_parameters` for 'rope_type'='default': {'mrope_section'}
[2026-01-18 20:21:04] [INFO] [0;36m(APIServer pid=179800)[0;0m Unrecognized keys in `rope_parameters` for 'rope_type'='default': {'mrope_section', 'partial_rotary_factor'}
[2026-01-18 20:21:04] [INFO] [0;36m(APIServer pid=179800)[0;0m Unrecognized keys in `rope_parameters` for 'rope_type'='default': {'mrope_section', 'partial_rotary_factor'}
[2026-01-18 20:21:29] [INFO] [0;36m(APIServer pid=179800)[0;0m INFO 01-18 20:21:29 [model.py:530] Resolved architecture: Glm4vMoeForConditionalGeneration
[2026-01-18 20:21:29] [INFO] [0;36m(APIServer pid=179800)[0;0m INFO 01-18 20:21:29 [model.py:1547] Using max model len 128000
[2026-01-18 20:21:29] [WARNING] [0;36m(APIServer pid=179800)[0;0m WARNING 01-18 20:21:29 [quark_ocp_mx.py:157] AITER is not found or QuarkOCP_MX is not supported on the current platform. QuarkOCP_MX quantization will not be available.
[2026-01-18 20:21:29] [INFO] [0;36m(APIServer pid=179800)[0;0m INFO 01-18 20:21:29 [cache.py:206] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor.
[2026-01-18 20:21:30] [INFO] [0;36m(APIServer pid=179800)[0;0m INFO 01-18 20:21:30 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[2026-01-18 20:21:30] [INFO] [0;36m(APIServer pid=179800)[0;0m INFO 01-18 20:21:30 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 20:21:30] [INFO] [0;36m(APIServer pid=179800)[0;0m INFO 01-18 20:21:30 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-18 20:22:00] [INFO] [0;36m(EngineCore_DP0 pid=181044)[0;0m INFO 01-18 20:22:00 [core.py:96] Initializing a V1 LLM engine (v0.14.0rc2.dev117+g9e078d058) with config: model='/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8', speculative_config=None, tokenizer='/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=fp8, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='glm45', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-18 20:22:00] [WARNING] [0;36m(EngineCore_DP0 pid=181044)[0;0m WARNING 01-18 20:22:00 [multiproc_executor.py:897] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-18 20:22:30] [INFO] INFO 01-18 20:22:30 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:37907 backend=nccl
[2026-01-18 20:22:30] [INFO] INFO 01-18 20:22:30 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:37907 backend=nccl
[2026-01-18 20:22:30] [INFO] INFO 01-18 20:22:30 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-18 20:22:30] [WARNING] WARNING 01-18 20:22:30 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 20:22:30] [WARNING] WARNING 01-18 20:22:30 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 20:22:30] [INFO] INFO 01-18 20:22:30 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
[2026-01-18 20:22:30] [INFO] INFO 01-18 20:22:30 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[2026-01-18 20:22:31] [INFO] Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[2026-01-18 20:22:31] [INFO] Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[2026-01-18 20:22:51] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m INFO 01-18 20:22:51 [gpu_model_runner.py:3803] Starting to load model /mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.6V-FP8...
[2026-01-18 20:22:52] [WARNING] [0;36m(Worker_TP0 pid=181654)[0;0m WARNING 01-18 20:22:52 [compressed_tensors.py:738] Acceleration for non-quantized schemes is not supported by Compressed Tensors. Falling back to UnquantizedLinearMethod
[2026-01-18 20:22:52] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m INFO 01-18 20:22:52 [mm_encoder_attention.py:86] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[2026-01-18 20:22:52] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m INFO 01-18 20:22:52 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 20:22:52] [WARNING] [0;36m(Worker_TP1 pid=181655)[0;0m WARNING 01-18 20:22:52 [compressed_tensors.py:738] Acceleration for non-quantized schemes is not supported by Compressed Tensors. Falling back to UnquantizedLinearMethod
[2026-01-18 20:22:52] [INFO] [0;36m(Worker_TP1 pid=181655)[0;0m INFO 01-18 20:22:52 [mm_encoder_attention.py:86] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[2026-01-18 20:22:52] [INFO] [0;36m(Worker_TP1 pid=181655)[0;0m INFO 01-18 20:22:52 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 20:22:56] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m INFO 01-18 20:22:56 [cuda.py:351] Using FLASHINFER attention backend out of potential backends: ('FLASHINFER', 'TRITON_ATTN')
[2026-01-18 20:22:56] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m INFO 01-18 20:22:56 [fp8.py:126] DeepGEMM is disabled because the platform does not support it.
[2026-01-18 20:22:56] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m INFO 01-18 20:22:56 [fp8.py:150] Using vLLM CUTLASS backend for FP8 MoE
[2026-01-18 20:22:57] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m
[2026-01-18 20:22:57] [INFO] Loading safetensors checkpoint shards:   0% Completed | 0/41 [00:00<?, ?it/s]
[2026-01-18 20:22:57] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m
[2026-01-18 20:22:57] [INFO] Loading safetensors checkpoint shards:   2% Completed | 1/41 [00:00<00:13,  3.03it/s]
[2026-01-18 20:22:57] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m
[2026-01-18 20:22:57] [INFO] Loading safetensors checkpoint shards:   5% Completed | 2/41 [00:00<00:13,  2.79it/s]
[2026-01-18 20:22:58] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m
[2026-01-18 20:22:58] [INFO] Loading safetensors checkpoint shards:   7% Completed | 3/41 [00:01<00:13,  2.88it/s]
[2026-01-18 20:22:58] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m
[2026-01-18 20:22:58] [INFO] Loading safetensors checkpoint shards:  10% Completed | 4/41 [00:01<00:13,  2.69it/s]
[2026-01-18 20:22:58] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m
[2026-01-18 20:22:58] [INFO] Loading safetensors checkpoint shards:  12% Completed | 5/41 [00:01<00:13,  2.70it/s]
[2026-01-18 20:22:59] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m
[2026-01-18 20:22:59] [INFO] Loading safetensors checkpoint shards:  15% Completed | 6/41 [00:02<00:13,  2.68it/s]
[2026-01-18 20:22:59] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m
[2026-01-18 20:22:59] [INFO] Loading safetensors checkpoint shards:  17% Completed | 7/41 [00:02<00:12,  2.71it/s]
[2026-01-18 20:22:59] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m
[2026-01-18 20:22:59] [INFO] Loading safetensors checkpoint shards:  20% Completed | 8/41 [00:02<00:12,  2.73it/s]
[2026-01-18 20:23:00] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m
[2026-01-18 20:23:00] [INFO] Loading safetensors checkpoint shards:  22% Completed | 9/41 [00:03<00:12,  2.63it/s]
[2026-01-18 20:23:00] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m
[2026-01-18 20:23:00] [INFO] Loading safetensors checkpoint shards:  24% Completed | 10/41 [00:03<00:12,  2.57it/s]
[2026-01-18 20:23:01] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m
[2026-01-18 20:23:01] [INFO] Loading safetensors checkpoint shards:  27% Completed | 11/41 [00:04<00:11,  2.52it/s]
[2026-01-18 20:23:01] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m
[2026-01-18 20:23:01] [INFO] Loading safetensors checkpoint shards:  29% Completed | 12/41 [00:04<00:11,  2.58it/s]
[2026-01-18 20:23:01] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m
[2026-01-18 20:23:01] [INFO] Loading safetensors checkpoint shards:  32% Completed | 13/41 [00:04<00:10,  2.63it/s]
[2026-01-18 20:23:02] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m
[2026-01-18 20:23:02] [INFO] Loading safetensors checkpoint shards:  34% Completed | 14/41 [00:05<00:10,  2.66it/s]
[2026-01-18 20:23:02] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m
[2026-01-18 20:23:02] [INFO] Loading safetensors checkpoint shards:  37% Completed | 15/41 [00:05<00:09,  2.67it/s]
[2026-01-18 20:23:03] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m
[2026-01-18 20:23:03] [INFO] Loading safetensors checkpoint shards:  39% Completed | 16/41 [00:06<00:09,  2.61it/s]
[2026-01-18 20:23:03] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m
[2026-01-18 20:23:03] [INFO] Loading safetensors checkpoint shards:  41% Completed | 17/41 [00:06<00:09,  2.54it/s]
[2026-01-18 20:23:03] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m
[2026-01-18 20:23:03] [INFO] Loading safetensors checkpoint shards:  44% Completed | 18/41 [00:06<00:08,  2.59it/s]
[2026-01-18 20:23:04] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m
[2026-01-18 20:23:04] [INFO] Loading safetensors checkpoint shards:  46% Completed | 19/41 [00:07<00:08,  2.64it/s]
[2026-01-18 20:23:04] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m
[2026-01-18 20:23:04] [INFO] Loading safetensors checkpoint shards:  49% Completed | 20/41 [00:07<00:07,  2.67it/s]
[2026-01-18 20:23:04] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m
[2026-01-18 20:23:04] [INFO] Loading safetensors checkpoint shards:  51% Completed | 21/41 [00:07<00:07,  2.70it/s]
[2026-01-18 20:23:05] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m
[2026-01-18 20:23:05] [INFO] Loading safetensors checkpoint shards:  54% Completed | 22/41 [00:08<00:07,  2.70it/s]
[2026-01-18 20:23:05] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m
[2026-01-18 20:23:05] [INFO] Loading safetensors checkpoint shards:  56% Completed | 23/41 [00:08<00:06,  2.66it/s]
[2026-01-18 20:23:06] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m
[2026-01-18 20:23:06] [INFO] Loading safetensors checkpoint shards:  59% Completed | 24/41 [00:09<00:06,  2.63it/s]
[2026-01-18 20:23:06] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m
[2026-01-18 20:23:06] [INFO] Loading safetensors checkpoint shards:  61% Completed | 25/41 [00:09<00:05,  2.68it/s]
[2026-01-18 20:23:06] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m
[2026-01-18 20:23:06] [INFO] Loading safetensors checkpoint shards:  63% Completed | 26/41 [00:09<00:05,  2.73it/s]
[2026-01-18 20:23:07] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m
[2026-01-18 20:23:07] [INFO] Loading safetensors checkpoint shards:  66% Completed | 27/41 [00:10<00:05,  2.72it/s]
[2026-01-18 20:23:07] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m
[2026-01-18 20:23:07] [INFO] Loading safetensors checkpoint shards:  68% Completed | 28/41 [00:10<00:04,  2.72it/s]
[2026-01-18 20:23:07] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m
[2026-01-18 20:23:07] [INFO] Loading safetensors checkpoint shards:  71% Completed | 29/41 [00:10<00:04,  2.67it/s]
[2026-01-18 20:23:08] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m
[2026-01-18 20:23:08] [INFO] Loading safetensors checkpoint shards:  73% Completed | 30/41 [00:11<00:04,  2.64it/s]
[2026-01-18 20:23:08] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m
[2026-01-18 20:23:08] [INFO] Loading safetensors checkpoint shards:  76% Completed | 31/41 [00:11<00:03,  2.68it/s]
[2026-01-18 20:23:09] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m
[2026-01-18 20:23:09] [INFO] Loading safetensors checkpoint shards:  78% Completed | 32/41 [00:11<00:03,  2.73it/s]
[2026-01-18 20:23:09] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m
[2026-01-18 20:23:09] [INFO] Loading safetensors checkpoint shards:  80% Completed | 33/41 [00:12<00:02,  2.74it/s]
[2026-01-18 20:23:09] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m
[2026-01-18 20:23:09] [INFO] Loading safetensors checkpoint shards:  83% Completed | 34/41 [00:12<00:02,  2.77it/s]
[2026-01-18 20:23:10] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m
[2026-01-18 20:23:10] [INFO] Loading safetensors checkpoint shards:  85% Completed | 35/41 [00:13<00:02,  2.73it/s]
[2026-01-18 20:23:10] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m
[2026-01-18 20:23:10] [INFO] Loading safetensors checkpoint shards:  88% Completed | 36/41 [00:13<00:01,  2.66it/s]
[2026-01-18 20:23:10] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m
[2026-01-18 20:23:10] [INFO] Loading safetensors checkpoint shards:  90% Completed | 37/41 [00:13<00:01,  2.63it/s]
[2026-01-18 20:23:11] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m
[2026-01-18 20:23:11] [INFO] Loading safetensors checkpoint shards:  93% Completed | 38/41 [00:14<00:01,  2.66it/s]
[2026-01-18 20:23:11] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m
[2026-01-18 20:23:11] [INFO] Loading safetensors checkpoint shards:  95% Completed | 39/41 [00:14<00:00,  2.69it/s]
[2026-01-18 20:23:11] [WARNING] [0;36m(Worker_TP1 pid=181655)[0;0m WARNING 01-18 20:23:11 [kv_cache.py:90] Checkpoint does not provide a q scaling factor. Setting it to k_scale. This only matters for FP8 Attention backends (flash-attn or flashinfer).
[2026-01-18 20:23:11] [WARNING] [0;36m(Worker_TP1 pid=181655)[0;0m WARNING 01-18 20:23:11 [kv_cache.py:104] Using KV cache scaling factor 1.0 for fp8_e4m3. If this is unintended, verify that k/v_scale scaling factors are properly set in the checkpoint.
[2026-01-18 20:23:11] [WARNING] [0;36m(Worker_TP1 pid=181655)[0;0m WARNING 01-18 20:23:11 [kv_cache.py:143] Using uncalibrated q_scale 1.0 and/or prob_scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure q/prob scaling factors are available in the fp8 checkpoint.
[2026-01-18 20:23:12] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m
[2026-01-18 20:23:12] [INFO] Loading safetensors checkpoint shards:  98% Completed | 40/41 [00:15<00:00,  2.56it/s]
[2026-01-18 20:23:12] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m
[2026-01-18 20:23:12] [INFO] Loading safetensors checkpoint shards: 100% Completed | 41/41 [00:15<00:00,  3.00it/s]
[2026-01-18 20:23:12] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m
[2026-01-18 20:23:12] [INFO] Loading safetensors checkpoint shards: 100% Completed | 41/41 [00:15<00:00,  2.69it/s]
[2026-01-18 20:23:12] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m
[2026-01-18 20:23:12] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m INFO 01-18 20:23:12 [default_loader.py:291] Loading weights took 15.25 seconds
[2026-01-18 20:23:12] [WARNING] [0;36m(Worker_TP0 pid=181654)[0;0m WARNING 01-18 20:23:12 [kv_cache.py:90] Checkpoint does not provide a q scaling factor. Setting it to k_scale. This only matters for FP8 Attention backends (flash-attn or flashinfer).
[2026-01-18 20:23:12] [WARNING] [0;36m(Worker_TP0 pid=181654)[0;0m WARNING 01-18 20:23:12 [kv_cache.py:104] Using KV cache scaling factor 1.0 for fp8_e4m3. If this is unintended, verify that k/v_scale scaling factors are properly set in the checkpoint.
[2026-01-18 20:23:12] [WARNING] [0;36m(Worker_TP0 pid=181654)[0;0m WARNING 01-18 20:23:12 [kv_cache.py:143] Using uncalibrated q_scale 1.0 and/or prob_scale 1.0 with fp8 attention. This may cause accuracy issues. Please make sure q/prob scaling factors are available in the fp8 checkpoint.
[2026-01-18 20:23:12] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m INFO 01-18 20:23:12 [gpu_model_runner.py:3900] Model loading took 52.22 GiB memory and 20.115516 seconds
[2026-01-18 20:23:13] [INFO] [0;36m(Worker_TP1 pid=181655)[0;0m INFO 01-18 20:23:13 [gpu_model_runner.py:4711] Encoder cache will be initialized with a budget of 30000 tokens, and profiled with 1 video items of the maximum feature size.
[2026-01-18 20:23:13] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m INFO 01-18 20:23:13 [gpu_model_runner.py:4711] Encoder cache will be initialized with a budget of 30000 tokens, and profiled with 1 video items of the maximum feature size.
[2026-01-18 20:23:31] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m INFO 01-18 20:23:31 [backends.py:644] Using cache directory: /home/wuwen/.cache/vllm/torch_compile_cache/e118bc0db9/rank_0_0/backbone for vLLM's torch.compile
[2026-01-18 20:23:31] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m INFO 01-18 20:23:31 [backends.py:704] Dynamo bytecode transform time: 13.13 s
[2026-01-18 20:23:43] [INFO] [0;36m(Worker_TP0 pid=181654)[0;0m INFO 01-18 20:23:43 [backends.py:261] Cache the graph of compile range (1, 8192) for later use
[2026-01-18 20:23:43] [INFO] [0;36m(Worker_TP1 pid=181655)[0;0m INFO 01-18 20:23:43 [backends.py:261] Cache the graph of compile range (1, 8192) for later use
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839] WorkerProc hit an exception.
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839] Traceback (most recent call last):
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 834, in worker_busy_loop
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     output = func(*args, **kwargs)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]              ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return func(*args, **kwargs)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 321, in determine_available_memory
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     self.model_runner.profile_run()
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4739, in profile_run
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     hidden_states, last_hidden_states = self._dummy_run(
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]                                         ^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return func(*args, **kwargs)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4457, in _dummy_run
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     outputs = self.model(
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]               ^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 222, in __call__
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return self.runnable(*args, **kwargs)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/glm4_1v.py", line 1764, in forward
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     hidden_states = self.language_model.model(
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 557, in __call__
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)  # type: ignore[arg-type]
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 228, in __call__
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return self._call_with_optional_nvtx_range(
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 119, in _call_with_optional_nvtx_range
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return callable_fn(*args, **kwargs)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return fn(*args, **kwargs)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/glm4_moe.py", line 452, in forward
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     def forward(
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return fn(*args, **kwargs)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/caching.py", line 64, in __call__
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return self.optimized_call(*args, **kwargs)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return self._wrapped_call(self, *args, **kwargs)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/fx/graph_module.py", line 413, in __call__
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     raise e
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/fx/graph_module.py", line 400, in __call__
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "<eval_with_key>.94", line 393, in forward
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     submod_4 = self.submod_4(getitem_7, s59, s18, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_8, l_self_modules_layers_modules_1_modules_mlp_modules_gate_parameters_weight_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_, l_positions_, s7);  getitem_7 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_8 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_parameters_weight_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 222, in __call__
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return self.runnable(*args, **kwargs)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/piecewise_backend.py", line 191, in __call__
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return range_entry.runnable(*args)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return self._compiled_fn(*args)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return fn(*args, **kwargs)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return compiled_fn(full_args)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     all_outs = call_func_at_runtime_with_args(
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     out = normalize_as_list(f(args))
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]                             ^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return compiled_fn(runtime_args)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_inductor/output_code.py", line 613, in __call__
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return self.current_callable(inputs)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_inductor/utils.py", line 3017, in run
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     out = model(new_inputs)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]           ^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/tmp/torchinductor_wuwen/ct/cctdwzzfufdzd76q46mh5ef2a3dt4kas62lnlxc54qvfsslr3v7f.py", line 1504, in call
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     buf13 = torch.ops.vllm.moe_forward_shared.default(buf12, buf11, 'language_model.model.layers.1.mlp.experts')
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_ops.py", line 841, in __call__
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return self._op(*args, **kwargs)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 2131, in moe_forward_shared
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return self.forward_impl(hidden_states, router_logits)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1979, in forward_impl
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     final_hidden_states = self.quant_method.apply(
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]                           ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py", line 1122, in apply
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     result = self.kernel(
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]              ^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/modular_kernel.py", line 1228, in forward
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     fused_out = self._fused_experts(
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]                 ^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/modular_kernel.py", line 1082, in _fused_experts
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     self.fused_experts.apply(
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/fallback.py", line 111, in apply
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     experts.apply(
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/cutlass_moe.py", line 298, in apply
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     run_cutlass_moe_fp8(
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/cutlass_moe.py", line 188, in run_cutlass_moe_fp8
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     ops.cutlass_moe_mm(
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/_custom_ops.py", line 1175, in cutlass_moe_mm
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return torch.ops._C.cutlass_moe_mm(
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_ops.py", line 1255, in __call__
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return self._op(*args, **kwargs)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839] NotImplementedError: No compiled cutlass_scaled_mm for CUDA device capability: 120. Required capability: 90 or 100
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839] Traceback (most recent call last):
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 834, in worker_busy_loop
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     output = func(*args, **kwargs)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]              ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return func(*args, **kwargs)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 321, in determine_available_memory
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     self.model_runner.profile_run()
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4739, in profile_run
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     hidden_states, last_hidden_states = self._dummy_run(
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]                                         ^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return func(*args, **kwargs)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4457, in _dummy_run
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     outputs = self.model(
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]               ^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 222, in __call__
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return self.runnable(*args, **kwargs)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/glm4_1v.py", line 1764, in forward
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     hidden_states = self.language_model.model(
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/decorators.py", line 557, in __call__
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)  # type: ignore[arg-type]
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 228, in __call__
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return self._call_with_optional_nvtx_range(
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/wrapper.py", line 119, in _call_with_optional_nvtx_range
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return callable_fn(*args, **kwargs)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return fn(*args, **kwargs)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/glm4_moe.py", line 452, in forward
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     def forward(
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return fn(*args, **kwargs)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/caching.py", line 64, in __call__
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return self.optimized_call(*args, **kwargs)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/fx/graph_module.py", line 837, in call_wrapped
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return self._wrapped_call(self, *args, **kwargs)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/fx/graph_module.py", line 413, in __call__
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     raise e
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/fx/graph_module.py", line 400, in __call__
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "<eval_with_key>.94", line 393, in forward
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     submod_4 = self.submod_4(getitem_7, s59, s18, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_8, l_self_modules_layers_modules_1_modules_mlp_modules_gate_parameters_weight_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_, l_positions_, s7);  getitem_7 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_8 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_parameters_weight_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/cuda_graph.py", line 222, in __call__
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return self.runnable(*args, **kwargs)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/compilation/piecewise_backend.py", line 191, in __call__
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return range_entry.runnable(*args)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return self._compiled_fn(*args)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return fn(*args, **kwargs)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return compiled_fn(full_args)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     all_outs = call_func_at_runtime_with_args(
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     out = normalize_as_list(f(args))
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]                             ^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return compiled_fn(runtime_args)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_inductor/output_code.py", line 613, in __call__
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return self.current_callable(inputs)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_inductor/utils.py", line 3017, in run
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     out = model(new_inputs)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]           ^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/tmp/torchinductor_wuwen/ct/cctdwzzfufdzd76q46mh5ef2a3dt4kas62lnlxc54qvfsslr3v7f.py", line 1504, in call
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     buf13 = torch.ops.vllm.moe_forward_shared.default(buf12, buf11, 'language_model.model.layers.1.mlp.experts')
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_ops.py", line 841, in __call__
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return self._op(*args, **kwargs)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 2131, in moe_forward_shared
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return self.forward_impl(hidden_states, router_logits)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/layer.py", line 1979, in forward_impl
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     final_hidden_states = self.quant_method.apply(
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]                           ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py", line 1122, in apply
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     result = self.kernel(
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]              ^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return self._call_impl(*args, **kwargs)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return forward_call(*args, **kwargs)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/modular_kernel.py", line 1228, in forward
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     fused_out = self._fused_experts(
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]                 ^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/modular_kernel.py", line 1082, in _fused_experts
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     self.fused_experts.apply(
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/fallback.py", line 111, in apply
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     experts.apply(
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/cutlass_moe.py", line 298, in apply
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     run_cutlass_moe_fp8(
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/cutlass_moe.py", line 188, in run_cutlass_moe_fp8
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     ops.cutlass_moe_mm(
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/_custom_ops.py", line 1175, in cutlass_moe_mm
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return torch.ops._C.cutlass_moe_mm(
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/_ops.py", line 1255, in __call__
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]     return self._op(*args, **kwargs)
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]            ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839] NotImplementedError: No compiled cutlass_scaled_mm for CUDA device capability: 120. Required capability: 90 or 100
[2026-01-18 20:23:44] [ERROR] [0;36m(Worker_TP0 pid=181654)[0;0m ERROR 01-18 20:23:44 [multiproc_executor.py:839]
[2026-01-18 20:23:44] [ERROR] [0;36m(EngineCore_DP0 pid=181044)[0;0m ERROR 01-18 20:23:44 [core.py:935] EngineCore failed to start.
[2026-01-18 20:23:44] [ERROR] [0;36m(EngineCore_DP0 pid=181044)[0;0m ERROR 01-18 20:23:44 [core.py:935] Traceback (most recent call last):
[2026-01-18 20:23:44] [ERROR] [0;36m(EngineCore_DP0 pid=181044)[0;0m ERROR 01-18 20:23:44 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-18 20:23:44] [ERROR] [0;36m(EngineCore_DP0 pid=181044)[0;0m ERROR 01-18 20:23:44 [core.py:935]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 20:23:44] [ERROR] [0;36m(EngineCore_DP0 pid=181044)[0;0m ERROR 01-18 20:23:44 [core.py:935]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(EngineCore_DP0 pid=181044)[0;0m ERROR 01-18 20:23:44 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-18 20:23:44] [ERROR] [0;36m(EngineCore_DP0 pid=181044)[0;0m ERROR 01-18 20:23:44 [core.py:935]     super().__init__(
[2026-01-18 20:23:44] [ERROR] [0;36m(EngineCore_DP0 pid=181044)[0;0m ERROR 01-18 20:23:44 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 112, in __init__
[2026-01-18 20:23:44] [ERROR] [0;36m(EngineCore_DP0 pid=181044)[0;0m ERROR 01-18 20:23:44 [core.py:935]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[2026-01-18 20:23:44] [ERROR] [0;36m(EngineCore_DP0 pid=181044)[0;0m ERROR 01-18 20:23:44 [core.py:935]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(EngineCore_DP0 pid=181044)[0;0m ERROR 01-18 20:23:44 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 242, in _initialize_kv_caches
[2026-01-18 20:23:44] [ERROR] [0;36m(EngineCore_DP0 pid=181044)[0;0m ERROR 01-18 20:23:44 [core.py:935]     available_gpu_memory = self.model_executor.determine_available_memory()
[2026-01-18 20:23:44] [ERROR] [0;36m(EngineCore_DP0 pid=181044)[0;0m ERROR 01-18 20:23:44 [core.py:935]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(EngineCore_DP0 pid=181044)[0;0m ERROR 01-18 20:23:44 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[2026-01-18 20:23:44] [ERROR] [0;36m(EngineCore_DP0 pid=181044)[0;0m ERROR 01-18 20:23:44 [core.py:935]     return self.collective_rpc("determine_available_memory")
[2026-01-18 20:23:44] [ERROR] [0;36m(EngineCore_DP0 pid=181044)[0;0m ERROR 01-18 20:23:44 [core.py:935]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(EngineCore_DP0 pid=181044)[0;0m ERROR 01-18 20:23:44 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 374, in collective_rpc
[2026-01-18 20:23:44] [ERROR] [0;36m(EngineCore_DP0 pid=181044)[0;0m ERROR 01-18 20:23:44 [core.py:935]     return aggregate(get_response())
[2026-01-18 20:23:44] [ERROR] [0;36m(EngineCore_DP0 pid=181044)[0;0m ERROR 01-18 20:23:44 [core.py:935]                      ^^^^^^^^^^^^^^
[2026-01-18 20:23:44] [ERROR] [0;36m(EngineCore_DP0 pid=181044)[0;0m ERROR 01-18 20:23:44 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 357, in get_response
[2026-01-18 20:23:44] [ERROR] [0;36m(EngineCore_DP0 pid=181044)[0;0m ERROR 01-18 20:23:44 [core.py:935]     raise RuntimeError(
[2026-01-18 20:23:44] [ERROR] [0;36m(EngineCore_DP0 pid=181044)[0;0m ERROR 01-18 20:23:44 [core.py:935] RuntimeError: Worker failed with error 'No compiled cutlass_scaled_mm for CUDA device capability: 120. Required capability: 90 or 100', please check the stack trace above for the root cause
[2026-01-18 20:23:46] [ERROR] [0;36m(EngineCore_DP0 pid=181044)[0;0m ERROR 01-18 20:23:46 [multiproc_executor.py:246] Worker proc VllmWorker-1 died unexpectedly, shutting down executor.
[2026-01-18 20:23:47] [INFO] [0;36m(EngineCore_DP0 pid=181044)[0;0m Process EngineCore_DP0:
[2026-01-18 20:23:47] [ERROR] [0;36m(EngineCore_DP0 pid=181044)[0;0m Traceback (most recent call last):
[2026-01-18 20:23:47] [INFO] [0;36m(EngineCore_DP0 pid=181044)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[2026-01-18 20:23:47] [INFO] [0;36m(EngineCore_DP0 pid=181044)[0;0m     self.run()
[2026-01-18 20:23:47] [INFO] [0;36m(EngineCore_DP0 pid=181044)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/process.py", line 108, in run
[2026-01-18 20:23:47] [INFO] [0;36m(EngineCore_DP0 pid=181044)[0;0m     self._target(*self._args, **self._kwargs)
[2026-01-18 20:23:47] [INFO] [0;36m(EngineCore_DP0 pid=181044)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 939, in run_engine_core
[2026-01-18 20:23:47] [INFO] [0;36m(EngineCore_DP0 pid=181044)[0;0m     raise e
[2026-01-18 20:23:47] [INFO] [0;36m(EngineCore_DP0 pid=181044)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-18 20:23:47] [INFO] [0;36m(EngineCore_DP0 pid=181044)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 20:23:47] [INFO] [0;36m(EngineCore_DP0 pid=181044)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:47] [INFO] [0;36m(EngineCore_DP0 pid=181044)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-18 20:23:47] [INFO] [0;36m(EngineCore_DP0 pid=181044)[0;0m     super().__init__(
[2026-01-18 20:23:47] [INFO] [0;36m(EngineCore_DP0 pid=181044)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 112, in __init__
[2026-01-18 20:23:47] [INFO] [0;36m(EngineCore_DP0 pid=181044)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[2026-01-18 20:23:47] [INFO] [0;36m(EngineCore_DP0 pid=181044)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:47] [INFO] [0;36m(EngineCore_DP0 pid=181044)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 242, in _initialize_kv_caches
[2026-01-18 20:23:47] [INFO] [0;36m(EngineCore_DP0 pid=181044)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[2026-01-18 20:23:47] [INFO] [0;36m(EngineCore_DP0 pid=181044)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:47] [INFO] [0;36m(EngineCore_DP0 pid=181044)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[2026-01-18 20:23:47] [INFO] [0;36m(EngineCore_DP0 pid=181044)[0;0m     return self.collective_rpc("determine_available_memory")
[2026-01-18 20:23:47] [INFO] [0;36m(EngineCore_DP0 pid=181044)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:47] [INFO] [0;36m(EngineCore_DP0 pid=181044)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 374, in collective_rpc
[2026-01-18 20:23:47] [INFO] [0;36m(EngineCore_DP0 pid=181044)[0;0m     return aggregate(get_response())
[2026-01-18 20:23:47] [INFO] [0;36m(EngineCore_DP0 pid=181044)[0;0m                      ^^^^^^^^^^^^^^
[2026-01-18 20:23:47] [INFO] [0;36m(EngineCore_DP0 pid=181044)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 357, in get_response
[2026-01-18 20:23:47] [INFO] [0;36m(EngineCore_DP0 pid=181044)[0;0m     raise RuntimeError(
[2026-01-18 20:23:47] [INFO] [0;36m(EngineCore_DP0 pid=181044)[0;0m RuntimeError: Worker failed with error 'No compiled cutlass_scaled_mm for CUDA device capability: 120. Required capability: 90 or 100', please check the stack trace above for the root cause
[2026-01-18 20:23:48] [ERROR] [0;36m(APIServer pid=179800)[0;0m Traceback (most recent call last):
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/bin/vllm", line 7, in <module>
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m     sys.exit(main())
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m              ^^^^^^
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m     args.dispatch_function(args)
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m     uvloop.run(run_server(args))
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m     return __asyncio.run(
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m            ^^^^^^^^^^^^^^
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m     return runner.run(main)
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m     return self._loop.run_until_complete(task)
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m     return await main
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m            ^^^^^^^^^^
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m     async with build_async_engine_client(
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m     return await anext(self.gen)
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 146, in build_async_engine_client
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m     return await anext(self.gen)
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 187, in build_async_engine_client_from_engine_args
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 202, in from_vllm_config
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m     return cls(
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m            ^^^^
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 129, in __init__
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m     return AsyncMPClient(*client_args)
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m     super().__init__(
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 144, in __exit__
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m     next(self.gen)
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 918, in launch_core_engines
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m     wait_for_engine_startup(
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 977, in wait_for_engine_startup
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m     raise RuntimeError(
[2026-01-18 20:23:48] [INFO] [0;36m(APIServer pid=179800)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[2026-01-18 20:23:56] [INFO] nvitop监控已启动
[2026-01-18 20:48:48] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 20:48:48] [INFO] nvitop监控已启动
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m INFO 01-18 20:49:13 [api_server.py:1278] vLLM API server version 0.14.0rc1.dev492+g19504ac07
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m INFO 01-18 20:49:13 [utils.py:254] non-default args: {'model_tag': '/mnt/i/AI-Chat/models/gemma-3/gemma-3-27b-abliterated/', 'host': '0.0.0.0', 'port': 8005, 'model': '/mnt/i/AI-Chat/models/gemma-3/gemma-3-27b-abliterated/', 'trust_remote_code': True, 'allowed_local_media_path': '/', 'max_model_len': 128000, 'served_model_name': ['VLLM-MODEL'], 'tensor_parallel_size': 2, 'kv_cache_dtype': 'fp8', 'mm_processor_cache_type': 'shm', 'mm_encoder_tp_mode': 'data', 'max_num_seqs': 256, 'enable_chunked_prefill': True, 'async_scheduling': True}
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 20:49:13] [ERROR] [0;36m(APIServer pid=203757)[0;0m Traceback (most recent call last):
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/transformers/utils/hub.py", line 479, in cached_files
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m     hf_hub_download(
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m     validate_repo_id(arg_value)
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m     raise HFValidationError(
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/mnt/i/AI-Chat/models/gemma-3/gemma-3-27b-abliterated/'. Use `repo_type` argument if needed.
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m During handling of the above exception, another exception occurred:
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m
[2026-01-18 20:49:13] [ERROR] [0;36m(APIServer pid=203757)[0;0m Traceback (most recent call last):
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m     resolved_config_file = cached_file(
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m                            ^^^^^^^^^^^^
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/transformers/utils/hub.py", line 322, in cached_file
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m     file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/transformers/utils/hub.py", line 532, in cached_files
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m     _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m     resolved_file = try_to_load_from_cache(
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m                     ^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m     validate_repo_id(arg_value)
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m     raise HFValidationError(
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/mnt/i/AI-Chat/models/gemma-3/gemma-3-27b-abliterated/'. Use `repo_type` argument if needed.
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m During handling of the above exception, another exception occurred:
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m
[2026-01-18 20:49:13] [ERROR] [0;36m(APIServer pid=203757)[0;0m Traceback (most recent call last):
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/bin/vllm", line 7, in <module>
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m     sys.exit(main())
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m              ^^^^^^
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m     args.dispatch_function(args)
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m     uvloop.run(run_server(args))
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m     return __asyncio.run(
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m            ^^^^^^^^^^^^^^
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m     return runner.run(main)
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m     return self._loop.run_until_complete(task)
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m     return await main
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m            ^^^^^^^^^^
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 1325, in run_server
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 1344, in run_server_worker
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m     async with build_async_engine_client(
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m     return await anext(self.gen)
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 173, in build_async_engine_client
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m     return await anext(self.gen)
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 199, in build_async_engine_client_from_engine_args
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/engine/arg_utils.py", line 1356, in create_engine_config
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m     maybe_override_with_speculators(
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/transformers_utils/config.py", line 528, in maybe_override_with_speculators
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m     config_dict, _ = PretrainedConfig.get_config_dict(
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m     config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m     raise OSError(
[2026-01-18 20:49:13] [INFO] [0;36m(APIServer pid=203757)[0;0m OSError: Can't load the configuration of '/mnt/i/AI-Chat/models/gemma-3/gemma-3-27b-abliterated/'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/mnt/i/AI-Chat/models/gemma-3/gemma-3-27b-abliterated/' is the correct path to a directory containing a config.json file
[2026-01-18 20:49:16] [INFO] nvitop监控已启动
[2026-01-18 20:52:35] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 20:52:35] [INFO] nvitop监控已启动
[2026-01-18 20:53:00] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:53:00 [api_server.py:1278] vLLM API server version 0.14.0rc1.dev492+g19504ac07
[2026-01-18 20:53:00] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:53:00 [utils.py:254] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma-3-27b-abliterated/', 'host': '0.0.0.0', 'port': 8005, 'model': '/mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma-3-27b-abliterated/', 'trust_remote_code': True, 'allowed_local_media_path': '/', 'max_model_len': 128000, 'served_model_name': ['VLLM-MODEL'], 'tensor_parallel_size': 2, 'kv_cache_dtype': 'fp8', 'mm_processor_cache_type': 'shm', 'mm_encoder_tp_mode': 'data', 'max_num_seqs': 256, 'enable_chunked_prefill': True, 'async_scheduling': True}
[2026-01-18 20:53:00] [INFO] [0;36m(APIServer pid=206720)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 20:53:00] [INFO] [0;36m(APIServer pid=206720)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 20:53:22] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:53:22 [model.py:528] Resolved architecture: Gemma3ForConditionalGeneration
[2026-01-18 20:53:22] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:53:22 [model.py:1543] Using max model len 128000
[2026-01-18 20:53:22] [WARNING] [0;36m(APIServer pid=206720)[0;0m WARNING 01-18 20:53:22 [model.py:570] This model does not support `--mm-encoder-tp-mode data`. Falling back to `--mm-encoder-tp-mode weights`.
[2026-01-18 20:53:22] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:53:22 [cache.py:206] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor.
[2026-01-18 20:53:22] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:53:22 [scheduler.py:231] Chunked prefill is enabled with max_num_batched_tokens=8192.
[2026-01-18 20:53:22] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:53:22 [vllm.py:640] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-18 20:53:22] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:53:22 [vllm.py:645] Asynchronous scheduling is enabled.
[2026-01-18 20:53:22] [WARNING] [0;36m(APIServer pid=206720)[0;0m WARNING 01-18 20:53:22 [cuda.py:244] Forcing --disable_chunked_mm_input for models with multimodal-bidirectional attention.
[2026-01-18 20:53:23] [INFO] [0;36m(APIServer pid=206720)[0;0m The tokenizer you are loading from '/mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma-3-27b-abliterated/' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[2026-01-18 20:53:46] [INFO] [0;36m(EngineCore_DP0 pid=207815)[0;0m INFO 01-18 20:53:46 [core.py:97] Initializing a V1 LLM engine (v0.14.0rc1.dev492+g19504ac07) with config: model='/mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma-3-27b-abliterated/', speculative_config=None, tokenizer='/mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma-3-27b-abliterated/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=fp8, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-18 20:53:46] [WARNING] [0;36m(EngineCore_DP0 pid=207815)[0;0m WARNING 01-18 20:53:46 [multiproc_executor.py:880] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-18 20:54:11] [INFO] The tokenizer you are loading from '/mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma-3-27b-abliterated/' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[2026-01-18 20:54:11] [INFO] The tokenizer you are loading from '/mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma-3-27b-abliterated/' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[2026-01-18 20:54:12] [INFO] INFO 01-18 20:54:12 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:45077 backend=nccl
[2026-01-18 20:54:12] [INFO] INFO 01-18 20:54:12 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:45077 backend=nccl
[2026-01-18 20:54:12] [INFO] INFO 01-18 20:54:12 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-18 20:54:13] [WARNING] WARNING 01-18 20:54:13 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 20:54:13] [WARNING] WARNING 01-18 20:54:13 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 20:54:13] [INFO] INFO 01-18 20:54:13 [parallel_state.py:1425] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
[2026-01-18 20:54:13] [INFO] INFO 01-18 20:54:13 [parallel_state.py:1425] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[2026-01-18 20:54:14] [INFO] Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[2026-01-18 20:54:14] [INFO] Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[2026-01-18 20:54:15] [INFO] The tokenizer you are loading from '/mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma-3-27b-abliterated/' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[2026-01-18 20:54:15] [INFO] The tokenizer you are loading from '/mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma-3-27b-abliterated/' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[2026-01-18 20:54:18] [INFO] The tokenizer you are loading from '/mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma-3-27b-abliterated/' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[2026-01-18 20:54:18] [INFO] The tokenizer you are loading from '/mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma-3-27b-abliterated/' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[2026-01-18 20:54:21] [INFO] [0;36m(Worker_TP0 pid=208405)[0;0m INFO 01-18 20:54:21 [gpu_model_runner.py:3789] Starting to load model /mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma-3-27b-abliterated/...
[2026-01-18 20:54:21] [INFO] [0;36m(Worker_TP0 pid=208405)[0;0m INFO 01-18 20:54:21 [mm_encoder_attention.py:86] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[2026-01-18 20:54:21] [INFO] [0;36m(Worker_TP0 pid=208405)[0;0m INFO 01-18 20:54:21 [vllm.py:645] Asynchronous scheduling is enabled.
[2026-01-18 20:54:21] [INFO] [0;36m(Worker_TP1 pid=208406)[0;0m INFO 01-18 20:54:21 [mm_encoder_attention.py:86] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[2026-01-18 20:54:21] [INFO] [0;36m(Worker_TP1 pid=208406)[0;0m INFO 01-18 20:54:21 [vllm.py:645] Asynchronous scheduling is enabled.
[2026-01-18 20:54:25] [INFO] [0;36m(Worker_TP0 pid=208405)[0;0m INFO 01-18 20:54:25 [cuda.py:351] Using TRITON_ATTN attention backend out of potential backends: ('TRITON_ATTN',)
[2026-01-18 20:54:25] [INFO] [0;36m(Worker_TP0 pid=208405)[0;0m
[2026-01-18 20:54:25] [INFO] Loading safetensors checkpoint shards:   0% Completed | 0/12 [00:00<?, ?it/s]
[2026-01-18 20:54:30] [INFO] [0;36m(Worker_TP0 pid=208405)[0;0m
[2026-01-18 20:54:30] [INFO] Loading safetensors checkpoint shards:   8% Completed | 1/12 [00:05<00:56,  5.10s/it]
[2026-01-18 20:54:35] [INFO] [0;36m(Worker_TP0 pid=208405)[0;0m
[2026-01-18 20:54:35] [INFO] Loading safetensors checkpoint shards:  17% Completed | 2/12 [00:10<00:51,  5.12s/it]
[2026-01-18 20:54:40] [INFO] [0;36m(Worker_TP0 pid=208405)[0;0m
[2026-01-18 20:54:40] [INFO] Loading safetensors checkpoint shards:  25% Completed | 3/12 [00:15<00:45,  5.03s/it]
[2026-01-18 20:54:45] [INFO] [0;36m(Worker_TP0 pid=208405)[0;0m
[2026-01-18 20:54:45] [INFO] Loading safetensors checkpoint shards:  33% Completed | 4/12 [00:20<00:40,  5.04s/it]
[2026-01-18 20:54:50] [INFO] [0;36m(Worker_TP0 pid=208405)[0;0m
[2026-01-18 20:54:50] [INFO] Loading safetensors checkpoint shards:  42% Completed | 5/12 [00:25<00:35,  5.03s/it]
[2026-01-18 20:54:55] [INFO] [0;36m(Worker_TP0 pid=208405)[0;0m
[2026-01-18 20:54:55] [INFO] Loading safetensors checkpoint shards:  50% Completed | 6/12 [00:30<00:30,  5.01s/it]
[2026-01-18 20:55:01] [INFO] [0;36m(Worker_TP0 pid=208405)[0;0m
[2026-01-18 20:55:01] [INFO] Loading safetensors checkpoint shards:  58% Completed | 7/12 [00:35<00:25,  5.05s/it]
[2026-01-18 20:55:06] [INFO] [0;36m(Worker_TP0 pid=208405)[0;0m
[2026-01-18 20:55:06] [INFO] Loading safetensors checkpoint shards:  67% Completed | 8/12 [00:40<00:20,  5.09s/it]
[2026-01-18 20:55:11] [INFO] [0;36m(Worker_TP0 pid=208405)[0;0m
[2026-01-18 20:55:11] [INFO] Loading safetensors checkpoint shards:  75% Completed | 9/12 [00:45<00:15,  5.11s/it]
[2026-01-18 20:55:16] [INFO] [0;36m(Worker_TP0 pid=208405)[0;0m
[2026-01-18 20:55:16] [INFO] Loading safetensors checkpoint shards:  83% Completed | 10/12 [00:50<00:10,  5.13s/it]
[2026-01-18 20:55:21] [INFO] [0;36m(Worker_TP0 pid=208405)[0;0m
[2026-01-18 20:55:21] [INFO] Loading safetensors checkpoint shards:  92% Completed | 11/12 [00:56<00:05,  5.15s/it]
[2026-01-18 20:55:22] [INFO] [0;36m(Worker_TP0 pid=208405)[0;0m
[2026-01-18 20:55:22] [INFO] Loading safetensors checkpoint shards: 100% Completed | 12/12 [00:56<00:00,  3.77s/it]
[2026-01-18 20:55:22] [INFO] [0;36m(Worker_TP0 pid=208405)[0;0m
[2026-01-18 20:55:22] [INFO] Loading safetensors checkpoint shards: 100% Completed | 12/12 [00:56<00:00,  4.72s/it]
[2026-01-18 20:55:22] [INFO] [0;36m(Worker_TP0 pid=208405)[0;0m
[2026-01-18 20:55:22] [INFO] [0;36m(Worker_TP0 pid=208405)[0;0m INFO 01-18 20:55:22 [default_loader.py:291] Loading weights took 56.64 seconds
[2026-01-18 20:55:22] [INFO] [0;36m(Worker_TP0 pid=208405)[0;0m INFO 01-18 20:55:22 [gpu_model_runner.py:3886] Model loading took 25.91 GiB memory and 60.570993 seconds
[2026-01-18 20:55:23] [INFO] [0;36m(Worker_TP0 pid=208405)[0;0m INFO 01-18 20:55:23 [gpu_model_runner.py:4696] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.
[2026-01-18 20:55:23] [INFO] [0;36m(Worker_TP1 pid=208406)[0;0m INFO 01-18 20:55:23 [gpu_model_runner.py:4696] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.
[2026-01-18 20:55:49] [INFO] [0;36m(Worker_TP0 pid=208405)[0;0m INFO 01-18 20:55:49 [backends.py:644] Using cache directory: /home/wuwen/.cache/vllm/torch_compile_cache/f98baa5859/rank_0_0/backbone for vLLM's torch.compile
[2026-01-18 20:55:49] [INFO] [0;36m(Worker_TP0 pid=208405)[0;0m INFO 01-18 20:55:49 [backends.py:704] Dynamo bytecode transform time: 13.85 s
[2026-01-18 20:56:16] [INFO] [0;36m(Worker_TP1 pid=208406)[0;0m INFO 01-18 20:56:16 [backends.py:261] Cache the graph of compile range (1, 8192) for later use
[2026-01-18 20:56:16] [INFO] [0;36m(Worker_TP0 pid=208405)[0;0m INFO 01-18 20:56:16 [backends.py:261] Cache the graph of compile range (1, 8192) for later use
[2026-01-18 20:56:22] [INFO] [0;36m(EngineCore_DP0 pid=207815)[0;0m INFO 01-18 20:56:22 [shm_broadcast.py:542] No available shared memory broadcast block found in 60 seconds. This typically happens when some processes are hanging or doing some time-consuming work (e.g. compilation, weight/kv cache quantization).
[2026-01-18 20:56:52] [INFO] [0;36m(Worker_TP0 pid=208405)[0;0m INFO 01-18 20:56:52 [backends.py:278] Compiling a graph for compile range (1, 8192) takes 47.38 s
[2026-01-18 20:56:52] [INFO] [0;36m(Worker_TP0 pid=208405)[0;0m INFO 01-18 20:56:52 [monitor.py:34] torch.compile takes 61.23 s in total
[2026-01-18 20:56:53] [INFO] [0;36m(Worker_TP0 pid=208405)[0;0m INFO 01-18 20:56:53 [gpu_worker.py:358] Available KV cache memory: 56.77 GiB
[2026-01-18 20:56:54] [WARNING] [0;36m(EngineCore_DP0 pid=207815)[0;0m WARNING 01-18 20:56:54 [kv_cache_utils.py:1047] Add 8 padding layers, may waste at most 15.38% KV cache memory
[2026-01-18 20:56:54] [INFO] [0;36m(EngineCore_DP0 pid=207815)[0;0m INFO 01-18 20:56:54 [kv_cache_utils.py:1305] GPU KV cache size: 425,184 tokens
[2026-01-18 20:56:54] [INFO] [0;36m(EngineCore_DP0 pid=207815)[0;0m INFO 01-18 20:56:54 [kv_cache_utils.py:1310] Maximum concurrency for 128,000 tokens per request: 16.23x
[2026-01-18 20:56:54] [INFO] [0;36m(Worker_TP1 pid=208406)[0;0m [0;36m(Worker_TP0 pid=208405)[0;0m 2026-01-18 20:56:54,492 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[2026-01-18 20:56:54] [INFO] 2026-01-18 20:56:54,492 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[2026-01-18 20:56:54] [INFO] [0;36m(Worker_TP1 pid=208406)[0;0m 2026-01-18 20:56:54,522 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[2026-01-18 20:56:54] [INFO] [0;36m(Worker_TP0 pid=208405)[0;0m 2026-01-18 20:56:54,523 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[2026-01-18 20:56:55] [INFO] [0;36m(Worker_TP0 pid=208405)[0;0m
[2026-01-18 20:56:55] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
[2026-01-18 20:56:55] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:17,  2.94it/s]
[2026-01-18 20:56:55] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:06,  7.05it/s]
[2026-01-18 20:56:55] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:00<00:04,  9.42it/s]
[2026-01-18 20:56:56] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▎        | 7/51 [00:00<00:04, 10.43it/s]
[2026-01-18 20:56:56] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:00<00:04, 10.49it/s]
[2026-01-18 20:56:56] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:01<00:03, 11.07it/s]
[2026-01-18 20:56:56] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:01<00:03, 11.54it/s]
[2026-01-18 20:56:56] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:01<00:03, 11.92it/s]
[2026-01-18 20:56:56] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:01<00:02, 12.45it/s]
[2026-01-18 20:56:56] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 19/51 [00:01<00:02, 13.17it/s]
[2026-01-18 20:56:57] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:01<00:02, 13.52it/s]
[2026-01-18 20:56:57] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:02<00:02, 13.73it/s]
[2026-01-18 20:56:57] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 25/51 [00:02<00:01, 14.15it/s]
[2026-01-18 20:56:57] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:02<00:01, 14.37it/s]
[2026-01-18 20:56:57] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:02<00:01, 14.28it/s]
[2026-01-18 20:56:57] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 31/51 [00:02<00:01, 13.63it/s]
[2026-01-18 20:56:58] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:02<00:01, 12.42it/s]
[2026-01-18 20:56:58] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:02<00:01, 12.81it/s]
[2026-01-18 20:56:58] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 37/51 [00:03<00:01, 12.97it/s]
[2026-01-18 20:56:58] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▋  | 39/51 [00:03<00:00, 13.21it/s]
[2026-01-18 20:56:58] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:03<00:00, 13.73it/s]
[2026-01-18 20:56:58] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 43/51 [00:03<00:00, 14.18it/s]
[2026-01-18 20:56:58] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 45/51 [00:03<00:00, 14.61it/s]
[2026-01-18 20:56:58] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:03<00:00, 14.92it/s]
[2026-01-18 20:56:59] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▌| 49/51 [00:03<00:00, 15.10it/s]
[2026-01-18 20:56:59] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:04<00:00, 14.19it/s]
[2026-01-18 20:56:59] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:04<00:00, 12.66it/s]
[2026-01-18 20:56:59] [INFO] [0;36m(Worker_TP0 pid=208405)[0;0m
[2026-01-18 20:57:00] [INFO] Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
[2026-01-18 20:57:01] [INFO] Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:01<00:50,  1.47s/it]
[2026-01-18 20:57:01] [INFO] Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:02<00:36,  1.10s/it]
[2026-01-18 20:57:01] [INFO] Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:02<00:14,  2.18it/s]
[2026-01-18 20:57:01] [INFO] Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:02<00:08,  3.61it/s]
[2026-01-18 20:57:02] [INFO] Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:02<00:05,  5.12it/s]
[2026-01-18 20:57:02] [INFO] Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:02<00:03,  6.61it/s]
[2026-01-18 20:57:02] [INFO] Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:03<00:02,  8.02it/s]
[2026-01-18 20:57:02] [INFO] Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:03<00:02,  9.28it/s]
[2026-01-18 20:57:02] [INFO] Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:03<00:01, 10.30it/s]
[2026-01-18 20:57:02] [INFO] Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:03<00:01, 11.16it/s]
[2026-01-18 20:57:02] [INFO] Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:03<00:01, 11.85it/s]
[2026-01-18 20:57:03] [INFO] Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:03<00:01, 12.41it/s]
[2026-01-18 20:57:03] [INFO] Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:03<00:00, 12.77it/s]
[2026-01-18 20:57:03] [INFO] Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:04<00:00, 13.11it/s]
[2026-01-18 20:57:03] [INFO] Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:04<00:00, 13.45it/s]
[2026-01-18 20:57:06] [INFO] Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:04<00:00, 13.63it/s]
[2026-01-18 20:57:06] [INFO] Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:06<00:01,  2.31it/s]
[2026-01-18 20:57:07] [INFO] Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:07<00:00,  3.08it/s]
[2026-01-18 20:57:07] [INFO] Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:08<00:00,  4.34it/s]
[2026-01-18 20:57:07] [INFO] [0;36m(Worker_TP0 pid=208405)[0;0m INFO 01-18 20:57:07 [custom_all_reduce.py:216] Registering 10664 cuda graph addresses
[2026-01-18 20:57:07] [INFO] [0;36m(Worker_TP1 pid=208406)[0;0m INFO 01-18 20:57:07 [custom_all_reduce.py:216] Registering 10664 cuda graph addresses
[2026-01-18 20:57:07] [INFO] [0;36m(Worker_TP0 pid=208405)[0;0m INFO 01-18 20:57:07 [gpu_model_runner.py:4837] Graph capturing finished in 13 secs, took -0.35 GiB
[2026-01-18 20:57:07] [INFO] [0;36m(EngineCore_DP0 pid=207815)[0;0m INFO 01-18 20:57:07 [core.py:273] init engine (profile, create kv cache, warmup model) took 105.03 seconds
[2026-01-18 20:57:08] [INFO] [0;36m(EngineCore_DP0 pid=207815)[0;0m The tokenizer you are loading from '/mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma-3-27b-abliterated/' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[2026-01-18 20:57:09] [INFO] [0;36m(EngineCore_DP0 pid=207815)[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[2026-01-18 20:57:10] [INFO] [0;36m(EngineCore_DP0 pid=207815)[0;0m The tokenizer you are loading from '/mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma-3-27b-abliterated/' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[2026-01-18 20:57:13] [INFO] [0;36m(EngineCore_DP0 pid=207815)[0;0m The tokenizer you are loading from '/mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma-3-27b-abliterated/' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[2026-01-18 20:57:17] [INFO] [0;36m(EngineCore_DP0 pid=207815)[0;0m INFO 01-18 20:57:17 [core.py:186] Batch queue is enabled with size 2
[2026-01-18 20:57:17] [INFO] [0;36m(EngineCore_DP0 pid=207815)[0;0m INFO 01-18 20:57:17 [vllm.py:645] Asynchronous scheduling is enabled.
[2026-01-18 20:57:17] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:17 [api_server.py:1020] Supported tasks: ['generate']
[2026-01-18 20:57:17] [WARNING] [0;36m(APIServer pid=206720)[0;0m WARNING 01-18 20:57:17 [model.py:1356] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[2026-01-18 20:57:17] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:17 [serving_responses.py:224] Using default chat sampling params from model: {'top_k': 64, 'top_p': 0.95}
[2026-01-18 20:57:17] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:17 [serving_chat.py:146] Using default chat sampling params from model: {'top_k': 64, 'top_p': 0.95}
[2026-01-18 20:57:17] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:17 [serving_chat.py:182] Warming up chat template processing...
[2026-01-18 20:57:17] [INFO] [0;36m(APIServer pid=206720)[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[2026-01-18 20:57:18] [INFO] [0;36m(APIServer pid=206720)[0;0m The tokenizer you are loading from '/mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma-3-27b-abliterated/' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:19 [chat_utils.py:599] Detected the chat template content format to be 'openai'. You can set `--chat-template-content-format` to override this.
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:19 [serving_chat.py:218] Chat template warmup completed in 2048.6ms
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:19 [serving_completion.py:78] Using default completion sampling params from model: {'top_k': 64, 'top_p': 0.95}
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:19 [serving_chat.py:146] Using default chat sampling params from model: {'top_k': 64, 'top_p': 0.95}
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:19 [api_server.py:1352] Starting vLLM API server 0 on http://0.0.0.0:8005
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:19 [launcher.py:38] Available routes are:
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:19 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:19 [launcher.py:46] Route: /docs, Methods: HEAD, GET
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:19 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:19 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:19 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:19 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:19 [launcher.py:46] Route: /tokenize, Methods: POST
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:19 [launcher.py:46] Route: /detokenize, Methods: POST
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:19 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:19 [launcher.py:46] Route: /pause, Methods: POST
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:19 [launcher.py:46] Route: /resume, Methods: POST
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:19 [launcher.py:46] Route: /is_paused, Methods: GET
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:19 [launcher.py:46] Route: /metrics, Methods: GET
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:19 [launcher.py:46] Route: /health, Methods: GET
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:19 [launcher.py:46] Route: /load, Methods: GET
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:19 [launcher.py:46] Route: /v1/models, Methods: GET
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:19 [launcher.py:46] Route: /version, Methods: GET
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:19 [launcher.py:46] Route: /v1/responses, Methods: POST
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:19 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:19 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:19 [launcher.py:46] Route: /v1/messages, Methods: POST
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:19 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:19 [launcher.py:46] Route: /v1/completions, Methods: POST
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:19 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:19 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:19 [launcher.py:46] Route: /ping, Methods: GET
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:19 [launcher.py:46] Route: /ping, Methods: POST
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:19 [launcher.py:46] Route: /invocations, Methods: POST
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:19 [launcher.py:46] Route: /classify, Methods: POST
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:19 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:19 [launcher.py:46] Route: /score, Methods: POST
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:19 [launcher.py:46] Route: /v1/score, Methods: POST
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:19 [launcher.py:46] Route: /rerank, Methods: POST
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:19 [launcher.py:46] Route: /v1/rerank, Methods: POST
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:19 [launcher.py:46] Route: /v2/rerank, Methods: POST
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:19 [launcher.py:46] Route: /pooling, Methods: POST
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO:     Started server process [206720]
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO:     Waiting for application startup.
[2026-01-18 20:57:19] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO:     Application startup complete.
[2026-01-18 20:57:26] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO:     127.0.0.1:50856 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-18 20:57:29] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:29 [loggers.py:257] Engine 000: Avg prompt throughput: 0.8 tokens/s, Avg generation throughput: 1.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[2026-01-18 20:57:39] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:57:39 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[2026-01-18 20:58:18] [INFO] [0;36m(APIServer pid=206720)[0;0m The tokenizer you are loading from '/mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma-3-27b-abliterated/' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[2026-01-18 20:58:22] [INFO] [0;36m(APIServer pid=206720)[0;0m The tokenizer you are loading from '/mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma-3-27b-abliterated/' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.
[2026-01-18 20:58:25] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO:     127.0.0.1:34236 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-18 20:58:25] [INFO] [0;36m(Worker_TP0 pid=208405)[0;0m /mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/torch/nested/__init__.py:250: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)
[2026-01-18 20:58:25] [INFO] [0;36m(Worker_TP0 pid=208405)[0;0m   return _nested.nested_tensor(
[2026-01-18 20:58:25] [INFO] [0;36m(Worker_TP1 pid=208406)[0;0m /mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/torch/nested/__init__.py:250: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)
[2026-01-18 20:58:25] [INFO] [0;36m(Worker_TP1 pid=208406)[0;0m   return _nested.nested_tensor(
[2026-01-18 20:58:35] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:58:35 [loggers.py:257] Engine 000: Avg prompt throughput: 29.7 tokens/s, Avg generation throughput: 23.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 5.2%, MM cache hit rate: 0.0%
[2026-01-18 20:58:45] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:58:45 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 5.2%, MM cache hit rate: 0.0%
[2026-01-18 20:59:25] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO:     127.0.0.1:55200 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-18 20:59:25] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:59:25 [loggers.py:257] Engine 000: Avg prompt throughput: 54.9 tokens/s, Avg generation throughput: 1.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 63.6%, MM cache hit rate: 50.0%
[2026-01-18 20:59:35] [INFO] [0;36m(APIServer pid=206720)[0;0m INFO 01-18 20:59:35 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 14.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 63.6%, MM cache hit rate: 50.0%
[2026-01-18 20:59:36] [INFO] [0;36m(Worker_TP1 pid=208406)[0;0m INFO 01-18 20:59:36 [multiproc_executor.py:707] Parent process exited, terminating worker
[2026-01-18 20:59:36] [ERROR] [0;36m(APIServer pid=206720)[0;0m ERROR 01-18 20:59:36 [core_client.py:610] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.
[2026-01-18 20:59:36] [INFO] [0;36m(Worker_TP0 pid=208405)[0;0m INFO 01-18 20:59:36 [multiproc_executor.py:707] Parent process exited, terminating worker
[2026-01-18 20:59:36] [INFO] [0;36m(Worker_TP1 pid=208406)[0;0m INFO 01-18 20:59:36 [multiproc_executor.py:751] WorkerProc shutting down.
[2026-01-18 20:59:36] [INFO] [0;36m(Worker_TP0 pid=208405)[0;0m INFO 01-18 20:59:36 [multiproc_executor.py:751] WorkerProc shutting down.
[2026-01-18 20:59:36] [INFO] [0;36m(Worker_TP0 pid=208405)[0;0m /mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py:147: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.
[2026-01-18 20:59:36] [INFO] [0;36m(Worker_TP0 pid=208405)[0;0m   warnings.warn('resource_tracker: process died unexpectedly, '
[2026-01-18 20:59:36] [INFO] [0;36m(Worker_TP1 pid=208406)[0;0m /mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py:147: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.
[2026-01-18 20:59:36] [INFO] [0;36m(Worker_TP1 pid=208406)[0;0m   warnings.warn('resource_tracker: process died unexpectedly, '
[2026-01-18 20:59:36] [ERROR] Traceback (most recent call last):
[2026-01-18 20:59:36] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 20:59:36] [ERROR] Traceback (most recent call last):
[2026-01-18 20:59:36] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 20:59:36] [INFO] cache[rtype].remove(name)
[2026-01-18 20:59:36] [INFO] KeyError: '/psm_9b1ddfd2'
[2026-01-18 20:59:36] [ERROR] Traceback (most recent call last):
[2026-01-18 20:59:36] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 20:59:36] [INFO] cache[rtype].remove(name)
[2026-01-18 20:59:36] [INFO] KeyError: '/psm_3ae02974'
[2026-01-18 20:59:36] [INFO] cache[rtype].remove(name)
[2026-01-18 20:59:36] [INFO] KeyError: '/psm_ab8f281d'
[2026-01-18 20:59:36] [ERROR] Traceback (most recent call last):
[2026-01-18 20:59:36] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 20:59:36] [INFO] cache[rtype].remove(name)
[2026-01-18 20:59:36] [INFO] KeyError: '/mp-hkouq4yn'
[2026-01-18 20:59:36] [ERROR] Traceback (most recent call last):
[2026-01-18 20:59:36] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 20:59:36] [INFO] cache[rtype].remove(name)
[2026-01-18 20:59:36] [INFO] KeyError: '/mp-sf9i40qw'
[2026-01-18 20:59:36] [INFO] 服务已停止
[2026-01-18 20:59:37] [INFO] nvitop监控已启动
[2026-01-18 20:59:39] [INFO] nvitop监控已启动
[2026-01-18 21:37:51] [SUCCESS] 方案已删除: gemma-3-27b-abliterated-TP2
[2026-01-18 21:38:25] [SUCCESS] 正在关闭服务器...
[2026-01-18 22:49:41] [INFO] VLLM GUI 服务器启动，端口: 5003
[2026-01-18 22:49:41] [INFO] nvitop监控已启动
[2026-01-18 22:49:42] [INFO] nvitop监控已启动
[2026-01-18 22:49:44] [INFO] 从conda info --base自动检测到conda路径: /mnt/AI-Acer4T/miniconda3
[2026-01-18 22:50:21] [SUCCESS] 方案已保存: gemma3_27b_inst_bg_en_nsfw
[2026-01-18 22:50:47] [SUCCESS] 方案已保存: gemma-3-27b-it-abliterated-normpreserve
[2026-01-18 22:51:09] [SUCCESS] 方案已保存: Gemma-3-27B-it-NP-Abliterated
[2026-01-18 22:51:10] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 22:51:10] [INFO] nvitop监控已启动
[2026-01-18 22:51:10] [INFO] nvitop监控已停止
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m INFO 01-18 22:51:38 [api_server.py:872] vLLM API server version 0.14.0rc2.dev126+g38bf2ffb2
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m INFO 01-18 22:51:38 [utils.py:267] non-default args: {'model_tag': '/mnt/i/AI-Chat/models/gemma-3/Gemma-3-27B-it-NP-Abliterated', 'host': '0.0.0.0', 'port': 8005, 'model': '/mnt/i/AI-Chat/models/gemma-3/Gemma-3-27B-it-NP-Abliterated', 'trust_remote_code': True, 'allowed_local_media_path': '/', 'max_model_len': 128000, 'served_model_name': ['VLLM-MODEL'], 'tensor_parallel_size': 2, 'kv_cache_dtype': 'fp8', 'mm_processor_cache_type': 'shm', 'mm_encoder_tp_mode': 'data', 'max_num_seqs': 256, 'enable_chunked_prefill': True, 'async_scheduling': True}
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 22:51:38] [ERROR] [0;36m(APIServer pid=267938)[0;0m Traceback (most recent call last):
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/transformers/utils/hub.py", line 479, in cached_files
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m     hf_hub_download(
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m     validate_repo_id(arg_value)
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m     raise HFValidationError(
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/mnt/i/AI-Chat/models/gemma-3/Gemma-3-27B-it-NP-Abliterated'. Use `repo_type` argument if needed.
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m During handling of the above exception, another exception occurred:
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m
[2026-01-18 22:51:38] [ERROR] [0;36m(APIServer pid=267938)[0;0m Traceback (most recent call last):
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/transformers/configuration_utils.py", line 721, in _get_config_dict
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m     resolved_config_file = cached_file(
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m                            ^^^^^^^^^^^^
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/transformers/utils/hub.py", line 322, in cached_file
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m     file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/transformers/utils/hub.py", line 532, in cached_files
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m     _get_cache_file_to_return(path_or_repo_id, filename, cache_dir, revision, repo_type)
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/transformers/utils/hub.py", line 143, in _get_cache_file_to_return
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m     resolved_file = try_to_load_from_cache(
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m                     ^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m     validate_repo_id(arg_value)
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m     raise HFValidationError(
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/mnt/i/AI-Chat/models/gemma-3/Gemma-3-27B-it-NP-Abliterated'. Use `repo_type` argument if needed.
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m During handling of the above exception, another exception occurred:
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m
[2026-01-18 22:51:38] [ERROR] [0;36m(APIServer pid=267938)[0;0m Traceback (most recent call last):
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/bin/vllm", line 7, in <module>
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m     sys.exit(main())
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m              ^^^^^^
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m     args.dispatch_function(args)
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m     uvloop.run(run_server(args))
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m     return __asyncio.run(
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m            ^^^^^^^^^^^^^^
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m     return runner.run(main)
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m     return self._loop.run_until_complete(task)
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m     return await main
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m            ^^^^^^^^^^
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m     async with build_async_engine_client(
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m     return await anext(self.gen)
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 146, in build_async_engine_client
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m     return await anext(self.gen)
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 172, in build_async_engine_client_from_engine_args
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/engine/arg_utils.py", line 1355, in create_engine_config
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m     maybe_override_with_speculators(
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/transformers_utils/config.py", line 528, in maybe_override_with_speculators
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m     config_dict, _ = PretrainedConfig.get_config_dict(
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/transformers/configuration_utils.py", line 662, in get_config_dict
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m     config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/transformers/configuration_utils.py", line 744, in _get_config_dict
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m     raise OSError(
[2026-01-18 22:51:38] [INFO] [0;36m(APIServer pid=267938)[0;0m OSError: Can't load the configuration of '/mnt/i/AI-Chat/models/gemma-3/Gemma-3-27B-it-NP-Abliterated'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/mnt/i/AI-Chat/models/gemma-3/Gemma-3-27B-it-NP-Abliterated' is the correct path to a directory containing a config.json file
[2026-01-18 22:51:40] [INFO] nvitop监控已启动
[2026-01-18 22:52:09] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 22:52:09] [INFO] nvitop监控已启动
[2026-01-18 22:52:09] [INFO] nvitop监控已停止
[2026-01-18 22:52:34] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:52:34 [api_server.py:872] vLLM API server version 0.14.0rc2.dev126+g38bf2ffb2
[2026-01-18 22:52:34] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:52:34 [utils.py:267] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/gemma-3/Gemma-3-27B-it-NP-Abliterated', 'host': '0.0.0.0', 'port': 8005, 'model': '/mnt/AI-Acer4T/AI-Chat/models/gemma-3/Gemma-3-27B-it-NP-Abliterated', 'trust_remote_code': True, 'allowed_local_media_path': '/', 'max_model_len': 128000, 'served_model_name': ['VLLM-MODEL'], 'tensor_parallel_size': 2, 'kv_cache_dtype': 'fp8', 'mm_processor_cache_type': 'shm', 'mm_encoder_tp_mode': 'data', 'max_num_seqs': 256, 'enable_chunked_prefill': True, 'async_scheduling': True}
[2026-01-18 22:52:34] [INFO] [0;36m(APIServer pid=268334)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 22:52:34] [INFO] [0;36m(APIServer pid=268334)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 22:52:55] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:52:55 [model.py:530] Resolved architecture: Gemma3ForConditionalGeneration
[2026-01-18 22:52:55] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:52:55 [model.py:1547] Using max model len 128000
[2026-01-18 22:52:55] [WARNING] [0;36m(APIServer pid=268334)[0;0m WARNING 01-18 22:52:55 [model.py:572] This model does not support `--mm-encoder-tp-mode data`. Falling back to `--mm-encoder-tp-mode weights`.
[2026-01-18 22:52:55] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:52:55 [cache.py:206] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor.
[2026-01-18 22:52:56] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:52:56 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[2026-01-18 22:52:56] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:52:56 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 22:52:56] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:52:56 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-18 22:52:56] [WARNING] [0;36m(APIServer pid=268334)[0;0m WARNING 01-18 22:52:56 [cuda.py:244] Forcing --disable_chunked_mm_input for models with multimodal-bidirectional attention.
[2026-01-18 22:53:20] [INFO] [0;36m(EngineCore_DP0 pid=268756)[0;0m INFO 01-18 22:53:20 [core.py:96] Initializing a V1 LLM engine (v0.14.0rc2.dev126+g38bf2ffb2) with config: model='/mnt/AI-Acer4T/AI-Chat/models/gemma-3/Gemma-3-27B-it-NP-Abliterated', speculative_config=None, tokenizer='/mnt/AI-Acer4T/AI-Chat/models/gemma-3/Gemma-3-27B-it-NP-Abliterated', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=fp8, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-18 22:53:20] [WARNING] [0;36m(EngineCore_DP0 pid=268756)[0;0m WARNING 01-18 22:53:20 [multiproc_executor.py:897] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-18 22:53:47] [INFO] INFO 01-18 22:53:47 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:56575 backend=nccl
[2026-01-18 22:53:47] [INFO] INFO 01-18 22:53:47 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:56575 backend=nccl
[2026-01-18 22:53:47] [INFO] INFO 01-18 22:53:47 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-18 22:53:47] [WARNING] WARNING 01-18 22:53:47 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 22:53:47] [WARNING] WARNING 01-18 22:53:47 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 22:53:47] [INFO] INFO 01-18 22:53:47 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
[2026-01-18 22:53:47] [INFO] INFO 01-18 22:53:47 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[2026-01-18 22:53:48] [INFO] Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[2026-01-18 22:53:48] [INFO] Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[2026-01-18 22:53:56] [INFO] [0;36m(Worker_TP0 pid=268980)[0;0m INFO 01-18 22:53:56 [gpu_model_runner.py:3803] Starting to load model /mnt/AI-Acer4T/AI-Chat/models/gemma-3/Gemma-3-27B-it-NP-Abliterated...
[2026-01-18 22:53:56] [INFO] [0;36m(Worker_TP1 pid=268981)[0;0m INFO 01-18 22:53:56 [mm_encoder_attention.py:86] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[2026-01-18 22:53:56] [INFO] [0;36m(Worker_TP1 pid=268981)[0;0m INFO 01-18 22:53:56 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 22:53:56] [INFO] [0;36m(Worker_TP0 pid=268980)[0;0m INFO 01-18 22:53:56 [mm_encoder_attention.py:86] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[2026-01-18 22:53:56] [INFO] [0;36m(Worker_TP0 pid=268980)[0;0m INFO 01-18 22:53:56 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 22:54:01] [INFO] [0;36m(Worker_TP0 pid=268980)[0;0m INFO 01-18 22:54:01 [cuda.py:351] Using TRITON_ATTN attention backend out of potential backends: ('TRITON_ATTN',)
[2026-01-18 22:54:01] [INFO] [0;36m(Worker_TP0 pid=268980)[0;0m
[2026-01-18 22:54:01] [INFO] Loading safetensors checkpoint shards:   0% Completed | 0/12 [00:00<?, ?it/s]
[2026-01-18 22:54:05] [INFO] [0;36m(Worker_TP0 pid=268980)[0;0m
[2026-01-18 22:54:05] [INFO] Loading safetensors checkpoint shards:   8% Completed | 1/12 [00:03<00:36,  3.35s/it]
[2026-01-18 22:54:09] [INFO] [0;36m(Worker_TP0 pid=268980)[0;0m
[2026-01-18 22:54:09] [INFO] Loading safetensors checkpoint shards:  17% Completed | 2/12 [00:08<00:41,  4.12s/it]
[2026-01-18 22:54:14] [INFO] [0;36m(Worker_TP0 pid=268980)[0;0m
[2026-01-18 22:54:14] [INFO] Loading safetensors checkpoint shards:  25% Completed | 3/12 [00:13<00:40,  4.55s/it]
[2026-01-18 22:54:20] [INFO] [0;36m(Worker_TP0 pid=268980)[0;0m
[2026-01-18 22:54:20] [INFO] Loading safetensors checkpoint shards:  33% Completed | 4/12 [00:18<00:38,  4.80s/it]
[2026-01-18 22:54:24] [INFO] [0;36m(Worker_TP0 pid=268980)[0;0m
[2026-01-18 22:54:24] [INFO] Loading safetensors checkpoint shards:  42% Completed | 5/12 [00:22<00:33,  4.74s/it]
[2026-01-18 22:54:29] [INFO] [0;36m(Worker_TP0 pid=268980)[0;0m
[2026-01-18 22:54:29] [INFO] Loading safetensors checkpoint shards:  50% Completed | 6/12 [00:27<00:27,  4.66s/it]
[2026-01-18 22:54:34] [INFO] [0;36m(Worker_TP0 pid=268980)[0;0m
[2026-01-18 22:54:34] [INFO] Loading safetensors checkpoint shards:  58% Completed | 7/12 [00:32<00:23,  4.78s/it]
[2026-01-18 22:54:39] [INFO] [0;36m(Worker_TP0 pid=268980)[0;0m
[2026-01-18 22:54:39] [INFO] Loading safetensors checkpoint shards:  67% Completed | 8/12 [00:37<00:19,  4.98s/it]
[2026-01-18 22:54:44] [INFO] [0;36m(Worker_TP0 pid=268980)[0;0m
[2026-01-18 22:54:44] [INFO] Loading safetensors checkpoint shards:  75% Completed | 9/12 [00:42<00:14,  4.87s/it]
[2026-01-18 22:54:49] [INFO] [0;36m(Worker_TP0 pid=268980)[0;0m
[2026-01-18 22:54:49] [INFO] Loading safetensors checkpoint shards:  83% Completed | 10/12 [00:47<00:09,  4.98s/it]
[2026-01-18 22:54:55] [INFO] [0;36m(Worker_TP0 pid=268980)[0;0m
[2026-01-18 22:54:55] [INFO] Loading safetensors checkpoint shards:  92% Completed | 11/12 [00:53<00:05,  5.14s/it]
[2026-01-18 22:54:55] [INFO] [0;36m(Worker_TP0 pid=268980)[0;0m
[2026-01-18 22:54:55] [INFO] Loading safetensors checkpoint shards: 100% Completed | 12/12 [00:53<00:00,  3.76s/it]
[2026-01-18 22:54:55] [INFO] [0;36m(Worker_TP0 pid=268980)[0;0m
[2026-01-18 22:54:55] [INFO] Loading safetensors checkpoint shards: 100% Completed | 12/12 [00:53<00:00,  4.48s/it]
[2026-01-18 22:54:55] [INFO] [0;36m(Worker_TP0 pid=268980)[0;0m
[2026-01-18 22:54:55] [INFO] [0;36m(Worker_TP0 pid=268980)[0;0m INFO 01-18 22:54:55 [default_loader.py:291] Loading weights took 53.80 seconds
[2026-01-18 22:54:56] [INFO] [0;36m(Worker_TP0 pid=268980)[0;0m INFO 01-18 22:54:56 [gpu_model_runner.py:3900] Model loading took 25.91 GiB memory and 58.781760 seconds
[2026-01-18 22:54:56] [INFO] [0;36m(Worker_TP0 pid=268980)[0;0m INFO 01-18 22:54:56 [gpu_model_runner.py:4711] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.
[2026-01-18 22:54:56] [INFO] [0;36m(Worker_TP1 pid=268981)[0;0m INFO 01-18 22:54:56 [gpu_model_runner.py:4711] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.
[2026-01-18 22:55:12] [INFO] [0;36m(Worker_TP0 pid=268980)[0;0m INFO 01-18 22:55:12 [backends.py:644] Using cache directory: /home/wuwen/.cache/vllm/torch_compile_cache/ef5d3129dd/rank_0_0/backbone for vLLM's torch.compile
[2026-01-18 22:55:12] [INFO] [0;36m(Worker_TP0 pid=268980)[0;0m INFO 01-18 22:55:12 [backends.py:704] Dynamo bytecode transform time: 14.41 s
[2026-01-18 22:55:26] [INFO] [0;36m(Worker_TP1 pid=268981)[0;0m INFO 01-18 22:55:26 [backends.py:261] Cache the graph of compile range (1, 8192) for later use
[2026-01-18 22:55:27] [INFO] [0;36m(Worker_TP0 pid=268980)[0;0m INFO 01-18 22:55:27 [backends.py:261] Cache the graph of compile range (1, 8192) for later use
[2026-01-18 22:55:45] [INFO] [0;36m(Worker_TP0 pid=268980)[0;0m INFO 01-18 22:55:45 [backends.py:278] Compiling a graph for compile range (1, 8192) takes 18.69 s
[2026-01-18 22:55:45] [INFO] [0;36m(Worker_TP0 pid=268980)[0;0m INFO 01-18 22:55:45 [monitor.py:34] torch.compile takes 33.09 s in total
[2026-01-18 22:55:46] [INFO] [0;36m(Worker_TP0 pid=268980)[0;0m INFO 01-18 22:55:46 [gpu_worker.py:355] Available KV cache memory: 56.77 GiB
[2026-01-18 22:55:46] [WARNING] [0;36m(EngineCore_DP0 pid=268756)[0;0m WARNING 01-18 22:55:46 [kv_cache_utils.py:1047] Add 8 padding layers, may waste at most 15.38% KV cache memory
[2026-01-18 22:55:46] [INFO] [0;36m(EngineCore_DP0 pid=268756)[0;0m INFO 01-18 22:55:46 [kv_cache_utils.py:1307] GPU KV cache size: 425,184 tokens
[2026-01-18 22:55:46] [INFO] [0;36m(EngineCore_DP0 pid=268756)[0;0m INFO 01-18 22:55:46 [kv_cache_utils.py:1312] Maximum concurrency for 128,000 tokens per request: 16.23x
[2026-01-18 22:55:46] [INFO] [0;36m(Worker_TP0 pid=268980)[0;0m 2026-01-18 22:55:46,882 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[2026-01-18 22:55:46] [INFO] [0;36m(Worker_TP1 pid=268981)[0;0m 2026-01-18 22:55:46,882 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[2026-01-18 22:55:46] [INFO] [0;36m(Worker_TP0 pid=268980)[0;0m 2026-01-18 22:55:46,915 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[2026-01-18 22:55:46] [INFO] [0;36m(Worker_TP1 pid=268981)[0;0m 2026-01-18 22:55:46,916 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[2026-01-18 22:55:47] [INFO] [0;36m(Worker_TP0 pid=268980)[0;0m
[2026-01-18 22:55:47] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
[2026-01-18 22:55:47] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:20,  2.49it/s]
[2026-01-18 22:55:48] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:07,  6.35it/s]
[2026-01-18 22:55:48] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:00<00:05,  8.79it/s]
[2026-01-18 22:55:48] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▎        | 7/51 [00:00<00:04,  9.97it/s]
[2026-01-18 22:55:48] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:01<00:04, 10.22it/s]
[2026-01-18 22:55:48] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:01<00:03, 10.97it/s]
[2026-01-18 22:55:48] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:01<00:03, 11.62it/s]
[2026-01-18 22:55:49] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:01<00:03, 12.00it/s]
[2026-01-18 22:55:49] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:01<00:02, 12.52it/s]
[2026-01-18 22:55:49] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 19/51 [00:01<00:02, 13.16it/s]
[2026-01-18 22:55:49] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:01<00:02, 13.58it/s]
[2026-01-18 22:55:49] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:02<00:02, 13.87it/s]
[2026-01-18 22:55:49] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 25/51 [00:02<00:01, 14.05it/s]
[2026-01-18 22:55:49] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:02<00:01, 14.25it/s]
[2026-01-18 22:55:50] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:02<00:01, 14.35it/s]
[2026-01-18 22:55:50] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 31/51 [00:02<00:01, 13.73it/s]
[2026-01-18 22:55:50] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:02<00:01, 14.19it/s]
[2026-01-18 22:55:50] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:02<00:01, 14.55it/s]
[2026-01-18 22:55:50] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 37/51 [00:03<00:00, 14.78it/s]
[2026-01-18 22:55:50] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▋  | 39/51 [00:03<00:00, 14.89it/s]
[2026-01-18 22:55:50] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:03<00:00, 15.11it/s]
[2026-01-18 22:55:50] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 43/51 [00:03<00:00, 15.24it/s]
[2026-01-18 22:55:51] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 45/51 [00:03<00:00, 15.47it/s]
[2026-01-18 22:55:51] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:03<00:00, 15.70it/s]
[2026-01-18 22:55:51] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▌| 49/51 [00:03<00:00, 15.83it/s]
[2026-01-18 22:55:51] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:03<00:00, 14.37it/s]
[2026-01-18 22:55:51] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:03<00:00, 12.91it/s]
[2026-01-18 22:55:51] [INFO] [0;36m(Worker_TP0 pid=268980)[0;0m
[2026-01-18 22:55:52] [INFO] Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
[2026-01-18 22:55:53] [INFO] Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:01<00:51,  1.52s/it]
[2026-01-18 22:55:53] [INFO] Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:02<00:36,  1.11s/it]
[2026-01-18 22:55:53] [INFO] Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:02<00:14,  2.18it/s]
[2026-01-18 22:55:54] [INFO] Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:02<00:08,  3.60it/s]
[2026-01-18 22:55:54] [INFO] Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:02<00:05,  5.09it/s]
[2026-01-18 22:55:54] [INFO] Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:02<00:03,  6.59it/s]
[2026-01-18 22:55:54] [INFO] Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:03<00:02,  7.95it/s]
[2026-01-18 22:55:54] [INFO] Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:03<00:02,  9.15it/s]
[2026-01-18 22:55:54] [INFO] Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:03<00:01, 10.14it/s]
[2026-01-18 22:55:55] [INFO] Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:03<00:01, 10.91it/s]
[2026-01-18 22:55:55] [INFO] Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:03<00:01, 11.53it/s]
[2026-01-18 22:55:55] [INFO] Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:03<00:01, 12.08it/s]
[2026-01-18 22:55:55] [INFO] Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:03<00:00, 12.34it/s]
[2026-01-18 22:55:55] [INFO] Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:04<00:00, 12.84it/s]
[2026-01-18 22:55:55] [INFO] Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:04<00:00, 13.16it/s]
[2026-01-18 22:55:58] [INFO] Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:04<00:00, 13.36it/s]
[2026-01-18 22:55:58] [INFO] Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:06<00:01,  2.31it/s]
[2026-01-18 22:55:59] [INFO] Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:07<00:00,  3.09it/s][0;36m(Worker_TP1 pid=268981)[0;0m INFO 01-18 22:55:59 [custom_all_reduce.py:216] Registering 10664 cuda graph addresses
[2026-01-18 22:55:59] [INFO] 
[2026-01-18 22:55:59] [INFO] Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:08<00:00,  4.27it/s]
[2026-01-18 22:55:59] [INFO] [0;36m(Worker_TP0 pid=268980)[0;0m INFO 01-18 22:55:59 [custom_all_reduce.py:216] Registering 10664 cuda graph addresses
[2026-01-18 22:56:00] [INFO] [0;36m(Worker_TP0 pid=268980)[0;0m INFO 01-18 22:56:00 [gpu_model_runner.py:4852] Graph capturing finished in 13 secs, took -0.35 GiB
[2026-01-18 22:56:00] [INFO] [0;36m(EngineCore_DP0 pid=268756)[0;0m INFO 01-18 22:56:00 [core.py:272] init engine (profile, create kv cache, warmup model) took 63.99 seconds
[2026-01-18 22:56:01] [INFO] [0;36m(EngineCore_DP0 pid=268756)[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[2026-01-18 22:56:09] [INFO] [0;36m(EngineCore_DP0 pid=268756)[0;0m INFO 01-18 22:56:09 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 22:56:09] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:09 [api_server.py:663] Supported tasks: ['generate']
[2026-01-18 22:56:09] [WARNING] [0;36m(APIServer pid=268334)[0;0m WARNING 01-18 22:56:09 [model.py:1360] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[2026-01-18 22:56:09] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:09 [serving.py:227] Using default chat sampling params from model: {'top_k': 64, 'top_p': 0.95}
[2026-01-18 22:56:09] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:09 [serving.py:149] Using default chat sampling params from model: {'top_k': 64, 'top_p': 0.95}
[2026-01-18 22:56:09] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:09 [serving.py:185] Warming up chat template processing...
[2026-01-18 22:56:09] [INFO] [0;36m(APIServer pid=268334)[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[2026-01-18 22:56:11] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:11 [chat_utils.py:599] Detected the chat template content format to be 'openai'. You can set `--chat-template-content-format` to override this.
[2026-01-18 22:56:11] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:11 [serving.py:221] Chat template warmup completed in 2125.5ms
[2026-01-18 22:56:11] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:11 [serving.py:80] Using default completion sampling params from model: {'top_k': 64, 'top_p': 0.95}
[2026-01-18 22:56:11] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:11 [serving.py:149] Using default chat sampling params from model: {'top_k': 64, 'top_p': 0.95}
[2026-01-18 22:56:11] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:11 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:8005
[2026-01-18 22:56:11] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:11 [launcher.py:38] Available routes are:
[2026-01-18 22:56:11] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:11 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
[2026-01-18 22:56:11] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:11 [launcher.py:46] Route: /docs, Methods: HEAD, GET
[2026-01-18 22:56:11] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:11 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[2026-01-18 22:56:11] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:11 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
[2026-01-18 22:56:11] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:11 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[2026-01-18 22:56:11] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:11 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[2026-01-18 22:56:11] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:11 [launcher.py:46] Route: /tokenize, Methods: POST
[2026-01-18 22:56:11] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:11 [launcher.py:46] Route: /detokenize, Methods: POST
[2026-01-18 22:56:11] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:11 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[2026-01-18 22:56:11] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:11 [launcher.py:46] Route: /pause, Methods: POST
[2026-01-18 22:56:11] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:11 [launcher.py:46] Route: /resume, Methods: POST
[2026-01-18 22:56:11] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:11 [launcher.py:46] Route: /is_paused, Methods: GET
[2026-01-18 22:56:11] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:11 [launcher.py:46] Route: /metrics, Methods: GET
[2026-01-18 22:56:11] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:11 [launcher.py:46] Route: /health, Methods: GET
[2026-01-18 22:56:11] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:11 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[2026-01-18 22:56:11] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:11 [launcher.py:46] Route: /v1/responses, Methods: POST
[2026-01-18 22:56:11] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:11 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[2026-01-18 22:56:11] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:11 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[2026-01-18 22:56:11] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:11 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[2026-01-18 22:56:11] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:11 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[2026-01-18 22:56:11] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:11 [launcher.py:46] Route: /v1/completions, Methods: POST
[2026-01-18 22:56:11] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:11 [launcher.py:46] Route: /v1/messages, Methods: POST
[2026-01-18 22:56:11] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:11 [launcher.py:46] Route: /v1/models, Methods: GET
[2026-01-18 22:56:11] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:11 [launcher.py:46] Route: /load, Methods: GET
[2026-01-18 22:56:11] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:11 [launcher.py:46] Route: /version, Methods: GET
[2026-01-18 22:56:11] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:11 [launcher.py:46] Route: /ping, Methods: GET
[2026-01-18 22:56:11] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:11 [launcher.py:46] Route: /ping, Methods: POST
[2026-01-18 22:56:11] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:11 [launcher.py:46] Route: /invocations, Methods: POST
[2026-01-18 22:56:11] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:11 [launcher.py:46] Route: /classify, Methods: POST
[2026-01-18 22:56:11] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:11 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[2026-01-18 22:56:11] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:11 [launcher.py:46] Route: /score, Methods: POST
[2026-01-18 22:56:11] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:11 [launcher.py:46] Route: /v1/score, Methods: POST
[2026-01-18 22:56:11] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:11 [launcher.py:46] Route: /rerank, Methods: POST
[2026-01-18 22:56:11] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:11 [launcher.py:46] Route: /v1/rerank, Methods: POST
[2026-01-18 22:56:11] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:11 [launcher.py:46] Route: /v2/rerank, Methods: POST
[2026-01-18 22:56:11] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:11 [launcher.py:46] Route: /pooling, Methods: POST
[2026-01-18 22:56:11] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO:     Started server process [268334]
[2026-01-18 22:56:11] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO:     Waiting for application startup.
[2026-01-18 22:56:12] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO:     Application startup complete.
[2026-01-18 22:56:36] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO:     127.0.0.1:55804 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-18 22:56:36] [INFO] [0;36m(Worker_TP1 pid=268981)[0;0m /mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/torch/nested/__init__.py:250: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)
[2026-01-18 22:56:36] [INFO] [0;36m(Worker_TP1 pid=268981)[0;0m   return _nested.nested_tensor(
[2026-01-18 22:56:36] [INFO] [0;36m(Worker_TP0 pid=268980)[0;0m /mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/torch/nested/__init__.py:250: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)
[2026-01-18 22:56:36] [INFO] [0;36m(Worker_TP0 pid=268980)[0;0m   return _nested.nested_tensor(
[2026-01-18 22:56:46] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:46 [loggers.py:257] Engine 000: Avg prompt throughput: 29.7 tokens/s, Avg generation throughput: 14.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[2026-01-18 22:56:56] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:56:56 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[2026-01-18 22:57:03] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO:     127.0.0.1:59386 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-18 22:57:06] [INFO] [0;36m(APIServer pid=268334)[0;0m INFO 01-18 22:57:06 [loggers.py:257] Engine 000: Avg prompt throughput: 46.2 tokens/s, Avg generation throughput: 10.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.1%, Prefix cache hit rate: 56.9%, MM cache hit rate: 50.0%
[2026-01-18 22:57:12] [ERROR] [0;36m(Worker_TP1 pid=268981)[0;0m [0;36m(APIServer pid=268334)[0;0m [0;36m(Worker_TP0 pid=268980)[0;0m ERROR 01-18 22:57:12 [core_client.py:605] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.
[2026-01-18 22:57:12] [INFO] INFO 01-18 22:57:12 [multiproc_executor.py:724] Parent process exited, terminating worker
[2026-01-18 22:57:12] [INFO] INFO 01-18 22:57:12 [multiproc_executor.py:724] Parent process exited, terminating worker
[2026-01-18 22:57:12] [INFO] [0;36m(Worker_TP0 pid=268980)[0;0m INFO 01-18 22:57:12 [multiproc_executor.py:768] WorkerProc shutting down.
[2026-01-18 22:57:12] [INFO] [0;36m(Worker_TP1 pid=268981)[0;0m INFO 01-18 22:57:12 [multiproc_executor.py:768] WorkerProc shutting down.
[2026-01-18 22:57:12] [INFO] [0;36m(Worker_TP1 pid=268981)[0;0m /mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py:147: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.
[2026-01-18 22:57:12] [INFO] [0;36m(Worker_TP1 pid=268981)[0;0m   warnings.warn('resource_tracker: process died unexpectedly, '
[2026-01-18 22:57:12] [INFO] [0;36m(Worker_TP0 pid=268980)[0;0m /mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py:147: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.
[2026-01-18 22:57:12] [INFO] [0;36m(Worker_TP0 pid=268980)[0;0m   warnings.warn('resource_tracker: process died unexpectedly, '
[2026-01-18 22:57:12] [ERROR] Traceback (most recent call last):
[2026-01-18 22:57:12] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 22:57:12] [INFO] cache[rtype].remove(name)
[2026-01-18 22:57:12] [INFO] KeyError: '/psm_b32d8e28'
[2026-01-18 22:57:12] [ERROR] Traceback (most recent call last):
[2026-01-18 22:57:12] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 22:57:12] [INFO] cache[rtype].remove(name)
[2026-01-18 22:57:12] [INFO] KeyError: '/psm_cd1b3065'
[2026-01-18 22:57:12] [INFO] 服务已停止
[2026-01-18 22:57:12] [INFO] nvitop监控已启动
[2026-01-18 22:57:13] [ERROR] Traceback (most recent call last):
[2026-01-18 22:57:13] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 22:57:13] [INFO] cache[rtype].remove(name)
[2026-01-18 22:57:13] [INFO] KeyError: '/psm_f6b1f04a'
[2026-01-18 22:57:13] [ERROR] Traceback (most recent call last):
[2026-01-18 22:57:13] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 22:57:13] [INFO] cache[rtype].remove(name)
[2026-01-18 22:57:13] [INFO] KeyError: '/mp-7uocuqg4'
[2026-01-18 22:57:13] [ERROR] Traceback (most recent call last):
[2026-01-18 22:57:13] [INFO] File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 22:57:13] [INFO] cache[rtype].remove(name)
[2026-01-18 22:57:13] [INFO] KeyError: '/mp-lcpljfye'
[2026-01-18 22:57:13] [INFO] nvitop监控已启动
[2026-01-18 22:57:16] [INFO] nvitop监控已启动
[2026-01-18 22:57:22] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 22:57:22] [INFO] nvitop监控已启动
[2026-01-18 22:57:22] [INFO] nvitop监控已停止
[2026-01-18 22:57:47] [INFO] [0;36m(APIServer pid=271957)[0;0m INFO 01-18 22:57:47 [api_server.py:872] vLLM API server version 0.14.0rc2.dev126+g38bf2ffb2
[2026-01-18 22:57:47] [INFO] [0;36m(APIServer pid=271957)[0;0m INFO 01-18 22:57:47 [utils.py:267] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma3_27b_inst_bg_en_nsfw', 'host': '0.0.0.0', 'port': 8005, 'model': '/mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma3_27b_inst_bg_en_nsfw', 'trust_remote_code': True, 'allowed_local_media_path': '/', 'max_model_len': 128000, 'served_model_name': ['VLLM-MODEL'], 'tensor_parallel_size': 2, 'kv_cache_dtype': 'fp8', 'mm_processor_cache_type': 'shm', 'mm_encoder_tp_mode': 'data', 'max_num_seqs': 256, 'enable_chunked_prefill': True, 'async_scheduling': True}
[2026-01-18 22:57:47] [INFO] [0;36m(APIServer pid=271957)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 22:57:47] [INFO] [0;36m(APIServer pid=271957)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 22:58:08] [INFO] [0;36m(APIServer pid=271957)[0;0m INFO 01-18 22:58:08 [model.py:530] Resolved architecture: Gemma3ForCausalLM
[2026-01-18 22:58:08] [INFO] [0;36m(APIServer pid=271957)[0;0m INFO 01-18 22:58:08 [model.py:1547] Using max model len 128000
[2026-01-18 22:58:08] [INFO] [0;36m(APIServer pid=271957)[0;0m INFO 01-18 22:58:08 [cache.py:206] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor.
[2026-01-18 22:58:08] [INFO] [0;36m(APIServer pid=271957)[0;0m INFO 01-18 22:58:08 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[2026-01-18 22:58:08] [INFO] [0;36m(APIServer pid=271957)[0;0m INFO 01-18 22:58:08 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 22:58:08] [INFO] [0;36m(APIServer pid=271957)[0;0m INFO 01-18 22:58:08 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-18 22:58:32] [INFO] [0;36m(EngineCore_DP0 pid=272407)[0;0m INFO 01-18 22:58:32 [core.py:96] Initializing a V1 LLM engine (v0.14.0rc2.dev126+g38bf2ffb2) with config: model='/mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma3_27b_inst_bg_en_nsfw', speculative_config=None, tokenizer='/mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma3_27b_inst_bg_en_nsfw', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=fp8, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-18 22:58:32] [WARNING] [0;36m(EngineCore_DP0 pid=272407)[0;0m WARNING 01-18 22:58:32 [multiproc_executor.py:897] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-18 22:58:56] [INFO] INFO 01-18 22:58:56 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:32973 backend=nccl
[2026-01-18 22:58:56] [INFO] INFO 01-18 22:58:56 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:32973 backend=nccl
[2026-01-18 22:58:56] [INFO] INFO 01-18 22:58:56 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-18 22:58:56] [WARNING] WARNING 01-18 22:58:56 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 22:58:56] [WARNING] WARNING 01-18 22:58:56 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 22:58:56] [INFO] INFO 01-18 22:58:56 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
[2026-01-18 22:58:56] [INFO] INFO 01-18 22:58:56 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[2026-01-18 22:58:57] [INFO] [0;36m(Worker_TP0 pid=272706)[0;0m INFO 01-18 22:58:57 [gpu_model_runner.py:3803] Starting to load model /mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma3_27b_inst_bg_en_nsfw...
[2026-01-18 22:59:00] [INFO] [0;36m(Worker_TP0 pid=272706)[0;0m INFO 01-18 22:59:00 [cuda.py:351] Using FLASHINFER attention backend out of potential backends: ('FLASHINFER', 'TRITON_ATTN')
[2026-01-18 22:59:00] [INFO] [0;36m(Worker_TP0 pid=272706)[0;0m
[2026-01-18 22:59:00] [INFO] Loading safetensors checkpoint shards:   0% Completed | 0/12 [00:00<?, ?it/s]
[2026-01-18 22:59:01] [ERROR] [0;36m(Worker_TP1 pid=272707)[0;0m ERROR 01-18 22:59:01 [multiproc_executor.py:766] WorkerProc failed to start.
[2026-01-18 22:59:01] [ERROR] [0;36m(Worker_TP1 pid=272707)[0;0m ERROR 01-18 22:59:01 [multiproc_executor.py:766] Traceback (most recent call last):
[2026-01-18 22:59:01] [ERROR] [0;36m(Worker_TP1 pid=272707)[0;0m ERROR 01-18 22:59:01 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 737, in worker_main
[2026-01-18 22:59:01] [ERROR] [0;36m(Worker_TP1 pid=272707)[0;0m ERROR 01-18 22:59:01 [multiproc_executor.py:766]     worker = WorkerProc(*args, **kwargs)
[2026-01-18 22:59:01] [ERROR] [0;36m(Worker_TP1 pid=272707)[0;0m ERROR 01-18 22:59:01 [multiproc_executor.py:766]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 22:59:01] [ERROR] [0;36m(Worker_TP1 pid=272707)[0;0m ERROR 01-18 22:59:01 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 575, in __init__
[2026-01-18 22:59:01] [ERROR] [0;36m(Worker_TP1 pid=272707)[0;0m ERROR 01-18 22:59:01 [multiproc_executor.py:766]     self.worker.load_model()
[2026-01-18 22:59:01] [ERROR] [0;36m(Worker_TP1 pid=272707)[0;0m ERROR 01-18 22:59:01 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/worker/gpu_worker.py", line 274, in load_model
[2026-01-18 22:59:01] [ERROR] [0;36m(Worker_TP1 pid=272707)[0;0m ERROR 01-18 22:59:01 [multiproc_executor.py:766]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[2026-01-18 22:59:01] [ERROR] [0;36m(Worker_TP1 pid=272707)[0;0m ERROR 01-18 22:59:01 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/worker/gpu_model_runner.py", line 3822, in load_model
[2026-01-18 22:59:01] [ERROR] [0;36m(Worker_TP1 pid=272707)[0;0m ERROR 01-18 22:59:01 [multiproc_executor.py:766]     self.model = model_loader.load_model(
[2026-01-18 22:59:01] [ERROR] [0;36m(Worker_TP1 pid=272707)[0;0m ERROR 01-18 22:59:01 [multiproc_executor.py:766]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 22:59:01] [ERROR] [0;36m(Worker_TP1 pid=272707)[0;0m ERROR 01-18 22:59:01 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/model_loader/base_loader.py", line 58, in load_model
[2026-01-18 22:59:01] [ERROR] [0;36m(Worker_TP1 pid=272707)[0;0m ERROR 01-18 22:59:01 [multiproc_executor.py:766]     self.load_weights(model, model_config)
[2026-01-18 22:59:01] [ERROR] [0;36m(Worker_TP1 pid=272707)[0;0m ERROR 01-18 22:59:01 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/model_loader/default_loader.py", line 288, in load_weights
[2026-01-18 22:59:01] [ERROR] [0;36m(Worker_TP1 pid=272707)[0;0m ERROR 01-18 22:59:01 [multiproc_executor.py:766]     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))
[2026-01-18 22:59:01] [ERROR] [0;36m(Worker_TP1 pid=272707)[0;0m ERROR 01-18 22:59:01 [multiproc_executor.py:766]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 22:59:01] [ERROR] [0;36m(Worker_TP1 pid=272707)[0;0m ERROR 01-18 22:59:01 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/gemma3.py", line 520, in load_weights
[2026-01-18 22:59:01] [ERROR] [0;36m(Worker_TP1 pid=272707)[0;0m ERROR 01-18 22:59:01 [multiproc_executor.py:766]     return loader.load_weights(weights)
[2026-01-18 22:59:01] [ERROR] [0;36m(Worker_TP1 pid=272707)[0;0m ERROR 01-18 22:59:01 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 22:59:01] [ERROR] [0;36m(Worker_TP1 pid=272707)[0;0m ERROR 01-18 22:59:01 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
[2026-01-18 22:59:01] [ERROR] [0;36m(Worker_TP1 pid=272707)[0;0m ERROR 01-18 22:59:01 [multiproc_executor.py:766]     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
[2026-01-18 22:59:01] [ERROR] [0;36m(Worker_TP1 pid=272707)[0;0m ERROR 01-18 22:59:01 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 22:59:01] [ERROR] [0;36m(Worker_TP1 pid=272707)[0;0m ERROR 01-18 22:59:01 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/utils.py", line 335, in load_weights
[2026-01-18 22:59:01] [ERROR] [0;36m(Worker_TP1 pid=272707)[0;0m ERROR 01-18 22:59:01 [multiproc_executor.py:766]     autoloaded_weights = set(self._load_module("", self.module, weights))
[2026-01-18 22:59:01] [ERROR] [0;36m(Worker_TP1 pid=272707)[0;0m ERROR 01-18 22:59:01 [multiproc_executor.py:766]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 22:59:01] [ERROR] [0;36m(Worker_TP1 pid=272707)[0;0m ERROR 01-18 22:59:01 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/utils.py", line 319, in _load_module
[2026-01-18 22:59:01] [ERROR] [0;36m(Worker_TP1 pid=272707)[0;0m ERROR 01-18 22:59:01 [multiproc_executor.py:766]     raise ValueError(msg)
[2026-01-18 22:59:01] [ERROR] [0;36m(Worker_TP1 pid=272707)[0;0m ERROR 01-18 22:59:01 [multiproc_executor.py:766] ValueError: There is no module or parameter named 'language_model' in Gemma3ForCausalLM
[2026-01-18 22:59:01] [INFO] [0;36m(Worker_TP1 pid=272707)[0;0m INFO 01-18 22:59:01 [multiproc_executor.py:724] Parent process exited, terminating worker
[2026-01-18 22:59:01] [INFO] [0;36m(Worker_TP0 pid=272706)[0;0m INFO 01-18 22:59:01 [multiproc_executor.py:724] Parent process exited, terminating worker
[2026-01-18 22:59:01] [INFO] [0;36m(Worker_TP0 pid=272706)[0;0m
[2026-01-18 22:59:01] [INFO] Loading safetensors checkpoint shards:   0% Completed | 0/12 [00:00<?, ?it/s]
[2026-01-18 22:59:01] [INFO] [0;36m(Worker_TP0 pid=272706)[0;0m
[2026-01-18 22:59:01] [WARNING] [rank0]:[W118 22:59:01.319706775 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2026-01-18 22:59:02] [ERROR] [0;36m(EngineCore_DP0 pid=272407)[0;0m ERROR 01-18 22:59:02 [core.py:935] EngineCore failed to start.
[2026-01-18 22:59:02] [ERROR] [0;36m(EngineCore_DP0 pid=272407)[0;0m ERROR 01-18 22:59:02 [core.py:935] Traceback (most recent call last):
[2026-01-18 22:59:02] [ERROR] [0;36m(EngineCore_DP0 pid=272407)[0;0m ERROR 01-18 22:59:02 [core.py:935]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-18 22:59:02] [ERROR] [0;36m(EngineCore_DP0 pid=272407)[0;0m ERROR 01-18 22:59:02 [core.py:935]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 22:59:02] [ERROR] [0;36m(EngineCore_DP0 pid=272407)[0;0m ERROR 01-18 22:59:02 [core.py:935]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 22:59:02] [ERROR] [0;36m(EngineCore_DP0 pid=272407)[0;0m ERROR 01-18 22:59:02 [core.py:935]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-18 22:59:02] [ERROR] [0;36m(EngineCore_DP0 pid=272407)[0;0m ERROR 01-18 22:59:02 [core.py:935]     super().__init__(
[2026-01-18 22:59:02] [ERROR] [0;36m(EngineCore_DP0 pid=272407)[0;0m ERROR 01-18 22:59:02 [core.py:935]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core.py", line 105, in __init__
[2026-01-18 22:59:02] [ERROR] [0;36m(EngineCore_DP0 pid=272407)[0;0m ERROR 01-18 22:59:02 [core.py:935]     self.model_executor = executor_class(vllm_config)
[2026-01-18 22:59:02] [ERROR] [0;36m(EngineCore_DP0 pid=272407)[0;0m ERROR 01-18 22:59:02 [core.py:935]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 22:59:02] [ERROR] [0;36m(EngineCore_DP0 pid=272407)[0;0m ERROR 01-18 22:59:02 [core.py:935]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[2026-01-18 22:59:02] [ERROR] [0;36m(EngineCore_DP0 pid=272407)[0;0m ERROR 01-18 22:59:02 [core.py:935]     super().__init__(vllm_config)
[2026-01-18 22:59:02] [ERROR] [0;36m(EngineCore_DP0 pid=272407)[0;0m ERROR 01-18 22:59:02 [core.py:935]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/abstract.py", line 101, in __init__
[2026-01-18 22:59:02] [ERROR] [0;36m(EngineCore_DP0 pid=272407)[0;0m ERROR 01-18 22:59:02 [core.py:935]     self._init_executor()
[2026-01-18 22:59:02] [ERROR] [0;36m(EngineCore_DP0 pid=272407)[0;0m ERROR 01-18 22:59:02 [core.py:935]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[2026-01-18 22:59:02] [ERROR] [0;36m(EngineCore_DP0 pid=272407)[0;0m ERROR 01-18 22:59:02 [core.py:935]     self.workers = WorkerProc.wait_for_ready(unready_workers)
[2026-01-18 22:59:02] [ERROR] [0;36m(EngineCore_DP0 pid=272407)[0;0m ERROR 01-18 22:59:02 [core.py:935]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 22:59:02] [ERROR] [0;36m(EngineCore_DP0 pid=272407)[0;0m ERROR 01-18 22:59:02 [core.py:935]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 675, in wait_for_ready
[2026-01-18 22:59:02] [ERROR] [0;36m(EngineCore_DP0 pid=272407)[0;0m ERROR 01-18 22:59:02 [core.py:935]     raise e from None
[2026-01-18 22:59:02] [ERROR] [0;36m(EngineCore_DP0 pid=272407)[0;0m ERROR 01-18 22:59:02 [core.py:935] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[2026-01-18 22:59:02] [INFO] [0;36m(EngineCore_DP0 pid=272407)[0;0m Process EngineCore_DP0:
[2026-01-18 22:59:02] [ERROR] [0;36m(EngineCore_DP0 pid=272407)[0;0m Traceback (most recent call last):
[2026-01-18 22:59:02] [INFO] [0;36m(EngineCore_DP0 pid=272407)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[2026-01-18 22:59:02] [INFO] [0;36m(EngineCore_DP0 pid=272407)[0;0m     self.run()
[2026-01-18 22:59:02] [INFO] [0;36m(EngineCore_DP0 pid=272407)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/process.py", line 108, in run
[2026-01-18 22:59:02] [INFO] [0;36m(EngineCore_DP0 pid=272407)[0;0m     self._target(*self._args, **self._kwargs)
[2026-01-18 22:59:02] [INFO] [0;36m(EngineCore_DP0 pid=272407)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core.py", line 939, in run_engine_core
[2026-01-18 22:59:02] [INFO] [0;36m(EngineCore_DP0 pid=272407)[0;0m     raise e
[2026-01-18 22:59:02] [INFO] [0;36m(EngineCore_DP0 pid=272407)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-18 22:59:02] [INFO] [0;36m(EngineCore_DP0 pid=272407)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 22:59:02] [INFO] [0;36m(EngineCore_DP0 pid=272407)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 22:59:02] [INFO] [0;36m(EngineCore_DP0 pid=272407)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-18 22:59:02] [INFO] [0;36m(EngineCore_DP0 pid=272407)[0;0m     super().__init__(
[2026-01-18 22:59:02] [INFO] [0;36m(EngineCore_DP0 pid=272407)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core.py", line 105, in __init__
[2026-01-18 22:59:02] [INFO] [0;36m(EngineCore_DP0 pid=272407)[0;0m     self.model_executor = executor_class(vllm_config)
[2026-01-18 22:59:02] [INFO] [0;36m(EngineCore_DP0 pid=272407)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 22:59:02] [INFO] [0;36m(EngineCore_DP0 pid=272407)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[2026-01-18 22:59:02] [INFO] [0;36m(EngineCore_DP0 pid=272407)[0;0m     super().__init__(vllm_config)
[2026-01-18 22:59:02] [INFO] [0;36m(EngineCore_DP0 pid=272407)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/abstract.py", line 101, in __init__
[2026-01-18 22:59:02] [INFO] [0;36m(EngineCore_DP0 pid=272407)[0;0m     self._init_executor()
[2026-01-18 22:59:02] [INFO] [0;36m(EngineCore_DP0 pid=272407)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[2026-01-18 22:59:02] [INFO] [0;36m(EngineCore_DP0 pid=272407)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
[2026-01-18 22:59:02] [INFO] [0;36m(EngineCore_DP0 pid=272407)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 22:59:02] [INFO] [0;36m(EngineCore_DP0 pid=272407)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 675, in wait_for_ready
[2026-01-18 22:59:02] [INFO] [0;36m(EngineCore_DP0 pid=272407)[0;0m     raise e from None
[2026-01-18 22:59:02] [INFO] [0;36m(EngineCore_DP0 pid=272407)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[2026-01-18 22:59:04] [ERROR] [0;36m(APIServer pid=271957)[0;0m Traceback (most recent call last):
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/bin/vllm", line 7, in <module>
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m     sys.exit(main())
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m              ^^^^^^
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m     args.dispatch_function(args)
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m     uvloop.run(run_server(args))
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m     return __asyncio.run(
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m            ^^^^^^^^^^^^^^
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m     return runner.run(main)
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m     return self._loop.run_until_complete(task)
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m     return await main
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m            ^^^^^^^^^^
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m     async with build_async_engine_client(
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m     return await anext(self.gen)
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 146, in build_async_engine_client
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m     return await anext(self.gen)
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 187, in build_async_engine_client_from_engine_args
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/async_llm.py", line 202, in from_vllm_config
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m     return cls(
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m            ^^^^
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/async_llm.py", line 129, in __init__
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m     return AsyncMPClient(*client_args)
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core_client.py", line 819, in __init__
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m     super().__init__(
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core_client.py", line 479, in __init__
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m   File "/mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/contextlib.py", line 144, in __exit__
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m     next(self.gen)
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/utils.py", line 918, in launch_core_engines
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m     wait_for_engine_startup(
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/utils.py", line 977, in wait_for_engine_startup
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m     raise RuntimeError(
[2026-01-18 22:59:04] [INFO] [0;36m(APIServer pid=271957)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[2026-01-18 22:59:04] [INFO] /mnt/AI-Acer4T/conda_envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py:279: UserWarning: resource_tracker: There appear to be 3 leaked shared_memory objects to clean up at shutdown
[2026-01-18 22:59:04] [INFO] warnings.warn('resource_tracker: There appear to be %d '
[2026-01-18 22:59:05] [INFO] nvitop监控已启动
[2026-01-18 22:59:23] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 22:59:23] [INFO] nvitop监控已启动
[2026-01-18 22:59:23] [INFO] nvitop监控已停止
[2026-01-18 22:59:54] [INFO] [0;36m(APIServer pid=273169)[0;0m INFO 01-18 22:59:54 [api_server.py:872] vLLM API server version 0.14.0rc2.dev117+g9e078d058
[2026-01-18 22:59:54] [INFO] [0;36m(APIServer pid=273169)[0;0m INFO 01-18 22:59:54 [utils.py:267] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma3_27b_inst_bg_en_nsfw', 'host': '0.0.0.0', 'port': 8005, 'model': '/mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma3_27b_inst_bg_en_nsfw', 'trust_remote_code': True, 'allowed_local_media_path': '/', 'max_model_len': 128000, 'served_model_name': ['VLLM-MODEL'], 'tensor_parallel_size': 2, 'kv_cache_dtype': 'fp8', 'mm_processor_cache_type': 'shm', 'mm_encoder_tp_mode': 'data', 'max_num_seqs': 256, 'enable_chunked_prefill': True, 'async_scheduling': True}
[2026-01-18 22:59:54] [INFO] [0;36m(APIServer pid=273169)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 22:59:54] [INFO] [0;36m(APIServer pid=273169)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 22:59:54] [INFO] [0;36m(APIServer pid=273169)[0;0m INFO 01-18 22:59:54 [model.py:530] Resolved architecture: Gemma3ForCausalLM
[2026-01-18 22:59:54] [INFO] [0;36m(APIServer pid=273169)[0;0m INFO 01-18 22:59:54 [model.py:1547] Using max model len 128000
[2026-01-18 22:59:54] [INFO] [0;36m(APIServer pid=273169)[0;0m INFO 01-18 22:59:54 [cache.py:206] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor.
[2026-01-18 22:59:54] [INFO] [0;36m(APIServer pid=273169)[0;0m INFO 01-18 22:59:54 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[2026-01-18 22:59:54] [INFO] [0;36m(APIServer pid=273169)[0;0m INFO 01-18 22:59:54 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 22:59:54] [INFO] [0;36m(APIServer pid=273169)[0;0m INFO 01-18 22:59:54 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-18 23:00:25] [INFO] [0;36m(EngineCore_DP0 pid=273514)[0;0m INFO 01-18 23:00:25 [core.py:96] Initializing a V1 LLM engine (v0.14.0rc2.dev117+g9e078d058) with config: model='/mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma3_27b_inst_bg_en_nsfw', speculative_config=None, tokenizer='/mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma3_27b_inst_bg_en_nsfw', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=fp8, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-18 23:00:25] [WARNING] [0;36m(EngineCore_DP0 pid=273514)[0;0m WARNING 01-18 23:00:25 [multiproc_executor.py:897] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-18 23:00:56] [INFO] INFO 01-18 23:00:56 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:34981 backend=nccl
[2026-01-18 23:00:56] [INFO] INFO 01-18 23:00:56 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:34981 backend=nccl
[2026-01-18 23:00:56] [INFO] INFO 01-18 23:00:56 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-18 23:00:56] [WARNING] WARNING 01-18 23:00:56 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 23:00:56] [WARNING] WARNING 01-18 23:00:56 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 23:00:56] [INFO] INFO 01-18 23:00:56 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
[2026-01-18 23:00:56] [INFO] INFO 01-18 23:00:56 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[2026-01-18 23:00:57] [INFO] [0;36m(Worker_TP0 pid=273806)[0;0m INFO 01-18 23:00:57 [gpu_model_runner.py:3803] Starting to load model /mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma3_27b_inst_bg_en_nsfw...
[2026-01-18 23:01:02] [INFO] [0;36m(Worker_TP0 pid=273806)[0;0m INFO 01-18 23:01:02 [cuda.py:351] Using FLASHINFER attention backend out of potential backends: ('FLASHINFER', 'TRITON_ATTN')
[2026-01-18 23:01:02] [INFO] [0;36m(Worker_TP0 pid=273806)[0;0m
[2026-01-18 23:01:02] [INFO] Loading safetensors checkpoint shards:   0% Completed | 0/12 [00:00<?, ?it/s]
[2026-01-18 23:01:03] [ERROR] [0;36m(Worker_TP1 pid=273807)[0;0m ERROR 01-18 23:01:03 [multiproc_executor.py:766] WorkerProc failed to start.
[2026-01-18 23:01:03] [ERROR] [0;36m(Worker_TP1 pid=273807)[0;0m ERROR 01-18 23:01:03 [multiproc_executor.py:766] Traceback (most recent call last):
[2026-01-18 23:01:03] [ERROR] [0;36m(Worker_TP1 pid=273807)[0;0m ERROR 01-18 23:01:03 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 737, in worker_main
[2026-01-18 23:01:03] [ERROR] [0;36m(Worker_TP1 pid=273807)[0;0m ERROR 01-18 23:01:03 [multiproc_executor.py:766]     worker = WorkerProc(*args, **kwargs)
[2026-01-18 23:01:03] [ERROR] [0;36m(Worker_TP1 pid=273807)[0;0m ERROR 01-18 23:01:03 [multiproc_executor.py:766]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:01:03] [ERROR] [0;36m(Worker_TP1 pid=273807)[0;0m ERROR 01-18 23:01:03 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 575, in __init__
[2026-01-18 23:01:03] [ERROR] [0;36m(Worker_TP1 pid=273807)[0;0m ERROR 01-18 23:01:03 [multiproc_executor.py:766]     self.worker.load_model()
[2026-01-18 23:01:03] [ERROR] [0;36m(Worker_TP1 pid=273807)[0;0m ERROR 01-18 23:01:03 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 274, in load_model
[2026-01-18 23:01:03] [ERROR] [0;36m(Worker_TP1 pid=273807)[0;0m ERROR 01-18 23:01:03 [multiproc_executor.py:766]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[2026-01-18 23:01:03] [ERROR] [0;36m(Worker_TP1 pid=273807)[0;0m ERROR 01-18 23:01:03 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3822, in load_model
[2026-01-18 23:01:03] [ERROR] [0;36m(Worker_TP1 pid=273807)[0;0m ERROR 01-18 23:01:03 [multiproc_executor.py:766]     self.model = model_loader.load_model(
[2026-01-18 23:01:03] [ERROR] [0;36m(Worker_TP1 pid=273807)[0;0m ERROR 01-18 23:01:03 [multiproc_executor.py:766]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:01:03] [ERROR] [0;36m(Worker_TP1 pid=273807)[0;0m ERROR 01-18 23:01:03 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 58, in load_model
[2026-01-18 23:01:03] [ERROR] [0;36m(Worker_TP1 pid=273807)[0;0m ERROR 01-18 23:01:03 [multiproc_executor.py:766]     self.load_weights(model, model_config)
[2026-01-18 23:01:03] [ERROR] [0;36m(Worker_TP1 pid=273807)[0;0m ERROR 01-18 23:01:03 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 288, in load_weights
[2026-01-18 23:01:03] [ERROR] [0;36m(Worker_TP1 pid=273807)[0;0m ERROR 01-18 23:01:03 [multiproc_executor.py:766]     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))
[2026-01-18 23:01:03] [ERROR] [0;36m(Worker_TP1 pid=273807)[0;0m ERROR 01-18 23:01:03 [multiproc_executor.py:766]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:01:03] [ERROR] [0;36m(Worker_TP1 pid=273807)[0;0m ERROR 01-18 23:01:03 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/gemma3.py", line 520, in load_weights
[2026-01-18 23:01:03] [ERROR] [0;36m(Worker_TP1 pid=273807)[0;0m ERROR 01-18 23:01:03 [multiproc_executor.py:766]     return loader.load_weights(weights)
[2026-01-18 23:01:03] [ERROR] [0;36m(Worker_TP1 pid=273807)[0;0m ERROR 01-18 23:01:03 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:01:03] [ERROR] [0;36m(Worker_TP1 pid=273807)[0;0m ERROR 01-18 23:01:03 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
[2026-01-18 23:01:03] [ERROR] [0;36m(Worker_TP1 pid=273807)[0;0m ERROR 01-18 23:01:03 [multiproc_executor.py:766]     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
[2026-01-18 23:01:03] [ERROR] [0;36m(Worker_TP1 pid=273807)[0;0m ERROR 01-18 23:01:03 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:01:03] [ERROR] [0;36m(Worker_TP1 pid=273807)[0;0m ERROR 01-18 23:01:03 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 335, in load_weights
[2026-01-18 23:01:03] [ERROR] [0;36m(Worker_TP1 pid=273807)[0;0m ERROR 01-18 23:01:03 [multiproc_executor.py:766]     autoloaded_weights = set(self._load_module("", self.module, weights))
[2026-01-18 23:01:03] [ERROR] [0;36m(Worker_TP1 pid=273807)[0;0m ERROR 01-18 23:01:03 [multiproc_executor.py:766]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:01:03] [ERROR] [0;36m(Worker_TP1 pid=273807)[0;0m ERROR 01-18 23:01:03 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 319, in _load_module
[2026-01-18 23:01:03] [ERROR] [0;36m(Worker_TP1 pid=273807)[0;0m ERROR 01-18 23:01:03 [multiproc_executor.py:766]     raise ValueError(msg)
[2026-01-18 23:01:03] [ERROR] [0;36m(Worker_TP1 pid=273807)[0;0m ERROR 01-18 23:01:03 [multiproc_executor.py:766] ValueError: There is no module or parameter named 'language_model' in Gemma3ForCausalLM
[2026-01-18 23:01:03] [INFO] [0;36m(Worker_TP1 pid=273807)[0;0m INFO 01-18 23:01:03 [multiproc_executor.py:724] Parent process exited, terminating worker
[2026-01-18 23:01:03] [INFO] [0;36m(Worker_TP0 pid=273806)[0;0m INFO 01-18 23:01:03 [multiproc_executor.py:724] Parent process exited, terminating worker
[2026-01-18 23:01:03] [INFO] [0;36m(Worker_TP0 pid=273806)[0;0m
[2026-01-18 23:01:03] [INFO] Loading safetensors checkpoint shards:   0% Completed | 0/12 [00:00<?, ?it/s]
[2026-01-18 23:01:03] [INFO] [0;36m(Worker_TP0 pid=273806)[0;0m
[2026-01-18 23:01:03] [WARNING] [rank0]:[W118 23:01:03.255433285 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2026-01-18 23:01:04] [ERROR] [0;36m(EngineCore_DP0 pid=273514)[0;0m ERROR 01-18 23:01:04 [core.py:935] EngineCore failed to start.
[2026-01-18 23:01:04] [ERROR] [0;36m(EngineCore_DP0 pid=273514)[0;0m ERROR 01-18 23:01:04 [core.py:935] Traceback (most recent call last):
[2026-01-18 23:01:04] [ERROR] [0;36m(EngineCore_DP0 pid=273514)[0;0m ERROR 01-18 23:01:04 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-18 23:01:04] [ERROR] [0;36m(EngineCore_DP0 pid=273514)[0;0m ERROR 01-18 23:01:04 [core.py:935]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 23:01:04] [ERROR] [0;36m(EngineCore_DP0 pid=273514)[0;0m ERROR 01-18 23:01:04 [core.py:935]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:01:04] [ERROR] [0;36m(EngineCore_DP0 pid=273514)[0;0m ERROR 01-18 23:01:04 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-18 23:01:04] [ERROR] [0;36m(EngineCore_DP0 pid=273514)[0;0m ERROR 01-18 23:01:04 [core.py:935]     super().__init__(
[2026-01-18 23:01:04] [ERROR] [0;36m(EngineCore_DP0 pid=273514)[0;0m ERROR 01-18 23:01:04 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[2026-01-18 23:01:04] [ERROR] [0;36m(EngineCore_DP0 pid=273514)[0;0m ERROR 01-18 23:01:04 [core.py:935]     self.model_executor = executor_class(vllm_config)
[2026-01-18 23:01:04] [ERROR] [0;36m(EngineCore_DP0 pid=273514)[0;0m ERROR 01-18 23:01:04 [core.py:935]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:01:04] [ERROR] [0;36m(EngineCore_DP0 pid=273514)[0;0m ERROR 01-18 23:01:04 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[2026-01-18 23:01:04] [ERROR] [0;36m(EngineCore_DP0 pid=273514)[0;0m ERROR 01-18 23:01:04 [core.py:935]     super().__init__(vllm_config)
[2026-01-18 23:01:04] [ERROR] [0;36m(EngineCore_DP0 pid=273514)[0;0m ERROR 01-18 23:01:04 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[2026-01-18 23:01:04] [ERROR] [0;36m(EngineCore_DP0 pid=273514)[0;0m ERROR 01-18 23:01:04 [core.py:935]     self._init_executor()
[2026-01-18 23:01:04] [ERROR] [0;36m(EngineCore_DP0 pid=273514)[0;0m ERROR 01-18 23:01:04 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[2026-01-18 23:01:04] [ERROR] [0;36m(EngineCore_DP0 pid=273514)[0;0m ERROR 01-18 23:01:04 [core.py:935]     self.workers = WorkerProc.wait_for_ready(unready_workers)
[2026-01-18 23:01:04] [ERROR] [0;36m(EngineCore_DP0 pid=273514)[0;0m ERROR 01-18 23:01:04 [core.py:935]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:01:04] [ERROR] [0;36m(EngineCore_DP0 pid=273514)[0;0m ERROR 01-18 23:01:04 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 675, in wait_for_ready
[2026-01-18 23:01:04] [ERROR] [0;36m(EngineCore_DP0 pid=273514)[0;0m ERROR 01-18 23:01:04 [core.py:935]     raise e from None
[2026-01-18 23:01:04] [ERROR] [0;36m(EngineCore_DP0 pid=273514)[0;0m ERROR 01-18 23:01:04 [core.py:935] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[2026-01-18 23:01:04] [INFO] [0;36m(EngineCore_DP0 pid=273514)[0;0m Process EngineCore_DP0:
[2026-01-18 23:01:04] [ERROR] [0;36m(EngineCore_DP0 pid=273514)[0;0m Traceback (most recent call last):
[2026-01-18 23:01:04] [INFO] [0;36m(EngineCore_DP0 pid=273514)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[2026-01-18 23:01:04] [INFO] [0;36m(EngineCore_DP0 pid=273514)[0;0m     self.run()
[2026-01-18 23:01:04] [INFO] [0;36m(EngineCore_DP0 pid=273514)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/process.py", line 108, in run
[2026-01-18 23:01:04] [INFO] [0;36m(EngineCore_DP0 pid=273514)[0;0m     self._target(*self._args, **self._kwargs)
[2026-01-18 23:01:04] [INFO] [0;36m(EngineCore_DP0 pid=273514)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 939, in run_engine_core
[2026-01-18 23:01:04] [INFO] [0;36m(EngineCore_DP0 pid=273514)[0;0m     raise e
[2026-01-18 23:01:04] [INFO] [0;36m(EngineCore_DP0 pid=273514)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-18 23:01:04] [INFO] [0;36m(EngineCore_DP0 pid=273514)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 23:01:04] [INFO] [0;36m(EngineCore_DP0 pid=273514)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:01:04] [INFO] [0;36m(EngineCore_DP0 pid=273514)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-18 23:01:04] [INFO] [0;36m(EngineCore_DP0 pid=273514)[0;0m     super().__init__(
[2026-01-18 23:01:04] [INFO] [0;36m(EngineCore_DP0 pid=273514)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[2026-01-18 23:01:04] [INFO] [0;36m(EngineCore_DP0 pid=273514)[0;0m     self.model_executor = executor_class(vllm_config)
[2026-01-18 23:01:04] [INFO] [0;36m(EngineCore_DP0 pid=273514)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:01:04] [INFO] [0;36m(EngineCore_DP0 pid=273514)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[2026-01-18 23:01:04] [INFO] [0;36m(EngineCore_DP0 pid=273514)[0;0m     super().__init__(vllm_config)
[2026-01-18 23:01:04] [INFO] [0;36m(EngineCore_DP0 pid=273514)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[2026-01-18 23:01:04] [INFO] [0;36m(EngineCore_DP0 pid=273514)[0;0m     self._init_executor()
[2026-01-18 23:01:04] [INFO] [0;36m(EngineCore_DP0 pid=273514)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[2026-01-18 23:01:04] [INFO] [0;36m(EngineCore_DP0 pid=273514)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
[2026-01-18 23:01:04] [INFO] [0;36m(EngineCore_DP0 pid=273514)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:01:04] [INFO] [0;36m(EngineCore_DP0 pid=273514)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 675, in wait_for_ready
[2026-01-18 23:01:04] [INFO] [0;36m(EngineCore_DP0 pid=273514)[0;0m     raise e from None
[2026-01-18 23:01:04] [INFO] [0;36m(EngineCore_DP0 pid=273514)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[2026-01-18 23:01:05] [ERROR] [0;36m(APIServer pid=273169)[0;0m Traceback (most recent call last):
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/bin/vllm", line 7, in <module>
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m     sys.exit(main())
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m              ^^^^^^
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m     args.dispatch_function(args)
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m     uvloop.run(run_server(args))
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m     return __asyncio.run(
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m            ^^^^^^^^^^^^^^
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m     return runner.run(main)
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m     return self._loop.run_until_complete(task)
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m     return await main
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m            ^^^^^^^^^^
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m     async with build_async_engine_client(
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m     return await anext(self.gen)
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 146, in build_async_engine_client
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m     return await anext(self.gen)
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 187, in build_async_engine_client_from_engine_args
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 202, in from_vllm_config
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m     return cls(
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m            ^^^^
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 129, in __init__
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m     return AsyncMPClient(*client_args)
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m     super().__init__(
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 144, in __exit__
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m     next(self.gen)
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 918, in launch_core_engines
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m     wait_for_engine_startup(
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 977, in wait_for_engine_startup
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m     raise RuntimeError(
[2026-01-18 23:01:05] [INFO] [0;36m(APIServer pid=273169)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[2026-01-18 23:01:06] [INFO] /mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/resource_tracker.py:279: UserWarning: resource_tracker: There appear to be 3 leaked shared_memory objects to clean up at shutdown
[2026-01-18 23:01:06] [INFO] warnings.warn('resource_tracker: There appear to be %d '
[2026-01-18 23:01:07] [INFO] nvitop监控已启动
[2026-01-18 23:06:49] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 23:06:49] [INFO] nvitop监控已启动
[2026-01-18 23:06:49] [INFO] nvitop监控已停止
[2026-01-18 23:07:00] [INFO] nvitop监控已启动
[2026-01-18 23:07:00] [INFO] 服务已停止
[2026-01-18 23:07:01] [INFO] nvitop监控已启动
[2026-01-18 23:07:09] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 23:07:09] [INFO] nvitop监控已启动
[2026-01-18 23:07:09] [INFO] nvitop监控已停止
[2026-01-18 23:07:39] [INFO] [0;36m(APIServer pid=276496)[0;0m INFO 01-18 23:07:39 [api_server.py:872] vLLM API server version 0.14.0rc2.dev117+g9e078d058
[2026-01-18 23:07:39] [INFO] [0;36m(APIServer pid=276496)[0;0m INFO 01-18 23:07:39 [utils.py:267] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma3_27b_inst_bg_en_nsfw', 'host': '0.0.0.0', 'port': 8005, 'model': '/mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma3_27b_inst_bg_en_nsfw', 'trust_remote_code': True, 'allowed_local_media_path': '/', 'max_model_len': 32000, 'served_model_name': ['VLLM-MODEL'], 'kv_cache_dtype': 'fp8', 'mm_processor_cache_type': 'shm', 'mm_encoder_tp_mode': 'data', 'max_num_seqs': 256, 'enable_chunked_prefill': True, 'async_scheduling': True}
[2026-01-18 23:07:39] [INFO] [0;36m(APIServer pid=276496)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 23:07:39] [INFO] [0;36m(APIServer pid=276496)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 23:07:39] [INFO] [0;36m(APIServer pid=276496)[0;0m INFO 01-18 23:07:39 [model.py:530] Resolved architecture: Gemma3ForCausalLM
[2026-01-18 23:07:39] [INFO] [0;36m(APIServer pid=276496)[0;0m INFO 01-18 23:07:39 [model.py:1547] Using max model len 32000
[2026-01-18 23:07:39] [INFO] [0;36m(APIServer pid=276496)[0;0m INFO 01-18 23:07:39 [cache.py:206] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor.
[2026-01-18 23:07:39] [INFO] [0;36m(APIServer pid=276496)[0;0m INFO 01-18 23:07:39 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[2026-01-18 23:07:39] [INFO] [0;36m(APIServer pid=276496)[0;0m INFO 01-18 23:07:39 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 23:07:39] [INFO] [0;36m(APIServer pid=276496)[0;0m INFO 01-18 23:07:39 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-18 23:08:10] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m INFO 01-18 23:08:10 [core.py:96] Initializing a V1 LLM engine (v0.14.0rc2.dev117+g9e078d058) with config: model='/mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma3_27b_inst_bg_en_nsfw', speculative_config=None, tokenizer='/mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma3_27b_inst_bg_en_nsfw', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=fp8, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-18 23:08:11] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m INFO 01-18 23:08:11 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://192.168.31.143:36275 backend=nccl
[2026-01-18 23:08:11] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m INFO 01-18 23:08:11 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[2026-01-18 23:08:12] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m INFO 01-18 23:08:12 [gpu_model_runner.py:3803] Starting to load model /mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma3_27b_inst_bg_en_nsfw...
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m INFO 01-18 23:08:15 [cuda.py:351] Using FLASHINFER attention backend out of potential backends: ('FLASHINFER', 'TRITON_ATTN')
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m
[2026-01-18 23:08:15] [INFO] Loading safetensors checkpoint shards:   0% Completed | 0/12 [00:00<?, ?it/s]
[2026-01-18 23:08:15] [ERROR] [0;36m(EngineCore_DP0 pid=276839)[0;0m ERROR 01-18 23:08:15 [core.py:935] EngineCore failed to start.
[2026-01-18 23:08:15] [ERROR] [0;36m(EngineCore_DP0 pid=276839)[0;0m ERROR 01-18 23:08:15 [core.py:935] Traceback (most recent call last):
[2026-01-18 23:08:15] [ERROR] [0;36m(EngineCore_DP0 pid=276839)[0;0m ERROR 01-18 23:08:15 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-18 23:08:15] [ERROR] [0;36m(EngineCore_DP0 pid=276839)[0;0m ERROR 01-18 23:08:15 [core.py:935]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 23:08:15] [ERROR] [0;36m(EngineCore_DP0 pid=276839)[0;0m ERROR 01-18 23:08:15 [core.py:935]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:08:15] [ERROR] [0;36m(EngineCore_DP0 pid=276839)[0;0m ERROR 01-18 23:08:15 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-18 23:08:15] [ERROR] [0;36m(EngineCore_DP0 pid=276839)[0;0m ERROR 01-18 23:08:15 [core.py:935]     super().__init__(
[2026-01-18 23:08:15] [ERROR] [0;36m(EngineCore_DP0 pid=276839)[0;0m ERROR 01-18 23:08:15 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[2026-01-18 23:08:15] [ERROR] [0;36m(EngineCore_DP0 pid=276839)[0;0m ERROR 01-18 23:08:15 [core.py:935]     self.model_executor = executor_class(vllm_config)
[2026-01-18 23:08:15] [ERROR] [0;36m(EngineCore_DP0 pid=276839)[0;0m ERROR 01-18 23:08:15 [core.py:935]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:08:15] [ERROR] [0;36m(EngineCore_DP0 pid=276839)[0;0m ERROR 01-18 23:08:15 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[2026-01-18 23:08:15] [ERROR] [0;36m(EngineCore_DP0 pid=276839)[0;0m ERROR 01-18 23:08:15 [core.py:935]     self._init_executor()
[2026-01-18 23:08:15] [ERROR] [0;36m(EngineCore_DP0 pid=276839)[0;0m ERROR 01-18 23:08:15 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[2026-01-18 23:08:15] [ERROR] [0;36m(EngineCore_DP0 pid=276839)[0;0m ERROR 01-18 23:08:15 [core.py:935]     self.driver_worker.load_model()
[2026-01-18 23:08:15] [ERROR] [0;36m(EngineCore_DP0 pid=276839)[0;0m ERROR 01-18 23:08:15 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 274, in load_model
[2026-01-18 23:08:15] [ERROR] [0;36m(EngineCore_DP0 pid=276839)[0;0m ERROR 01-18 23:08:15 [core.py:935]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[2026-01-18 23:08:15] [ERROR] [0;36m(EngineCore_DP0 pid=276839)[0;0m ERROR 01-18 23:08:15 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3822, in load_model
[2026-01-18 23:08:15] [ERROR] [0;36m(EngineCore_DP0 pid=276839)[0;0m ERROR 01-18 23:08:15 [core.py:935]     self.model = model_loader.load_model(
[2026-01-18 23:08:15] [ERROR] [0;36m(EngineCore_DP0 pid=276839)[0;0m ERROR 01-18 23:08:15 [core.py:935]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:08:15] [ERROR] [0;36m(EngineCore_DP0 pid=276839)[0;0m ERROR 01-18 23:08:15 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 58, in load_model
[2026-01-18 23:08:15] [ERROR] [0;36m(EngineCore_DP0 pid=276839)[0;0m ERROR 01-18 23:08:15 [core.py:935]     self.load_weights(model, model_config)
[2026-01-18 23:08:15] [ERROR] [0;36m(EngineCore_DP0 pid=276839)[0;0m ERROR 01-18 23:08:15 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 288, in load_weights
[2026-01-18 23:08:15] [ERROR] [0;36m(EngineCore_DP0 pid=276839)[0;0m ERROR 01-18 23:08:15 [core.py:935]     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))
[2026-01-18 23:08:15] [ERROR] [0;36m(EngineCore_DP0 pid=276839)[0;0m ERROR 01-18 23:08:15 [core.py:935]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:08:15] [ERROR] [0;36m(EngineCore_DP0 pid=276839)[0;0m ERROR 01-18 23:08:15 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/gemma3.py", line 520, in load_weights
[2026-01-18 23:08:15] [ERROR] [0;36m(EngineCore_DP0 pid=276839)[0;0m ERROR 01-18 23:08:15 [core.py:935]     return loader.load_weights(weights)
[2026-01-18 23:08:15] [ERROR] [0;36m(EngineCore_DP0 pid=276839)[0;0m ERROR 01-18 23:08:15 [core.py:935]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:08:15] [ERROR] [0;36m(EngineCore_DP0 pid=276839)[0;0m ERROR 01-18 23:08:15 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
[2026-01-18 23:08:15] [ERROR] [0;36m(EngineCore_DP0 pid=276839)[0;0m ERROR 01-18 23:08:15 [core.py:935]     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
[2026-01-18 23:08:15] [ERROR] [0;36m(EngineCore_DP0 pid=276839)[0;0m ERROR 01-18 23:08:15 [core.py:935]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:08:15] [ERROR] [0;36m(EngineCore_DP0 pid=276839)[0;0m ERROR 01-18 23:08:15 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 335, in load_weights
[2026-01-18 23:08:15] [ERROR] [0;36m(EngineCore_DP0 pid=276839)[0;0m ERROR 01-18 23:08:15 [core.py:935]     autoloaded_weights = set(self._load_module("", self.module, weights))
[2026-01-18 23:08:15] [ERROR] [0;36m(EngineCore_DP0 pid=276839)[0;0m ERROR 01-18 23:08:15 [core.py:935]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:08:15] [ERROR] [0;36m(EngineCore_DP0 pid=276839)[0;0m ERROR 01-18 23:08:15 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 319, in _load_module
[2026-01-18 23:08:15] [ERROR] [0;36m(EngineCore_DP0 pid=276839)[0;0m ERROR 01-18 23:08:15 [core.py:935]     raise ValueError(msg)
[2026-01-18 23:08:15] [ERROR] [0;36m(EngineCore_DP0 pid=276839)[0;0m ERROR 01-18 23:08:15 [core.py:935] ValueError: There is no module or parameter named 'language_model' in Gemma3ForCausalLM
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m Process EngineCore_DP0:
[2026-01-18 23:08:15] [ERROR] [0;36m(EngineCore_DP0 pid=276839)[0;0m Traceback (most recent call last):
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m     self.run()
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/process.py", line 108, in run
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m     self._target(*self._args, **self._kwargs)
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 939, in run_engine_core
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m     raise e
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m     super().__init__(
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m     self.model_executor = executor_class(vllm_config)
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m     self._init_executor()
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m     self.driver_worker.load_model()
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 274, in load_model
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3822, in load_model
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m     self.model = model_loader.load_model(
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 58, in load_model
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m     self.load_weights(model, model_config)
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 288, in load_weights
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/gemma3.py", line 520, in load_weights
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m     return loader.load_weights(weights)
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 335, in load_weights
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m     autoloaded_weights = set(self._load_module("", self.module, weights))
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 319, in _load_module
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m     raise ValueError(msg)
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m ValueError: There is no module or parameter named 'language_model' in Gemma3ForCausalLM
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m
[2026-01-18 23:08:15] [INFO] Loading safetensors checkpoint shards:   0% Completed | 0/12 [00:00<?, ?it/s]
[2026-01-18 23:08:15] [INFO] [0;36m(EngineCore_DP0 pid=276839)[0;0m
[2026-01-18 23:08:16] [WARNING] [rank0]:[W118 23:08:16.787666376 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2026-01-18 23:08:17] [ERROR] [0;36m(APIServer pid=276496)[0;0m Traceback (most recent call last):
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/bin/vllm", line 7, in <module>
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m     sys.exit(main())
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m              ^^^^^^
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m     args.dispatch_function(args)
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m     uvloop.run(run_server(args))
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m     return __asyncio.run(
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m            ^^^^^^^^^^^^^^
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m     return runner.run(main)
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m     return self._loop.run_until_complete(task)
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m     return await main
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m            ^^^^^^^^^^
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m     async with build_async_engine_client(
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m     return await anext(self.gen)
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 146, in build_async_engine_client
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m     return await anext(self.gen)
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 187, in build_async_engine_client_from_engine_args
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 202, in from_vllm_config
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m     return cls(
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m            ^^^^
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 129, in __init__
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m     return AsyncMPClient(*client_args)
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m     super().__init__(
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 144, in __exit__
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m     next(self.gen)
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 918, in launch_core_engines
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m     wait_for_engine_startup(
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 977, in wait_for_engine_startup
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m     raise RuntimeError(
[2026-01-18 23:08:17] [INFO] [0;36m(APIServer pid=276496)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[2026-01-18 23:08:18] [INFO] nvitop监控已启动
[2026-01-18 23:16:46] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 23:16:46] [INFO] nvitop监控已启动
[2026-01-18 23:16:46] [INFO] nvitop监控已停止
[2026-01-18 23:17:17] [INFO] [0;36m(APIServer pid=281259)[0;0m INFO 01-18 23:17:17 [api_server.py:872] vLLM API server version 0.14.0rc2.dev117+g9e078d058
[2026-01-18 23:17:17] [INFO] [0;36m(APIServer pid=281259)[0;0m INFO 01-18 23:17:17 [utils.py:267] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma3_27b_inst_bg_en_nsfw', 'host': '0.0.0.0', 'port': 8005, 'model': '/mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma3_27b_inst_bg_en_nsfw', 'trust_remote_code': True, 'allowed_local_media_path': '/', 'max_model_len': 32000, 'served_model_name': ['VLLM-MODEL'], 'kv_cache_dtype': 'fp8', 'mm_processor_cache_type': 'shm', 'mm_encoder_tp_mode': 'data', 'max_num_seqs': 256, 'enable_chunked_prefill': True, 'async_scheduling': True}
[2026-01-18 23:17:17] [INFO] [0;36m(APIServer pid=281259)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 23:17:17] [INFO] [0;36m(APIServer pid=281259)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 23:17:43] [INFO] [0;36m(APIServer pid=281259)[0;0m INFO 01-18 23:17:43 [model.py:530] Resolved architecture: Gemma3ForCausalLM
[2026-01-18 23:17:43] [INFO] [0;36m(APIServer pid=281259)[0;0m INFO 01-18 23:17:43 [model.py:1547] Using max model len 32000
[2026-01-18 23:17:43] [INFO] [0;36m(APIServer pid=281259)[0;0m INFO 01-18 23:17:43 [cache.py:206] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor.
[2026-01-18 23:17:44] [INFO] [0;36m(APIServer pid=281259)[0;0m INFO 01-18 23:17:44 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[2026-01-18 23:17:44] [INFO] [0;36m(APIServer pid=281259)[0;0m INFO 01-18 23:17:44 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 23:17:44] [INFO] [0;36m(APIServer pid=281259)[0;0m INFO 01-18 23:17:44 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-18 23:18:12] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m INFO 01-18 23:18:12 [core.py:96] Initializing a V1 LLM engine (v0.14.0rc2.dev117+g9e078d058) with config: model='/mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma3_27b_inst_bg_en_nsfw', speculative_config=None, tokenizer='/mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma3_27b_inst_bg_en_nsfw', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=fp8, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-18 23:18:13] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m INFO 01-18 23:18:13 [parallel_state.py:1212] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://192.168.31.143:52927 backend=nccl
[2026-01-18 23:18:13] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m INFO 01-18 23:18:13 [parallel_state.py:1423] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[2026-01-18 23:18:14] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m INFO 01-18 23:18:14 [gpu_model_runner.py:3803] Starting to load model /mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma3_27b_inst_bg_en_nsfw...
[2026-01-18 23:18:18] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m INFO 01-18 23:18:18 [cuda.py:351] Using FLASHINFER attention backend out of potential backends: ('FLASHINFER', 'TRITON_ATTN')
[2026-01-18 23:18:18] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m
[2026-01-18 23:18:18] [INFO] Loading safetensors checkpoint shards:   0% Completed | 0/12 [00:00<?, ?it/s]
[2026-01-18 23:18:18] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m
[2026-01-18 23:18:18] [INFO] Loading safetensors checkpoint shards:  17% Completed | 2/12 [00:00<00:00, 14.78it/s]
[2026-01-18 23:18:18] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m
[2026-01-18 23:18:18] [INFO] Loading safetensors checkpoint shards:  58% Completed | 7/12 [00:00<00:00, 29.30it/s]
[2026-01-18 23:18:18] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m
[2026-01-18 23:18:18] [INFO] Loading safetensors checkpoint shards: 100% Completed | 12/12 [00:00<00:00, 33.44it/s]
[2026-01-18 23:18:18] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m
[2026-01-18 23:18:19] [ERROR] [0;36m(EngineCore_DP0 pid=282079)[0;0m ERROR 01-18 23:18:19 [core.py:935] EngineCore failed to start.
[2026-01-18 23:18:19] [ERROR] [0;36m(EngineCore_DP0 pid=282079)[0;0m ERROR 01-18 23:18:19 [core.py:935] Traceback (most recent call last):
[2026-01-18 23:18:19] [ERROR] [0;36m(EngineCore_DP0 pid=282079)[0;0m ERROR 01-18 23:18:19 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-18 23:18:19] [ERROR] [0;36m(EngineCore_DP0 pid=282079)[0;0m ERROR 01-18 23:18:19 [core.py:935]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 23:18:19] [ERROR] [0;36m(EngineCore_DP0 pid=282079)[0;0m ERROR 01-18 23:18:19 [core.py:935]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:18:19] [ERROR] [0;36m(EngineCore_DP0 pid=282079)[0;0m ERROR 01-18 23:18:19 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-18 23:18:19] [ERROR] [0;36m(EngineCore_DP0 pid=282079)[0;0m ERROR 01-18 23:18:19 [core.py:935]     super().__init__(
[2026-01-18 23:18:19] [ERROR] [0;36m(EngineCore_DP0 pid=282079)[0;0m ERROR 01-18 23:18:19 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[2026-01-18 23:18:19] [ERROR] [0;36m(EngineCore_DP0 pid=282079)[0;0m ERROR 01-18 23:18:19 [core.py:935]     self.model_executor = executor_class(vllm_config)
[2026-01-18 23:18:19] [ERROR] [0;36m(EngineCore_DP0 pid=282079)[0;0m ERROR 01-18 23:18:19 [core.py:935]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:18:19] [ERROR] [0;36m(EngineCore_DP0 pid=282079)[0;0m ERROR 01-18 23:18:19 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[2026-01-18 23:18:19] [ERROR] [0;36m(EngineCore_DP0 pid=282079)[0;0m ERROR 01-18 23:18:19 [core.py:935]     self._init_executor()
[2026-01-18 23:18:19] [ERROR] [0;36m(EngineCore_DP0 pid=282079)[0;0m ERROR 01-18 23:18:19 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[2026-01-18 23:18:19] [ERROR] [0;36m(EngineCore_DP0 pid=282079)[0;0m ERROR 01-18 23:18:19 [core.py:935]     self.driver_worker.load_model()
[2026-01-18 23:18:19] [ERROR] [0;36m(EngineCore_DP0 pid=282079)[0;0m ERROR 01-18 23:18:19 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 274, in load_model
[2026-01-18 23:18:19] [ERROR] [0;36m(EngineCore_DP0 pid=282079)[0;0m ERROR 01-18 23:18:19 [core.py:935]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[2026-01-18 23:18:19] [ERROR] [0;36m(EngineCore_DP0 pid=282079)[0;0m ERROR 01-18 23:18:19 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3822, in load_model
[2026-01-18 23:18:19] [ERROR] [0;36m(EngineCore_DP0 pid=282079)[0;0m ERROR 01-18 23:18:19 [core.py:935]     self.model = model_loader.load_model(
[2026-01-18 23:18:19] [ERROR] [0;36m(EngineCore_DP0 pid=282079)[0;0m ERROR 01-18 23:18:19 [core.py:935]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:18:19] [ERROR] [0;36m(EngineCore_DP0 pid=282079)[0;0m ERROR 01-18 23:18:19 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 58, in load_model
[2026-01-18 23:18:19] [ERROR] [0;36m(EngineCore_DP0 pid=282079)[0;0m ERROR 01-18 23:18:19 [core.py:935]     self.load_weights(model, model_config)
[2026-01-18 23:18:19] [ERROR] [0;36m(EngineCore_DP0 pid=282079)[0;0m ERROR 01-18 23:18:19 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 288, in load_weights
[2026-01-18 23:18:19] [ERROR] [0;36m(EngineCore_DP0 pid=282079)[0;0m ERROR 01-18 23:18:19 [core.py:935]     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))
[2026-01-18 23:18:19] [ERROR] [0;36m(EngineCore_DP0 pid=282079)[0;0m ERROR 01-18 23:18:19 [core.py:935]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:18:19] [ERROR] [0;36m(EngineCore_DP0 pid=282079)[0;0m ERROR 01-18 23:18:19 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/gemma3.py", line 530, in load_weights
[2026-01-18 23:18:19] [ERROR] [0;36m(EngineCore_DP0 pid=282079)[0;0m ERROR 01-18 23:18:19 [core.py:935]     return loader.load_weights(modified_weights)
[2026-01-18 23:18:19] [ERROR] [0;36m(EngineCore_DP0 pid=282079)[0;0m ERROR 01-18 23:18:19 [core.py:935]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:18:19] [ERROR] [0;36m(EngineCore_DP0 pid=282079)[0;0m ERROR 01-18 23:18:19 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
[2026-01-18 23:18:19] [ERROR] [0;36m(EngineCore_DP0 pid=282079)[0;0m ERROR 01-18 23:18:19 [core.py:935]     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
[2026-01-18 23:18:19] [ERROR] [0;36m(EngineCore_DP0 pid=282079)[0;0m ERROR 01-18 23:18:19 [core.py:935]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:18:19] [ERROR] [0;36m(EngineCore_DP0 pid=282079)[0;0m ERROR 01-18 23:18:19 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 335, in load_weights
[2026-01-18 23:18:19] [ERROR] [0;36m(EngineCore_DP0 pid=282079)[0;0m ERROR 01-18 23:18:19 [core.py:935]     autoloaded_weights = set(self._load_module("", self.module, weights))
[2026-01-18 23:18:19] [ERROR] [0;36m(EngineCore_DP0 pid=282079)[0;0m ERROR 01-18 23:18:19 [core.py:935]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:18:19] [ERROR] [0;36m(EngineCore_DP0 pid=282079)[0;0m ERROR 01-18 23:18:19 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 288, in _load_module
[2026-01-18 23:18:19] [ERROR] [0;36m(EngineCore_DP0 pid=282079)[0;0m ERROR 01-18 23:18:19 [core.py:935]     yield from self._load_module(
[2026-01-18 23:18:19] [ERROR] [0;36m(EngineCore_DP0 pid=282079)[0;0m ERROR 01-18 23:18:19 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 261, in _load_module
[2026-01-18 23:18:19] [ERROR] [0;36m(EngineCore_DP0 pid=282079)[0;0m ERROR 01-18 23:18:19 [core.py:935]     loaded_params = module_load_weights(weights)
[2026-01-18 23:18:19] [ERROR] [0;36m(EngineCore_DP0 pid=282079)[0;0m ERROR 01-18 23:18:19 [core.py:935]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:18:19] [ERROR] [0;36m(EngineCore_DP0 pid=282079)[0;0m ERROR 01-18 23:18:19 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/gemma3.py", line 444, in load_weights
[2026-01-18 23:18:19] [ERROR] [0;36m(EngineCore_DP0 pid=282079)[0;0m ERROR 01-18 23:18:19 [core.py:935]     param = params_dict[name]
[2026-01-18 23:18:19] [ERROR] [0;36m(EngineCore_DP0 pid=282079)[0;0m ERROR 01-18 23:18:19 [core.py:935]             ~~~~~~~~~~~^^^^^^
[2026-01-18 23:18:19] [ERROR] [0;36m(EngineCore_DP0 pid=282079)[0;0m ERROR 01-18 23:18:19 [core.py:935] KeyError: 'model.layers.31.input_layernorm.weight'
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m Process EngineCore_DP0:
[2026-01-18 23:18:19] [ERROR] [0;36m(EngineCore_DP0 pid=282079)[0;0m Traceback (most recent call last):
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m     self.run()
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/process.py", line 108, in run
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m     self._target(*self._args, **self._kwargs)
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 939, in run_engine_core
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m     raise e
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m     super().__init__(
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m     self.model_executor = executor_class(vllm_config)
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m     self._init_executor()
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m     self.driver_worker.load_model()
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 274, in load_model
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3822, in load_model
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m     self.model = model_loader.load_model(
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 58, in load_model
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m     self.load_weights(model, model_config)
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 288, in load_weights
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/gemma3.py", line 530, in load_weights
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m     return loader.load_weights(modified_weights)
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 335, in load_weights
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m     autoloaded_weights = set(self._load_module("", self.module, weights))
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 288, in _load_module
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m     yield from self._load_module(
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 261, in _load_module
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m     loaded_params = module_load_weights(weights)
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/gemma3.py", line 444, in load_weights
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m     param = params_dict[name]
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m             ~~~~~~~~~~~^^^^^^
[2026-01-18 23:18:19] [INFO] [0;36m(EngineCore_DP0 pid=282079)[0;0m KeyError: 'model.layers.31.input_layernorm.weight'
[2026-01-18 23:18:19] [WARNING] [rank0]:[W118 23:18:19.266180927 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2026-01-18 23:18:20] [ERROR] [0;36m(APIServer pid=281259)[0;0m Traceback (most recent call last):
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/bin/vllm", line 7, in <module>
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m     sys.exit(main())
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m              ^^^^^^
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m     args.dispatch_function(args)
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m     uvloop.run(run_server(args))
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m     return __asyncio.run(
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m            ^^^^^^^^^^^^^^
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m     return runner.run(main)
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m     return self._loop.run_until_complete(task)
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m     return await main
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m            ^^^^^^^^^^
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m     async with build_async_engine_client(
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m     return await anext(self.gen)
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 146, in build_async_engine_client
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m     return await anext(self.gen)
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 187, in build_async_engine_client_from_engine_args
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 202, in from_vllm_config
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m     return cls(
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m            ^^^^
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 129, in __init__
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m     return AsyncMPClient(*client_args)
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m     super().__init__(
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 144, in __exit__
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m     next(self.gen)
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 918, in launch_core_engines
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m     wait_for_engine_startup(
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 977, in wait_for_engine_startup
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m     raise RuntimeError(
[2026-01-18 23:18:20] [INFO] [0;36m(APIServer pid=281259)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[2026-01-18 23:18:22] [INFO] nvitop监控已启动
[2026-01-18 23:21:30] [SUCCESS] 方案已删除: gemma3_27b_inst_bg_en_nsfw
[2026-01-18 23:21:39] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 23:21:39] [INFO] nvitop监控已启动
[2026-01-18 23:21:39] [INFO] nvitop监控已停止
[2026-01-18 23:22:37] [INFO] nvitop监控已启动
[2026-01-18 23:22:37] [INFO] 服务已停止
[2026-01-18 23:22:37] [INFO] nvitop监控已启动
[2026-01-18 23:22:38] [INFO] nvitop监控已启动
[2026-01-18 23:22:39] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 23:22:39] [INFO] nvitop监控已启动
[2026-01-18 23:22:39] [INFO] nvitop监控已停止
[2026-01-18 23:23:49] [INFO] [0;36m(APIServer pid=284809)[0;0m INFO 01-18 23:23:49 [api_server.py:872] vLLM API server version 0.14.0rc2.dev117+g9e078d058
[2026-01-18 23:23:49] [INFO] [0;36m(APIServer pid=284809)[0;0m INFO 01-18 23:23:49 [utils.py:267] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma-3-27b-it-abliterated-normpreserve', 'host': '0.0.0.0', 'port': 8005, 'model': '/mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma-3-27b-it-abliterated-normpreserve', 'trust_remote_code': True, 'allowed_local_media_path': '/', 'max_model_len': 128000, 'served_model_name': ['VLLM-MODEL'], 'tensor_parallel_size': 2, 'kv_cache_dtype': 'fp8', 'mm_processor_cache_type': 'shm', 'mm_encoder_tp_mode': 'data', 'max_num_seqs': 256, 'enable_chunked_prefill': True, 'async_scheduling': True}
[2026-01-18 23:23:49] [INFO] [0;36m(APIServer pid=284809)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 23:23:49] [INFO] [0;36m(APIServer pid=284809)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 23:23:49] [INFO] [0;36m(APIServer pid=284809)[0;0m INFO 01-18 23:23:49 [model.py:530] Resolved architecture: Gemma3ForConditionalGeneration
[2026-01-18 23:23:49] [INFO] [0;36m(APIServer pid=284809)[0;0m INFO 01-18 23:23:49 [model.py:1547] Using max model len 128000
[2026-01-18 23:23:49] [WARNING] [0;36m(APIServer pid=284809)[0;0m WARNING 01-18 23:23:49 [model.py:572] This model does not support `--mm-encoder-tp-mode data`. Falling back to `--mm-encoder-tp-mode weights`.
[2026-01-18 23:23:49] [INFO] [0;36m(APIServer pid=284809)[0;0m INFO 01-18 23:23:49 [cache.py:206] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor.
[2026-01-18 23:23:50] [INFO] [0;36m(APIServer pid=284809)[0;0m INFO 01-18 23:23:50 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[2026-01-18 23:23:50] [INFO] [0;36m(APIServer pid=284809)[0;0m INFO 01-18 23:23:50 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 23:23:50] [INFO] [0;36m(APIServer pid=284809)[0;0m INFO 01-18 23:23:50 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-18 23:23:50] [WARNING] [0;36m(APIServer pid=284809)[0;0m WARNING 01-18 23:23:50 [cuda.py:244] Forcing --disable_chunked_mm_input for models with multimodal-bidirectional attention.
[2026-01-18 23:23:54] [ERROR] [0;36m(APIServer pid=284809)[0;0m Traceback (most recent call last):
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/bin/vllm", line 7, in <module>
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m     sys.exit(main())
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m              ^^^^^^
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m     args.dispatch_function(args)
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m     uvloop.run(run_server(args))
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m     return __asyncio.run(
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m            ^^^^^^^^^^^^^^
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m     return runner.run(main)
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m     return self._loop.run_until_complete(task)
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m     return await main
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m            ^^^^^^^^^^
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m     async with build_async_engine_client(
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m     return await anext(self.gen)
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 146, in build_async_engine_client
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m     return await anext(self.gen)
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 187, in build_async_engine_client_from_engine_args
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 202, in from_vllm_config
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m     return cls(
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m            ^^^^
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 111, in __init__
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m     self.input_processor = InputProcessor(self.vllm_config, tokenizer)
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/input_processor.py", line 60, in __init__
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m     self.mm_processor_cache = mm_registry.processor_cache_from_config(vllm_config)
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/multimodal/registry.py", line 407, in processor_cache_from_config
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m     return ShmObjectStoreSenderCache(vllm_config)
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/multimodal/cache.py", line 451, in __init__
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m     ring_buffer = SingleWriterShmRingBuffer(
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/distributed/device_communicators/shm_object_storage.py", line 131, in __init__
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m     self.shared_memory = shared_memory.SharedMemory(
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/shared_memory.py", line 104, in __init__
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m     self._fd = _posixshmem.shm_open(
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m                ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:23:54] [INFO] [0;36m(APIServer pid=284809)[0;0m FileExistsError: [Errno 17] File exists: '/VLLM_OBJECT_STORAGE_SHM_BUFFER'
[2026-01-18 23:23:57] [INFO] nvitop监控已启动
[2026-01-18 23:24:26] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 23:24:26] [INFO] nvitop监控已启动
[2026-01-18 23:24:26] [INFO] nvitop监控已停止
[2026-01-18 23:25:42] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:25:42 [api_server.py:872] vLLM API server version 0.14.0rc2.dev117+g9e078d058
[2026-01-18 23:25:42] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:25:42 [utils.py:267] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma-3-27b-it-abliterated-normpreserve', 'host': '0.0.0.0', 'port': 8005, 'model': '/mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma-3-27b-it-abliterated-normpreserve', 'trust_remote_code': True, 'max_model_len': 128000, 'served_model_name': ['VLLM-MODEL'], 'tensor_parallel_size': 2, 'max_num_seqs': 256, 'async_scheduling': True}
[2026-01-18 23:25:42] [INFO] [0;36m(APIServer pid=286251)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 23:25:42] [INFO] [0;36m(APIServer pid=286251)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 23:25:42] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:25:42 [model.py:530] Resolved architecture: Gemma3ForConditionalGeneration
[2026-01-18 23:25:42] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:25:42 [model.py:1547] Using max model len 128000
[2026-01-18 23:25:43] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:25:43 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[2026-01-18 23:25:43] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:25:43 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 23:25:43] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:25:43 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-18 23:25:43] [WARNING] [0;36m(APIServer pid=286251)[0;0m WARNING 01-18 23:25:43 [cuda.py:244] Forcing --disable_chunked_mm_input for models with multimodal-bidirectional attention.
[2026-01-18 23:26:39] [INFO] [0;36m(EngineCore_DP0 pid=287778)[0;0m INFO 01-18 23:26:39 [core.py:96] Initializing a V1 LLM engine (v0.14.0rc2.dev117+g9e078d058) with config: model='/mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma-3-27b-it-abliterated-normpreserve', speculative_config=None, tokenizer='/mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma-3-27b-it-abliterated-normpreserve', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-18 23:26:39] [WARNING] [0;36m(EngineCore_DP0 pid=287778)[0;0m WARNING 01-18 23:26:39 [multiproc_executor.py:897] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-18 23:27:25] [INFO] INFO 01-18 23:27:25 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:37173 backend=nccl
[2026-01-18 23:27:25] [INFO] INFO 01-18 23:27:25 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:37173 backend=nccl
[2026-01-18 23:27:25] [INFO] INFO 01-18 23:27:25 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-18 23:27:26] [WARNING] WARNING 01-18 23:27:26 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 23:27:26] [WARNING] WARNING 01-18 23:27:26 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 23:27:26] [INFO] INFO 01-18 23:27:26 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank N/A
[2026-01-18 23:27:26] [INFO] INFO 01-18 23:27:26 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A
[2026-01-18 23:27:28] [INFO] Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[2026-01-18 23:27:28] [INFO] Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[2026-01-18 23:27:45] [INFO] [0;36m(Worker_TP0 pid=288619)[0;0m INFO 01-18 23:27:45 [gpu_model_runner.py:3803] Starting to load model /mnt/AI-Acer4T/AI-Chat/models/gemma-3/gemma-3-27b-it-abliterated-normpreserve...
[2026-01-18 23:27:46] [INFO] [0;36m(Worker_TP1 pid=288620)[0;0m INFO 01-18 23:27:46 [mm_encoder_attention.py:86] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[2026-01-18 23:27:46] [INFO] [0;36m(Worker_TP0 pid=288619)[0;0m INFO 01-18 23:27:46 [mm_encoder_attention.py:86] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[2026-01-18 23:27:46] [INFO] [0;36m(Worker_TP1 pid=288620)[0;0m INFO 01-18 23:27:46 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 23:27:46] [INFO] [0;36m(Worker_TP0 pid=288619)[0;0m INFO 01-18 23:27:46 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 23:27:51] [INFO] [0;36m(Worker_TP0 pid=288619)[0;0m INFO 01-18 23:27:51 [cuda.py:351] Using TRITON_ATTN attention backend out of potential backends: ('TRITON_ATTN', 'FLEX_ATTENTION')
[2026-01-18 23:27:52] [INFO] [0;36m(Worker_TP0 pid=288619)[0;0m
[2026-01-18 23:27:52] [INFO] Loading safetensors checkpoint shards:   0% Completed | 0/12 [00:00<?, ?it/s]
[2026-01-18 23:27:57] [INFO] [0;36m(Worker_TP0 pid=288619)[0;0m
[2026-01-18 23:27:57] [INFO] Loading safetensors checkpoint shards:   8% Completed | 1/12 [00:05<00:56,  5.14s/it]
[2026-01-18 23:28:03] [INFO] [0;36m(Worker_TP0 pid=288619)[0;0m
[2026-01-18 23:28:03] [INFO] Loading safetensors checkpoint shards:  17% Completed | 2/12 [00:10<00:54,  5.41s/it]
[2026-01-18 23:28:08] [INFO] [0;36m(Worker_TP0 pid=288619)[0;0m
[2026-01-18 23:28:08] [INFO] Loading safetensors checkpoint shards:  25% Completed | 3/12 [00:16<00:48,  5.42s/it]
[2026-01-18 23:28:13] [INFO] [0;36m(Worker_TP0 pid=288619)[0;0m
[2026-01-18 23:28:13] [INFO] Loading safetensors checkpoint shards:  33% Completed | 4/12 [00:21<00:43,  5.39s/it]
[2026-01-18 23:28:19] [INFO] [0;36m(Worker_TP0 pid=288619)[0;0m
[2026-01-18 23:28:19] [INFO] Loading safetensors checkpoint shards:  42% Completed | 5/12 [00:27<00:38,  5.51s/it]
[2026-01-18 23:28:25] [INFO] [0;36m(Worker_TP0 pid=288619)[0;0m
[2026-01-18 23:28:25] [INFO] Loading safetensors checkpoint shards:  50% Completed | 6/12 [00:33<00:34,  5.69s/it]
[2026-01-18 23:28:31] [INFO] [0;36m(Worker_TP0 pid=288619)[0;0m
[2026-01-18 23:28:31] [INFO] Loading safetensors checkpoint shards:  58% Completed | 7/12 [00:39<00:28,  5.73s/it]
[2026-01-18 23:28:37] [INFO] [0;36m(Worker_TP0 pid=288619)[0;0m
[2026-01-18 23:28:37] [INFO] Loading safetensors checkpoint shards:  67% Completed | 8/12 [00:45<00:23,  5.83s/it]
[2026-01-18 23:28:43] [INFO] [0;36m(Worker_TP0 pid=288619)[0;0m
[2026-01-18 23:28:43] [INFO] Loading safetensors checkpoint shards:  75% Completed | 9/12 [00:51<00:17,  5.91s/it]
[2026-01-18 23:28:49] [INFO] [0;36m(Worker_TP0 pid=288619)[0;0m
[2026-01-18 23:28:49] [INFO] Loading safetensors checkpoint shards:  83% Completed | 10/12 [00:56<00:11,  5.86s/it]
[2026-01-18 23:28:55] [INFO] [0;36m(Worker_TP0 pid=288619)[0;0m
[2026-01-18 23:28:55] [INFO] Loading safetensors checkpoint shards:  92% Completed | 11/12 [01:02<00:05,  5.92s/it]
[2026-01-18 23:28:56] [INFO] [0;36m(Worker_TP0 pid=288619)[0;0m
[2026-01-18 23:28:56] [INFO] Loading safetensors checkpoint shards: 100% Completed | 12/12 [01:03<00:00,  4.33s/it]
[2026-01-18 23:28:56] [INFO] [0;36m(Worker_TP0 pid=288619)[0;0m
[2026-01-18 23:28:56] [INFO] Loading safetensors checkpoint shards: 100% Completed | 12/12 [01:03<00:00,  5.31s/it]
[2026-01-18 23:28:56] [INFO] [0;36m(Worker_TP0 pid=288619)[0;0m
[2026-01-18 23:28:56] [INFO] [0;36m(Worker_TP0 pid=288619)[0;0m INFO 01-18 23:28:56 [default_loader.py:291] Loading weights took 63.70 seconds
[2026-01-18 23:28:56] [INFO] [0;36m(Worker_TP0 pid=288619)[0;0m INFO 01-18 23:28:56 [gpu_model_runner.py:3900] Model loading took 25.91 GiB memory and 69.899381 seconds
[2026-01-18 23:28:57] [INFO] [0;36m(Worker_TP1 pid=288620)[0;0m INFO 01-18 23:28:57 [gpu_model_runner.py:4711] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.
[2026-01-18 23:28:57] [INFO] [0;36m(Worker_TP0 pid=288619)[0;0m INFO 01-18 23:28:57 [gpu_model_runner.py:4711] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 32 image items of the maximum feature size.
[2026-01-18 23:29:45] [INFO] [0;36m(Worker_TP0 pid=288619)[0;0m INFO 01-18 23:29:45 [backends.py:644] Using cache directory: /home/wuwen/.cache/vllm/torch_compile_cache/723515666a/rank_0_0/backbone for vLLM's torch.compile
[2026-01-18 23:29:45] [INFO] [0;36m(Worker_TP0 pid=288619)[0;0m INFO 01-18 23:29:45 [backends.py:704] Dynamo bytecode transform time: 23.97 s
[2026-01-18 23:29:56] [INFO] [0;36m(EngineCore_DP0 pid=287778)[0;0m INFO 01-18 23:29:56 [shm_broadcast.py:542] No available shared memory broadcast block found in 60 seconds. This typically happens when some processes are hanging or doing some time-consuming work (e.g. compilation, weight/kv cache quantization).
[2026-01-18 23:30:17] [INFO] [0;36m(Worker_TP0 pid=288619)[0;0m INFO 01-18 23:30:17 [backends.py:261] Cache the graph of compile range (1, 8192) for later use
[2026-01-18 23:30:17] [INFO] [0;36m(Worker_TP1 pid=288620)[0;0m INFO 01-18 23:30:17 [backends.py:261] Cache the graph of compile range (1, 8192) for later use
[2026-01-18 23:30:34] [INFO] [0;36m(Worker_TP0 pid=288619)[0;0m INFO 01-18 23:30:34 [backends.py:278] Compiling a graph for compile range (1, 8192) takes 29.63 s
[2026-01-18 23:30:34] [INFO] [0;36m(Worker_TP0 pid=288619)[0;0m INFO 01-18 23:30:34 [monitor.py:34] torch.compile takes 53.60 s in total
[2026-01-18 23:30:36] [INFO] [0;36m(Worker_TP0 pid=288619)[0;0m INFO 01-18 23:30:36 [gpu_worker.py:355] Available KV cache memory: 56.78 GiB
[2026-01-18 23:30:36] [WARNING] [0;36m(EngineCore_DP0 pid=287778)[0;0m WARNING 01-18 23:30:36 [kv_cache_utils.py:1047] Add 8 padding layers, may waste at most 15.38% KV cache memory
[2026-01-18 23:30:36] [INFO] [0;36m(EngineCore_DP0 pid=287778)[0;0m INFO 01-18 23:30:36 [kv_cache_utils.py:1307] GPU KV cache size: 212,624 tokens
[2026-01-18 23:30:36] [INFO] [0;36m(EngineCore_DP0 pid=287778)[0;0m INFO 01-18 23:30:36 [kv_cache_utils.py:1312] Maximum concurrency for 128,000 tokens per request: 8.12x
[2026-01-18 23:30:36] [INFO] [0;36m(Worker_TP1 pid=288620)[0;0m 2026-01-18 23:30:36,872 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[2026-01-18 23:30:36] [INFO] [0;36m(Worker_TP0 pid=288619)[0;0m 2026-01-18 23:30:36,873 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[2026-01-18 23:30:37] [INFO] [0;36m(Worker_TP1 pid=288620)[0;0m 2026-01-18 23:30:36,997 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[2026-01-18 23:30:37] [INFO] [0;36m(Worker_TP0 pid=288619)[0;0m 2026-01-18 23:30:37,028 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[2026-01-18 23:30:38] [INFO] [0;36m(Worker_TP0 pid=288619)[0;0m
[2026-01-18 23:30:38] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
[2026-01-18 23:30:38] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:05,  8.60it/s]
[2026-01-18 23:30:38] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:05,  8.16it/s]
[2026-01-18 23:30:38] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:00<00:05,  8.96it/s]
[2026-01-18 23:30:38] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▎        | 7/51 [00:00<00:04,  9.21it/s]
[2026-01-18 23:30:39] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:00<00:04,  9.29it/s]
[2026-01-18 23:30:39] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:01<00:04,  8.51it/s]
[2026-01-18 23:30:39] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:01<00:04,  9.24it/s]
[2026-01-18 23:30:39] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:01<00:04,  9.14it/s]
[2026-01-18 23:30:39] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:01<00:04,  8.98it/s]
[2026-01-18 23:30:39] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:01<00:04,  8.85it/s]
[2026-01-18 23:30:40] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 16/51 [00:01<00:04,  8.62it/s]
[2026-01-18 23:30:40] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:02<00:05,  6.60it/s]
[2026-01-18 23:30:40] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:02<00:04,  7.03it/s]
[2026-01-18 23:30:40] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 19/51 [00:02<00:04,  6.68it/s]
[2026-01-18 23:30:40] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:02<00:06,  5.14it/s]
[2026-01-18 23:30:40] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:02<00:05,  5.81it/s]
[2026-01-18 23:30:41] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 22/51 [00:02<00:04,  6.48it/s]
[2026-01-18 23:30:41] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:03<00:04,  6.54it/s]
[2026-01-18 23:30:41] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 25/51 [00:03<00:03,  7.83it/s]
[2026-01-18 23:30:41] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:03<00:03,  7.91it/s]
[2026-01-18 23:30:41] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:03<00:03,  6.58it/s]
[2026-01-18 23:30:41] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:03<00:03,  7.20it/s]
[2026-01-18 23:30:42] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:03<00:03,  5.69it/s]
[2026-01-18 23:30:42] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:04<00:04,  5.05it/s]
[2026-01-18 23:30:42] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 31/51 [00:04<00:03,  5.30it/s]
[2026-01-18 23:30:42] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:04<00:03,  5.51it/s]
[2026-01-18 23:30:42] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:04<00:03,  5.02it/s]
[2026-01-18 23:30:43] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:04<00:03,  5.51it/s]
[2026-01-18 23:30:43] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:05<00:03,  5.28it/s]
[2026-01-18 23:30:43] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:05<00:03,  4.84it/s]
[2026-01-18 23:30:43] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 37/51 [00:05<00:02,  4.68it/s]
[2026-01-18 23:30:44] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:05<00:02,  4.59it/s]
[2026-01-18 23:30:44] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▋  | 39/51 [00:06<00:02,  4.53it/s]
[2026-01-18 23:30:44] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:06<00:02,  4.93it/s]
[2026-01-18 23:30:44] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:06<00:01,  5.30it/s]
[2026-01-18 23:30:44] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:06<00:01,  5.79it/s]
[2026-01-18 23:30:44] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 43/51 [00:06<00:01,  5.54it/s]
[2026-01-18 23:30:45] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:06<00:01,  5.07it/s]
[2026-01-18 23:30:45] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 45/51 [00:07<00:01,  5.28it/s]
[2026-01-18 23:30:45] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|█████████ | 46/51 [00:07<00:00,  5.34it/s]
[2026-01-18 23:30:45] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:07<00:00,  5.88it/s]
[2026-01-18 23:30:45] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 48/51 [00:07<00:00,  5.10it/s]
[2026-01-18 23:30:45] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▌| 49/51 [00:07<00:00,  5.98it/s]
[2026-01-18 23:30:46] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:07<00:00,  6.75it/s]
[2026-01-18 23:30:46] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:08<00:00,  5.19it/s]
[2026-01-18 23:30:46] [INFO] Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:08<00:00,  6.24it/s]
[2026-01-18 23:30:46] [INFO] [0;36m(Worker_TP0 pid=288619)[0;0m
[2026-01-18 23:30:48] [INFO] Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
[2026-01-18 23:30:50] [INFO] Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:01<01:05,  1.93s/it]
[2026-01-18 23:30:50] [INFO] Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:04<01:07,  2.03s/it]
[2026-01-18 23:30:50] [INFO] Capturing CUDA graphs (decode, FULL):   9%|▊         | 3/35 [00:04<00:37,  1.17s/it]
[2026-01-18 23:30:50] [INFO] Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:04<00:23,  1.32it/s]
[2026-01-18 23:30:50] [INFO] Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:04<00:15,  1.90it/s]
[2026-01-18 23:30:50] [INFO] Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:04<00:11,  2.59it/s]
[2026-01-18 23:30:51] [INFO] Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:04<00:08,  3.17it/s]
[2026-01-18 23:30:51] [INFO] Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:04<00:07,  3.83it/s]
[2026-01-18 23:30:51] [INFO] Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:04<00:05,  4.61it/s]
[2026-01-18 23:30:51] [INFO] Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:05<00:04,  5.45it/s]
[2026-01-18 23:30:51] [INFO] Capturing CUDA graphs (decode, FULL):  31%|███▏      | 11/35 [00:05<00:03,  6.02it/s]
[2026-01-18 23:30:51] [INFO] Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:05<00:03,  6.10it/s]
[2026-01-18 23:30:51] [INFO] Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:05<00:03,  6.43it/s]
[2026-01-18 23:30:51] [INFO] Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:05<00:03,  6.72it/s]
[2026-01-18 23:30:52] [INFO] Capturing CUDA graphs (decode, FULL):  43%|████▎     | 15/35 [00:05<00:02,  7.06it/s]
[2026-01-18 23:30:52] [INFO] Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:05<00:02,  7.15it/s]
[2026-01-18 23:30:52] [INFO] Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:06<00:02,  7.03it/s]
[2026-01-18 23:30:52] [INFO] Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:06<00:02,  6.85it/s]
[2026-01-18 23:30:52] [INFO] Capturing CUDA graphs (decode, FULL):  54%|█████▍    | 19/35 [00:06<00:02,  7.28it/s]
[2026-01-18 23:30:52] [INFO] Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:06<00:01,  7.52it/s]
[2026-01-18 23:30:52] [INFO] Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:06<00:02,  6.88it/s]
[2026-01-18 23:30:53] [INFO] Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:06<00:01,  6.74it/s]
[2026-01-18 23:30:53] [INFO] Capturing CUDA graphs (decode, FULL):  66%|██████▌   | 23/35 [00:06<00:02,  5.75it/s]
[2026-01-18 23:30:53] [INFO] Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:07<00:02,  4.95it/s]
[2026-01-18 23:30:53] [INFO] Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:07<00:02,  4.59it/s]
[2026-01-18 23:30:54] [INFO] Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:07<00:01,  4.70it/s]
[2026-01-18 23:30:54] [INFO] Capturing CUDA graphs (decode, FULL):  77%|███████▋  | 27/35 [00:07<00:01,  4.85it/s]
[2026-01-18 23:30:54] [INFO] Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:08<00:01,  5.11it/s]
[2026-01-18 23:30:54] [INFO] Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:08<00:01,  4.87it/s]
[2026-01-18 23:30:57] [INFO] Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:08<00:00,  5.46it/s]
[2026-01-18 23:30:59] [INFO] Capturing CUDA graphs (decode, FULL):  89%|████████▊ | 31/35 [00:10<00:03,  1.17it/s]
[2026-01-18 23:30:59] [INFO] Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:13<00:04,  1.34s/it]
[2026-01-18 23:30:59] [INFO] Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:13<00:01,  1.03it/s]
[2026-01-18 23:31:00] [INFO] Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:13<00:00,  1.39it/s][0;36m(Worker_TP1 pid=288620)[0;0m INFO 01-18 23:31:00 [custom_all_reduce.py:216] Registering 10664 cuda graph addresses
[2026-01-18 23:31:01] [INFO] 
[2026-01-18 23:31:01] [INFO] Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:15<00:00,  1.16s/it]
[2026-01-18 23:31:01] [INFO] Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:15<00:00,  2.22it/s]
[2026-01-18 23:31:01] [INFO] [0;36m(Worker_TP0 pid=288619)[0;0m INFO 01-18 23:31:01 [custom_all_reduce.py:216] Registering 10664 cuda graph addresses
[2026-01-18 23:31:03] [INFO] [0;36m(Worker_TP0 pid=288619)[0;0m INFO 01-18 23:31:03 [gpu_model_runner.py:4852] Graph capturing finished in 26 secs, took -0.35 GiB
[2026-01-18 23:31:03] [INFO] [0;36m(EngineCore_DP0 pid=287778)[0;0m INFO 01-18 23:31:03 [core.py:272] init engine (profile, create kv cache, warmup model) took 126.21 seconds
[2026-01-18 23:31:07] [INFO] [0;36m(EngineCore_DP0 pid=287778)[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[2026-01-18 23:31:25] [INFO] [0;36m(EngineCore_DP0 pid=287778)[0;0m INFO 01-18 23:31:25 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 23:31:26] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:26 [api_server.py:663] Supported tasks: ['generate']
[2026-01-18 23:31:26] [WARNING] [0;36m(APIServer pid=286251)[0;0m WARNING 01-18 23:31:26 [model.py:1360] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[2026-01-18 23:31:26] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:26 [serving.py:227] Using default chat sampling params from model: {'top_k': 64, 'top_p': 0.95}
[2026-01-18 23:31:26] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:26 [serving.py:149] Using default chat sampling params from model: {'top_k': 64, 'top_p': 0.95}
[2026-01-18 23:31:26] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:26 [serving.py:185] Warming up chat template processing...
[2026-01-18 23:31:26] [INFO] [0;36m(APIServer pid=286251)[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:32 [chat_utils.py:599] Detected the chat template content format to be 'openai'. You can set `--chat-template-content-format` to override this.
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:32 [serving.py:221] Chat template warmup completed in 5484.0ms
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:32 [serving.py:80] Using default completion sampling params from model: {'top_k': 64, 'top_p': 0.95}
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:32 [serving.py:149] Using default chat sampling params from model: {'top_k': 64, 'top_p': 0.95}
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:32 [api_server.py:946] Starting vLLM API server 0 on http://0.0.0.0:8005
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:32 [launcher.py:38] Available routes are:
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:32 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:32 [launcher.py:46] Route: /docs, Methods: HEAD, GET
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:32 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:32 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:32 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:32 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:32 [launcher.py:46] Route: /tokenize, Methods: POST
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:32 [launcher.py:46] Route: /detokenize, Methods: POST
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:32 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:32 [launcher.py:46] Route: /pause, Methods: POST
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:32 [launcher.py:46] Route: /resume, Methods: POST
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:32 [launcher.py:46] Route: /is_paused, Methods: GET
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:32 [launcher.py:46] Route: /metrics, Methods: GET
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:32 [launcher.py:46] Route: /health, Methods: GET
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:32 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:32 [launcher.py:46] Route: /v1/responses, Methods: POST
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:32 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:32 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:32 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:32 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:32 [launcher.py:46] Route: /v1/completions, Methods: POST
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:32 [launcher.py:46] Route: /v1/messages, Methods: POST
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:32 [launcher.py:46] Route: /v1/models, Methods: GET
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:32 [launcher.py:46] Route: /load, Methods: GET
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:32 [launcher.py:46] Route: /version, Methods: GET
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:32 [launcher.py:46] Route: /ping, Methods: GET
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:32 [launcher.py:46] Route: /ping, Methods: POST
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:32 [launcher.py:46] Route: /invocations, Methods: POST
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:32 [launcher.py:46] Route: /classify, Methods: POST
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:32 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:32 [launcher.py:46] Route: /score, Methods: POST
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:32 [launcher.py:46] Route: /v1/score, Methods: POST
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:32 [launcher.py:46] Route: /rerank, Methods: POST
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:32 [launcher.py:46] Route: /v1/rerank, Methods: POST
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:32 [launcher.py:46] Route: /v2/rerank, Methods: POST
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:31:32 [launcher.py:46] Route: /pooling, Methods: POST
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO:     Started server process [286251]
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO:     Waiting for application startup.
[2026-01-18 23:31:32] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO:     Application startup complete.
[2026-01-18 23:31:59] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO:     127.0.0.1:38374 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-18 23:31:59] [INFO] [0;36m(Worker_TP0 pid=288619)[0;0m /mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nested/__init__.py:250: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)
[2026-01-18 23:31:59] [INFO] [0;36m(Worker_TP0 pid=288619)[0;0m   return _nested.nested_tensor(
[2026-01-18 23:31:59] [INFO] [0;36m(Worker_TP1 pid=288620)[0;0m /mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/torch/nested/__init__.py:250: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)
[2026-01-18 23:31:59] [INFO] [0;36m(Worker_TP1 pid=288620)[0;0m   return _nested.nested_tensor(
[2026-01-18 23:32:09] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:32:09 [loggers.py:257] Engine 000: Avg prompt throughput: 46.2 tokens/s, Avg generation throughput: 21.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[2026-01-18 23:32:19] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:32:19 [loggers.py:257] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, MM cache hit rate: 0.0%
[2026-01-18 23:32:22] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO:     127.0.0.1:51826 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[2026-01-18 23:32:29] [INFO] [0;36m(APIServer pid=286251)[0;0m INFO 01-18 23:32:29 [loggers.py:257] Engine 000: Avg prompt throughput: 69.0 tokens/s, Avg generation throughput: 23.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 58.3%, MM cache hit rate: 50.0%
[2026-01-18 23:32:37] [INFO] [0;36m(Worker_TP1 pid=288620)[0;0m INFO 01-18 23:32:37 [multiproc_executor.py:724] Parent process exited, terminating worker
[2026-01-18 23:32:37] [INFO] [0;36m(Worker_TP1 pid=288620)[0;0m INFO 01-18 23:32:37 [multiproc_executor.py:768] WorkerProc shutting down.
[2026-01-18 23:32:37] [INFO] [0;36m(Worker_TP0 pid=288619)[0;0m INFO 01-18 23:32:37 [multiproc_executor.py:724] Parent process exited, terminating worker
[2026-01-18 23:32:37] [INFO] [0;36m(Worker_TP0 pid=288619)[0;0m INFO 01-18 23:32:37 [multiproc_executor.py:768] WorkerProc shutting down.
[2026-01-18 23:32:37] [ERROR] [0;36m(APIServer pid=286251)[0;0m ERROR 01-18 23:32:37 [core_client.py:605] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.
[2026-01-18 23:32:37] [INFO] [0;36m(Worker_TP0 pid=288619)[0;0m /mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/resource_tracker.py:147: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.
[2026-01-18 23:32:37] [INFO] [0;36m(Worker_TP0 pid=288619)[0;0m   warnings.warn('resource_tracker: process died unexpectedly, '
[2026-01-18 23:32:37] [INFO] [0;36m(Worker_TP1 pid=288620)[0;0m /mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/resource_tracker.py:147: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.
[2026-01-18 23:32:37] [INFO] [0;36m(Worker_TP1 pid=288620)[0;0m   warnings.warn('resource_tracker: process died unexpectedly, '
[2026-01-18 23:32:37] [ERROR] Traceback (most recent call last):
[2026-01-18 23:32:37] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 23:32:37] [ERROR] Traceback (most recent call last):
[2026-01-18 23:32:37] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 23:32:37] [INFO] cache[rtype].remove(name)
[2026-01-18 23:32:37] [INFO] KeyError: '/psm_6e2ca160'
[2026-01-18 23:32:37] [ERROR] Traceback (most recent call last):
[2026-01-18 23:32:37] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 23:32:37] [INFO] cache[rtype].remove(name)
[2026-01-18 23:32:37] [INFO] KeyError: '/mp-_udjgxz1'
[2026-01-18 23:32:37] [INFO] cache[rtype].remove(name)
[2026-01-18 23:32:37] [INFO] KeyError: '/psm_34f257e5'
[2026-01-18 23:32:37] [ERROR] Traceback (most recent call last):
[2026-01-18 23:32:37] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 23:32:37] [INFO] cache[rtype].remove(name)
[2026-01-18 23:32:37] [INFO] KeyError: '/psm_22166c9d'
[2026-01-18 23:32:37] [ERROR] Traceback (most recent call last):
[2026-01-18 23:32:37] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/resource_tracker.py", line 264, in main
[2026-01-18 23:32:37] [INFO] cache[rtype].remove(name)
[2026-01-18 23:32:37] [INFO] KeyError: '/mp-4ifve90d'
[2026-01-18 23:32:37] [INFO] 服务已停止
[2026-01-18 23:32:38] [INFO] nvitop监控已启动
[2026-01-18 23:32:39] [INFO] nvitop监控已启动
[2026-01-18 23:32:42] [INFO] nvitop监控已启动
[2026-01-18 23:34:22] [SUCCESS] 方案已保存: Qwen3-VL-235B-A22B-Thinking-AWQ
[2026-01-18 23:34:25] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 23:34:25] [INFO] nvitop监控已启动
[2026-01-18 23:34:25] [INFO] nvitop监控已停止
[2026-01-18 23:35:13] [ERROR] Traceback (most recent call last):
[2026-01-18 23:35:13] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/bin/vllm", line 7, in <module>
[2026-01-18 23:35:13] [INFO] sys.exit(main())
[2026-01-18 23:35:13] [INFO] ^^^^^^
[2026-01-18 23:35:13] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 66, in main
[2026-01-18 23:35:13] [INFO] cmd.subparser_init(subparsers).set_defaults(dispatch_function=cmd.cmd)
[2026-01-18 23:35:13] [INFO] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:35:13] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 76, in subparser_init
[2026-01-18 23:35:13] [INFO] serve_parser = make_arg_parser(serve_parser)
[2026-01-18 23:35:13] [INFO] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:35:13] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/cli_args.py", line 296, in make_arg_parser
[2026-01-18 23:35:13] [INFO] parser = AsyncEngineArgs.add_cli_args(parser)
[2026-01-18 23:35:13] [INFO] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:35:13] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/engine/arg_utils.py", line 2044, in add_cli_args
[2026-01-18 23:35:13] [INFO] parser = EngineArgs.add_cli_args(parser)
[2026-01-18 23:35:13] [INFO] ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:35:13] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/engine/arg_utils.py", line 1144, in add_cli_args
[2026-01-18 23:35:13] [INFO] vllm_kwargs = get_kwargs(VllmConfig)
[2026-01-18 23:35:13] [INFO] ^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:35:13] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/engine/arg_utils.py", line 345, in get_kwargs
[2026-01-18 23:35:13] [INFO] return copy.deepcopy(_compute_kwargs(cls))
[2026-01-18 23:35:13] [INFO] ^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:35:13] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/engine/arg_utils.py", line 257, in _compute_kwargs
[2026-01-18 23:35:13] [INFO] default = default.default_factory()
[2026-01-18 23:35:13] [INFO] ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:35:13] [INFO] File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/pydantic/_internal/_dataclasses.py", line 121, in __init__
[2026-01-18 23:35:13] [INFO] s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)
[2026-01-18 23:35:13] [INFO] pydantic_core._pydantic_core.ValidationError: 1 validation error for AttentionConfig
[2026-01-18 23:35:13] [INFO] Value error, Invalid value 'DUAL_CHUNK_FLASH_ATTN' for VLLM_ATTENTION_BACKEND. Valid options: ['FLASH_ATTN', 'FLASH_ATTN_DIFFKV', 'TRITON_ATTN', 'ROCM_ATTN', 'ROCM_AITER_MLA', 'ROCM_AITER_TRITON_MLA', 'ROCM_AITER_FA', 'ROCM_AITER_MLA_SPARSE', 'TORCH_SDPA', 'FLASHINFER', 'FLASHINFER_MLA', 'TRITON_MLA', 'CUTLASS_MLA', 'FLASHMLA', 'FLASHMLA_SPARSE', 'FLASH_ATTN_MLA', 'IPEX', 'NO_ATTENTION', 'FLEX_ATTENTION', 'TREE_ATTN', 'ROCM_AITER_UNIFIED_ATTN', 'CPU_ATTN', 'CUSTOM']. [type=value_error, input_value=ArgsKwargs(()), input_type=ArgsKwargs]
[2026-01-18 23:35:13] [INFO] For further information visit https://errors.pydantic.dev/2.12/v/value_error
[2026-01-18 23:35:15] [INFO] nvitop监控已启动
[2026-01-18 23:35:43] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 23:35:43] [INFO] nvitop监控已启动
[2026-01-18 23:35:43] [INFO] nvitop监控已停止
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m INFO 01-18 23:36:11 [api_server.py:872] vLLM API server version 0.14.0rc2.dev117+g9e078d058
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m INFO 01-18 23:36:11 [utils.py:267] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3//mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-VL-235B-A22B-Thinking-AWQ/', 'host': '0.0.0.0', 'port': 8005, 'model': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3//mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-VL-235B-A22B-Thinking-AWQ/', 'trust_remote_code': True, 'max_model_len': 1010000, 'served_model_name': ['VLLM-MODEL'], 'tensor_parallel_size': 2, 'max_num_seqs': 256, 'async_scheduling': True}
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 23:36:11] [ERROR] [0;36m(APIServer pid=300591)[0;0m Traceback (most recent call last):
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/transformers/utils/hub.py", line 425, in cached_files
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m     hf_hub_download(
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 85, in _inner_fn
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m     validate_repo_id(arg_value)
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 129, in validate_repo_id
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m     raise HFValidationError(
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3//mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-VL-235B-A22B-Thinking-AWQ/'. Use `repo_type` argument if needed.
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m The above exception was the direct cause of the following exception:
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m
[2026-01-18 23:36:11] [ERROR] [0;36m(APIServer pid=300591)[0;0m Traceback (most recent call last):
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/bin/vllm", line 7, in <module>
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m     sys.exit(main())
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m              ^^^^^^
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m     args.dispatch_function(args)
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m     uvloop.run(run_server(args))
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m     return __asyncio.run(
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m            ^^^^^^^^^^^^^^
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m     return runner.run(main)
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m     return self._loop.run_until_complete(task)
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m     return await main
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m            ^^^^^^^^^^
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m     async with build_async_engine_client(
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m     return await anext(self.gen)
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 146, in build_async_engine_client
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m     return await anext(self.gen)
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 172, in build_async_engine_client_from_engine_args
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/engine/arg_utils.py", line 1355, in create_engine_config
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m     maybe_override_with_speculators(
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/transformers_utils/config.py", line 528, in maybe_override_with_speculators
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m     config_dict, _ = PretrainedConfig.get_config_dict(
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/transformers/configuration_utils.py", line 619, in get_config_dict
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m     config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/transformers/configuration_utils.py", line 674, in _get_config_dict
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m     resolved_config_file = cached_file(
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m                            ^^^^^^^^^^^^
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/transformers/utils/hub.py", line 282, in cached_file
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m     file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/transformers/utils/hub.py", line 474, in cached_files
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m     raise OSError(f"{e}") from e
[2026-01-18 23:36:11] [INFO] [0;36m(APIServer pid=300591)[0;0m OSError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3//mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-VL-235B-A22B-Thinking-AWQ/'. Use `repo_type` argument if needed.
[2026-01-18 23:36:15] [INFO] nvitop监控已启动
[2026-01-18 23:39:22] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 23:39:22] [INFO] nvitop监控已启动
[2026-01-18 23:39:22] [INFO] nvitop监控已停止
[2026-01-18 23:39:58] [INFO] [0;36m(APIServer pid=302909)[0;0m INFO 01-18 23:39:58 [api_server.py:872] vLLM API server version 0.14.0rc2.dev117+g9e078d058
[2026-01-18 23:39:58] [INFO] [0;36m(APIServer pid=302909)[0;0m INFO 01-18 23:39:58 [utils.py:267] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-VL-235B-A22B-Thinking-AWQ/', 'host': '0.0.0.0', 'port': 8005, 'model': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-VL-235B-A22B-Thinking-AWQ/', 'trust_remote_code': True, 'max_model_len': 1010000, 'served_model_name': ['VLLM-MODEL'], 'tensor_parallel_size': 2, 'max_num_seqs': 256, 'async_scheduling': True}
[2026-01-18 23:39:58] [INFO] [0;36m(APIServer pid=302909)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 23:39:58] [INFO] [0;36m(APIServer pid=302909)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 23:39:58] [INFO] [0;36m(APIServer pid=302909)[0;0m Unrecognized keys in `rope_parameters` for 'rope_type'='default': {'mrope_interleaved', 'mrope_section'}
[2026-01-18 23:39:58] [INFO] [0;36m(APIServer pid=302909)[0;0m Unrecognized keys in `rope_parameters` for 'rope_type'='default': {'mrope_interleaved', 'mrope_section'}
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m INFO 01-18 23:40:30 [model.py:530] Resolved architecture: Qwen3VLMoeForConditionalGeneration
[2026-01-18 23:40:30] [ERROR] [0;36m(APIServer pid=302909)[0;0m Traceback (most recent call last):
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/bin/vllm", line 7, in <module>
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m     sys.exit(main())
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m              ^^^^^^
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m     args.dispatch_function(args)
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m     uvloop.run(run_server(args))
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m     return __asyncio.run(
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m            ^^^^^^^^^^^^^^
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m     return runner.run(main)
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m     return self._loop.run_until_complete(task)
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m     return await main
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m            ^^^^^^^^^^
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m     async with build_async_engine_client(
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m     return await anext(self.gen)
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 146, in build_async_engine_client
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m     return await anext(self.gen)
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 172, in build_async_engine_client_from_engine_args
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/engine/arg_utils.py", line 1364, in create_engine_config
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m     model_config = self.create_model_config()
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/engine/arg_utils.py", line 1219, in create_model_config
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m     return ModelConfig(
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m            ^^^^^^^^^^^^
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/pydantic/_internal/_dataclasses.py", line 121, in __init__
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m   Value error, User-specified max_model_len (1010000) is greater than the derived max_model_len (max_position_embeddings=262144.0 or model_max_length=None in model's config.json). To allow overriding this maximum, set the env var VLLM_ALLOW_LONG_MAX_MODEL_LEN=1. VLLM_ALLOW_LONG_MAX_MODEL_LEN must be used with extreme caution. If the model uses relative position encoding (RoPE), positions exceeding derived_max_model_len lead to nan. If the model uses absolute position encoding, positions exceeding derived_max_model_len will cause a CUDA array out-of-bounds error. [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]
[2026-01-18 23:40:30] [INFO] [0;36m(APIServer pid=302909)[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error
[2026-01-18 23:40:33] [INFO] nvitop监控已启动
[2026-01-18 23:46:53] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 23:46:53] [INFO] nvitop监控已启动
[2026-01-18 23:46:53] [INFO] nvitop监控已停止
[2026-01-18 23:47:23] [INFO] [0;36m(APIServer pid=305610)[0;0m INFO 01-18 23:47:23 [api_server.py:872] vLLM API server version 0.14.0rc2.dev117+g9e078d058
[2026-01-18 23:47:23] [INFO] [0;36m(APIServer pid=305610)[0;0m INFO 01-18 23:47:23 [utils.py:267] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-VL-235B-A22B-Thinking-AWQ/', 'host': '0.0.0.0', 'port': 8005, 'model': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-VL-235B-A22B-Thinking-AWQ/', 'trust_remote_code': True, 'max_model_len': 128000, 'served_model_name': ['VLLM-MODEL'], 'tensor_parallel_size': 2, 'max_num_seqs': 256, 'async_scheduling': True}
[2026-01-18 23:47:23] [INFO] [0;36m(APIServer pid=305610)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 23:47:23] [INFO] [0;36m(APIServer pid=305610)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 23:47:23] [INFO] [0;36m(APIServer pid=305610)[0;0m Unrecognized keys in `rope_parameters` for 'rope_type'='default': {'mrope_interleaved', 'mrope_section'}
[2026-01-18 23:47:23] [INFO] [0;36m(APIServer pid=305610)[0;0m Unrecognized keys in `rope_parameters` for 'rope_type'='default': {'mrope_interleaved', 'mrope_section'}
[2026-01-18 23:47:23] [INFO] [0;36m(APIServer pid=305610)[0;0m INFO 01-18 23:47:23 [model.py:530] Resolved architecture: Qwen3VLMoeForConditionalGeneration
[2026-01-18 23:47:23] [INFO] [0;36m(APIServer pid=305610)[0;0m INFO 01-18 23:47:23 [model.py:1547] Using max model len 128000
[2026-01-18 23:47:23] [WARNING] [0;36m(APIServer pid=305610)[0;0m WARNING 01-18 23:47:23 [quark_ocp_mx.py:157] AITER is not found or QuarkOCP_MX is not supported on the current platform. QuarkOCP_MX quantization will not be available.
[2026-01-18 23:47:24] [INFO] [0;36m(APIServer pid=305610)[0;0m INFO 01-18 23:47:24 [awq_marlin.py:163] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.
[2026-01-18 23:47:24] [INFO] [0;36m(APIServer pid=305610)[0;0m INFO 01-18 23:47:24 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[2026-01-18 23:47:24] [INFO] [0;36m(APIServer pid=305610)[0;0m INFO 01-18 23:47:24 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 23:47:24] [INFO] [0;36m(APIServer pid=305610)[0;0m INFO 01-18 23:47:24 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-18 23:47:54] [INFO] [0;36m(EngineCore_DP0 pid=305955)[0;0m INFO 01-18 23:47:54 [core.py:96] Initializing a V1 LLM engine (v0.14.0rc2.dev117+g9e078d058) with config: model='/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-VL-235B-A22B-Thinking-AWQ/', speculative_config=None, tokenizer='/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-VL-235B-A22B-Thinking-AWQ/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-18 23:47:54] [WARNING] [0;36m(EngineCore_DP0 pid=305955)[0;0m WARNING 01-18 23:47:54 [multiproc_executor.py:897] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-18 23:48:24] [INFO] INFO 01-18 23:48:24 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:44541 backend=nccl
[2026-01-18 23:48:24] [INFO] INFO 01-18 23:48:24 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:44541 backend=nccl
[2026-01-18 23:48:24] [INFO] INFO 01-18 23:48:24 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-18 23:48:24] [WARNING] WARNING 01-18 23:48:24 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 23:48:24] [WARNING] WARNING 01-18 23:48:24 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 23:48:24] [INFO] INFO 01-18 23:48:24 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
[2026-01-18 23:48:24] [INFO] INFO 01-18 23:48:24 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[2026-01-18 23:48:30] [INFO] [0;36m(Worker_TP0 pid=306275)[0;0m INFO 01-18 23:48:30 [gpu_model_runner.py:3803] Starting to load model /mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-VL-235B-A22B-Thinking-AWQ/...
[2026-01-18 23:48:30] [INFO] [0;36m(Worker_TP1 pid=306276)[0;0m INFO 01-18 23:48:30 [mm_encoder_attention.py:86] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[2026-01-18 23:48:30] [INFO] [0;36m(Worker_TP0 pid=306275)[0;0m INFO 01-18 23:48:30 [mm_encoder_attention.py:86] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[2026-01-18 23:48:34] [INFO] [0;36m(Worker_TP0 pid=306275)[0;0m INFO 01-18 23:48:34 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[2026-01-18 23:48:35] [INFO] [0;36m(Worker_TP0 pid=306275)[0;0m
[2026-01-18 23:48:35] [INFO] Loading safetensors checkpoint shards:   0% Completed | 0/42 [00:00<?, ?it/s]
[2026-01-18 23:48:37] [INFO] [0;36m(Worker_TP0 pid=306275)[0;0m
[2026-01-18 23:48:37] [INFO] Loading safetensors checkpoint shards:   2% Completed | 1/42 [00:01<01:07,  1.65s/it]
[2026-01-18 23:48:40] [INFO] [0;36m(Worker_TP0 pid=306275)[0;0m
[2026-01-18 23:48:40] [INFO] Loading safetensors checkpoint shards:   5% Completed | 2/42 [00:04<01:33,  2.34s/it]
[2026-01-18 23:48:43] [INFO] [0;36m(Worker_TP0 pid=306275)[0;0m
[2026-01-18 23:48:43] [INFO] Loading safetensors checkpoint shards:   7% Completed | 3/42 [00:07<01:44,  2.69s/it]
[2026-01-18 23:48:46] [INFO] [0;36m(Worker_TP0 pid=306275)[0;0m
[2026-01-18 23:48:46] [INFO] Loading safetensors checkpoint shards:  10% Completed | 4/42 [00:10<01:48,  2.85s/it]
[2026-01-18 23:48:49] [INFO] [0;36m(Worker_TP0 pid=306275)[0;0m
[2026-01-18 23:48:49] [INFO] Loading safetensors checkpoint shards:  12% Completed | 5/42 [00:13<01:48,  2.93s/it]
[2026-01-18 23:48:52] [INFO] [0;36m(Worker_TP0 pid=306275)[0;0m
[2026-01-18 23:48:52] [INFO] Loading safetensors checkpoint shards:  14% Completed | 6/42 [00:16<01:47,  2.99s/it]
[2026-01-18 23:48:55] [INFO] [0;36m(Worker_TP0 pid=306275)[0;0m
[2026-01-18 23:48:55] [INFO] Loading safetensors checkpoint shards:  17% Completed | 7/42 [00:20<01:47,  3.07s/it]
[2026-01-18 23:48:55] [INFO] [0;36m(Worker_TP0 pid=306275)[0;0m
[2026-01-18 23:48:55] [INFO] Loading safetensors checkpoint shards:  17% Completed | 7/42 [00:20<01:40,  2.87s/it]
[2026-01-18 23:48:55] [INFO] [0;36m(Worker_TP0 pid=306275)[0;0m
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766] WorkerProc failed to start.
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766] Traceback (most recent call last):
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 737, in worker_main
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     worker = WorkerProc(*args, **kwargs)
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 575, in __init__
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     self.worker.load_model()
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 274, in load_model
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3822, in load_model
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     self.model = model_loader.load_model(
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 58, in load_model
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     self.load_weights(model, model_config)
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 288, in load_weights
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_vl.py", line 2083, in load_weights
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     return loader.load_weights(weights, mapper=self.hf_to_vllm_mapper)
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 335, in load_weights
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     autoloaded_weights = set(self._load_module("", self.module, weights))
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 288, in _load_module
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     yield from self._load_module(
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 261, in _load_module
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     loaded_params = module_load_weights(weights)
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 749, in load_weights
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     return loader.load_weights(weights)
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 335, in load_weights
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     autoloaded_weights = set(self._load_module("", self.module, weights))
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 288, in _load_module
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     yield from self._load_module(
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 261, in _load_module
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     loaded_params = module_load_weights(weights)
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_vl_moe.py", line 194, in load_weights
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     for name, loaded_weight in weights:
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]                                ^^^^^^^
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 170, in <genexpr>
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     for parts, weights_data in group
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]                                ^^^^^
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 160, in <genexpr>
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     for weight_name, weight_data in weights
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]                                     ^^^^^^^
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 332, in <genexpr>
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     (name, weight) for name, weight in weights if not self._can_skip(name)
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]                                        ^^^^^^^
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 170, in <genexpr>
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     for parts, weights_data in group
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]                                ^^^^^
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 160, in <genexpr>
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     for weight_name, weight_data in weights
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]                                     ^^^^^^^
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 332, in <genexpr>
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     (name, weight) for name, weight in weights if not self._can_skip(name)
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]                                        ^^^^^^^
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 91, in <genexpr>
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     for name, data in weights
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]                       ^^^^^^^
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 260, in get_all_weights
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     yield from self._get_weights_iterator(primary_weights)
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 246, in <genexpr>
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     return ((source.prefix + name, tensor) for (name, tensor) in weights_iterator)
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]                                                                  ^^^^^^^^^^^^^^^^
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py", line 708, in safetensors_weights_iterator
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     with safe_open(st_file, framework="pt") as f:
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP1 pid=306276)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766] safetensors_rust.SafetensorError: Error while deserializing header: header too large
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766] WorkerProc failed to start.
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766] Traceback (most recent call last):
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 737, in worker_main
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     worker = WorkerProc(*args, **kwargs)
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 575, in __init__
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     self.worker.load_model()
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 274, in load_model
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3822, in load_model
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     self.model = model_loader.load_model(
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 58, in load_model
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     self.load_weights(model, model_config)
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 288, in load_weights
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_vl.py", line 2083, in load_weights
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     return loader.load_weights(weights, mapper=self.hf_to_vllm_mapper)
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 335, in load_weights
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     autoloaded_weights = set(self._load_module("", self.module, weights))
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 288, in _load_module
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     yield from self._load_module(
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 261, in _load_module
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     loaded_params = module_load_weights(weights)
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 749, in load_weights
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     return loader.load_weights(weights)
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 335, in load_weights
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     autoloaded_weights = set(self._load_module("", self.module, weights))
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 288, in _load_module
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     yield from self._load_module(
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 261, in _load_module
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     loaded_params = module_load_weights(weights)
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_vl_moe.py", line 194, in load_weights
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     for name, loaded_weight in weights:
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]                                ^^^^^^^
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 170, in <genexpr>
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     for parts, weights_data in group
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]                                ^^^^^
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 160, in <genexpr>
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     for weight_name, weight_data in weights
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]                                     ^^^^^^^
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 332, in <genexpr>
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     (name, weight) for name, weight in weights if not self._can_skip(name)
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]                                        ^^^^^^^
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 170, in <genexpr>
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     for parts, weights_data in group
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]                                ^^^^^
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 160, in <genexpr>
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     for weight_name, weight_data in weights
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]                                     ^^^^^^^
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 332, in <genexpr>
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     (name, weight) for name, weight in weights if not self._can_skip(name)
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]                                        ^^^^^^^
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 91, in <genexpr>
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     for name, data in weights
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]                       ^^^^^^^
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 260, in get_all_weights
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     yield from self._get_weights_iterator(primary_weights)
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 246, in <genexpr>
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     return ((source.prefix + name, tensor) for (name, tensor) in weights_iterator)
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]                                                                  ^^^^^^^^^^^^^^^^
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py", line 708, in safetensors_weights_iterator
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]     with safe_open(st_file, framework="pt") as f:
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:48:56] [ERROR] [0;36m(Worker_TP0 pid=306275)[0;0m ERROR 01-18 23:48:56 [multiproc_executor.py:766] safetensors_rust.SafetensorError: Error while deserializing header: header too large
[2026-01-18 23:48:56] [INFO] [0;36m(Worker_TP0 pid=306275)[0;0m INFO 01-18 23:48:56 [multiproc_executor.py:724] Parent process exited, terminating worker
[2026-01-18 23:48:56] [INFO] [0;36m(Worker_TP1 pid=306276)[0;0m INFO 01-18 23:48:56 [multiproc_executor.py:724] Parent process exited, terminating worker
[2026-01-18 23:48:57] [WARNING] [rank0]:[W118 23:48:57.647344832 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2026-01-18 23:48:58] [ERROR] [0;36m(EngineCore_DP0 pid=305955)[0;0m ERROR 01-18 23:48:58 [core.py:935] EngineCore failed to start.
[2026-01-18 23:48:58] [ERROR] [0;36m(EngineCore_DP0 pid=305955)[0;0m ERROR 01-18 23:48:58 [core.py:935] Traceback (most recent call last):
[2026-01-18 23:48:58] [ERROR] [0;36m(EngineCore_DP0 pid=305955)[0;0m ERROR 01-18 23:48:58 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-18 23:48:58] [ERROR] [0;36m(EngineCore_DP0 pid=305955)[0;0m ERROR 01-18 23:48:58 [core.py:935]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 23:48:58] [ERROR] [0;36m(EngineCore_DP0 pid=305955)[0;0m ERROR 01-18 23:48:58 [core.py:935]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:48:58] [ERROR] [0;36m(EngineCore_DP0 pid=305955)[0;0m ERROR 01-18 23:48:58 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-18 23:48:58] [ERROR] [0;36m(EngineCore_DP0 pid=305955)[0;0m ERROR 01-18 23:48:58 [core.py:935]     super().__init__(
[2026-01-18 23:48:58] [ERROR] [0;36m(EngineCore_DP0 pid=305955)[0;0m ERROR 01-18 23:48:58 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[2026-01-18 23:48:58] [ERROR] [0;36m(EngineCore_DP0 pid=305955)[0;0m ERROR 01-18 23:48:58 [core.py:935]     self.model_executor = executor_class(vllm_config)
[2026-01-18 23:48:58] [ERROR] [0;36m(EngineCore_DP0 pid=305955)[0;0m ERROR 01-18 23:48:58 [core.py:935]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:48:58] [ERROR] [0;36m(EngineCore_DP0 pid=305955)[0;0m ERROR 01-18 23:48:58 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[2026-01-18 23:48:58] [ERROR] [0;36m(EngineCore_DP0 pid=305955)[0;0m ERROR 01-18 23:48:58 [core.py:935]     super().__init__(vllm_config)
[2026-01-18 23:48:58] [ERROR] [0;36m(EngineCore_DP0 pid=305955)[0;0m ERROR 01-18 23:48:58 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[2026-01-18 23:48:58] [ERROR] [0;36m(EngineCore_DP0 pid=305955)[0;0m ERROR 01-18 23:48:58 [core.py:935]     self._init_executor()
[2026-01-18 23:48:58] [ERROR] [0;36m(EngineCore_DP0 pid=305955)[0;0m ERROR 01-18 23:48:58 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[2026-01-18 23:48:58] [ERROR] [0;36m(EngineCore_DP0 pid=305955)[0;0m ERROR 01-18 23:48:58 [core.py:935]     self.workers = WorkerProc.wait_for_ready(unready_workers)
[2026-01-18 23:48:58] [ERROR] [0;36m(EngineCore_DP0 pid=305955)[0;0m ERROR 01-18 23:48:58 [core.py:935]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:48:58] [ERROR] [0;36m(EngineCore_DP0 pid=305955)[0;0m ERROR 01-18 23:48:58 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 675, in wait_for_ready
[2026-01-18 23:48:58] [ERROR] [0;36m(EngineCore_DP0 pid=305955)[0;0m ERROR 01-18 23:48:58 [core.py:935]     raise e from None
[2026-01-18 23:48:58] [ERROR] [0;36m(EngineCore_DP0 pid=305955)[0;0m ERROR 01-18 23:48:58 [core.py:935] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[2026-01-18 23:48:58] [INFO] [0;36m(EngineCore_DP0 pid=305955)[0;0m Process EngineCore_DP0:
[2026-01-18 23:48:58] [ERROR] [0;36m(EngineCore_DP0 pid=305955)[0;0m Traceback (most recent call last):
[2026-01-18 23:48:58] [INFO] [0;36m(EngineCore_DP0 pid=305955)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[2026-01-18 23:48:58] [INFO] [0;36m(EngineCore_DP0 pid=305955)[0;0m     self.run()
[2026-01-18 23:48:58] [INFO] [0;36m(EngineCore_DP0 pid=305955)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/process.py", line 108, in run
[2026-01-18 23:48:58] [INFO] [0;36m(EngineCore_DP0 pid=305955)[0;0m     self._target(*self._args, **self._kwargs)
[2026-01-18 23:48:58] [INFO] [0;36m(EngineCore_DP0 pid=305955)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 939, in run_engine_core
[2026-01-18 23:48:58] [INFO] [0;36m(EngineCore_DP0 pid=305955)[0;0m     raise e
[2026-01-18 23:48:58] [INFO] [0;36m(EngineCore_DP0 pid=305955)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-18 23:48:58] [INFO] [0;36m(EngineCore_DP0 pid=305955)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 23:48:58] [INFO] [0;36m(EngineCore_DP0 pid=305955)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:48:58] [INFO] [0;36m(EngineCore_DP0 pid=305955)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-18 23:48:58] [INFO] [0;36m(EngineCore_DP0 pid=305955)[0;0m     super().__init__(
[2026-01-18 23:48:58] [INFO] [0;36m(EngineCore_DP0 pid=305955)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[2026-01-18 23:48:58] [INFO] [0;36m(EngineCore_DP0 pid=305955)[0;0m     self.model_executor = executor_class(vllm_config)
[2026-01-18 23:48:58] [INFO] [0;36m(EngineCore_DP0 pid=305955)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:48:58] [INFO] [0;36m(EngineCore_DP0 pid=305955)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[2026-01-18 23:48:58] [INFO] [0;36m(EngineCore_DP0 pid=305955)[0;0m     super().__init__(vllm_config)
[2026-01-18 23:48:58] [INFO] [0;36m(EngineCore_DP0 pid=305955)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[2026-01-18 23:48:58] [INFO] [0;36m(EngineCore_DP0 pid=305955)[0;0m     self._init_executor()
[2026-01-18 23:48:58] [INFO] [0;36m(EngineCore_DP0 pid=305955)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[2026-01-18 23:48:58] [INFO] [0;36m(EngineCore_DP0 pid=305955)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
[2026-01-18 23:48:58] [INFO] [0;36m(EngineCore_DP0 pid=305955)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:48:58] [INFO] [0;36m(EngineCore_DP0 pid=305955)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 675, in wait_for_ready
[2026-01-18 23:48:58] [INFO] [0;36m(EngineCore_DP0 pid=305955)[0;0m     raise e from None
[2026-01-18 23:48:58] [INFO] [0;36m(EngineCore_DP0 pid=305955)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[2026-01-18 23:48:59] [ERROR] [0;36m(APIServer pid=305610)[0;0m Traceback (most recent call last):
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/bin/vllm", line 7, in <module>
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m     sys.exit(main())
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m              ^^^^^^
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m     args.dispatch_function(args)
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m     uvloop.run(run_server(args))
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m     return __asyncio.run(
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m            ^^^^^^^^^^^^^^
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m     return runner.run(main)
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m     return self._loop.run_until_complete(task)
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m     return await main
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m            ^^^^^^^^^^
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m     async with build_async_engine_client(
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m     return await anext(self.gen)
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 146, in build_async_engine_client
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m     return await anext(self.gen)
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 187, in build_async_engine_client_from_engine_args
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 202, in from_vllm_config
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m     return cls(
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m            ^^^^
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 129, in __init__
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m     return AsyncMPClient(*client_args)
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m     super().__init__(
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 144, in __exit__
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m     next(self.gen)
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 918, in launch_core_engines
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m     wait_for_engine_startup(
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 977, in wait_for_engine_startup
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m     raise RuntimeError(
[2026-01-18 23:48:59] [INFO] [0;36m(APIServer pid=305610)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[2026-01-18 23:49:00] [INFO] /mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/resource_tracker.py:279: UserWarning: resource_tracker: There appear to be 3 leaked shared_memory objects to clean up at shutdown
[2026-01-18 23:49:00] [INFO] warnings.warn('resource_tracker: There appear to be %d '
[2026-01-18 23:49:02] [INFO] nvitop监控已启动
[2026-01-18 23:51:55] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 23:51:55] [INFO] nvitop监控已启动
[2026-01-18 23:51:55] [INFO] nvitop监控已停止
[2026-01-18 23:52:26] [INFO] [0;36m(APIServer pid=308429)[0;0m INFO 01-18 23:52:26 [api_server.py:872] vLLM API server version 0.14.0rc2.dev117+g9e078d058
[2026-01-18 23:52:26] [INFO] [0;36m(APIServer pid=308429)[0;0m INFO 01-18 23:52:26 [utils.py:267] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-VL-235B-A22B-Thinking-AWQ/', 'host': '0.0.0.0', 'port': 8005, 'model': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-VL-235B-A22B-Thinking-AWQ/', 'trust_remote_code': True, 'max_model_len': 32000, 'quantization': 'awq', 'served_model_name': ['VLLM-MODEL'], 'tensor_parallel_size': 2, 'max_num_seqs': 64, 'async_scheduling': True}
[2026-01-18 23:52:26] [INFO] [0;36m(APIServer pid=308429)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 23:52:26] [INFO] [0;36m(APIServer pid=308429)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 23:52:26] [INFO] [0;36m(APIServer pid=308429)[0;0m Unrecognized keys in `rope_parameters` for 'rope_type'='default': {'mrope_section', 'mrope_interleaved'}
[2026-01-18 23:52:26] [INFO] [0;36m(APIServer pid=308429)[0;0m Unrecognized keys in `rope_parameters` for 'rope_type'='default': {'mrope_section', 'mrope_interleaved'}
[2026-01-18 23:52:26] [INFO] [0;36m(APIServer pid=308429)[0;0m INFO 01-18 23:52:26 [model.py:530] Resolved architecture: Qwen3VLMoeForConditionalGeneration
[2026-01-18 23:52:26] [INFO] [0;36m(APIServer pid=308429)[0;0m INFO 01-18 23:52:26 [model.py:1547] Using max model len 32000
[2026-01-18 23:52:26] [WARNING] [0;36m(APIServer pid=308429)[0;0m WARNING 01-18 23:52:26 [quark_ocp_mx.py:157] AITER is not found or QuarkOCP_MX is not supported on the current platform. QuarkOCP_MX quantization will not be available.
[2026-01-18 23:52:26] [INFO] [0;36m(APIServer pid=308429)[0;0m INFO 01-18 23:52:26 [awq_marlin.py:167] Detected that the model can run with awq_marlin, however you specified quantization=awq explicitly, so forcing awq. Use quantization=awq_marlin for faster inference
[2026-01-18 23:52:27] [INFO] [0;36m(APIServer pid=308429)[0;0m INFO 01-18 23:52:27 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[2026-01-18 23:52:27] [INFO] [0;36m(APIServer pid=308429)[0;0m INFO 01-18 23:52:27 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 23:52:27] [INFO] [0;36m(APIServer pid=308429)[0;0m INFO 01-18 23:52:27 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-18 23:53:03] [INFO] [0;36m(EngineCore_DP0 pid=308885)[0;0m INFO 01-18 23:53:03 [core.py:96] Initializing a V1 LLM engine (v0.14.0rc2.dev117+g9e078d058) with config: model='/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-VL-235B-A22B-Thinking-AWQ/', speculative_config=None, tokenizer='/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-VL-235B-A22B-Thinking-AWQ/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=32000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 128, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-18 23:53:03] [WARNING] [0;36m(EngineCore_DP0 pid=308885)[0;0m WARNING 01-18 23:53:03 [multiproc_executor.py:897] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-18 23:53:45] [INFO] INFO 01-18 23:53:45 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:39465 backend=nccl
[2026-01-18 23:53:46] [INFO] INFO 01-18 23:53:46 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:39465 backend=nccl
[2026-01-18 23:53:47] [INFO] INFO 01-18 23:53:47 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-18 23:53:48] [WARNING] WARNING 01-18 23:53:48 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 23:53:48] [WARNING] WARNING 01-18 23:53:48 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 23:53:48] [INFO] INFO 01-18 23:53:48 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[2026-01-18 23:53:48] [INFO] INFO 01-18 23:53:48 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
[2026-01-18 23:53:52] [INFO] [0;36m(Worker_TP0 pid=309500)[0;0m INFO 01-18 23:53:52 [gpu_model_runner.py:3803] Starting to load model /mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-VL-235B-A22B-Thinking-AWQ/...
[2026-01-18 23:53:53] [INFO] [0;36m(Worker_TP0 pid=309500)[0;0m INFO 01-18 23:53:53 [mm_encoder_attention.py:86] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[2026-01-18 23:53:53] [INFO] [0;36m(Worker_TP1 pid=309501)[0;0m INFO 01-18 23:53:53 [mm_encoder_attention.py:86] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[2026-01-18 23:53:57] [INFO] [0;36m(Worker_TP0 pid=309500)[0;0m INFO 01-18 23:53:57 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[2026-01-18 23:53:57] [INFO] [0;36m(Worker_TP0 pid=309500)[0;0m
[2026-01-18 23:53:57] [INFO] Loading safetensors checkpoint shards:   0% Completed | 0/42 [00:00<?, ?it/s]
[2026-01-18 23:53:58] [INFO] [0;36m(Worker_TP0 pid=309500)[0;0m
[2026-01-18 23:53:58] [INFO] Loading safetensors checkpoint shards:   2% Completed | 1/42 [00:00<00:12,  3.39it/s]
[2026-01-18 23:53:58] [INFO] [0;36m(Worker_TP0 pid=309500)[0;0m
[2026-01-18 23:53:58] [INFO] Loading safetensors checkpoint shards:   5% Completed | 2/42 [00:00<00:19,  2.01it/s]
[2026-01-18 23:53:59] [INFO] [0;36m(Worker_TP0 pid=309500)[0;0m
[2026-01-18 23:53:59] [INFO] Loading safetensors checkpoint shards:   7% Completed | 3/42 [00:01<00:21,  1.83it/s]
[2026-01-18 23:54:00] [INFO] [0;36m(Worker_TP0 pid=309500)[0;0m
[2026-01-18 23:54:00] [INFO] Loading safetensors checkpoint shards:  10% Completed | 4/42 [00:02<00:21,  1.77it/s]
[2026-01-18 23:54:00] [INFO] [0;36m(Worker_TP0 pid=309500)[0;0m
[2026-01-18 23:54:00] [INFO] Loading safetensors checkpoint shards:  12% Completed | 5/42 [00:02<00:21,  1.72it/s]
[2026-01-18 23:54:01] [INFO] [0;36m(Worker_TP0 pid=309500)[0;0m
[2026-01-18 23:54:01] [INFO] Loading safetensors checkpoint shards:  14% Completed | 6/42 [00:03<00:21,  1.70it/s]
[2026-01-18 23:54:01] [INFO] [0;36m(Worker_TP0 pid=309500)[0;0m
[2026-01-18 23:54:01] [INFO] Loading safetensors checkpoint shards:  17% Completed | 7/42 [00:03<00:20,  1.71it/s]
[2026-01-18 23:54:01] [INFO] [0;36m(Worker_TP0 pid=309500)[0;0m
[2026-01-18 23:54:01] [INFO] Loading safetensors checkpoint shards:  17% Completed | 7/42 [00:03<00:19,  1.78it/s]
[2026-01-18 23:54:01] [INFO] [0;36m(Worker_TP0 pid=309500)[0;0m
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766] WorkerProc failed to start.
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766] Traceback (most recent call last):
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 737, in worker_main
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]     worker = WorkerProc(*args, **kwargs)
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 575, in __init__
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]     self.worker.load_model()
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 274, in load_model
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3822, in load_model
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]     self.model = model_loader.load_model(
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 58, in load_model
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]     self.load_weights(model, model_config)
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 288, in load_weights
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_vl.py", line 2083, in load_weights
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]     return loader.load_weights(weights, mapper=self.hf_to_vllm_mapper)
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 335, in load_weights
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]     autoloaded_weights = set(self._load_module("", self.module, weights))
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 288, in _load_module
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]     yield from self._load_module(
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 261, in _load_module
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]     loaded_params = module_load_weights(weights)
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 749, in load_weights
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]     return loader.load_weights(weights)
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 335, in load_weights
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]     autoloaded_weights = set(self._load_module("", self.module, weights))
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 288, in _load_module
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]     yield from self._load_module(
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 261, in _load_module
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]     loaded_params = module_load_weights(weights)
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_vl_moe.py", line 194, in load_weights
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]     for name, loaded_weight in weights:
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]                                ^^^^^^^
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 170, in <genexpr>
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]     for parts, weights_data in group
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]                                ^^^^^
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 160, in <genexpr>
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]     for weight_name, weight_data in weights
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]                                     ^^^^^^^
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 332, in <genexpr>
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]     (name, weight) for name, weight in weights if not self._can_skip(name)
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]                                        ^^^^^^^
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 170, in <genexpr>
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]     for parts, weights_data in group
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]                                ^^^^^
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 160, in <genexpr>
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]     for weight_name, weight_data in weights
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]                                     ^^^^^^^
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 332, in <genexpr>
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]     (name, weight) for name, weight in weights if not self._can_skip(name)
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]                                        ^^^^^^^
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 91, in <genexpr>
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]     for name, data in weights
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]                       ^^^^^^^
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 260, in get_all_weights
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]     yield from self._get_weights_iterator(primary_weights)
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 246, in <genexpr>
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]     return ((source.prefix + name, tensor) for (name, tensor) in weights_iterator)
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]                                                                  ^^^^^^^^^^^^^^^^
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py", line 708, in safetensors_weights_iterator
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]     with safe_open(st_file, framework="pt") as f:
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:54:02] [ERROR] [0;36m(Worker_TP0 pid=309500)[0;0m ERROR 01-18 23:54:02 [multiproc_executor.py:766] safetensors_rust.SafetensorError: Error while deserializing header: header too large
[2026-01-18 23:54:02] [INFO] [0;36m(Worker_TP0 pid=309500)[0;0m INFO 01-18 23:54:02 [multiproc_executor.py:724] Parent process exited, terminating worker
[2026-01-18 23:54:02] [INFO] [0;36m(Worker_TP1 pid=309501)[0;0m INFO 01-18 23:54:02 [multiproc_executor.py:724] Parent process exited, terminating worker
[2026-01-18 23:54:03] [WARNING] [rank0]:[W118 23:54:03.605032746 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2026-01-18 23:54:04] [ERROR] [0;36m(EngineCore_DP0 pid=308885)[0;0m ERROR 01-18 23:54:04 [core.py:935] EngineCore failed to start.
[2026-01-18 23:54:04] [ERROR] [0;36m(EngineCore_DP0 pid=308885)[0;0m ERROR 01-18 23:54:04 [core.py:935] Traceback (most recent call last):
[2026-01-18 23:54:04] [ERROR] [0;36m(EngineCore_DP0 pid=308885)[0;0m ERROR 01-18 23:54:04 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-18 23:54:04] [ERROR] [0;36m(EngineCore_DP0 pid=308885)[0;0m ERROR 01-18 23:54:04 [core.py:935]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 23:54:04] [ERROR] [0;36m(EngineCore_DP0 pid=308885)[0;0m ERROR 01-18 23:54:04 [core.py:935]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:54:04] [ERROR] [0;36m(EngineCore_DP0 pid=308885)[0;0m ERROR 01-18 23:54:04 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-18 23:54:04] [ERROR] [0;36m(EngineCore_DP0 pid=308885)[0;0m ERROR 01-18 23:54:04 [core.py:935]     super().__init__(
[2026-01-18 23:54:04] [ERROR] [0;36m(EngineCore_DP0 pid=308885)[0;0m ERROR 01-18 23:54:04 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[2026-01-18 23:54:04] [ERROR] [0;36m(EngineCore_DP0 pid=308885)[0;0m ERROR 01-18 23:54:04 [core.py:935]     self.model_executor = executor_class(vllm_config)
[2026-01-18 23:54:04] [ERROR] [0;36m(EngineCore_DP0 pid=308885)[0;0m ERROR 01-18 23:54:04 [core.py:935]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:54:04] [ERROR] [0;36m(EngineCore_DP0 pid=308885)[0;0m ERROR 01-18 23:54:04 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[2026-01-18 23:54:04] [ERROR] [0;36m(EngineCore_DP0 pid=308885)[0;0m ERROR 01-18 23:54:04 [core.py:935]     super().__init__(vllm_config)
[2026-01-18 23:54:04] [ERROR] [0;36m(EngineCore_DP0 pid=308885)[0;0m ERROR 01-18 23:54:04 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[2026-01-18 23:54:04] [ERROR] [0;36m(EngineCore_DP0 pid=308885)[0;0m ERROR 01-18 23:54:04 [core.py:935]     self._init_executor()
[2026-01-18 23:54:04] [ERROR] [0;36m(EngineCore_DP0 pid=308885)[0;0m ERROR 01-18 23:54:04 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[2026-01-18 23:54:04] [ERROR] [0;36m(EngineCore_DP0 pid=308885)[0;0m ERROR 01-18 23:54:04 [core.py:935]     self.workers = WorkerProc.wait_for_ready(unready_workers)
[2026-01-18 23:54:04] [ERROR] [0;36m(EngineCore_DP0 pid=308885)[0;0m ERROR 01-18 23:54:04 [core.py:935]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:54:04] [ERROR] [0;36m(EngineCore_DP0 pid=308885)[0;0m ERROR 01-18 23:54:04 [core.py:935]   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 675, in wait_for_ready
[2026-01-18 23:54:04] [ERROR] [0;36m(EngineCore_DP0 pid=308885)[0;0m ERROR 01-18 23:54:04 [core.py:935]     raise e from None
[2026-01-18 23:54:04] [ERROR] [0;36m(EngineCore_DP0 pid=308885)[0;0m ERROR 01-18 23:54:04 [core.py:935] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[2026-01-18 23:54:04] [INFO] [0;36m(EngineCore_DP0 pid=308885)[0;0m Process EngineCore_DP0:
[2026-01-18 23:54:04] [ERROR] [0;36m(EngineCore_DP0 pid=308885)[0;0m Traceback (most recent call last):
[2026-01-18 23:54:04] [INFO] [0;36m(EngineCore_DP0 pid=308885)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[2026-01-18 23:54:04] [INFO] [0;36m(EngineCore_DP0 pid=308885)[0;0m     self.run()
[2026-01-18 23:54:04] [INFO] [0;36m(EngineCore_DP0 pid=308885)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/process.py", line 108, in run
[2026-01-18 23:54:04] [INFO] [0;36m(EngineCore_DP0 pid=308885)[0;0m     self._target(*self._args, **self._kwargs)
[2026-01-18 23:54:04] [INFO] [0;36m(EngineCore_DP0 pid=308885)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 939, in run_engine_core
[2026-01-18 23:54:04] [INFO] [0;36m(EngineCore_DP0 pid=308885)[0;0m     raise e
[2026-01-18 23:54:04] [INFO] [0;36m(EngineCore_DP0 pid=308885)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-18 23:54:04] [INFO] [0;36m(EngineCore_DP0 pid=308885)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 23:54:04] [INFO] [0;36m(EngineCore_DP0 pid=308885)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:54:04] [INFO] [0;36m(EngineCore_DP0 pid=308885)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-18 23:54:04] [INFO] [0;36m(EngineCore_DP0 pid=308885)[0;0m     super().__init__(
[2026-01-18 23:54:04] [INFO] [0;36m(EngineCore_DP0 pid=308885)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 105, in __init__
[2026-01-18 23:54:04] [INFO] [0;36m(EngineCore_DP0 pid=308885)[0;0m     self.model_executor = executor_class(vllm_config)
[2026-01-18 23:54:04] [INFO] [0;36m(EngineCore_DP0 pid=308885)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:54:04] [INFO] [0;36m(EngineCore_DP0 pid=308885)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[2026-01-18 23:54:04] [INFO] [0;36m(EngineCore_DP0 pid=308885)[0;0m     super().__init__(vllm_config)
[2026-01-18 23:54:04] [INFO] [0;36m(EngineCore_DP0 pid=308885)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[2026-01-18 23:54:04] [INFO] [0;36m(EngineCore_DP0 pid=308885)[0;0m     self._init_executor()
[2026-01-18 23:54:04] [INFO] [0;36m(EngineCore_DP0 pid=308885)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[2026-01-18 23:54:04] [INFO] [0;36m(EngineCore_DP0 pid=308885)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
[2026-01-18 23:54:04] [INFO] [0;36m(EngineCore_DP0 pid=308885)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:54:04] [INFO] [0;36m(EngineCore_DP0 pid=308885)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 675, in wait_for_ready
[2026-01-18 23:54:04] [INFO] [0;36m(EngineCore_DP0 pid=308885)[0;0m     raise e from None
[2026-01-18 23:54:04] [INFO] [0;36m(EngineCore_DP0 pid=308885)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[2026-01-18 23:54:05] [ERROR] [0;36m(APIServer pid=308429)[0;0m Traceback (most recent call last):
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/bin/vllm", line 7, in <module>
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m     sys.exit(main())
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m              ^^^^^^
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m     args.dispatch_function(args)
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m     uvloop.run(run_server(args))
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m     return __asyncio.run(
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m            ^^^^^^^^^^^^^^
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m     return runner.run(main)
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m     return self._loop.run_until_complete(task)
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m     return await main
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m            ^^^^^^^^^^
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m     async with build_async_engine_client(
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m     return await anext(self.gen)
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 146, in build_async_engine_client
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m     return await anext(self.gen)
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 187, in build_async_engine_client_from_engine_args
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 202, in from_vllm_config
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m     return cls(
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m            ^^^^
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 129, in __init__
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m     return AsyncMPClient(*client_args)
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m     super().__init__(
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/contextlib.py", line 144, in __exit__
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m     next(self.gen)
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 918, in launch_core_engines
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m     wait_for_engine_startup(
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 977, in wait_for_engine_startup
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m     raise RuntimeError(
[2026-01-18 23:54:05] [INFO] [0;36m(APIServer pid=308429)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[2026-01-18 23:54:06] [INFO] /mnt/AI-Acer4T/miniconda3/envs/vllm-tool/lib/python3.12/multiprocessing/resource_tracker.py:279: UserWarning: resource_tracker: There appear to be 3 leaked shared_memory objects to clean up at shutdown
[2026-01-18 23:54:06] [INFO] warnings.warn('resource_tracker: There appear to be %d '
[2026-01-18 23:54:10] [INFO] nvitop监控已启动
[2026-01-18 23:54:29] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 23:54:29] [INFO] nvitop监控已启动
[2026-01-18 23:54:29] [INFO] nvitop监控已停止
[2026-01-18 23:54:38] [INFO] 服务已停止
[2026-01-18 23:54:38] [INFO] nvitop监控已启动
[2026-01-18 23:54:39] [INFO] nvitop监控已启动
[2026-01-18 23:54:39] [INFO] nvitop监控已启动
[2026-01-18 23:54:45] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 23:54:45] [INFO] nvitop监控已启动
[2026-01-18 23:54:46] [INFO] nvitop监控已停止
[2026-01-18 23:55:24] [INFO] [0;36m(APIServer pid=310598)[0;0m INFO 01-18 23:55:24 [api_server.py:872] vLLM API server version 0.14.0rc2.dev126+g38bf2ffb2
[2026-01-18 23:55:24] [INFO] [0;36m(APIServer pid=310598)[0;0m INFO 01-18 23:55:24 [utils.py:267] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-VL-235B-A22B-Thinking-AWQ/', 'host': '0.0.0.0', 'port': 8005, 'model': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-VL-235B-A22B-Thinking-AWQ/', 'trust_remote_code': True, 'max_model_len': 32000, 'quantization': 'awq', 'served_model_name': ['VLLM-MODEL'], 'tensor_parallel_size': 2, 'gpu_memory_utilization': 0.95, 'max_num_seqs': 64, 'async_scheduling': True}
[2026-01-18 23:55:24] [INFO] [0;36m(APIServer pid=310598)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 23:55:24] [INFO] [0;36m(APIServer pid=310598)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-18 23:55:24] [INFO] [0;36m(APIServer pid=310598)[0;0m INFO 01-18 23:55:24 [model.py:530] Resolved architecture: Qwen3VLMoeForConditionalGeneration
[2026-01-18 23:55:24] [INFO] [0;36m(APIServer pid=310598)[0;0m INFO 01-18 23:55:24 [model.py:1547] Using max model len 32000
[2026-01-18 23:55:24] [WARNING] [0;36m(APIServer pid=310598)[0;0m WARNING 01-18 23:55:24 [quark_ocp_mx.py:157] AITER is not found or QuarkOCP_MX is not supported on the current platform. QuarkOCP_MX quantization will not be available.
[2026-01-18 23:55:25] [INFO] [0;36m(APIServer pid=310598)[0;0m INFO 01-18 23:55:25 [awq_marlin.py:167] Detected that the model can run with awq_marlin, however you specified quantization=awq explicitly, so forcing awq. Use quantization=awq_marlin for faster inference
[2026-01-18 23:55:26] [INFO] [0;36m(APIServer pid=310598)[0;0m INFO 01-18 23:55:26 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[2026-01-18 23:55:26] [INFO] [0;36m(APIServer pid=310598)[0;0m INFO 01-18 23:55:26 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-18 23:55:26] [INFO] [0;36m(APIServer pid=310598)[0;0m INFO 01-18 23:55:26 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-18 23:56:03] [INFO] [0;36m(EngineCore_DP0 pid=311170)[0;0m INFO 01-18 23:56:03 [core.py:96] Initializing a V1 LLM engine (v0.14.0rc2.dev126+g38bf2ffb2) with config: model='/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-VL-235B-A22B-Thinking-AWQ/', speculative_config=None, tokenizer='/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-VL-235B-A22B-Thinking-AWQ/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=32000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 128, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-18 23:56:03] [WARNING] [0;36m(EngineCore_DP0 pid=311170)[0;0m WARNING 01-18 23:56:03 [multiproc_executor.py:897] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-18 23:56:31] [INFO] INFO 01-18 23:56:31 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:39247 backend=nccl
[2026-01-18 23:56:32] [INFO] INFO 01-18 23:56:32 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:39247 backend=nccl
[2026-01-18 23:56:33] [INFO] INFO 01-18 23:56:33 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-18 23:56:33] [WARNING] WARNING 01-18 23:56:33 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 23:56:33] [WARNING] WARNING 01-18 23:56:33 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-18 23:56:33] [INFO] INFO 01-18 23:56:33 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
[2026-01-18 23:56:33] [INFO] INFO 01-18 23:56:33 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[2026-01-18 23:56:37] [INFO] [0;36m(Worker_TP0 pid=311691)[0;0m INFO 01-18 23:56:37 [gpu_model_runner.py:3803] Starting to load model /mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-VL-235B-A22B-Thinking-AWQ/...
[2026-01-18 23:56:38] [INFO] [0;36m(Worker_TP1 pid=311692)[0;0m INFO 01-18 23:56:38 [mm_encoder_attention.py:86] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[2026-01-18 23:56:38] [INFO] [0;36m(Worker_TP0 pid=311691)[0;0m INFO 01-18 23:56:38 [mm_encoder_attention.py:86] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[2026-01-18 23:56:42] [INFO] [0;36m(Worker_TP0 pid=311691)[0;0m INFO 01-18 23:56:42 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[2026-01-18 23:56:43] [INFO] [0;36m(Worker_TP0 pid=311691)[0;0m
[2026-01-18 23:56:43] [INFO] Loading safetensors checkpoint shards:   0% Completed | 0/42 [00:00<?, ?it/s]
[2026-01-18 23:56:43] [INFO] [0;36m(Worker_TP0 pid=311691)[0;0m
[2026-01-18 23:56:43] [INFO] Loading safetensors checkpoint shards:   2% Completed | 1/42 [00:00<00:11,  3.54it/s]
[2026-01-18 23:56:43] [INFO] [0;36m(Worker_TP0 pid=311691)[0;0m
[2026-01-18 23:56:43] [INFO] Loading safetensors checkpoint shards:   5% Completed | 2/42 [00:00<00:18,  2.13it/s]
[2026-01-18 23:56:44] [INFO] [0;36m(Worker_TP0 pid=311691)[0;0m
[2026-01-18 23:56:44] [INFO] Loading safetensors checkpoint shards:   7% Completed | 3/42 [00:01<00:20,  1.90it/s]
[2026-01-18 23:56:45] [INFO] [0;36m(Worker_TP0 pid=311691)[0;0m
[2026-01-18 23:56:45] [INFO] Loading safetensors checkpoint shards:  10% Completed | 4/42 [00:02<00:20,  1.85it/s]
[2026-01-18 23:56:45] [INFO] [0;36m(Worker_TP0 pid=311691)[0;0m
[2026-01-18 23:56:45] [INFO] Loading safetensors checkpoint shards:  12% Completed | 5/42 [00:02<00:20,  1.77it/s]
[2026-01-18 23:56:46] [INFO] [0;36m(Worker_TP0 pid=311691)[0;0m
[2026-01-18 23:56:46] [INFO] Loading safetensors checkpoint shards:  14% Completed | 6/42 [00:03<00:20,  1.73it/s]
[2026-01-18 23:56:46] [INFO] [0;36m(Worker_TP0 pid=311691)[0;0m
[2026-01-18 23:56:46] [INFO] Loading safetensors checkpoint shards:  17% Completed | 7/42 [00:03<00:20,  1.72it/s]
[2026-01-18 23:56:46] [INFO] [0;36m(Worker_TP0 pid=311691)[0;0m
[2026-01-18 23:56:46] [INFO] Loading safetensors checkpoint shards:  17% Completed | 7/42 [00:03<00:19,  1.83it/s]
[2026-01-18 23:56:46] [INFO] [0;36m(Worker_TP0 pid=311691)[0;0m
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766] WorkerProc failed to start.
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766] Traceback (most recent call last):
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 737, in worker_main
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     worker = WorkerProc(*args, **kwargs)
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 575, in __init__
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     self.worker.load_model()
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/worker/gpu_worker.py", line 274, in load_model
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/worker/gpu_model_runner.py", line 3822, in load_model
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     self.model = model_loader.load_model(
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/model_loader/base_loader.py", line 58, in load_model
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     self.load_weights(model, model_config)
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/model_loader/default_loader.py", line 288, in load_weights
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/qwen3_vl.py", line 2083, in load_weights
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     return loader.load_weights(weights, mapper=self.hf_to_vllm_mapper)
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/utils.py", line 335, in load_weights
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     autoloaded_weights = set(self._load_module("", self.module, weights))
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/utils.py", line 288, in _load_module
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     yield from self._load_module(
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/utils.py", line 261, in _load_module
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     loaded_params = module_load_weights(weights)
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/qwen3_moe.py", line 749, in load_weights
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     return loader.load_weights(weights)
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/utils.py", line 335, in load_weights
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     autoloaded_weights = set(self._load_module("", self.module, weights))
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/utils.py", line 288, in _load_module
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     yield from self._load_module(
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/utils.py", line 261, in _load_module
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     loaded_params = module_load_weights(weights)
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/qwen3_vl_moe.py", line 194, in load_weights
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     for name, loaded_weight in weights:
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]                                ^^^^^^^
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/utils.py", line 170, in <genexpr>
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     for parts, weights_data in group
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]                                ^^^^^
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/utils.py", line 160, in <genexpr>
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     for weight_name, weight_data in weights
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]                                     ^^^^^^^
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/utils.py", line 332, in <genexpr>
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     (name, weight) for name, weight in weights if not self._can_skip(name)
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]                                        ^^^^^^^
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/utils.py", line 170, in <genexpr>
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     for parts, weights_data in group
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]                                ^^^^^
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/utils.py", line 160, in <genexpr>
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     for weight_name, weight_data in weights
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]                                     ^^^^^^^
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/utils.py", line 332, in <genexpr>
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     (name, weight) for name, weight in weights if not self._can_skip(name)
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]                                        ^^^^^^^
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/utils.py", line 91, in <genexpr>
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     for name, data in weights
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]                       ^^^^^^^
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/model_loader/default_loader.py", line 260, in get_all_weights
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     yield from self._get_weights_iterator(primary_weights)
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/model_loader/default_loader.py", line 246, in <genexpr>
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     return ((source.prefix + name, tensor) for (name, tensor) in weights_iterator)
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]                                                                  ^^^^^^^^^^^^^^^^
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/model_loader/weight_utils.py", line 708, in safetensors_weights_iterator
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     with safe_open(st_file, framework="pt") as f:
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP0 pid=311691)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766] safetensors_rust.SafetensorError: Error while deserializing header: header too large
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766] WorkerProc failed to start.
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766] Traceback (most recent call last):
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 737, in worker_main
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     worker = WorkerProc(*args, **kwargs)
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 575, in __init__
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     self.worker.load_model()
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/worker/gpu_worker.py", line 274, in load_model
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/worker/gpu_model_runner.py", line 3822, in load_model
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     self.model = model_loader.load_model(
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/model_loader/base_loader.py", line 58, in load_model
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     self.load_weights(model, model_config)
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/model_loader/default_loader.py", line 288, in load_weights
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/qwen3_vl.py", line 2083, in load_weights
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     return loader.load_weights(weights, mapper=self.hf_to_vllm_mapper)
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/utils.py", line 335, in load_weights
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     autoloaded_weights = set(self._load_module("", self.module, weights))
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/utils.py", line 288, in _load_module
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     yield from self._load_module(
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/utils.py", line 261, in _load_module
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     loaded_params = module_load_weights(weights)
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/qwen3_moe.py", line 749, in load_weights
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     return loader.load_weights(weights)
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/utils.py", line 335, in load_weights
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     autoloaded_weights = set(self._load_module("", self.module, weights))
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/utils.py", line 288, in _load_module
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     yield from self._load_module(
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/utils.py", line 261, in _load_module
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     loaded_params = module_load_weights(weights)
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/qwen3_vl_moe.py", line 194, in load_weights
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     for name, loaded_weight in weights:
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]                                ^^^^^^^
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/utils.py", line 170, in <genexpr>
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     for parts, weights_data in group
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]                                ^^^^^
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/utils.py", line 160, in <genexpr>
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     for weight_name, weight_data in weights
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]                                     ^^^^^^^
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/utils.py", line 332, in <genexpr>
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     (name, weight) for name, weight in weights if not self._can_skip(name)
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]                                        ^^^^^^^
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/utils.py", line 170, in <genexpr>
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     for parts, weights_data in group
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]                                ^^^^^
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/utils.py", line 160, in <genexpr>
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     for weight_name, weight_data in weights
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]                                     ^^^^^^^
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/utils.py", line 332, in <genexpr>
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     (name, weight) for name, weight in weights if not self._can_skip(name)
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]                                        ^^^^^^^
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/models/utils.py", line 91, in <genexpr>
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     for name, data in weights
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]                       ^^^^^^^
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/model_loader/default_loader.py", line 260, in get_all_weights
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     yield from self._get_weights_iterator(primary_weights)
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/model_loader/default_loader.py", line 246, in <genexpr>
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     return ((source.prefix + name, tensor) for (name, tensor) in weights_iterator)
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]                                                                  ^^^^^^^^^^^^^^^^
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/model_executor/model_loader/weight_utils.py", line 708, in safetensors_weights_iterator
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]     with safe_open(st_file, framework="pt") as f:
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:56:47] [ERROR] [0;36m(Worker_TP1 pid=311692)[0;0m ERROR 01-18 23:56:47 [multiproc_executor.py:766] safetensors_rust.SafetensorError: Error while deserializing header: header too large
[2026-01-18 23:56:47] [INFO] [0;36m(Worker_TP1 pid=311692)[0;0m INFO 01-18 23:56:47 [multiproc_executor.py:724] Parent process exited, terminating worker
[2026-01-18 23:56:47] [INFO] [0;36m(Worker_TP0 pid=311691)[0;0m INFO 01-18 23:56:47 [multiproc_executor.py:724] Parent process exited, terminating worker
[2026-01-18 23:56:48] [WARNING] [rank0]:[W118 23:56:48.592084512 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2026-01-18 23:56:49] [ERROR] [0;36m(EngineCore_DP0 pid=311170)[0;0m ERROR 01-18 23:56:49 [core.py:935] EngineCore failed to start.
[2026-01-18 23:56:49] [ERROR] [0;36m(EngineCore_DP0 pid=311170)[0;0m ERROR 01-18 23:56:49 [core.py:935] Traceback (most recent call last):
[2026-01-18 23:56:49] [ERROR] [0;36m(EngineCore_DP0 pid=311170)[0;0m ERROR 01-18 23:56:49 [core.py:935]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-18 23:56:49] [ERROR] [0;36m(EngineCore_DP0 pid=311170)[0;0m ERROR 01-18 23:56:49 [core.py:935]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 23:56:49] [ERROR] [0;36m(EngineCore_DP0 pid=311170)[0;0m ERROR 01-18 23:56:49 [core.py:935]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:56:49] [ERROR] [0;36m(EngineCore_DP0 pid=311170)[0;0m ERROR 01-18 23:56:49 [core.py:935]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-18 23:56:49] [ERROR] [0;36m(EngineCore_DP0 pid=311170)[0;0m ERROR 01-18 23:56:49 [core.py:935]     super().__init__(
[2026-01-18 23:56:49] [ERROR] [0;36m(EngineCore_DP0 pid=311170)[0;0m ERROR 01-18 23:56:49 [core.py:935]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core.py", line 105, in __init__
[2026-01-18 23:56:49] [ERROR] [0;36m(EngineCore_DP0 pid=311170)[0;0m ERROR 01-18 23:56:49 [core.py:935]     self.model_executor = executor_class(vllm_config)
[2026-01-18 23:56:49] [ERROR] [0;36m(EngineCore_DP0 pid=311170)[0;0m ERROR 01-18 23:56:49 [core.py:935]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:56:49] [ERROR] [0;36m(EngineCore_DP0 pid=311170)[0;0m ERROR 01-18 23:56:49 [core.py:935]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[2026-01-18 23:56:49] [ERROR] [0;36m(EngineCore_DP0 pid=311170)[0;0m ERROR 01-18 23:56:49 [core.py:935]     super().__init__(vllm_config)
[2026-01-18 23:56:49] [ERROR] [0;36m(EngineCore_DP0 pid=311170)[0;0m ERROR 01-18 23:56:49 [core.py:935]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/abstract.py", line 101, in __init__
[2026-01-18 23:56:49] [ERROR] [0;36m(EngineCore_DP0 pid=311170)[0;0m ERROR 01-18 23:56:49 [core.py:935]     self._init_executor()
[2026-01-18 23:56:49] [ERROR] [0;36m(EngineCore_DP0 pid=311170)[0;0m ERROR 01-18 23:56:49 [core.py:935]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[2026-01-18 23:56:49] [ERROR] [0;36m(EngineCore_DP0 pid=311170)[0;0m ERROR 01-18 23:56:49 [core.py:935]     self.workers = WorkerProc.wait_for_ready(unready_workers)
[2026-01-18 23:56:49] [ERROR] [0;36m(EngineCore_DP0 pid=311170)[0;0m ERROR 01-18 23:56:49 [core.py:935]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:56:49] [ERROR] [0;36m(EngineCore_DP0 pid=311170)[0;0m ERROR 01-18 23:56:49 [core.py:935]   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 675, in wait_for_ready
[2026-01-18 23:56:49] [ERROR] [0;36m(EngineCore_DP0 pid=311170)[0;0m ERROR 01-18 23:56:49 [core.py:935]     raise e from None
[2026-01-18 23:56:49] [ERROR] [0;36m(EngineCore_DP0 pid=311170)[0;0m ERROR 01-18 23:56:49 [core.py:935] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[2026-01-18 23:56:49] [INFO] [0;36m(EngineCore_DP0 pid=311170)[0;0m Process EngineCore_DP0:
[2026-01-18 23:56:49] [ERROR] [0;36m(EngineCore_DP0 pid=311170)[0;0m Traceback (most recent call last):
[2026-01-18 23:56:49] [INFO] [0;36m(EngineCore_DP0 pid=311170)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-git/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[2026-01-18 23:56:49] [INFO] [0;36m(EngineCore_DP0 pid=311170)[0;0m     self.run()
[2026-01-18 23:56:49] [INFO] [0;36m(EngineCore_DP0 pid=311170)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-git/lib/python3.12/multiprocessing/process.py", line 108, in run
[2026-01-18 23:56:49] [INFO] [0;36m(EngineCore_DP0 pid=311170)[0;0m     self._target(*self._args, **self._kwargs)
[2026-01-18 23:56:49] [INFO] [0;36m(EngineCore_DP0 pid=311170)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core.py", line 939, in run_engine_core
[2026-01-18 23:56:49] [INFO] [0;36m(EngineCore_DP0 pid=311170)[0;0m     raise e
[2026-01-18 23:56:49] [INFO] [0;36m(EngineCore_DP0 pid=311170)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-18 23:56:49] [INFO] [0;36m(EngineCore_DP0 pid=311170)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-18 23:56:49] [INFO] [0;36m(EngineCore_DP0 pid=311170)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:56:49] [INFO] [0;36m(EngineCore_DP0 pid=311170)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-18 23:56:49] [INFO] [0;36m(EngineCore_DP0 pid=311170)[0;0m     super().__init__(
[2026-01-18 23:56:49] [INFO] [0;36m(EngineCore_DP0 pid=311170)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core.py", line 105, in __init__
[2026-01-18 23:56:49] [INFO] [0;36m(EngineCore_DP0 pid=311170)[0;0m     self.model_executor = executor_class(vllm_config)
[2026-01-18 23:56:49] [INFO] [0;36m(EngineCore_DP0 pid=311170)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:56:49] [INFO] [0;36m(EngineCore_DP0 pid=311170)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[2026-01-18 23:56:49] [INFO] [0;36m(EngineCore_DP0 pid=311170)[0;0m     super().__init__(vllm_config)
[2026-01-18 23:56:49] [INFO] [0;36m(EngineCore_DP0 pid=311170)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/abstract.py", line 101, in __init__
[2026-01-18 23:56:49] [INFO] [0;36m(EngineCore_DP0 pid=311170)[0;0m     self._init_executor()
[2026-01-18 23:56:49] [INFO] [0;36m(EngineCore_DP0 pid=311170)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 165, in _init_executor
[2026-01-18 23:56:49] [INFO] [0;36m(EngineCore_DP0 pid=311170)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
[2026-01-18 23:56:49] [INFO] [0;36m(EngineCore_DP0 pid=311170)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:56:49] [INFO] [0;36m(EngineCore_DP0 pid=311170)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/executor/multiproc_executor.py", line 675, in wait_for_ready
[2026-01-18 23:56:49] [INFO] [0;36m(EngineCore_DP0 pid=311170)[0;0m     raise e from None
[2026-01-18 23:56:49] [INFO] [0;36m(EngineCore_DP0 pid=311170)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[2026-01-18 23:56:50] [ERROR] [0;36m(APIServer pid=310598)[0;0m Traceback (most recent call last):
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-git/bin/vllm", line 7, in <module>
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m     sys.exit(main())
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m              ^^^^^^
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m     args.dispatch_function(args)
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m     uvloop.run(run_server(args))
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-git/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m     return __asyncio.run(
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m            ^^^^^^^^^^^^^^
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-git/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m     return runner.run(main)
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-git/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m     return self._loop.run_until_complete(task)
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-git/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m     return await main
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m            ^^^^^^^^^^
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m     async with build_async_engine_client(
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-git/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m     return await anext(self.gen)
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 146, in build_async_engine_client
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-git/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m     return await anext(self.gen)
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/entrypoints/openai/api_server.py", line 187, in build_async_engine_client_from_engine_args
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/async_llm.py", line 202, in from_vllm_config
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m     return cls(
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m            ^^^^
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/async_llm.py", line 129, in __init__
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m     return AsyncMPClient(*client_args)
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core_client.py", line 819, in __init__
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m     super().__init__(
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/core_client.py", line 479, in __init__
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m   File "/mnt/AI-Acer4T/miniconda3/envs/vllm-git/lib/python3.12/contextlib.py", line 144, in __exit__
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m     next(self.gen)
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/utils.py", line 918, in launch_core_engines
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m     wait_for_engine_startup(
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m   File "/mnt/AI-Acer4T/AI-Chat/vllm/vllm/v1/engine/utils.py", line 977, in wait_for_engine_startup
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m     raise RuntimeError(
[2026-01-18 23:56:50] [INFO] [0;36m(APIServer pid=310598)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[2026-01-18 23:56:51] [INFO] /mnt/AI-Acer4T/miniconda3/envs/vllm-git/lib/python3.12/multiprocessing/resource_tracker.py:279: UserWarning: resource_tracker: There appear to be 3 leaked shared_memory objects to clean up at shutdown
[2026-01-18 23:56:51] [INFO] warnings.warn('resource_tracker: There appear to be %d '
[2026-01-18 23:56:56] [INFO] nvitop监控已启动
[2026-01-18 23:58:07] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-18 23:58:07] [INFO] nvitop监控已启动
[2026-01-18 23:58:43] [INFO] nvitop监控已启动
[2026-01-18 23:58:43] [INFO] 服务已停止
[2026-01-18 23:58:43] [INFO] nvitop监控已启动
[2026-01-18 23:58:44] [INFO] nvitop监控已启动
[2026-01-18 23:59:41] [SUCCESS] 正在关闭服务器...
[2026-01-19 00:36:59] [INFO] VLLM GUI 服务器启动，端口: 5003
[2026-01-19 00:37:00] [INFO] nvitop监控已启动
[2026-01-19 00:37:01] [INFO] nvitop监控已启动
[2026-01-19 00:37:02] [INFO] 从conda info --base自动检测到conda路径: /mnt/AI-Acer4T/miniconda3
[2026-01-19 00:38:33] [SUCCESS] 方案已保存: Qwen3-VL-32B-Instruct-TP2
[2026-01-19 00:39:57] [SUCCESS] 方案已保存: Qwen3-VL-30B-A3B-Instruct
[2026-01-19 00:43:53] [SUCCESS] 正在关闭服务器...
[2026-01-19 19:27:01] [INFO] VLLM GUI 服务器启动，端口: 5003
[2026-01-19 19:27:47] [INFO] VLLM GUI 服务器启动，端口: 5001
[2026-01-19 19:31:59] [INFO] VLLM GUI 服务器启动，端口: 5002
[2026-01-19 19:32:01] [WARNING] 未找到conda安装，使用默认路径: C:\Users\wuwen/miniconda3
[2026-01-19 19:32:01] [INFO] nvitop监控已启动
[2026-01-19 19:32:29] [INFO] 启动命令: wsl bash -c "source /etc/profile 2>/dev/null || true && source ~/.bashrc 2>/dev/null || true && sour...
[2026-01-19 19:32:29] [INFO] nvitop监控已启动
[2026-01-19 19:32:29] [INFO] nvitop监控已停止
[2026-01-19 19:32:32] [INFO] bash: line 1: conda: command not found
[2026-01-19 19:32:32] [INFO] nvitop监控已启动
[2026-01-19 19:32:41] [INFO] 启动命令: wsl bash -c "source /etc/profile 2>/dev/null || true && source ~/.bashrc 2>/dev/null || true && sour...
[2026-01-19 19:32:41] [INFO] nvitop监控已启动
[2026-01-19 19:32:41] [INFO] bash: line 1: conda: command not found
[2026-01-19 19:32:41] [INFO] nvitop监控已启动
[2026-01-19 19:32:41] [INFO] nvitop监控已停止
[2026-01-19 19:34:18] [INFO] 启动命令: wsl bash -c "source /etc/profile 2>/dev/null || true && source ~/.bashrc 2>/dev/null || true && sour...
[2026-01-19 19:34:18] [INFO] nvitop监控已启动
[2026-01-19 19:34:19] [INFO] nvitop监控已停止
[2026-01-19 19:34:26] [INFO] bash: line 1: conda: command not found
[2026-01-19 19:34:26] [INFO] nvitop监控已启动
[2026-01-19 19:47:41] [WARNING] 未找到conda安装，使用默认路径: C:\Users\wuwen/miniconda3
[2026-01-19 19:47:41] [WARNING] 未找到conda安装，使用默认路径: C:\Users\wuwen/miniconda3
[2026-01-19 19:47:41] [WARNING] 未找到conda安装，使用默认路径: C:\Users\wuwen/miniconda3
[2026-01-19 19:47:41] [WARNING] 未找到conda安装，使用默认路径: C:\Users\wuwen/miniconda3
[2026-01-19 19:47:41] [WARNING] 未找到conda安装，使用默认路径: C:\Users\wuwen/miniconda3
[2026-01-19 19:47:41] [WARNING] 配置验证警告（仍将生成命令）: 模型路径不能为空
[2026-01-19 19:47:41] [WARNING] 未找到conda安装，使用默认路径: C:\Users\wuwen/miniconda3
[2026-01-19 19:47:41] [INFO] 启动命令: wsl bash -c "conda activate vllm && vllm serve /mnt/i/model"...
[2026-01-19 19:47:41] [INFO] nvitop监控已启动
[2026-01-19 19:47:41] [INFO] 启动命令: /bin/bash -c "conda activate vllm && vllm serve /home/user/model"...
[2026-01-19 19:47:41] [INFO] nvitop监控已启动
[2026-01-19 19:47:41] [INFO] 启动命令: wsl bash -c "conda activate vllm && vllm serve /mnt/i/model"...
[2026-01-19 19:47:41] [ERROR] 子进程执行失败: Subprocess error
[2026-01-19 19:47:41] [INFO] 服务已停止
[2026-01-19 19:47:41] [INFO] 服务已停止
[2026-01-19 19:47:42] [INFO] nvitop监控已启动
[2026-01-19 19:47:42] [INFO] nvitop监控已启动
[2026-01-19 19:52:18] [INFO] VLLM GUI 服务器启动，端口: 5002
[2026-01-19 19:52:19] [WARNING] 未找到conda安装，使用默认路径: C:\Users\wuwen/miniconda3
[2026-01-19 19:52:19] [INFO] nvitop监控已启动
[2026-01-19 19:52:26] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-19 19:52:31] [INFO] 启动命令: wsl bash -c "source /etc/profile 2>/dev/null || true && source ~/.bashrc 2>/dev/null || true && sour...
[2026-01-19 19:52:31] [INFO] nvitop监控已启动
[2026-01-19 19:52:32] [INFO] bash: line 1: conda: command not found
[2026-01-19 19:52:32] [INFO] nvitop监控已启动
[2026-01-19 19:52:32] [INFO] nvitop监控已停止
[2026-01-19 19:53:06] [SUCCESS] 正在关闭服务器...
[2026-01-19 19:54:29] [WARNING] 未在WSL中找到conda安装，使用默认路径: /home/wuwen-ubu/miniconda3
[2026-01-19 19:54:30] [WARNING] 未在WSL中找到conda安装，使用默认路径: /home/wuwen-ubu/miniconda3
[2026-01-19 19:54:31] [WARNING] 未在WSL中找到conda安装，使用默认路径: /home/wuwen-ubu/miniconda3
[2026-01-19 19:54:31] [WARNING] 未在WSL中找到conda安装，使用默认路径: /home/wuwen-ubu/miniconda3
[2026-01-19 19:54:32] [WARNING] 未在WSL中找到conda安装，使用默认路径: /home/wuwen-ubu/miniconda3
[2026-01-19 19:54:32] [WARNING] 配置验证警告（仍将生成命令）: 模型路径不能为空
[2026-01-19 19:54:33] [WARNING] 未在WSL中找到conda安装，使用默认路径: /home/wuwen-ubu/miniconda3
[2026-01-19 19:54:33] [INFO] 启动命令: wsl bash -c "conda activate vllm && vllm serve /mnt/i/model"...
[2026-01-19 19:54:33] [INFO] nvitop监控已启动
[2026-01-19 19:54:33] [INFO] 启动命令: /bin/bash -c "conda activate vllm && vllm serve /home/user/model"...
[2026-01-19 19:54:33] [INFO] nvitop监控已启动
[2026-01-19 19:54:33] [INFO] 启动命令: wsl bash -c "conda activate vllm && vllm serve /mnt/i/model"...
[2026-01-19 19:54:33] [ERROR] 子进程执行失败: Subprocess error
[2026-01-19 19:54:33] [INFO] 服务已停止
[2026-01-19 19:54:33] [INFO] 服务已停止
[2026-01-19 19:54:34] [INFO] nvitop监控已启动
[2026-01-19 19:54:34] [INFO] nvitop监控已启动
[2026-01-19 19:56:07] [INFO] VLLM GUI 服务器启动，端口: 5002
[2026-01-19 19:56:08] [INFO] nvitop监控已启动
[2026-01-19 19:56:14] [INFO] 启动命令: wsl bash -c "source /etc/profile 2>/dev/null || true && source ~/.bashrc 2>/dev/null || true && sour...
[2026-01-19 19:56:15] [INFO] nvitop监控已停止
[2026-01-19 19:56:35] [WARNING] 未在WSL中找到conda安装，使用默认路径: /root/miniconda3
[2026-01-19 20:01:24] [INFO] 服务已停止
[2026-01-19 20:01:25] [INFO] nvitop监控已启动
[2026-01-19 20:01:25] [INFO] nvitop监控已启动
[2026-01-19 20:01:25] [INFO] nvitop监控已启动
[2026-01-19 20:05:28] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:05:29] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:05:29] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:05:29] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:05:30] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:05:30] [WARNING] 配置验证警告（仍将生成命令）: 模型路径不能为空
[2026-01-19 20:05:30] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:05:30] [INFO] 启动命令: wsl bash -c "conda activate vllm && vllm serve /mnt/i/model"...
[2026-01-19 20:05:30] [INFO] nvitop监控已启动
[2026-01-19 20:05:30] [INFO] 启动命令: /bin/bash -c "conda activate vllm && vllm serve /home/user/model"...
[2026-01-19 20:05:30] [INFO] nvitop监控已启动
[2026-01-19 20:05:30] [INFO] 启动命令: wsl bash -c "conda activate vllm && vllm serve /mnt/i/model"...
[2026-01-19 20:05:30] [ERROR] 子进程执行失败: Subprocess error
[2026-01-19 20:05:30] [INFO] 服务已停止
[2026-01-19 20:05:30] [INFO] 服务已停止
[2026-01-19 20:05:31] [INFO] nvitop监控已启动
[2026-01-19 20:05:31] [INFO] nvitop监控已启动
[2026-01-19 20:07:14] [INFO] VLLM GUI 服务器启动，端口: 5002
[2026-01-19 20:07:14] [INFO] nvitop监控已启动
[2026-01-19 20:07:15] [INFO] nvitop监控已启动
[2026-01-19 20:07:15] [WARNING] 未在WSL中找到conda安装，使用默认路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:07:22] [INFO] 启动命令: /bin/bash -c "source /mnt/AI-Acer4T/miniconda3/etc/profile.d/conda.sh 2>/dev/null || true && conda a...
[2026-01-19 20:07:26] [INFO] 启动命令: wsl bash -c "source /etc/profile 2>/dev/null || true && source ~/.bashrc 2>/dev/null || true && sour...
[2026-01-19 20:07:26] [INFO] nvitop监控已启动
[2026-01-19 20:07:26] [INFO] bash: line 1: conda: command not found
[2026-01-19 20:07:26] [INFO] nvitop监控已启动
[2026-01-19 20:09:26] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:09:26] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:09:27] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:09:27] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:09:27] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:09:27] [WARNING] 配置验证警告（仍将生成命令）: 模型路径不能为空
[2026-01-19 20:09:28] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:09:39] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:15:37] [INFO] VLLM GUI 服务器启动，端口: 5002
[2026-01-19 20:15:38] [INFO] nvitop监控已启动
[2026-01-19 20:15:38] [WARNING] 未在WSL中找到conda安装，使用默认路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:15:38] [INFO] nvitop监控已启动
[2026-01-19 20:15:53] [INFO] 启动命令: wsl bash -c "source /etc/profile 2>/dev/null || true && source ~/.bashrc 2>/dev/null || true && sour...
[2026-01-19 20:15:53] [INFO] nvitop监控已启动
[2026-01-19 20:15:53] [INFO] bash: line 1: conda: command not found
[2026-01-19 20:15:53] [INFO] nvitop监控已启动
[2026-01-19 20:20:38] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:20:54] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:20:54] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:20:55] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:20:55] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:20:55] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:20:55] [WARNING] 配置验证警告（仍将生成命令）: 模型路径不能为空
[2026-01-19 20:20:56] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:21:30] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:21:31] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:21:31] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:21:31] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:21:32] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:21:32] [WARNING] 配置验证警告（仍将生成命令）: 模型路径不能为空
[2026-01-19 20:21:32] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:22:02] [INFO] VLLM GUI 服务器启动，端口: 5002
[2026-01-19 20:22:03] [INFO] nvitop监控已启动
[2026-01-19 20:22:06] [WARNING] 未在WSL中找到conda安装，使用默认路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:22:11] [INFO] 启动命令: wsl bash -c "source /etc/profile 2>/dev/null || true && source ~/.bashrc 2>/dev/null || true && sour...
[2026-01-19 20:22:11] [INFO] nvitop监控已启动
[2026-01-19 20:22:11] [INFO] bash: line 1: conda: command not found
[2026-01-19 20:22:11] [INFO] nvitop监控已启动
[2026-01-19 20:25:17] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:25:17] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:25:18] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:25:18] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:25:18] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:25:18] [WARNING] 配置验证警告（仍将生成命令）: 模型路径不能为空
[2026-01-19 20:25:19] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:25:19] [INFO] 启动命令: wsl bash -c "conda activate vllm && vllm serve /mnt/i/model"...
[2026-01-19 20:25:19] [INFO] nvitop监控已启动
[2026-01-19 20:25:19] [INFO] 启动命令: /bin/bash -c "conda activate vllm && vllm serve /home/user/model"...
[2026-01-19 20:25:19] [INFO] nvitop监控已启动
[2026-01-19 20:25:19] [INFO] 启动命令: wsl bash -c "conda activate vllm && vllm serve /mnt/i/model"...
[2026-01-19 20:25:19] [ERROR] 子进程执行失败: Subprocess error
[2026-01-19 20:25:19] [INFO] 服务已停止
[2026-01-19 20:25:19] [INFO] 服务已停止
[2026-01-19 20:25:20] [INFO] nvitop监控已启动
[2026-01-19 20:25:20] [INFO] nvitop监控已启动
[2026-01-19 20:26:06] [INFO] VLLM GUI 服务器启动，端口: 5002
[2026-01-19 20:26:07] [INFO] nvitop监控已启动
[2026-01-19 20:26:10] [WARNING] 未在WSL中找到conda安装，使用默认路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:26:17] [INFO] 启动命令: wsl bash -c "source /etc/profile 2>/dev/null || true && source ~/.bashrc 2>/dev/null || true && sour...
[2026-01-19 20:26:17] [INFO] nvitop监控已启动
[2026-01-19 20:26:17] [INFO] bash: line 1: /mnt/AI-Acer4T/miniconda3/bin/activate: No such file or directory
[2026-01-19 20:26:17] [INFO] nvitop监控已启动
[2026-01-19 20:29:11] [INFO] 从WSL HOME目录自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:29:11] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:29:11] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:29:12] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:29:12] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:29:12] [WARNING] 配置验证警告（仍将生成命令）: 模型路径不能为空
[2026-01-19 20:29:12] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:29:12] [INFO] 启动命令: wsl bash -c "conda activate vllm && vllm serve /mnt/i/model"...
[2026-01-19 20:29:12] [INFO] nvitop监控已启动
[2026-01-19 20:29:12] [INFO] 启动命令: /bin/bash -c "conda activate vllm && vllm serve /home/user/model"...
[2026-01-19 20:29:12] [INFO] nvitop监控已启动
[2026-01-19 20:29:12] [INFO] 启动命令: wsl bash -c "conda activate vllm && vllm serve /mnt/i/model"...
[2026-01-19 20:29:12] [ERROR] 子进程执行失败: Subprocess error
[2026-01-19 20:29:12] [INFO] 服务已停止
[2026-01-19 20:29:12] [INFO] 服务已停止
[2026-01-19 20:29:14] [INFO] nvitop监控已启动
[2026-01-19 20:29:14] [INFO] nvitop监控已启动
[2026-01-19 20:33:22] [INFO] VLLM GUI 服务器启动，端口: 5002
[2026-01-19 20:33:23] [INFO] nvitop监控已启动
[2026-01-19 20:33:31] [WARNING] 未在WSL中找到conda安装，使用默认路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:33:33] [INFO] 启动命令: wsl bash -c "source /etc/profile 2>/dev/null || true && source ~/.bashrc 2>/dev/null || true && sour...
[2026-01-19 20:33:33] [INFO] nvitop监控已启动
[2026-01-19 20:33:33] [INFO] bash: line 1: /mnt/AI-Acer4T/miniconda3/bin/activate: No such file or directory
[2026-01-19 20:33:33] [INFO] nvitop监控已停止
[2026-01-19 20:33:33] [INFO] nvitop监控已启动
[2026-01-19 20:36:37] [INFO] 从WSL HOME目录自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:36:46] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:36:47] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:36:47] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:36:47] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:36:48] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:36:48] [WARNING] 配置验证警告（仍将生成命令）: 模型路径不能为空
[2026-01-19 20:36:48] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 20:36:48] [INFO] 启动命令: wsl bash -c "conda activate vllm && vllm serve /mnt/i/model"...
[2026-01-19 20:36:48] [INFO] nvitop监控已启动
[2026-01-19 20:36:48] [INFO] 启动命令: /bin/bash -c "conda activate vllm && vllm serve /home/user/model"...
[2026-01-19 20:36:48] [INFO] nvitop监控已启动
[2026-01-19 20:36:48] [INFO] 启动命令: wsl bash -c "conda activate vllm && vllm serve /mnt/i/model"...
[2026-01-19 20:36:48] [ERROR] 子进程执行失败: Subprocess error
[2026-01-19 20:36:48] [INFO] 服务已停止
[2026-01-19 20:36:48] [INFO] 服务已停止
[2026-01-19 20:36:49] [INFO] nvitop监控已启动
[2026-01-19 20:36:49] [INFO] nvitop监控已启动
[2026-01-19 21:23:45] [INFO] VLLM GUI 服务器启动，端口: 5002
[2026-01-19 21:23:46] [INFO] nvitop监控已启动
[2026-01-19 21:23:54] [WARNING] 未在WSL中找到conda安装，使用默认路径: /home/wuwen-ubu/miniconda3
[2026-01-19 21:24:05] [INFO] nvitop监控已启动
[2026-01-19 21:25:45] [INFO] 从WSL HOME目录自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 21:27:34] [INFO] 从WSL HOME目录自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 21:27:34] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 21:27:35] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 21:27:35] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 21:27:35] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 21:27:35] [WARNING] 配置验证警告（仍将生成命令）: 模型路径不能为空
[2026-01-19 21:27:36] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 21:27:36] [INFO] 启动命令: wsl bash -c "conda activate vllm && vllm serve /mnt/i/model"...
[2026-01-19 21:27:36] [INFO] nvitop监控已启动
[2026-01-19 21:27:36] [INFO] 启动命令: /bin/bash -c "conda activate vllm && vllm serve /home/user/model"...
[2026-01-19 21:27:36] [INFO] nvitop监控已启动
[2026-01-19 21:27:36] [INFO] 启动命令: wsl bash -c "conda activate vllm && vllm serve /mnt/i/model"...
[2026-01-19 21:27:36] [ERROR] 子进程执行失败: Subprocess error
[2026-01-19 21:27:36] [INFO] 服务已停止
[2026-01-19 21:27:36] [INFO] 服务已停止
[2026-01-19 21:27:37] [INFO] nvitop监控已启动
[2026-01-19 21:27:37] [INFO] nvitop监控已启动
[2026-01-19 21:27:58] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 21:27:58] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 21:32:46] [INFO] VLLM GUI 服务器启动，端口: 5002
[2026-01-19 21:32:48] [INFO] nvitop监控已启动
[2026-01-19 21:32:55] [INFO] 从WSL HOME目录自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 21:32:55] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 21:32:55] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 21:32:55] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 21:32:55] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 21:32:56] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 21:32:56] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 21:33:00] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 21:33:00] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 21:33:13] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 21:33:13] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 21:35:51] [INFO] 从WSL HOME目录自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 21:35:51] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 21:35:51] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 21:35:52] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 21:35:52] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 21:35:52] [WARNING] 配置验证警告（仍将生成命令）: 模型路径不能为空
[2026-01-19 21:35:52] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 21:35:52] [INFO] 启动命令: wsl bash -c "conda activate vllm && vllm serve /mnt/i/model"...
[2026-01-19 21:35:52] [INFO] nvitop监控已启动
[2026-01-19 21:35:52] [INFO] 启动命令: /bin/bash -c "conda activate vllm && vllm serve /home/user/model"...
[2026-01-19 21:35:52] [INFO] nvitop监控已启动
[2026-01-19 21:35:52] [INFO] 启动命令: wsl bash -c "conda activate vllm && vllm serve /mnt/i/model"...
[2026-01-19 21:35:52] [ERROR] 子进程执行失败: Subprocess error
[2026-01-19 21:35:52] [INFO] 服务已停止
[2026-01-19 21:35:52] [INFO] 服务已停止
[2026-01-19 21:35:53] [INFO] nvitop监控已启动
[2026-01-19 21:35:53] [INFO] nvitop监控已启动
[2026-01-19 21:37:26] [INFO] VLLM GUI 服务器启动，端口: 5002
[2026-01-19 21:37:27] [INFO] nvitop监控已启动
[2026-01-19 21:37:35] [INFO] 从WSL HOME目录自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 21:37:35] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 21:37:35] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 21:37:35] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 21:37:35] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 21:37:35] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 21:37:35] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 21:37:35] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 21:37:35] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 21:37:36] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 21:37:36] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 21:37:36] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 21:37:36] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 21:37:36] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 21:37:36] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 21:37:36] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 21:37:36] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 21:37:43] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 21:37:43] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 21:37:47] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 21:37:47] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 21:37:48] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 21:37:48] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 21:40:16] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 21:40:16] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 21:40:18] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 21:40:27] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 21:40:28] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 21:40:28] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 21:40:28] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 21:40:29] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 21:40:29] [WARNING] 配置验证警告（仍将生成命令）: 模型路径不能为空
[2026-01-19 21:40:29] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 21:40:29] [INFO] 启动命令: wsl bash -c "conda activate vllm && vllm serve /mnt/i/model"...
[2026-01-19 21:40:29] [INFO] nvitop监控已启动
[2026-01-19 21:40:29] [INFO] 启动命令: /bin/bash -c "conda activate vllm && vllm serve /home/user/model"...
[2026-01-19 21:40:29] [INFO] nvitop监控已启动
[2026-01-19 21:40:29] [INFO] 启动命令: wsl bash -c "conda activate vllm && vllm serve /mnt/i/model"...
[2026-01-19 21:40:29] [ERROR] 子进程执行失败: Subprocess error
[2026-01-19 21:40:29] [INFO] 服务已停止
[2026-01-19 21:40:29] [INFO] 服务已停止
[2026-01-19 21:40:30] [INFO] nvitop监控已启动
[2026-01-19 21:40:30] [INFO] nvitop监控已启动
[2026-01-19 22:20:55] [INFO] VLLM GUI 服务器启动，端口: 5002
[2026-01-19 22:20:57] [INFO] nvitop监控已启动
[2026-01-19 22:21:05] [INFO] 从WSL HOME目录自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:21:06] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:21:06] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 22:21:06] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:21:06] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 22:21:19] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:21:19] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 22:21:19] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:21:19] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 22:24:41] [INFO] 从WSL HOME目录自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:24:41] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:24:41] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:24:42] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:24:42] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:24:42] [WARNING] 配置验证警告（仍将生成命令）: 模型路径不能为空
[2026-01-19 22:24:42] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:24:42] [INFO] 启动命令: wsl bash -c "conda activate vllm && vllm serve /mnt/i/model"...
[2026-01-19 22:24:42] [INFO] nvitop监控已启动
[2026-01-19 22:24:42] [INFO] 启动命令: /bin/bash -c "conda activate vllm && vllm serve /home/user/model"...
[2026-01-19 22:24:42] [INFO] nvitop监控已启动
[2026-01-19 22:24:42] [INFO] 启动命令: wsl bash -c "conda activate vllm && vllm serve /mnt/i/model"...
[2026-01-19 22:24:42] [ERROR] 子进程执行失败: Subprocess error
[2026-01-19 22:24:42] [INFO] 服务已停止
[2026-01-19 22:24:42] [INFO] 服务已停止
[2026-01-19 22:24:43] [INFO] nvitop监控已启动
[2026-01-19 22:24:43] [INFO] nvitop监控已启动
[2026-01-19 22:25:37] [INFO] VLLM GUI 服务器启动，端口: 5002
[2026-01-19 22:25:38] [INFO] nvitop监控已启动
[2026-01-19 22:25:41] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:25:44] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:25:44] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 22:25:44] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:25:44] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 22:25:47] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:25:47] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 22:27:52] [INFO] 从WSL HOME目录自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:27:53] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:27:53] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:27:53] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:27:54] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:27:54] [WARNING] 配置验证警告（仍将生成命令）: 模型路径不能为空
[2026-01-19 22:27:54] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:27:54] [INFO] 启动命令: wsl bash -c "conda activate vllm && vllm serve /mnt/i/model"...
[2026-01-19 22:27:54] [INFO] nvitop监控已启动
[2026-01-19 22:27:54] [INFO] 启动命令: /bin/bash -c "conda activate vllm && vllm serve /home/user/model"...
[2026-01-19 22:27:54] [INFO] nvitop监控已启动
[2026-01-19 22:27:54] [INFO] 启动命令: wsl bash -c "conda activate vllm && vllm serve /mnt/i/model"...
[2026-01-19 22:27:54] [ERROR] 子进程执行失败: Subprocess error
[2026-01-19 22:27:54] [INFO] 服务已停止
[2026-01-19 22:27:54] [INFO] 服务已停止
[2026-01-19 22:27:55] [INFO] nvitop监控已启动
[2026-01-19 22:27:55] [INFO] nvitop监控已启动
[2026-01-19 22:33:43] [INFO] VLLM GUI 服务器启动，端口: 5002
[2026-01-19 22:36:38] [INFO] 从WSL HOME目录自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:36:39] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:36:39] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:36:39] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:36:39] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:36:39] [WARNING] 配置验证警告（仍将生成命令）: 模型路径不能为空
[2026-01-19 22:36:40] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:36:40] [INFO] 启动命令: wsl bash -c "conda activate vllm && vllm serve /mnt/i/model"...
[2026-01-19 22:36:40] [INFO] nvitop监控已启动
[2026-01-19 22:36:40] [INFO] 启动命令: /bin/bash -c "conda activate vllm && vllm serve /home/user/model"...
[2026-01-19 22:36:40] [INFO] nvitop监控已启动
[2026-01-19 22:36:40] [INFO] 启动命令: wsl bash -c "conda activate vllm && vllm serve /mnt/i/model"...
[2026-01-19 22:36:40] [ERROR] 子进程执行失败: Subprocess error
[2026-01-19 22:36:40] [INFO] 服务已停止
[2026-01-19 22:36:40] [INFO] 服务已停止
[2026-01-19 22:36:41] [INFO] nvitop监控已启动
[2026-01-19 22:36:41] [INFO] nvitop监控已启动
[2026-01-19 22:38:16] [INFO] VLLM GUI 服务器启动，端口: 5002
[2026-01-19 22:38:17] [INFO] nvitop监控已启动
[2026-01-19 22:38:25] [INFO] 从WSL HOME目录自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:38:26] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:38:26] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 22:38:26] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:38:26] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 22:38:29] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:38:29] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 22:38:29] [INFO] 启动命令: wsl bash -c "source /etc/profile 2>/dev/null || true && source ~/.bashrc 2>/dev/null || true && sour...
[2026-01-19 22:38:29] [INFO] nvitop监控已启动
[2026-01-19 22:38:29] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:38:29] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 22:38:29] [INFO] 
[2026-01-19 22:38:29] [INFO] EnvironmentNameNotFound: Could not find conda environment: vllm-tool
[2026-01-19 22:38:29] [INFO] You can list all discoverable environments with `conda info --envs`.
[2026-01-19 22:38:29] [INFO] 
[2026-01-19 22:38:29] [INFO] 
[2026-01-19 22:38:30] [INFO] nvitop监控已启动
[2026-01-19 22:38:39] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:38:39] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 22:38:39] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:38:39] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 22:38:41] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:38:41] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 22:38:41] [INFO] 启动命令: wsl bash -c "source /etc/profile 2>/dev/null || true && source ~/.bashrc 2>/dev/null || true && sour...
[2026-01-19 22:38:41] [INFO] nvitop监控已启动
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m INFO 01-19 22:38:54 [api_server.py:1278] vLLM API server version 0.14.0rc1.dev538+g2a719e086
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m INFO 01-19 22:38:54 [utils.py:254] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-VL-30B-A3B-Instruct', 'host': '0.0.0.0', 'port': 8005, 'model': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-VL-30B-A3B-Instruct', 'trust_remote_code': True, 'max_model_len': 128000, 'quantization': 'awq', 'served_model_name': ['VLLM-MODEL'], 'tensor_parallel_size': 2, 'enable_expert_parallel': True, 'max_num_seqs': 256, 'async_scheduling': True}
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-19 22:38:54] [ERROR] [0;36m(APIServer pid=579)[0;0m Traceback (most recent call last):
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/transformers/utils/hub.py", line 425, in cached_files
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m     hf_hub_download(
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 85, in _inner_fn
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m     validate_repo_id(arg_value)
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 129, in validate_repo_id
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m     raise HFValidationError(
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-VL-30B-A3B-Instruct'. Use `repo_type` argument if needed.
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m The above exception was the direct cause of the following exception:
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m
[2026-01-19 22:38:54] [ERROR] [0;36m(APIServer pid=579)[0;0m Traceback (most recent call last):
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/bin/vllm", line 7, in <module>
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m     sys.exit(main())
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m              ^^^^^^
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m     args.dispatch_function(args)
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m     uvloop.run(run_server(args))
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m     return __asyncio.run(
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m            ^^^^^^^^^^^^^^
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m     return runner.run(main)
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m     return self._loop.run_until_complete(task)
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m     return await main
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m            ^^^^^^^^^^
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 1325, in run_server
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 1344, in run_server_worker
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m     async with build_async_engine_client(
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m     return await anext(self.gen)
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 173, in build_async_engine_client
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m     return await anext(self.gen)
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 199, in build_async_engine_client_from_engine_args
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/engine/arg_utils.py", line 1362, in create_engine_config
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m     maybe_override_with_speculators(
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/transformers_utils/config.py", line 528, in maybe_override_with_speculators
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m     config_dict, _ = PretrainedConfig.get_config_dict(
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/transformers/configuration_utils.py", line 619, in get_config_dict
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m     config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/transformers/configuration_utils.py", line 674, in _get_config_dict
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m     resolved_config_file = cached_file(
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m                            ^^^^^^^^^^^^
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/transformers/utils/hub.py", line 282, in cached_file
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m     file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/transformers/utils/hub.py", line 474, in cached_files
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m     raise OSError(f"{e}") from e
[2026-01-19 22:38:54] [INFO] [0;36m(APIServer pid=579)[0;0m OSError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-VL-30B-A3B-Instruct'. Use `repo_type` argument if needed.
[2026-01-19 22:38:57] [INFO] nvitop监控已启动
[2026-01-19 22:39:11] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:39:11] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 22:39:11] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:39:11] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 22:39:12] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:39:12] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 22:39:12] [INFO] 启动命令: wsl bash -c "source /etc/profile 2>/dev/null || true && source ~/.bashrc 2>/dev/null || true && sour...
[2026-01-19 22:39:12] [INFO] nvitop监控已启动
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m INFO 01-19 22:39:20 [api_server.py:1278] vLLM API server version 0.14.0rc1.dev538+g2a719e086
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m INFO 01-19 22:39:20 [utils.py:254] non-default args: {'model_tag': '/mnt/i/AI-Chat/models/Qwen3/Qwen3-VL-30B-A3B-Instruct', 'host': '0.0.0.0', 'port': 8005, 'model': '/mnt/i/AI-Chat/models/Qwen3/Qwen3-VL-30B-A3B-Instruct', 'trust_remote_code': True, 'max_model_len': 128000, 'quantization': 'awq', 'served_model_name': ['VLLM-MODEL'], 'tensor_parallel_size': 2, 'enable_expert_parallel': True, 'max_num_seqs': 256, 'async_scheduling': True}
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-19 22:39:20] [ERROR] [0;36m(APIServer pid=873)[0;0m Traceback (most recent call last):
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/bin/vllm", line 7, in <module>
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m     sys.exit(main())
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m              ^^^^^^
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m     args.dispatch_function(args)
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m     uvloop.run(run_server(args))
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m     return __asyncio.run(
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m            ^^^^^^^^^^^^^^
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m     return runner.run(main)
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m     return self._loop.run_until_complete(task)
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m     return await main
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m            ^^^^^^^^^^
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 1325, in run_server
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 1344, in run_server_worker
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m     async with build_async_engine_client(
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m     return await anext(self.gen)
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 173, in build_async_engine_client
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m     return await anext(self.gen)
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 199, in build_async_engine_client_from_engine_args
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/engine/arg_utils.py", line 1362, in create_engine_config
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m     maybe_override_with_speculators(
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/transformers_utils/config.py", line 528, in maybe_override_with_speculators
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m     config_dict, _ = PretrainedConfig.get_config_dict(
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/transformers/configuration_utils.py", line 619, in get_config_dict
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m     config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/transformers/configuration_utils.py", line 708, in _get_config_dict
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m     config_dict = cls._dict_from_json_file(resolved_config_file)
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/transformers/configuration_utils.py", line 814, in _dict_from_json_file
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m     with open(json_file, encoding="utf-8") as reader:
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:39:20] [INFO] [0;36m(APIServer pid=873)[0;0m PermissionError: [Errno 13] Permission denied: '/mnt/i/AI-Chat/models/Qwen3/Qwen3-VL-30B-A3B-Instruct/config.json'
[2026-01-19 22:39:22] [INFO] nvitop监控已启动
[2026-01-19 22:45:49] [INFO] VLLM GUI 服务器启动，端口: 5002
[2026-01-19 22:45:50] [INFO] nvitop监控已启动
[2026-01-19 22:45:53] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:45:58] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:45:58] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 22:45:59] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:45:59] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 22:45:59] [INFO] 启动命令: wsl bash -c "source /etc/profile 2>/dev/null || true && source ~/.bashrc 2>/dev/null || true && sour...
[2026-01-19 22:45:59] [INFO] nvitop监控已启动
[2026-01-19 22:46:00] [INFO] 
[2026-01-19 22:46:00] [INFO] EnvironmentNameNotFound: Could not find conda environment: vllm-tool
[2026-01-19 22:46:00] [INFO] You can list all discoverable environments with `conda info --envs`.
[2026-01-19 22:46:00] [INFO] 
[2026-01-19 22:46:00] [INFO] 
[2026-01-19 22:46:00] [INFO] nvitop监控已启动
[2026-01-19 22:46:14] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:46:14] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 22:46:14] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:46:14] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 22:46:15] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:46:15] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 22:46:15] [INFO] 启动命令: wsl bash -c "source /etc/profile 2>/dev/null || true && source ~/.bashrc 2>/dev/null || true && sour...
[2026-01-19 22:46:16] [INFO] nvitop监控已启动
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m INFO 01-19 22:46:29 [api_server.py:1278] vLLM API server version 0.14.0rc1.dev538+g2a719e086
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m INFO 01-19 22:46:29 [utils.py:254] non-default args: {'model_tag': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-VL-30B-A3B-Instruct', 'host': '0.0.0.0', 'port': 8005, 'model': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-VL-30B-A3B-Instruct', 'trust_remote_code': True, 'max_model_len': 128000, 'quantization': 'awq', 'served_model_name': ['VLLM-MODEL'], 'tensor_parallel_size': 2, 'enable_expert_parallel': True, 'max_num_seqs': 256, 'async_scheduling': True}
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-19 22:46:29] [ERROR] [0;36m(APIServer pid=619)[0;0m Traceback (most recent call last):
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/transformers/utils/hub.py", line 425, in cached_files
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m     hf_hub_download(
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 85, in _inner_fn
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m     validate_repo_id(arg_value)
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 129, in validate_repo_id
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m     raise HFValidationError(
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-VL-30B-A3B-Instruct'. Use `repo_type` argument if needed.
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m The above exception was the direct cause of the following exception:
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m
[2026-01-19 22:46:29] [ERROR] [0;36m(APIServer pid=619)[0;0m Traceback (most recent call last):
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/bin/vllm", line 7, in <module>
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m     sys.exit(main())
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m              ^^^^^^
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m     args.dispatch_function(args)
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m     uvloop.run(run_server(args))
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m     return __asyncio.run(
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m            ^^^^^^^^^^^^^^
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m     return runner.run(main)
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m     return self._loop.run_until_complete(task)
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m     return await main
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m            ^^^^^^^^^^
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 1325, in run_server
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 1344, in run_server_worker
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m     async with build_async_engine_client(
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m     return await anext(self.gen)
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 173, in build_async_engine_client
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m     return await anext(self.gen)
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 199, in build_async_engine_client_from_engine_args
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/engine/arg_utils.py", line 1362, in create_engine_config
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m     maybe_override_with_speculators(
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/transformers_utils/config.py", line 528, in maybe_override_with_speculators
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m     config_dict, _ = PretrainedConfig.get_config_dict(
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/transformers/configuration_utils.py", line 619, in get_config_dict
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m     config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/transformers/configuration_utils.py", line 674, in _get_config_dict
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m     resolved_config_file = cached_file(
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m                            ^^^^^^^^^^^^
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/transformers/utils/hub.py", line 282, in cached_file
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m     file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/transformers/utils/hub.py", line 474, in cached_files
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m     raise OSError(f"{e}") from e
[2026-01-19 22:46:29] [INFO] [0;36m(APIServer pid=619)[0;0m OSError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/mnt/AI-Acer4T/AI-Chat/models/Qwen3/Qwen3-VL-30B-A3B-Instruct'. Use `repo_type` argument if needed.
[2026-01-19 22:46:32] [INFO] nvitop监控已启动
[2026-01-19 22:46:39] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:46:39] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 22:46:40] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:46:40] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 22:46:41] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:46:41] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 22:46:41] [INFO] 启动命令: wsl bash -c "source /etc/profile 2>/dev/null || true && source ~/.bashrc 2>/dev/null || true && sour...
[2026-01-19 22:46:42] [INFO] nvitop监控已启动
[2026-01-19 22:46:50] [INFO] [0;36m(APIServer pid=889)[0;0m INFO 01-19 22:46:49 [api_server.py:1278] vLLM API server version 0.14.0rc1.dev538+g2a719e086
[2026-01-19 22:46:50] [INFO] [0;36m(APIServer pid=889)[0;0m INFO 01-19 22:46:49 [utils.py:254] non-default args: {'model_tag': '/mnt/i/AI-Chat/models/Qwen3/Qwen3-VL-30B-A3B-Instruct', 'host': '0.0.0.0', 'port': 8005, 'model': '/mnt/i/AI-Chat/models/Qwen3/Qwen3-VL-30B-A3B-Instruct', 'trust_remote_code': True, 'max_model_len': 128000, 'quantization': 'awq', 'served_model_name': ['VLLM-MODEL'], 'tensor_parallel_size': 2, 'enable_expert_parallel': True, 'max_num_seqs': 256, 'async_scheduling': True}
[2026-01-19 22:46:50] [INFO] [0;36m(APIServer pid=889)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-19 22:46:50] [INFO] [0;36m(APIServer pid=889)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-19 22:46:50] [INFO] [0;36m(APIServer pid=889)[0;0m Unrecognized keys in `rope_parameters` for 'rope_type'='default': {'mrope_section', 'mrope_interleaved'}
[2026-01-19 22:46:50] [INFO] [0;36m(APIServer pid=889)[0;0m Unrecognized keys in `rope_parameters` for 'rope_type'='default': {'mrope_section', 'mrope_interleaved'}
[2026-01-19 22:46:57] [INFO] [0;36m(APIServer pid=889)[0;0m INFO 01-19 22:46:57 [model.py:530] Resolved architecture: Qwen3VLMoeForConditionalGeneration
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m INFO 01-19 22:46:57 [model.py:1545] Using max model len 128000
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m INFO 01-19 22:46:58 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[2026-01-19 22:46:58] [ERROR] [0;36m(APIServer pid=889)[0;0m Traceback (most recent call last):
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/bin/vllm", line 7, in <module>
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m     sys.exit(main())
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m              ^^^^^^
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m     args.dispatch_function(args)
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m     uvloop.run(run_server(args))
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m     return __asyncio.run(
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m            ^^^^^^^^^^^^^^
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m     return runner.run(main)
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m     return self._loop.run_until_complete(task)
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m     return await main
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m            ^^^^^^^^^^
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 1325, in run_server
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 1344, in run_server_worker
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m     async with build_async_engine_client(
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m     return await anext(self.gen)
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 173, in build_async_engine_client
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m     return await anext(self.gen)
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 199, in build_async_engine_client_from_engine_args
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/engine/arg_utils.py", line 1750, in create_engine_config
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m     config = VllmConfig(
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m              ^^^^^^^^^^^
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/pydantic/_internal/_dataclasses.py", line 121, in __init__
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for VllmConfig
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m   Value error, Cannot find the config file for awq [type=value_error, input_value=ArgsKwargs((), {'model_co...timizationLevel.O2: 2>}), input_type=ArgsKwargs]
[2026-01-19 22:46:58] [INFO] [0;36m(APIServer pid=889)[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error
[2026-01-19 22:47:00] [INFO] nvitop监控已启动
[2026-01-19 22:47:12] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:47:12] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 22:47:14] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:47:14] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 22:47:16] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:47:16] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 22:47:16] [INFO] 启动命令: wsl bash -c "source /etc/profile 2>/dev/null || true && source ~/.bashrc 2>/dev/null || true && sour...
[2026-01-19 22:47:16] [INFO] nvitop监控已启动
[2026-01-19 22:47:24] [INFO] [0;36m(APIServer pid=1263)[0;0m INFO 01-19 22:47:24 [api_server.py:1278] vLLM API server version 0.14.0rc1.dev538+g2a719e086
[2026-01-19 22:47:24] [INFO] [0;36m(APIServer pid=1263)[0;0m INFO 01-19 22:47:24 [utils.py:254] non-default args: {'model_tag': '/mnt/i/AI-Chat/models/Qwen3/Qwen3-VL-30B-A3B-Instruct', 'host': '0.0.0.0', 'port': 8005, 'model': '/mnt/i/AI-Chat/models/Qwen3/Qwen3-VL-30B-A3B-Instruct', 'trust_remote_code': True, 'max_model_len': 128000, 'served_model_name': ['VLLM-MODEL'], 'tensor_parallel_size': 2, 'enable_expert_parallel': True, 'max_num_seqs': 256, 'async_scheduling': True}
[2026-01-19 22:47:24] [INFO] [0;36m(APIServer pid=1263)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-19 22:47:24] [INFO] [0;36m(APIServer pid=1263)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-19 22:47:24] [INFO] [0;36m(APIServer pid=1263)[0;0m Unrecognized keys in `rope_parameters` for 'rope_type'='default': {'mrope_section', 'mrope_interleaved'}
[2026-01-19 22:47:24] [INFO] [0;36m(APIServer pid=1263)[0;0m Unrecognized keys in `rope_parameters` for 'rope_type'='default': {'mrope_section', 'mrope_interleaved'}
[2026-01-19 22:47:24] [INFO] [0;36m(APIServer pid=1263)[0;0m INFO 01-19 22:47:24 [model.py:530] Resolved architecture: Qwen3VLMoeForConditionalGeneration
[2026-01-19 22:47:24] [INFO] [0;36m(APIServer pid=1263)[0;0m INFO 01-19 22:47:24 [model.py:1545] Using max model len 128000
[2026-01-19 22:47:24] [INFO] [0;36m(APIServer pid=1263)[0;0m INFO 01-19 22:47:24 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[2026-01-19 22:47:24] [INFO] [0;36m(APIServer pid=1263)[0;0m INFO 01-19 22:47:24 [vllm.py:632] Asynchronous scheduling is enabled.
[2026-01-19 22:47:24] [INFO] [0;36m(APIServer pid=1263)[0;0m INFO 01-19 22:47:24 [vllm.py:639] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-19 22:47:25] [WARNING] [0;36m(APIServer pid=1263)[0;0m WARNING 01-19 22:47:25 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[2026-01-19 22:47:33] [INFO] [0;36m(EngineCore_DP0 pid=1410)[0;0m INFO 01-19 22:47:34 [core.py:97] Initializing a V1 LLM engine (v0.14.0rc1.dev538+g2a719e086) with config: model='/mnt/i/AI-Chat/models/Qwen3/Qwen3-VL-30B-A3B-Instruct', speculative_config=None, tokenizer='/mnt/i/AI-Chat/models/Qwen3/Qwen3-VL-30B-A3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-19 22:47:33] [WARNING] [0;36m(EngineCore_DP0 pid=1410)[0;0m WARNING 01-19 22:47:34 [multiproc_executor.py:880] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-19 22:47:43] [INFO] INFO 01-19 22:47:42 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:52017 backend=nccl
[2026-01-19 22:47:43] [INFO] INFO 01-19 22:47:42 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:52017 backend=nccl
[2026-01-19 22:47:43] [INFO] INFO 01-19 22:47:42 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749] WorkerProc failed to start.
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749] Traceback (most recent call last):
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 720, in worker_main
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]     worker = WorkerProc(*args, **kwargs)
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 551, in __init__
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]     self.worker.init_device()
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 326, in init_device
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]     self.worker.init_device()  # type: ignore
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 217, in init_device
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]     init_worker_distributed_environment(
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 959, in init_worker_distributed_environment
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]     ensure_model_parallel_initialized(
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/distributed/parallel_state.py", line 1452, in ensure_model_parallel_initialized
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]     initialize_model_parallel(
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/distributed/parallel_state.py", line 1349, in initialize_model_parallel
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]     _TP = init_model_parallel_group(
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]           ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/distributed/parallel_state.py", line 1069, in init_model_parallel_group
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]     return GroupCoordinator(
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]            ^^^^^^^^^^^^^^^^^
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/distributed/parallel_state.py", line 364, in __init__
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]     self.device_communicator = device_comm_cls(
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]                                ^^^^^^^^^^^^^^^^
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/distributed/device_communicators/cuda_communicator.py", line 58, in __init__
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]     self.pynccl_comm = PyNcclCommunicator(
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]                        ^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/distributed/device_communicators/pynccl.py", line 139, in __init__
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]     self.comm: ncclComm_t = self.nccl.ncclCommInitRank(
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/distributed/device_communicators/pynccl_wrapper.py", line 403, in ncclCommInitRank
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]     self.NCCL_CHECK(
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/distributed/device_communicators/pynccl_wrapper.py", line 369, in NCCL_CHECK
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]     raise RuntimeError(f"NCCL error: {error_str}")
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749] RuntimeError: NCCL error: unhandled cuda error (run with NCCL_DEBUG=INFO for details)
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749] WorkerProc failed to start.
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749] Traceback (most recent call last):
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 720, in worker_main
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]     worker = WorkerProc(*args, **kwargs)
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 551, in __init__
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]     self.worker.init_device()
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 326, in init_device
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]     self.worker.init_device()  # type: ignore
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]     ^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 217, in init_device
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]     init_worker_distributed_environment(
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 959, in init_worker_distributed_environment
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]     ensure_model_parallel_initialized(
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/distributed/parallel_state.py", line 1452, in ensure_model_parallel_initialized
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]     initialize_model_parallel(
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/distributed/parallel_state.py", line 1349, in initialize_model_parallel
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]     _TP = init_model_parallel_group(
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]           ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/distributed/parallel_state.py", line 1069, in init_model_parallel_group
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]     return GroupCoordinator(
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]            ^^^^^^^^^^^^^^^^^
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/distributed/parallel_state.py", line 364, in __init__
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]     self.device_communicator = device_comm_cls(
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]                                ^^^^^^^^^^^^^^^^
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/distributed/device_communicators/cuda_communicator.py", line 58, in __init__
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]     self.pynccl_comm = PyNcclCommunicator(
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]                        ^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/distributed/device_communicators/pynccl.py", line 139, in __init__
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]     self.comm: ncclComm_t = self.nccl.ncclCommInitRank(
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/distributed/device_communicators/pynccl_wrapper.py", line 403, in ncclCommInitRank
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]     self.NCCL_CHECK(
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/distributed/device_communicators/pynccl_wrapper.py", line 369, in NCCL_CHECK
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749]     raise RuntimeError(f"NCCL error: {error_str}")
[2026-01-19 22:47:44] [ERROR] ERROR 01-19 22:47:43 [multiproc_executor.py:749] RuntimeError: NCCL error: unhandled cuda error (run with NCCL_DEBUG=INFO for details)
[2026-01-19 22:47:44] [INFO] INFO 01-19 22:47:43 [multiproc_executor.py:707] Parent process exited, terminating worker
[2026-01-19 22:47:44] [INFO] INFO 01-19 22:47:43 [multiproc_executor.py:707] Parent process exited, terminating worker
[2026-01-19 22:47:45] [WARNING] [rank0]:[W119 22:47:44.584130730 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2026-01-19 22:47:46] [ERROR] [0;36m(EngineCore_DP0 pid=1410)[0;0m ERROR 01-19 22:47:45 [core.py:936] EngineCore failed to start.
[2026-01-19 22:47:46] [ERROR] [0;36m(EngineCore_DP0 pid=1410)[0;0m ERROR 01-19 22:47:45 [core.py:936] Traceback (most recent call last):
[2026-01-19 22:47:46] [ERROR] [0;36m(EngineCore_DP0 pid=1410)[0;0m ERROR 01-19 22:47:45 [core.py:936]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 927, in run_engine_core
[2026-01-19 22:47:46] [ERROR] [0;36m(EngineCore_DP0 pid=1410)[0;0m ERROR 01-19 22:47:45 [core.py:936]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-19 22:47:46] [ERROR] [0;36m(EngineCore_DP0 pid=1410)[0;0m ERROR 01-19 22:47:45 [core.py:936]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:47:46] [ERROR] [0;36m(EngineCore_DP0 pid=1410)[0;0m ERROR 01-19 22:47:45 [core.py:936]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 692, in __init__
[2026-01-19 22:47:46] [ERROR] [0;36m(EngineCore_DP0 pid=1410)[0;0m ERROR 01-19 22:47:45 [core.py:936]     super().__init__(
[2026-01-19 22:47:46] [ERROR] [0;36m(EngineCore_DP0 pid=1410)[0;0m ERROR 01-19 22:47:45 [core.py:936]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 106, in __init__
[2026-01-19 22:47:46] [ERROR] [0;36m(EngineCore_DP0 pid=1410)[0;0m ERROR 01-19 22:47:45 [core.py:936]     self.model_executor = executor_class(vllm_config)
[2026-01-19 22:47:46] [ERROR] [0;36m(EngineCore_DP0 pid=1410)[0;0m ERROR 01-19 22:47:45 [core.py:936]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:47:46] [ERROR] [0;36m(EngineCore_DP0 pid=1410)[0;0m ERROR 01-19 22:47:45 [core.py:936]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[2026-01-19 22:47:46] [ERROR] [0;36m(EngineCore_DP0 pid=1410)[0;0m ERROR 01-19 22:47:45 [core.py:936]     super().__init__(vllm_config)
[2026-01-19 22:47:46] [ERROR] [0;36m(EngineCore_DP0 pid=1410)[0;0m ERROR 01-19 22:47:45 [core.py:936]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[2026-01-19 22:47:46] [ERROR] [0;36m(EngineCore_DP0 pid=1410)[0;0m ERROR 01-19 22:47:45 [core.py:936]     self._init_executor()
[2026-01-19 22:47:46] [ERROR] [0;36m(EngineCore_DP0 pid=1410)[0;0m ERROR 01-19 22:47:45 [core.py:936]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 172, in _init_executor
[2026-01-19 22:47:46] [ERROR] [0;36m(EngineCore_DP0 pid=1410)[0;0m ERROR 01-19 22:47:45 [core.py:936]     self.workers = WorkerProc.wait_for_ready(unready_workers)
[2026-01-19 22:47:46] [ERROR] [0;36m(EngineCore_DP0 pid=1410)[0;0m ERROR 01-19 22:47:45 [core.py:936]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:47:46] [ERROR] [0;36m(EngineCore_DP0 pid=1410)[0;0m ERROR 01-19 22:47:45 [core.py:936]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 658, in wait_for_ready
[2026-01-19 22:47:46] [ERROR] [0;36m(EngineCore_DP0 pid=1410)[0;0m ERROR 01-19 22:47:45 [core.py:936]     raise e from None
[2026-01-19 22:47:46] [ERROR] [0;36m(EngineCore_DP0 pid=1410)[0;0m ERROR 01-19 22:47:45 [core.py:936] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[2026-01-19 22:47:46] [INFO] [0;36m(EngineCore_DP0 pid=1410)[0;0m Process EngineCore_DP0:
[2026-01-19 22:47:46] [ERROR] [0;36m(EngineCore_DP0 pid=1410)[0;0m Traceback (most recent call last):
[2026-01-19 22:47:46] [INFO] [0;36m(EngineCore_DP0 pid=1410)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[2026-01-19 22:47:46] [INFO] [0;36m(EngineCore_DP0 pid=1410)[0;0m     self.run()
[2026-01-19 22:47:46] [INFO] [0;36m(EngineCore_DP0 pid=1410)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/multiprocessing/process.py", line 108, in run
[2026-01-19 22:47:46] [INFO] [0;36m(EngineCore_DP0 pid=1410)[0;0m     self._target(*self._args, **self._kwargs)
[2026-01-19 22:47:46] [INFO] [0;36m(EngineCore_DP0 pid=1410)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 940, in run_engine_core
[2026-01-19 22:47:46] [INFO] [0;36m(EngineCore_DP0 pid=1410)[0;0m     raise e
[2026-01-19 22:47:46] [INFO] [0;36m(EngineCore_DP0 pid=1410)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 927, in run_engine_core
[2026-01-19 22:47:46] [INFO] [0;36m(EngineCore_DP0 pid=1410)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-19 22:47:46] [INFO] [0;36m(EngineCore_DP0 pid=1410)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:47:46] [INFO] [0;36m(EngineCore_DP0 pid=1410)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 692, in __init__
[2026-01-19 22:47:46] [INFO] [0;36m(EngineCore_DP0 pid=1410)[0;0m     super().__init__(
[2026-01-19 22:47:46] [INFO] [0;36m(EngineCore_DP0 pid=1410)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 106, in __init__
[2026-01-19 22:47:46] [INFO] [0;36m(EngineCore_DP0 pid=1410)[0;0m     self.model_executor = executor_class(vllm_config)
[2026-01-19 22:47:46] [INFO] [0;36m(EngineCore_DP0 pid=1410)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:47:46] [INFO] [0;36m(EngineCore_DP0 pid=1410)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[2026-01-19 22:47:46] [INFO] [0;36m(EngineCore_DP0 pid=1410)[0;0m     super().__init__(vllm_config)
[2026-01-19 22:47:46] [INFO] [0;36m(EngineCore_DP0 pid=1410)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[2026-01-19 22:47:46] [INFO] [0;36m(EngineCore_DP0 pid=1410)[0;0m     self._init_executor()
[2026-01-19 22:47:46] [INFO] [0;36m(EngineCore_DP0 pid=1410)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 172, in _init_executor
[2026-01-19 22:47:46] [INFO] [0;36m(EngineCore_DP0 pid=1410)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
[2026-01-19 22:47:46] [INFO] [0;36m(EngineCore_DP0 pid=1410)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:47:46] [INFO] [0;36m(EngineCore_DP0 pid=1410)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 658, in wait_for_ready
[2026-01-19 22:47:46] [INFO] [0;36m(EngineCore_DP0 pid=1410)[0;0m     raise e from None
[2026-01-19 22:47:46] [INFO] [0;36m(EngineCore_DP0 pid=1410)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[2026-01-19 22:47:47] [ERROR] [0;36m(APIServer pid=1263)[0;0m Traceback (most recent call last):
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/bin/vllm", line 7, in <module>
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m     sys.exit(main())
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m              ^^^^^^
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m     args.dispatch_function(args)
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m     uvloop.run(run_server(args))
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m     return __asyncio.run(
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m            ^^^^^^^^^^^^^^
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m     return runner.run(main)
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m     return self._loop.run_until_complete(task)
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m     return await main
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m            ^^^^^^^^^^
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 1325, in run_server
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 1344, in run_server_worker
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m     async with build_async_engine_client(
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m     return await anext(self.gen)
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 173, in build_async_engine_client
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m     return await anext(self.gen)
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 214, in build_async_engine_client_from_engine_args
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 205, in from_vllm_config
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m     return cls(
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m            ^^^^
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 132, in __init__
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m     return AsyncMPClient(*client_args)
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 824, in __init__
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m     super().__init__(
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/contextlib.py", line 144, in __exit__
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m     next(self.gen)
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 921, in launch_core_engines
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m     wait_for_engine_startup(
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 980, in wait_for_engine_startup
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m     raise RuntimeError(
[2026-01-19 22:47:47] [INFO] [0;36m(APIServer pid=1263)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[2026-01-19 22:47:49] [INFO] nvitop监控已启动
[2026-01-19 22:48:48] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:48:48] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 22:48:48] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:48:48] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 22:48:49] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-19 22:48:49] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-19 22:48:49] [INFO] 启动命令: wsl bash -c "source /etc/profile 2>/dev/null || true && source ~/.bashrc 2>/dev/null || true && sour...
[2026-01-19 22:48:50] [INFO] nvitop监控已启动
[2026-01-19 22:48:50] [INFO] nvitop监控已停止
[2026-01-19 22:49:03] [INFO] [0;36m(APIServer pid=434)[0;0m INFO 01-19 22:49:02 [api_server.py:1278] vLLM API server version 0.14.0rc1.dev538+g2a719e086
[2026-01-19 22:49:03] [INFO] [0;36m(APIServer pid=434)[0;0m INFO 01-19 22:49:02 [utils.py:254] non-default args: {'model_tag': '/mnt/i/AI-Chat/models/Qwen3/Qwen3-VL-30B-A3B-Instruct', 'host': '0.0.0.0', 'port': 8005, 'model': '/mnt/i/AI-Chat/models/Qwen3/Qwen3-VL-30B-A3B-Instruct', 'trust_remote_code': True, 'max_model_len': 128000, 'served_model_name': ['VLLM-MODEL'], 'tensor_parallel_size': 2, 'enable_expert_parallel': True, 'max_num_seqs': 256, 'async_scheduling': True}
[2026-01-19 22:49:03] [INFO] [0;36m(APIServer pid=434)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-19 22:49:03] [INFO] [0;36m(APIServer pid=434)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-19 22:49:03] [INFO] [0;36m(APIServer pid=434)[0;0m Unrecognized keys in `rope_parameters` for 'rope_type'='default': {'mrope_interleaved', 'mrope_section'}
[2026-01-19 22:49:03] [INFO] [0;36m(APIServer pid=434)[0;0m Unrecognized keys in `rope_parameters` for 'rope_type'='default': {'mrope_interleaved', 'mrope_section'}
[2026-01-19 22:49:03] [INFO] [0;36m(APIServer pid=434)[0;0m INFO 01-19 22:49:02 [model.py:530] Resolved architecture: Qwen3VLMoeForConditionalGeneration
[2026-01-19 22:49:03] [INFO] [0;36m(APIServer pid=434)[0;0m INFO 01-19 22:49:02 [model.py:1545] Using max model len 128000
[2026-01-19 22:49:03] [INFO] [0;36m(APIServer pid=434)[0;0m INFO 01-19 22:49:02 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[2026-01-19 22:49:03] [INFO] [0;36m(APIServer pid=434)[0;0m INFO 01-19 22:49:02 [vllm.py:632] Asynchronous scheduling is enabled.
[2026-01-19 22:49:03] [INFO] [0;36m(APIServer pid=434)[0;0m INFO 01-19 22:49:02 [vllm.py:639] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-19 22:49:04] [WARNING] [0;36m(APIServer pid=434)[0;0m WARNING 01-19 22:49:03 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[2026-01-19 22:49:12] [INFO] [0;36m(EngineCore_DP0 pid=598)[0;0m INFO 01-19 22:49:12 [core.py:97] Initializing a V1 LLM engine (v0.14.0rc1.dev538+g2a719e086) with config: model='/mnt/i/AI-Chat/models/Qwen3/Qwen3-VL-30B-A3B-Instruct', speculative_config=None, tokenizer='/mnt/i/AI-Chat/models/Qwen3/Qwen3-VL-30B-A3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-19 22:49:12] [WARNING] [0;36m(EngineCore_DP0 pid=598)[0;0m WARNING 01-19 22:49:12 [multiproc_executor.py:880] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-19 22:49:22] [INFO] INFO 01-19 22:49:22 [parallel_state.py:1214] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:43875 backend=nccl
[2026-01-19 22:49:22] [INFO] INFO 01-19 22:49:22 [parallel_state.py:1214] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:43875 backend=nccl
[2026-01-19 22:49:22] [INFO] INFO 01-19 22:49:23 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-19 22:49:23] [WARNING] WARNING 01-19 22:49:23 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-19 22:49:23] [WARNING] WARNING 01-19 22:49:23 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-19 22:49:23] [WARNING] WARNING 01-19 22:49:23 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2026-01-19 22:49:23] [WARNING] WARNING 01-19 22:49:23 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2026-01-19 22:49:23] [INFO] INFO 01-19 22:49:24 [parallel_state.py:1425] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
[2026-01-19 22:49:23] [INFO] INFO 01-19 22:49:24 [parallel_state.py:1425] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[2026-01-19 22:49:24] [WARNING] WARNING 01-19 22:49:24 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[2026-01-19 22:49:24] [WARNING] WARNING 01-19 22:49:24 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[2026-01-19 22:49:30] [INFO] [0;36m(Worker_TP0_EP0 pid=715)[0;0m INFO 01-19 22:49:29 [gpu_model_runner.py:3808] Starting to load model /mnt/i/AI-Chat/models/Qwen3/Qwen3-VL-30B-A3B-Instruct...
[2026-01-19 22:49:30] [INFO] [0;36m(Worker_TP0_EP0 pid=715)[0;0m INFO 01-19 22:49:29 [mm_encoder_attention.py:86] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[2026-01-19 22:49:30] [INFO] [0;36m(Worker_TP1_EP1 pid=716)[0;0m INFO 01-19 22:49:29 [mm_encoder_attention.py:86] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[2026-01-19 22:49:31] [INFO] [0;36m(Worker_TP0_EP0 pid=715)[0;0m INFO 01-19 22:49:30 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[2026-01-19 22:49:31] [INFO] [0;36m(Worker_TP1_EP1 pid=716)[0;0m INFO 01-19 22:49:30 [layer.py:503] [EP Rank 1/2] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 64/128. Experts local to global index map: 0->64, 1->65, 2->66, 3->67, 4->68, 5->69, 6->70, 7->71, 8->72, 9->73, 10->74, 11->75, 12->76, 13->77, 14->78, 15->79, 16->80, 17->81, 18->82, 19->83, 20->84, 21->85, 22->86, 23->87, 24->88, 25->89, 26->90, 27->91, 28->92, 29->93, 30->94, 31->95, 32->96, 33->97, 34->98, 35->99, 36->100, 37->101, 38->102, 39->103, 40->104, 41->105, 42->106, 43->107, 44->108, 45->109, 46->110, 47->111, 48->112, 49->113, 50->114, 51->115, 52->116, 53->117, 54->118, 55->119, 56->120, 57->121, 58->122, 59->123, 60->124, 61->125, 62->126, 63->127.
[2026-01-19 22:49:31] [INFO] [0;36m(Worker_TP0_EP0 pid=715)[0;0m INFO 01-19 22:49:30 [layer.py:503] [EP Rank 0/2] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 64/128. Experts local to global index map: 0->0, 1->1, 2->2, 3->3, 4->4, 5->5, 6->6, 7->7, 8->8, 9->9, 10->10, 11->11, 12->12, 13->13, 14->14, 15->15, 16->16, 17->17, 18->18, 19->19, 20->20, 21->21, 22->22, 23->23, 24->24, 25->25, 26->26, 27->27, 28->28, 29->29, 30->30, 31->31, 32->32, 33->33, 34->34, 35->35, 36->36, 37->37, 38->38, 39->39, 40->40, 41->41, 42->42, 43->43, 44->44, 45->45, 46->46, 47->47, 48->48, 49->49, 50->50, 51->51, 52->52, 53->53, 54->54, 55->55, 56->56, 57->57, 58->58, 59->59, 60->60, 61->61, 62->62, 63->63.
[2026-01-19 22:49:31] [INFO] [0;36m(Worker_TP0_EP0 pid=715)[0;0m INFO 01-19 22:49:30 [unquantized_fused_moe_method.py:84] FlashInfer CUTLASS MoE is available for EP but not enabled, consider setting VLLM_USE_FLASHINFER_MOE_FP16=1 to enable it.
[2026-01-19 22:49:31] [INFO] [0;36m(Worker_TP0_EP0 pid=715)[0;0m
[2026-01-19 22:49:31] [INFO] Loading safetensors checkpoint shards:   0% Completed | 0/13 [00:00<?, ?it/s]
[2026-01-19 22:49:51] [INFO] [0;36m(Worker_TP0_EP0 pid=715)[0;0m
[2026-01-19 22:49:51] [INFO] Loading safetensors checkpoint shards:   8% Completed | 1/13 [00:20<04:01, 20.10s/it]
[2026-01-19 22:50:13] [INFO] [0;36m(Worker_TP0_EP0 pid=715)[0;0m
[2026-01-19 22:50:13] [INFO] Loading safetensors checkpoint shards:  15% Completed | 2/13 [00:42<03:55, 21.36s/it]
[2026-01-19 22:50:36] [INFO] [0;36m(Worker_TP0_EP0 pid=715)[0;0m
[2026-01-19 22:50:36] [INFO] Loading safetensors checkpoint shards:  23% Completed | 3/13 [01:04<03:37, 21.76s/it]
[2026-01-19 22:50:59] [INFO] [0;36m(Worker_TP0_EP0 pid=715)[0;0m
[2026-01-19 22:50:59] [INFO] Loading safetensors checkpoint shards:  31% Completed | 4/13 [01:28<03:23, 22.58s/it]
[2026-01-19 22:51:21] [INFO] [0;36m(Worker_TP0_EP0 pid=715)[0;0m
[2026-01-19 22:51:21] [INFO] Loading safetensors checkpoint shards:  38% Completed | 5/13 [01:50<02:58, 22.36s/it]
[2026-01-19 22:51:44] [INFO] [0;36m(Worker_TP0_EP0 pid=715)[0;0m
[2026-01-19 22:51:44] [INFO] Loading safetensors checkpoint shards:  46% Completed | 6/13 [02:13<02:38, 22.69s/it]
[2026-01-19 22:51:44] [INFO] [0;36m(Worker_TP0_EP0 pid=715)[0;0m
[2026-01-19 22:51:44] [INFO] Loading safetensors checkpoint shards:  46% Completed | 6/13 [02:13<02:36, 22.29s/it]
[2026-01-19 22:51:44] [INFO] [0;36m(Worker_TP0_EP0 pid=715)[0;0m
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749] WorkerProc failed to start.
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749] Traceback (most recent call last):
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 720, in worker_main
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]     worker = WorkerProc(*args, **kwargs)
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 560, in __init__
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]     self.worker.load_model()
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 274, in load_model
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3827, in load_model
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]     self.model = model_loader.load_model(
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]                  ^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/model_loader/base_loader.py", line 58, in load_model
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]     self.load_weights(model, model_config)
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 288, in load_weights
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]     loaded_weights = model.load_weights(self.get_all_weights(model_config, model))
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_vl.py", line 2083, in load_weights
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]     return loader.load_weights(weights, mapper=self.hf_to_vllm_mapper)
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 335, in load_weights
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]     autoloaded_weights = set(self._load_module("", self.module, weights))
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 288, in _load_module
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]     yield from self._load_module(
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 261, in _load_module
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]     loaded_params = module_load_weights(weights)
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_moe.py", line 749, in load_weights
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]     return loader.load_weights(weights)
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/model_loader/online_quantization.py", line 173, in patched_model_load_weights
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]     return original_load_weights(auto_weight_loader, weights, mapper=mapper)
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 335, in load_weights
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]     autoloaded_weights = set(self._load_module("", self.module, weights))
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 288, in _load_module
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]     yield from self._load_module(
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 261, in _load_module
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]     loaded_params = module_load_weights(weights)
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/models/qwen3_vl_moe.py", line 194, in load_weights
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]     for name, loaded_weight in weights:
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]                                ^^^^^^^
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 170, in <genexpr>
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]     for parts, weights_data in group
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]                                ^^^^^
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 160, in <genexpr>
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]     for weight_name, weight_data in weights
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]                                     ^^^^^^^
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 332, in <genexpr>
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]     (name, weight) for name, weight in weights if not self._can_skip(name)
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]                                        ^^^^^^^
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 170, in <genexpr>
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]     for parts, weights_data in group
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]                                ^^^^^
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 160, in <genexpr>
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]     for weight_name, weight_data in weights
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]                                     ^^^^^^^
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 332, in <genexpr>
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]     (name, weight) for name, weight in weights if not self._can_skip(name)
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]                                        ^^^^^^^
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py", line 91, in <genexpr>
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]     for name, data in weights
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]                       ^^^^^^^
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 260, in get_all_weights
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]     yield from self._get_weights_iterator(primary_weights)
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/model_loader/default_loader.py", line 246, in <genexpr>
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]     return ((source.prefix + name, tensor) for (name, tensor) in weights_iterator)
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]                                                                  ^^^^^^^^^^^^^^^^
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/model_loader/weight_utils.py", line 708, in safetensors_weights_iterator
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]     with safe_open(st_file, framework="pt") as f:
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:51:44] [ERROR] [0;36m(Worker_TP0_EP0 pid=715)[0;0m ERROR 01-19 22:51:45 [multiproc_executor.py:749] safetensors_rust.SafetensorError: Error while deserializing header: header too large
[2026-01-19 22:51:44] [INFO] [0;36m(Worker_TP1_EP1 pid=716)[0;0m INFO 01-19 22:51:45 [multiproc_executor.py:707] Parent process exited, terminating worker
[2026-01-19 22:51:44] [INFO] [0;36m(Worker_TP0_EP0 pid=715)[0;0m INFO 01-19 22:51:45 [multiproc_executor.py:707] Parent process exited, terminating worker
[2026-01-19 22:51:45] [WARNING] [rank0]:[W119 22:51:46.699813628 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2026-01-19 22:51:49] [ERROR] [0;36m(EngineCore_DP0 pid=598)[0;0m ERROR 01-19 22:51:48 [core.py:936] EngineCore failed to start.
[2026-01-19 22:51:49] [ERROR] [0;36m(EngineCore_DP0 pid=598)[0;0m ERROR 01-19 22:51:48 [core.py:936] Traceback (most recent call last):
[2026-01-19 22:51:49] [ERROR] [0;36m(EngineCore_DP0 pid=598)[0;0m ERROR 01-19 22:51:48 [core.py:936]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 927, in run_engine_core
[2026-01-19 22:51:49] [ERROR] [0;36m(EngineCore_DP0 pid=598)[0;0m ERROR 01-19 22:51:48 [core.py:936]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-19 22:51:49] [ERROR] [0;36m(EngineCore_DP0 pid=598)[0;0m ERROR 01-19 22:51:48 [core.py:936]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:51:49] [ERROR] [0;36m(EngineCore_DP0 pid=598)[0;0m ERROR 01-19 22:51:48 [core.py:936]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 692, in __init__
[2026-01-19 22:51:49] [ERROR] [0;36m(EngineCore_DP0 pid=598)[0;0m ERROR 01-19 22:51:48 [core.py:936]     super().__init__(
[2026-01-19 22:51:49] [ERROR] [0;36m(EngineCore_DP0 pid=598)[0;0m ERROR 01-19 22:51:48 [core.py:936]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 106, in __init__
[2026-01-19 22:51:49] [ERROR] [0;36m(EngineCore_DP0 pid=598)[0;0m ERROR 01-19 22:51:48 [core.py:936]     self.model_executor = executor_class(vllm_config)
[2026-01-19 22:51:49] [ERROR] [0;36m(EngineCore_DP0 pid=598)[0;0m ERROR 01-19 22:51:48 [core.py:936]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:51:49] [ERROR] [0;36m(EngineCore_DP0 pid=598)[0;0m ERROR 01-19 22:51:48 [core.py:936]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[2026-01-19 22:51:49] [ERROR] [0;36m(EngineCore_DP0 pid=598)[0;0m ERROR 01-19 22:51:48 [core.py:936]     super().__init__(vllm_config)
[2026-01-19 22:51:49] [ERROR] [0;36m(EngineCore_DP0 pid=598)[0;0m ERROR 01-19 22:51:48 [core.py:936]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[2026-01-19 22:51:49] [ERROR] [0;36m(EngineCore_DP0 pid=598)[0;0m ERROR 01-19 22:51:48 [core.py:936]     self._init_executor()
[2026-01-19 22:51:49] [ERROR] [0;36m(EngineCore_DP0 pid=598)[0;0m ERROR 01-19 22:51:48 [core.py:936]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 172, in _init_executor
[2026-01-19 22:51:49] [ERROR] [0;36m(EngineCore_DP0 pid=598)[0;0m ERROR 01-19 22:51:48 [core.py:936]     self.workers = WorkerProc.wait_for_ready(unready_workers)
[2026-01-19 22:51:49] [ERROR] [0;36m(EngineCore_DP0 pid=598)[0;0m ERROR 01-19 22:51:48 [core.py:936]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:51:49] [ERROR] [0;36m(EngineCore_DP0 pid=598)[0;0m ERROR 01-19 22:51:48 [core.py:936]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 658, in wait_for_ready
[2026-01-19 22:51:49] [ERROR] [0;36m(EngineCore_DP0 pid=598)[0;0m ERROR 01-19 22:51:48 [core.py:936]     raise e from None
[2026-01-19 22:51:49] [ERROR] [0;36m(EngineCore_DP0 pid=598)[0;0m ERROR 01-19 22:51:48 [core.py:936] Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[2026-01-19 22:51:49] [INFO] [0;36m(EngineCore_DP0 pid=598)[0;0m Process EngineCore_DP0:
[2026-01-19 22:51:49] [ERROR] [0;36m(EngineCore_DP0 pid=598)[0;0m Traceback (most recent call last):
[2026-01-19 22:51:49] [INFO] [0;36m(EngineCore_DP0 pid=598)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[2026-01-19 22:51:49] [INFO] [0;36m(EngineCore_DP0 pid=598)[0;0m     self.run()
[2026-01-19 22:51:49] [INFO] [0;36m(EngineCore_DP0 pid=598)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/multiprocessing/process.py", line 108, in run
[2026-01-19 22:51:49] [INFO] [0;36m(EngineCore_DP0 pid=598)[0;0m     self._target(*self._args, **self._kwargs)
[2026-01-19 22:51:49] [INFO] [0;36m(EngineCore_DP0 pid=598)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 940, in run_engine_core
[2026-01-19 22:51:49] [INFO] [0;36m(EngineCore_DP0 pid=598)[0;0m     raise e
[2026-01-19 22:51:49] [INFO] [0;36m(EngineCore_DP0 pid=598)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 927, in run_engine_core
[2026-01-19 22:51:49] [INFO] [0;36m(EngineCore_DP0 pid=598)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-19 22:51:49] [INFO] [0;36m(EngineCore_DP0 pid=598)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:51:49] [INFO] [0;36m(EngineCore_DP0 pid=598)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 692, in __init__
[2026-01-19 22:51:49] [INFO] [0;36m(EngineCore_DP0 pid=598)[0;0m     super().__init__(
[2026-01-19 22:51:49] [INFO] [0;36m(EngineCore_DP0 pid=598)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 106, in __init__
[2026-01-19 22:51:49] [INFO] [0;36m(EngineCore_DP0 pid=598)[0;0m     self.model_executor = executor_class(vllm_config)
[2026-01-19 22:51:49] [INFO] [0;36m(EngineCore_DP0 pid=598)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:51:49] [INFO] [0;36m(EngineCore_DP0 pid=598)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 97, in __init__
[2026-01-19 22:51:49] [INFO] [0;36m(EngineCore_DP0 pid=598)[0;0m     super().__init__(vllm_config)
[2026-01-19 22:51:49] [INFO] [0;36m(EngineCore_DP0 pid=598)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 101, in __init__
[2026-01-19 22:51:49] [INFO] [0;36m(EngineCore_DP0 pid=598)[0;0m     self._init_executor()
[2026-01-19 22:51:49] [INFO] [0;36m(EngineCore_DP0 pid=598)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 172, in _init_executor
[2026-01-19 22:51:49] [INFO] [0;36m(EngineCore_DP0 pid=598)[0;0m     self.workers = WorkerProc.wait_for_ready(unready_workers)
[2026-01-19 22:51:49] [INFO] [0;36m(EngineCore_DP0 pid=598)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:51:49] [INFO] [0;36m(EngineCore_DP0 pid=598)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 658, in wait_for_ready
[2026-01-19 22:51:49] [INFO] [0;36m(EngineCore_DP0 pid=598)[0;0m     raise e from None
[2026-01-19 22:51:49] [INFO] [0;36m(EngineCore_DP0 pid=598)[0;0m Exception: WorkerProc initialization failed due to an exception in a background process. See stack trace for root cause.
[2026-01-19 22:51:50] [ERROR] [0;36m(APIServer pid=434)[0;0m Traceback (most recent call last):
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/bin/vllm", line 7, in <module>
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m     sys.exit(main())
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m              ^^^^^^
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m     args.dispatch_function(args)
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m     uvloop.run(run_server(args))
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m     return __asyncio.run(
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m            ^^^^^^^^^^^^^^
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m     return runner.run(main)
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m     return self._loop.run_until_complete(task)
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m     return await main
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m            ^^^^^^^^^^
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 1325, in run_server
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 1344, in run_server_worker
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m     async with build_async_engine_client(
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m     return await anext(self.gen)
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 173, in build_async_engine_client
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m     return await anext(self.gen)
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 214, in build_async_engine_client_from_engine_args
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 205, in from_vllm_config
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m     return cls(
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m            ^^^^
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 132, in __init__
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m     return AsyncMPClient(*client_args)
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 824, in __init__
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m     super().__init__(
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/contextlib.py", line 144, in __exit__
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m     next(self.gen)
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 921, in launch_core_engines
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m     wait_for_engine_startup(
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 980, in wait_for_engine_startup
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m     raise RuntimeError(
[2026-01-19 22:51:50] [INFO] [0;36m(APIServer pid=434)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[2026-01-19 22:51:51] [INFO] /home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/multiprocessing/resource_tracker.py:279: UserWarning: resource_tracker: There appear to be 3 leaked shared_memory objects to clean up at shutdown
[2026-01-19 22:51:51] [INFO] warnings.warn('resource_tracker: There appear to be %d '
[2026-01-19 22:51:52] [INFO] nvitop监控已启动
[2026-01-20 09:37:35] [INFO] VLLM GUI 服务器启动，端口: 5002
[2026-01-20 09:37:36] [INFO] nvitop监控已启动
[2026-01-20 09:37:45] [INFO] 从WSL HOME目录自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-20 09:39:15] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-20 09:39:15] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-20 09:39:15] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-20 09:39:15] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-20 09:39:16] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-20 09:39:16] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-20 09:39:16] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-20 09:39:16] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-20 09:39:16] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-20 09:39:16] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-20 09:39:27] [SUCCESS] 方案已保存: GLM-4.7-Flash-GPU0
[2026-01-20 09:39:29] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-20 09:39:29] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-20 09:39:30] [INFO] 启动命令: wsl bash -c "source /etc/profile 2>/dev/null || true && source ~/.bashrc 2>/dev/null || true && sour...
[2026-01-20 09:39:30] [INFO] nvitop监控已启动
[2026-01-20 09:39:43] [ERROR] Traceback (most recent call last):
[2026-01-20 09:39:43] [INFO] File "/home/wuwen-ubu/miniconda3/envs/vllm/bin/vllm", line 7, in <module>
[2026-01-20 09:39:43] [INFO] sys.exit(main())
[2026-01-20 09:39:43] [INFO] ^^^^^^
[2026-01-20 09:39:43] [INFO] File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 70, in main
[2026-01-20 09:39:43] [INFO] cmds[args.subparser].validate(args)
[2026-01-20 09:39:43] [INFO] File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 63, in validate
[2026-01-20 09:39:43] [INFO] validate_parsed_serve_args(args)
[2026-01-20 09:39:43] [INFO] File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/cli_args.py", line 307, in validate_parsed_serve_args
[2026-01-20 09:39:43] [INFO] validate_chat_template(args.chat_template)
[2026-01-20 09:39:43] [INFO] File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/chat_utils.py", line 1199, in validate_chat_template
[2026-01-20 09:39:43] [INFO] raise ValueError(
[2026-01-20 09:39:43] [INFO] ValueError: The supplied chat template string (/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.7-REAP-40-W4A16/chat_template.jinja) appears path-like, but doesn't exist! Tried: /mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.7-REAP-40-W4A16/chat_template.jinja and /mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.7-REAP-40-W4A16/chat_template.jinja
[2026-01-20 09:39:45] [INFO] nvitop监控已启动
[2026-01-20 09:40:22] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-20 09:40:22] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-20 09:40:22] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-20 09:40:22] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-20 09:40:25] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-20 09:40:25] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-20 09:40:25] [INFO] 启动命令: wsl bash -c "source /etc/profile 2>/dev/null || true && source ~/.bashrc 2>/dev/null || true && sour...
[2026-01-20 09:40:25] [INFO] nvitop监控已启动
[2026-01-20 09:40:38] [ERROR] Traceback (most recent call last):
[2026-01-20 09:40:38] [INFO] File "/home/wuwen-ubu/miniconda3/envs/vllm/bin/vllm", line 7, in <module>
[2026-01-20 09:40:38] [INFO] sys.exit(main())
[2026-01-20 09:40:38] [INFO] ^^^^^^
[2026-01-20 09:40:38] [INFO] File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 70, in main
[2026-01-20 09:40:38] [INFO] cmds[args.subparser].validate(args)
[2026-01-20 09:40:38] [INFO] File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 63, in validate
[2026-01-20 09:40:38] [INFO] validate_parsed_serve_args(args)
[2026-01-20 09:40:38] [INFO] File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/cli_args.py", line 307, in validate_parsed_serve_args
[2026-01-20 09:40:38] [INFO] validate_chat_template(args.chat_template)
[2026-01-20 09:40:38] [INFO] File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/chat_utils.py", line 1199, in validate_chat_template
[2026-01-20 09:40:38] [INFO] raise ValueError(
[2026-01-20 09:40:38] [INFO] ValueError: The supplied chat template string (/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.7-Flash/chat_template.jinja) appears path-like, but doesn't exist! Tried: /mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.7-Flash/chat_template.jinja and /mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.7-Flash/chat_template.jinja
[2026-01-20 09:40:40] [INFO] nvitop监控已启动
[2026-01-20 09:41:28] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-20 09:41:28] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-20 09:41:28] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-20 09:41:28] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-20 09:41:29] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-20 09:41:29] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-20 09:41:29] [INFO] 启动命令: wsl bash -c "source /etc/profile 2>/dev/null || true && source ~/.bashrc 2>/dev/null || true && sour...
[2026-01-20 09:41:29] [INFO] nvitop监控已启动
[2026-01-20 09:41:30] [INFO] nvitop监控已停止
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m INFO 01-20 09:41:42 [api_server.py:1278] vLLM API server version 0.14.0rc1.dev538+g2a719e086
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m INFO 01-20 09:41:42 [utils.py:254] non-default args: {'model_tag': '/mnt/i/models/ZhipuAI/GLM-4.7-Flash/', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/i/AI-Chat/models/ZhipuAI/GLM-4.7-Flash/chat_template.jinja', 'enable_auto_tool_choice': True, 'tool_call_parser': 'glm47', 'model': '/mnt/i/models/ZhipuAI/GLM-4.7-Flash/', 'trust_remote_code': True, 'max_model_len': 64000, 'served_model_name': ['VLLM-MODEL'], 'reasoning_parser': 'glm45', 'enable_expert_parallel': True, 'kv_cache_dtype': 'fp8', 'max_num_seqs': 256, 'async_scheduling': True, 'speculative_config': {'method': 'mtp', 'num_speculative_tokens': 1}}
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-20 09:41:42] [ERROR] [0;36m(APIServer pid=436)[0;0m Traceback (most recent call last):
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/transformers/utils/hub.py", line 425, in cached_files
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m     hf_hub_download(
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 85, in _inner_fn
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m     validate_repo_id(arg_value)
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 129, in validate_repo_id
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m     raise HFValidationError(
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/mnt/i/models/ZhipuAI/GLM-4.7-Flash/'. Use `repo_type` argument if needed.
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m The above exception was the direct cause of the following exception:
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m
[2026-01-20 09:41:42] [ERROR] [0;36m(APIServer pid=436)[0;0m Traceback (most recent call last):
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/bin/vllm", line 7, in <module>
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m     sys.exit(main())
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m              ^^^^^^
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m     args.dispatch_function(args)
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m     uvloop.run(run_server(args))
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m     return __asyncio.run(
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m            ^^^^^^^^^^^^^^
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m     return runner.run(main)
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m     return self._loop.run_until_complete(task)
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m     return await main
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m            ^^^^^^^^^^
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 1325, in run_server
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 1344, in run_server_worker
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m     async with build_async_engine_client(
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m     return await anext(self.gen)
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 173, in build_async_engine_client
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m     return await anext(self.gen)
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 199, in build_async_engine_client_from_engine_args
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/engine/arg_utils.py", line 1362, in create_engine_config
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m     maybe_override_with_speculators(
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/transformers_utils/config.py", line 528, in maybe_override_with_speculators
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m     config_dict, _ = PretrainedConfig.get_config_dict(
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/transformers/configuration_utils.py", line 619, in get_config_dict
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m     config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/transformers/configuration_utils.py", line 674, in _get_config_dict
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m     resolved_config_file = cached_file(
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m                            ^^^^^^^^^^^^
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/transformers/utils/hub.py", line 282, in cached_file
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m     file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/transformers/utils/hub.py", line 474, in cached_files
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m     raise OSError(f"{e}") from e
[2026-01-20 09:41:42] [INFO] [0;36m(APIServer pid=436)[0;0m OSError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/mnt/i/models/ZhipuAI/GLM-4.7-Flash/'. Use `repo_type` argument if needed.
[2026-01-20 09:41:45] [INFO] nvitop监控已启动
[2026-01-20 09:42:21] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-20 09:42:21] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-20 09:42:21] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-20 09:42:21] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-20 09:42:23] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-20 09:42:23] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-20 09:42:23] [INFO] 启动命令: wsl bash -c "source /etc/profile 2>/dev/null || true && source ~/.bashrc 2>/dev/null || true && sour...
[2026-01-20 09:42:23] [INFO] nvitop监控已启动
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m INFO 01-20 09:42:36 [api_server.py:1278] vLLM API server version 0.14.0rc1.dev538+g2a719e086
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m INFO 01-20 09:42:36 [utils.py:254] non-default args: {'model_tag': '/mnt/i/AI-Chat/models/ZhipuAI/GLM-4.7-Flash/', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/i/AI-Chat/models/ZhipuAI/GLM-4.7-Flash/chat_template.jinja', 'enable_auto_tool_choice': True, 'tool_call_parser': 'glm47', 'model': '/mnt/i/AI-Chat/models/ZhipuAI/GLM-4.7-Flash/', 'trust_remote_code': True, 'max_model_len': 64000, 'served_model_name': ['VLLM-MODEL'], 'reasoning_parser': 'glm45', 'enable_expert_parallel': True, 'kv_cache_dtype': 'fp8', 'max_num_seqs': 256, 'async_scheduling': True, 'speculative_config': {'method': 'mtp', 'num_speculative_tokens': 1}}
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-20 09:42:36] [ERROR] [0;36m(APIServer pid=421)[0;0m Traceback (most recent call last):
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/bin/vllm", line 7, in <module>
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m     sys.exit(main())
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m              ^^^^^^
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m     args.dispatch_function(args)
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m     uvloop.run(run_server(args))
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m     return __asyncio.run(
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m            ^^^^^^^^^^^^^^
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m     return runner.run(main)
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m     return self._loop.run_until_complete(task)
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m     return await main
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m            ^^^^^^^^^^
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 1325, in run_server
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 1344, in run_server_worker
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m     async with build_async_engine_client(
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m     return await anext(self.gen)
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 173, in build_async_engine_client
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m     return await anext(self.gen)
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 199, in build_async_engine_client_from_engine_args
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/engine/arg_utils.py", line 1371, in create_engine_config
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m     model_config = self.create_model_config()
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/engine/arg_utils.py", line 1225, in create_model_config
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m     return ModelConfig(
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m            ^^^^^^^^^^^^
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/pydantic/_internal/_dataclasses.py", line 121, in __init__
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m   Value error, The checkpoint you are trying to load has model type `glm4_moe_lite` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m You can update Transformers with the command `pip install --upgrade transformers`. If this does not work, and the checkpoint is very new, then there may not be a release version that supports this model yet. In this case, you can get the most up-to-date code by installing Transformers from source with the command `pip install git+https://github.com/huggingface/transformers.git` [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]
[2026-01-20 09:42:36] [INFO] [0;36m(APIServer pid=421)[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error
[2026-01-20 09:42:38] [INFO] nvitop监控已启动
[2026-01-20 09:44:31] [SUCCESS] 正在关闭服务器...
[2026-01-20 09:44:51] [INFO] VLLM GUI 服务器启动，端口: 5002
[2026-01-20 09:44:53] [INFO] nvitop监控已启动
[2026-01-20 09:44:56] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-20 09:44:58] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-20 09:44:58] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-20 09:44:59] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-20 09:44:59] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-20 09:45:00] [INFO] 启动命令: wsl bash -c "source /etc/profile 2>/dev/null || true && source ~/.bashrc 2>/dev/null || true && sour...
[2026-01-20 09:45:00] [INFO] nvitop监控已启动
[2026-01-20 09:45:12] [ERROR] Traceback (most recent call last):
[2026-01-20 09:45:12] [INFO] File "/home/wuwen-ubu/miniconda3/envs/vllm/bin/vllm", line 7, in <module>
[2026-01-20 09:45:12] [INFO] sys.exit(main())
[2026-01-20 09:45:12] [INFO] ^^^^^^
[2026-01-20 09:45:12] [INFO] File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 70, in main
[2026-01-20 09:45:12] [INFO] cmds[args.subparser].validate(args)
[2026-01-20 09:45:12] [INFO] File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 63, in validate
[2026-01-20 09:45:12] [INFO] validate_parsed_serve_args(args)
[2026-01-20 09:45:12] [INFO] File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/cli_args.py", line 307, in validate_parsed_serve_args
[2026-01-20 09:45:12] [INFO] validate_chat_template(args.chat_template)
[2026-01-20 09:45:12] [INFO] File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/chat_utils.py", line 1199, in validate_chat_template
[2026-01-20 09:45:12] [INFO] raise ValueError(
[2026-01-20 09:45:12] [INFO] ValueError: The supplied chat template string (/mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.7-REAP-40-W4A16/chat_template.jinja) appears path-like, but doesn't exist! Tried: /mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.7-REAP-40-W4A16/chat_template.jinja and /mnt/AI-Acer4T/AI-Chat/models/ZhipuAI/GLM-4.7-REAP-40-W4A16/chat_template.jinja
[2026-01-20 09:45:15] [INFO] nvitop监控已启动
[2026-01-20 09:46:48] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-20 09:46:48] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-20 09:47:13] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-20 09:47:13] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-20 09:47:26] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-20 09:47:26] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-20 09:47:26] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-20 09:47:26] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-20 09:47:32] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-20 09:47:32] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-20 09:47:32] [INFO] 启动命令: wsl bash -c "source /etc/profile 2>/dev/null || true && source ~/.bashrc 2>/dev/null || true && sour...
[2026-01-20 09:47:32] [INFO] nvitop监控已启动
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m INFO 01-20 09:47:44 [api_server.py:1278] vLLM API server version 0.14.0rc1.dev538+g2a719e086
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m INFO 01-20 09:47:44 [utils.py:254] non-default args: {'model_tag': '/mnt/i/AI-Chat/models/ZhipuAI/GLM-4.7-Flash/', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/i/AI-Chat/models/ZhipuAI/GLM-4.7-Flash/chat_template.jinja', 'enable_auto_tool_choice': True, 'tool_call_parser': 'glm47', 'model': '/mnt/i/AI-Chat/models/ZhipuAI/GLM-4.7-Flash/', 'trust_remote_code': True, 'max_model_len': 64000, 'served_model_name': ['VLLM-MODEL'], 'reasoning_parser': 'glm45', 'enable_expert_parallel': True, 'kv_cache_dtype': 'fp8', 'max_num_seqs': 256, 'async_scheduling': True, 'speculative_config': {'method': 'mtp', 'num_speculative_tokens': 1}}
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-20 09:47:45] [ERROR] [0;36m(APIServer pid=537)[0;0m Traceback (most recent call last):
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/bin/vllm", line 7, in <module>
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m     sys.exit(main())
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m              ^^^^^^
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m     args.dispatch_function(args)
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m     uvloop.run(run_server(args))
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m     return __asyncio.run(
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m            ^^^^^^^^^^^^^^
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m     return runner.run(main)
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m     return self._loop.run_until_complete(task)
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m     return await main
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m            ^^^^^^^^^^
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 1325, in run_server
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 1344, in run_server_worker
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m     async with build_async_engine_client(
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m     return await anext(self.gen)
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 173, in build_async_engine_client
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m     return await anext(self.gen)
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 199, in build_async_engine_client_from_engine_args
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m     vllm_config = engine_args.create_engine_config(usage_context=usage_context)
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/engine/arg_utils.py", line 1371, in create_engine_config
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m     model_config = self.create_model_config()
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/engine/arg_utils.py", line 1225, in create_model_config
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m     return ModelConfig(
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m            ^^^^^^^^^^^^
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/pydantic/_internal/_dataclasses.py", line 121, in __init__
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m     s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m pydantic_core._pydantic_core.ValidationError: 1 validation error for ModelConfig
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m   Value error, The checkpoint you are trying to load has model type `glm4_moe_lite` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m You can update Transformers with the command `pip install --upgrade transformers`. If this does not work, and the checkpoint is very new, then there may not be a release version that supports this model yet. In this case, you can get the most up-to-date code by installing Transformers from source with the command `pip install git+https://github.com/huggingface/transformers.git` [type=value_error, input_value=ArgsKwargs((), {'model': ...rocessor_plugin': None}), input_type=ArgsKwargs]
[2026-01-20 09:47:45] [INFO] [0;36m(APIServer pid=537)[0;0m     For further information visit https://errors.pydantic.dev/2.12/v/value_error
[2026-01-20 09:47:48] [INFO] nvitop监控已启动
[2026-01-20 09:49:58] [SUCCESS] 正在关闭服务器...
[2026-01-20 10:05:51] [INFO] VLLM GUI 服务器启动，端口: 5002
[2026-01-20 10:05:52] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-20 10:05:52] [INFO] nvitop监控已启动
[2026-01-20 10:05:52] [INFO] nvitop监控已启动
[2026-01-20 10:06:21] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-20 10:06:21] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-20 10:07:43] [SUCCESS] 正在关闭服务器...
[2026-01-20 10:41:04] [INFO] VLLM GUI 服务器启动，端口: 5002
[2026-01-20 10:41:05] [INFO] nvitop监控已启动
[2026-01-20 10:41:08] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-20 10:42:12] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-20 10:42:12] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-20 10:42:45] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-20 10:42:45] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-20 10:43:07] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-20 10:43:07] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-20 10:43:07] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-20 10:43:07] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-20 10:43:14] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-20 10:43:14] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-20 10:43:18] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-20 10:43:18] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-20 10:43:30] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-20 10:43:30] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-20 10:43:34] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-20 10:43:34] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-20 10:43:35] [INFO] 启动命令: wsl bash -c "source /etc/profile 2>/dev/null || true && source ~/.bashrc 2>/dev/null || true && sour...
[2026-01-20 10:43:35] [INFO] nvitop监控已启动
[2026-01-20 10:43:35] [INFO] nvitop监控已停止
[2026-01-20 10:43:48] [INFO] [0;36m(APIServer pid=619)[0;0m INFO 01-20 10:43:47 [api_server.py:872] vLLM API server version 0.14.0rc2.dev159+g05dc4bfab
[2026-01-20 10:43:48] [INFO] [0;36m(APIServer pid=619)[0;0m INFO 01-20 10:43:47 [utils.py:267] non-default args: {'model_tag': '/mnt/i/AI-Chat/models/ZhipuAI/GLM-4.7-Flash/', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/i/AI-Chat/models/ZhipuAI/GLM-4.7-Flash/chat_template.jinja', 'enable_auto_tool_choice': True, 'tool_call_parser': 'glm47', 'model': '/mnt/i/AI-Chat/models/ZhipuAI/GLM-4.7-Flash/', 'trust_remote_code': True, 'max_model_len': 128000, 'served_model_name': ['VLLM-MODEL'], 'reasoning_parser': 'glm45', 'tensor_parallel_size': 2, 'enable_expert_parallel': True, 'max_num_seqs': 256, 'async_scheduling': True}
[2026-01-20 10:43:48] [INFO] [0;36m(APIServer pid=619)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-20 10:43:48] [INFO] [0;36m(APIServer pid=619)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-20 10:43:48] [INFO] [0;36m(APIServer pid=619)[0;0m INFO 01-20 10:43:47 [model.py:530] Resolved architecture: Glm4MoeLiteForCausalLM
[2026-01-20 10:43:48] [INFO] [0;36m(APIServer pid=619)[0;0m INFO 01-20 10:43:47 [model.py:1547] Using max model len 128000
[2026-01-20 10:43:48] [INFO] [0;36m(APIServer pid=619)[0;0m INFO 01-20 10:43:47 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[2026-01-20 10:43:48] [INFO] [0;36m(APIServer pid=619)[0;0m INFO 01-20 10:43:47 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-20 10:43:48] [INFO] [0;36m(APIServer pid=619)[0;0m INFO 01-20 10:43:47 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-20 10:43:50] [WARNING] [0;36m(APIServer pid=619)[0;0m WARNING 01-20 10:43:49 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[2026-01-20 10:43:58] [INFO] [0;36m(EngineCore_DP0 pid=790)[0;0m INFO 01-20 10:43:58 [core.py:96] Initializing a V1 LLM engine (v0.14.0rc2.dev159+g05dc4bfab) with config: model='/mnt/i/AI-Chat/models/ZhipuAI/GLM-4.7-Flash/', speculative_config=None, tokenizer='/mnt/i/AI-Chat/models/ZhipuAI/GLM-4.7-Flash/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='glm45', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-20 10:43:58] [WARNING] [0;36m(EngineCore_DP0 pid=790)[0;0m WARNING 01-20 10:43:58 [multiproc_executor.py:897] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-20 10:44:06] [INFO] INFO 01-20 10:44:07 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:48201 backend=nccl
[2026-01-20 10:44:06] [INFO] INFO 01-20 10:44:07 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:48201 backend=nccl
[2026-01-20 10:44:07] [INFO] INFO 01-20 10:44:07 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-20 10:44:08] [WARNING] WARNING 01-20 10:44:06 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-20 10:44:08] [WARNING] WARNING 01-20 10:44:06 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-20 10:44:08] [WARNING] WARNING 01-20 10:44:06 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2026-01-20 10:44:08] [WARNING] WARNING 01-20 10:44:06 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2026-01-20 10:44:08] [INFO] INFO 01-20 10:44:07 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[2026-01-20 10:44:08] [INFO] INFO 01-20 10:44:07 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
[2026-01-20 10:44:08] [WARNING] WARNING 01-20 10:44:07 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[2026-01-20 10:44:08] [WARNING] WARNING 01-20 10:44:07 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[2026-01-20 10:44:09] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m INFO 01-20 10:44:08 [gpu_model_runner.py:3825] Starting to load model /mnt/i/AI-Chat/models/ZhipuAI/GLM-4.7-Flash/...
[2026-01-20 10:44:10] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m INFO 01-20 10:44:08 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[2026-01-20 10:44:10] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m INFO 01-20 10:44:09 [layer.py:472] [EP Rank 0/2] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 32/64. Experts local to global index map: 0->0, 1->1, 2->2, 3->3, 4->4, 5->5, 6->6, 7->7, 8->8, 9->9, 10->10, 11->11, 12->12, 13->13, 14->14, 15->15, 16->16, 17->17, 18->18, 19->19, 20->20, 21->21, 22->22, 23->23, 24->24, 25->25, 26->26, 27->27, 28->28, 29->29, 30->30, 31->31.
[2026-01-20 10:44:10] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m INFO 01-20 10:44:09 [unquantized.py:82] FlashInfer CUTLASS MoE is available for EP but not enabled, consider setting VLLM_USE_FLASHINFER_MOE_FP16=1 to enable it.
[2026-01-20 10:44:10] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m INFO 01-20 10:44:09 [unquantized.py:103] Using TRITON backend for Unquantized MoE
[2026-01-20 10:44:10] [INFO] [0;36m(Worker_TP1_EP1 pid=907)[0;0m INFO 01-20 10:44:09 [layer.py:472] [EP Rank 1/2] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 32/64. Experts local to global index map: 0->32, 1->33, 2->34, 3->35, 4->36, 5->37, 6->38, 7->39, 8->40, 9->41, 10->42, 11->43, 12->44, 13->45, 14->46, 15->47, 16->48, 17->49, 18->50, 19->51, 20->52, 21->53, 22->54, 23->55, 24->56, 25->57, 26->58, 27->59, 28->60, 29->61, 30->62, 31->63.
[2026-01-20 10:44:11] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:44:11] [INFO] Loading safetensors checkpoint shards:   0% Completed | 0/48 [00:00<?, ?it/s]
[2026-01-20 10:44:13] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:44:13] [INFO] Loading safetensors checkpoint shards:   2% Completed | 1/48 [00:02<01:59,  2.54s/it]
[2026-01-20 10:44:17] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:44:17] [INFO] Loading safetensors checkpoint shards:   4% Completed | 2/48 [00:06<02:31,  3.29s/it]
[2026-01-20 10:44:20] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:44:20] [INFO] Loading safetensors checkpoint shards:   6% Completed | 3/48 [00:10<02:41,  3.59s/it]
[2026-01-20 10:44:24] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:44:24] [INFO] Loading safetensors checkpoint shards:   8% Completed | 4/48 [00:14<02:40,  3.64s/it]
[2026-01-20 10:44:27] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:44:27] [INFO] Loading safetensors checkpoint shards:  10% Completed | 5/48 [00:17<02:37,  3.67s/it]
[2026-01-20 10:44:31] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:44:31] [INFO] Loading safetensors checkpoint shards:  12% Completed | 6/48 [00:21<02:37,  3.75s/it]
[2026-01-20 10:44:34] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:44:34] [INFO] Loading safetensors checkpoint shards:  15% Completed | 7/48 [00:25<02:31,  3.69s/it]
[2026-01-20 10:44:38] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:44:38] [INFO] Loading safetensors checkpoint shards:  17% Completed | 8/48 [00:29<02:28,  3.72s/it]
[2026-01-20 10:44:42] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:44:42] [INFO] Loading safetensors checkpoint shards:  19% Completed | 9/48 [00:31<02:06,  3.25s/it]
[2026-01-20 10:44:46] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:44:46] [INFO] Loading safetensors checkpoint shards:  21% Completed | 10/48 [00:35<02:11,  3.47s/it]
[2026-01-20 10:44:49] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:44:49] [INFO] Loading safetensors checkpoint shards:  23% Completed | 11/48 [00:39<02:14,  3.62s/it]
[2026-01-20 10:44:53] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:44:53] [INFO] Loading safetensors checkpoint shards:  25% Completed | 12/48 [00:43<02:14,  3.73s/it]
[2026-01-20 10:44:57] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:44:57] [INFO] Loading safetensors checkpoint shards:  27% Completed | 13/48 [00:47<02:13,  3.80s/it]
[2026-01-20 10:45:00] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:45:00] [INFO] Loading safetensors checkpoint shards:  29% Completed | 14/48 [00:51<02:10,  3.84s/it]
[2026-01-20 10:45:04] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:45:04] [INFO] Loading safetensors checkpoint shards:  31% Completed | 15/48 [00:54<02:07,  3.86s/it]
[2026-01-20 10:45:08] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:45:08] [INFO] Loading safetensors checkpoint shards:  33% Completed | 16/48 [00:58<02:04,  3.89s/it]
[2026-01-20 10:45:12] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:45:12] [INFO] Loading safetensors checkpoint shards:  35% Completed | 17/48 [01:01<01:45,  3.40s/it]
[2026-01-20 10:45:15] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:45:15] [INFO] Loading safetensors checkpoint shards:  38% Completed | 18/48 [01:05<01:46,  3.54s/it]
[2026-01-20 10:45:19] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:45:19] [INFO] Loading safetensors checkpoint shards:  40% Completed | 19/48 [01:08<01:46,  3.66s/it]
[2026-01-20 10:45:23] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:45:23] [INFO] Loading safetensors checkpoint shards:  42% Completed | 20/48 [01:12<01:45,  3.76s/it]
[2026-01-20 10:45:27] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:45:27] [INFO] Loading safetensors checkpoint shards:  44% Completed | 21/48 [01:17<01:44,  3.86s/it]
[2026-01-20 10:45:30] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:45:30] [INFO] Loading safetensors checkpoint shards:  46% Completed | 22/48 [01:21<01:41,  3.90s/it]
[2026-01-20 10:45:34] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:45:34] [INFO] Loading safetensors checkpoint shards:  48% Completed | 23/48 [01:24<01:37,  3.89s/it]
[2026-01-20 10:45:38] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:45:38] [INFO] Loading safetensors checkpoint shards:  50% Completed | 24/48 [01:28<01:33,  3.89s/it]
[2026-01-20 10:45:42] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:45:42] [INFO] Loading safetensors checkpoint shards:  52% Completed | 25/48 [01:31<01:18,  3.41s/it]
[2026-01-20 10:45:45] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:45:45] [INFO] Loading safetensors checkpoint shards:  54% Completed | 26/48 [01:35<01:18,  3.56s/it]
[2026-01-20 10:45:49] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:45:49] [INFO] Loading safetensors checkpoint shards:  56% Completed | 27/48 [01:38<01:16,  3.62s/it]
[2026-01-20 10:45:52] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:45:52] [INFO] Loading safetensors checkpoint shards:  58% Completed | 28/48 [01:42<01:12,  3.62s/it]
[2026-01-20 10:45:56] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:45:56] [INFO] Loading safetensors checkpoint shards:  60% Completed | 29/48 [01:46<01:10,  3.71s/it]
[2026-01-20 10:46:00] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:46:00] [INFO] Loading safetensors checkpoint shards:  62% Completed | 30/48 [01:50<01:08,  3.80s/it]
[2026-01-20 10:46:03] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:46:03] [INFO] Loading safetensors checkpoint shards:  65% Completed | 31/48 [01:54<01:04,  3.80s/it]
[2026-01-20 10:46:07] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:46:07] [INFO] Loading safetensors checkpoint shards:  67% Completed | 32/48 [01:57<00:59,  3.74s/it]
[2026-01-20 10:46:10] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:46:10] [INFO] Loading safetensors checkpoint shards:  69% Completed | 33/48 [02:01<00:56,  3.74s/it]
[2026-01-20 10:46:14] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:46:14] [INFO] Loading safetensors checkpoint shards:  71% Completed | 34/48 [02:03<00:46,  3.30s/it]
[2026-01-20 10:46:18] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:46:18] [INFO] Loading safetensors checkpoint shards:  73% Completed | 35/48 [02:07<00:44,  3.44s/it]
[2026-01-20 10:46:21] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:46:21] [INFO] Loading safetensors checkpoint shards:  75% Completed | 36/48 [02:11<00:41,  3.49s/it]
[2026-01-20 10:46:25] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:46:25] [INFO] Loading safetensors checkpoint shards:  77% Completed | 37/48 [02:14<00:38,  3.53s/it]
[2026-01-20 10:46:28] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:46:28] [INFO] Loading safetensors checkpoint shards:  79% Completed | 38/48 [02:18<00:35,  3.59s/it]
[2026-01-20 10:46:32] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:46:32] [INFO] Loading safetensors checkpoint shards:  81% Completed | 39/48 [02:22<00:32,  3.64s/it]
[2026-01-20 10:46:35] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:46:35] [INFO] Loading safetensors checkpoint shards:  83% Completed | 40/48 [02:25<00:29,  3.63s/it]
[2026-01-20 10:46:39] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:46:39] [INFO] Loading safetensors checkpoint shards:  85% Completed | 41/48 [02:29<00:25,  3.61s/it]
[2026-01-20 10:46:42] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:46:42] [INFO] Loading safetensors checkpoint shards:  88% Completed | 42/48 [02:31<00:18,  3.13s/it]
[2026-01-20 10:46:45] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:46:45] [INFO] Loading safetensors checkpoint shards:  90% Completed | 43/48 [02:34<00:16,  3.27s/it]
[2026-01-20 10:46:49] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:46:49] [INFO] Loading safetensors checkpoint shards:  92% Completed | 44/48 [02:38<00:13,  3.36s/it]
[2026-01-20 10:46:52] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:46:52] [INFO] Loading safetensors checkpoint shards:  94% Completed | 45/48 [02:42<00:10,  3.45s/it]
[2026-01-20 10:46:56] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:46:56] [INFO] Loading safetensors checkpoint shards:  96% Completed | 46/48 [02:45<00:07,  3.51s/it]
[2026-01-20 10:47:01] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:47:01] [INFO] Loading safetensors checkpoint shards:  98% Completed | 47/48 [02:51<00:04,  4.06s/it]
[2026-01-20 10:47:01] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:47:01] [INFO] Loading safetensors checkpoint shards: 100% Completed | 48/48 [02:51<00:00,  2.89s/it]
[2026-01-20 10:47:01] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:47:01] [INFO] Loading safetensors checkpoint shards: 100% Completed | 48/48 [02:51<00:00,  3.57s/it]
[2026-01-20 10:47:01] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m
[2026-01-20 10:47:01] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m INFO 01-20 10:47:01 [default_loader.py:291] Loading weights took 179.60 seconds
[2026-01-20 10:47:01] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m INFO 01-20 10:47:01 [gpu_model_runner.py:3922] Model loading took 28.07 GiB memory and 181.064289 seconds
[2026-01-20 10:47:18] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m INFO 01-20 10:47:17 [backends.py:644] Using cache directory: /home/wuwen-ubu/.cache/vllm/torch_compile_cache/caffcfc8fb/rank_0_0/backbone for vLLM's torch.compile
[2026-01-20 10:47:18] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m INFO 01-20 10:47:17 [backends.py:704] Dynamo bytecode transform time: 12.92 s
[2026-01-20 10:47:34] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m INFO 01-20 10:47:34 [backends.py:261] Cache the graph of compile range (1, 8192) for later use
[2026-01-20 10:47:34] [INFO] [0;36m(Worker_TP1_EP1 pid=907)[0;0m INFO 01-20 10:47:34 [backends.py:261] Cache the graph of compile range (1, 8192) for later use
[2026-01-20 10:47:38] [INFO] [0;36m(Worker_TP1_EP1 pid=907)[0;0m /home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:312: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
[2026-01-20 10:47:38] [INFO] [0;36m(Worker_TP1_EP1 pid=907)[0;0m   warnings.warn(
[2026-01-20 10:47:38] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m /home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:312: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
[2026-01-20 10:47:38] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m   warnings.warn(
[2026-01-20 10:47:40] [WARNING] [0;36m(Worker_TP0_EP0 pid=906)[0;0m WARNING 01-20 10:47:40 [fused_moe.py:1084] Using default MoE config. Performance might be sub-optimal! Config file not found at /home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/configs/E=32,N=1536,device_name=NVIDIA_RTX_PRO_6000_Blackwell_Workstation_Edition.json
[2026-01-20 10:48:00] [INFO] [0;36m(EngineCore_DP0 pid=790)[0;0m INFO 01-20 10:48:00 [shm_broadcast.py:542] No available shared memory broadcast block found in 60 seconds. This typically happens when some processes are hanging or doing some time-consuming work (e.g. compilation, weight/kv cache quantization).
[2026-01-20 10:48:32] [SUCCESS] 方案已保存: GLM-4.7-Flash-TP2
[2026-01-20 10:48:53] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m INFO 01-20 10:48:52 [backends.py:278] Compiling a graph for compile range (1, 8192) takes 83.61 s
[2026-01-20 10:48:53] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m INFO 01-20 10:48:52 [monitor.py:34] torch.compile takes 96.53 s in total
[2026-01-20 10:48:55] [INFO] [0;36m(Worker_TP0_EP0 pid=906)[0;0m INFO 01-20 10:48:54 [gpu_worker.py:355] Available KV cache memory: 56.19 GiB
[2026-01-20 10:48:55] [ERROR] [0;36m(EngineCore_DP0 pid=790)[0;0m ERROR 01-20 10:48:54 [core.py:935] EngineCore failed to start.
[2026-01-20 10:48:55] [ERROR] [0;36m(EngineCore_DP0 pid=790)[0;0m ERROR 01-20 10:48:54 [core.py:935] Traceback (most recent call last):
[2026-01-20 10:48:55] [ERROR] [0;36m(EngineCore_DP0 pid=790)[0;0m ERROR 01-20 10:48:54 [core.py:935]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-20 10:48:55] [ERROR] [0;36m(EngineCore_DP0 pid=790)[0;0m ERROR 01-20 10:48:54 [core.py:935]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-20 10:48:55] [ERROR] [0;36m(EngineCore_DP0 pid=790)[0;0m ERROR 01-20 10:48:54 [core.py:935]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 10:48:55] [ERROR] [0;36m(EngineCore_DP0 pid=790)[0;0m ERROR 01-20 10:48:54 [core.py:935]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-20 10:48:55] [ERROR] [0;36m(EngineCore_DP0 pid=790)[0;0m ERROR 01-20 10:48:54 [core.py:935]     super().__init__(
[2026-01-20 10:48:55] [ERROR] [0;36m(EngineCore_DP0 pid=790)[0;0m ERROR 01-20 10:48:54 [core.py:935]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 112, in __init__
[2026-01-20 10:48:55] [ERROR] [0;36m(EngineCore_DP0 pid=790)[0;0m ERROR 01-20 10:48:54 [core.py:935]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[2026-01-20 10:48:55] [ERROR] [0;36m(EngineCore_DP0 pid=790)[0;0m ERROR 01-20 10:48:54 [core.py:935]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 10:48:55] [ERROR] [0;36m(EngineCore_DP0 pid=790)[0;0m ERROR 01-20 10:48:54 [core.py:935]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 253, in _initialize_kv_caches
[2026-01-20 10:48:55] [ERROR] [0;36m(EngineCore_DP0 pid=790)[0;0m ERROR 01-20 10:48:54 [core.py:935]     kv_cache_configs = get_kv_cache_configs(
[2026-01-20 10:48:55] [ERROR] [0;36m(EngineCore_DP0 pid=790)[0;0m ERROR 01-20 10:48:54 [core.py:935]                        ^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 10:48:55] [ERROR] [0;36m(EngineCore_DP0 pid=790)[0;0m ERROR 01-20 10:48:54 [core.py:935]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/core/kv_cache_utils.py", line 1516, in get_kv_cache_configs
[2026-01-20 10:48:55] [ERROR] [0;36m(EngineCore_DP0 pid=790)[0;0m ERROR 01-20 10:48:54 [core.py:935]     _check_enough_kv_cache_memory(
[2026-01-20 10:48:55] [ERROR] [0;36m(EngineCore_DP0 pid=790)[0;0m ERROR 01-20 10:48:54 [core.py:935]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/core/kv_cache_utils.py", line 634, in _check_enough_kv_cache_memory
[2026-01-20 10:48:55] [ERROR] [0;36m(EngineCore_DP0 pid=790)[0;0m ERROR 01-20 10:48:54 [core.py:935]     raise ValueError(
[2026-01-20 10:48:55] [ERROR] [0;36m(EngineCore_DP0 pid=790)[0;0m ERROR 01-20 10:48:54 [core.py:935] ValueError: To serve at least one request with the models's max seq len (128000), (57.37 GiB KV cache is needed, which is larger than the available KV cache memory (56.19 GiB). Based on the available memory, the estimated maximum model length is 125344. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[2026-01-20 10:48:59] [ERROR] [0;36m(EngineCore_DP0 pid=790)[0;0m ERROR 01-20 10:48:58 [multiproc_executor.py:246] Worker proc VllmWorker-1 died unexpectedly, shutting down executor.
[2026-01-20 10:48:59] [INFO] [0;36m(EngineCore_DP0 pid=790)[0;0m Process EngineCore_DP0:
[2026-01-20 10:48:59] [ERROR] [0;36m(EngineCore_DP0 pid=790)[0;0m Traceback (most recent call last):
[2026-01-20 10:48:59] [INFO] [0;36m(EngineCore_DP0 pid=790)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[2026-01-20 10:48:59] [INFO] [0;36m(EngineCore_DP0 pid=790)[0;0m     self.run()
[2026-01-20 10:48:59] [INFO] [0;36m(EngineCore_DP0 pid=790)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/multiprocessing/process.py", line 108, in run
[2026-01-20 10:48:59] [INFO] [0;36m(EngineCore_DP0 pid=790)[0;0m     self._target(*self._args, **self._kwargs)
[2026-01-20 10:48:59] [INFO] [0;36m(EngineCore_DP0 pid=790)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 939, in run_engine_core
[2026-01-20 10:48:59] [INFO] [0;36m(EngineCore_DP0 pid=790)[0;0m     raise e
[2026-01-20 10:48:59] [INFO] [0;36m(EngineCore_DP0 pid=790)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-20 10:48:59] [INFO] [0;36m(EngineCore_DP0 pid=790)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-20 10:48:59] [INFO] [0;36m(EngineCore_DP0 pid=790)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 10:48:59] [INFO] [0;36m(EngineCore_DP0 pid=790)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-20 10:48:59] [INFO] [0;36m(EngineCore_DP0 pid=790)[0;0m     super().__init__(
[2026-01-20 10:48:59] [INFO] [0;36m(EngineCore_DP0 pid=790)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 112, in __init__
[2026-01-20 10:48:59] [INFO] [0;36m(EngineCore_DP0 pid=790)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[2026-01-20 10:48:59] [INFO] [0;36m(EngineCore_DP0 pid=790)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 10:48:59] [INFO] [0;36m(EngineCore_DP0 pid=790)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 253, in _initialize_kv_caches
[2026-01-20 10:48:59] [INFO] [0;36m(EngineCore_DP0 pid=790)[0;0m     kv_cache_configs = get_kv_cache_configs(
[2026-01-20 10:48:59] [INFO] [0;36m(EngineCore_DP0 pid=790)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 10:48:59] [INFO] [0;36m(EngineCore_DP0 pid=790)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/core/kv_cache_utils.py", line 1516, in get_kv_cache_configs
[2026-01-20 10:48:59] [INFO] [0;36m(EngineCore_DP0 pid=790)[0;0m     _check_enough_kv_cache_memory(
[2026-01-20 10:48:59] [INFO] [0;36m(EngineCore_DP0 pid=790)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/core/kv_cache_utils.py", line 634, in _check_enough_kv_cache_memory
[2026-01-20 10:48:59] [INFO] [0;36m(EngineCore_DP0 pid=790)[0;0m     raise ValueError(
[2026-01-20 10:48:59] [INFO] [0;36m(EngineCore_DP0 pid=790)[0;0m ValueError: To serve at least one request with the models's max seq len (128000), (57.37 GiB KV cache is needed, which is larger than the available KV cache memory (56.19 GiB). Based on the available memory, the estimated maximum model length is 125344. Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[2026-01-20 10:49:01] [ERROR] [0;36m(APIServer pid=619)[0;0m Traceback (most recent call last):
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/bin/vllm", line 7, in <module>
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m     sys.exit(main())
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m              ^^^^^^
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m     args.dispatch_function(args)
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m     uvloop.run(run_server(args))
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m     return __asyncio.run(
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m            ^^^^^^^^^^^^^^
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m     return runner.run(main)
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m     return self._loop.run_until_complete(task)
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m     return await main
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m            ^^^^^^^^^^
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m     async with build_async_engine_client(
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m     return await anext(self.gen)
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 146, in build_async_engine_client
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m     return await anext(self.gen)
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 187, in build_async_engine_client_from_engine_args
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 202, in from_vllm_config
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m     return cls(
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m            ^^^^
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 129, in __init__
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m     return AsyncMPClient(*client_args)
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m     super().__init__(
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/contextlib.py", line 144, in __exit__
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m     next(self.gen)
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 918, in launch_core_engines
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m     wait_for_engine_startup(
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 977, in wait_for_engine_startup
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m     raise RuntimeError(
[2026-01-20 10:49:01] [INFO] [0;36m(APIServer pid=619)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[2026-01-20 10:49:03] [INFO] nvitop监控已启动
[2026-01-20 10:49:37] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-20 10:49:37] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-20 10:49:39] [INFO] 从WSL用户路径自动检测到conda路径: /home/wuwen-ubu/miniconda3
[2026-01-20 10:49:39] [WARNING] User-provided conda path invalid: /mnt/AI-Acer4T/miniconda3, using auto-detected: /home/wuwen-ubu/miniconda3
[2026-01-20 10:49:39] [INFO] 启动命令: wsl bash -c "source /etc/profile 2>/dev/null || true && source ~/.bashrc 2>/dev/null || true && sour...
[2026-01-20 10:49:39] [INFO] nvitop监控已启动
[2026-01-20 10:49:40] [INFO] nvitop监控已停止
[2026-01-20 10:49:52] [INFO] [0;36m(APIServer pid=432)[0;0m INFO 01-20 10:49:52 [api_server.py:872] vLLM API server version 0.14.0rc2.dev159+g05dc4bfab
[2026-01-20 10:49:52] [INFO] [0;36m(APIServer pid=432)[0;0m INFO 01-20 10:49:52 [utils.py:267] non-default args: {'model_tag': '/mnt/i/AI-Chat/models/ZhipuAI/GLM-4.7-Flash/', 'host': '0.0.0.0', 'port': 8005, 'chat_template': '/mnt/i/AI-Chat/models/ZhipuAI/GLM-4.7-Flash/chat_template.jinja', 'enable_auto_tool_choice': True, 'tool_call_parser': 'glm47', 'model': '/mnt/i/AI-Chat/models/ZhipuAI/GLM-4.7-Flash/', 'trust_remote_code': True, 'max_model_len': 128000, 'served_model_name': ['VLLM-MODEL'], 'reasoning_parser': 'glm45', 'tensor_parallel_size': 2, 'enable_expert_parallel': True, 'kv_cache_dtype': 'fp8', 'max_num_seqs': 256, 'async_scheduling': True}
[2026-01-20 10:49:52] [INFO] [0;36m(APIServer pid=432)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-20 10:49:52] [INFO] [0;36m(APIServer pid=432)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[2026-01-20 10:49:52] [INFO] [0;36m(APIServer pid=432)[0;0m INFO 01-20 10:49:52 [model.py:530] Resolved architecture: Glm4MoeLiteForCausalLM
[2026-01-20 10:49:52] [INFO] [0;36m(APIServer pid=432)[0;0m INFO 01-20 10:49:52 [model.py:1547] Using max model len 128000
[2026-01-20 10:49:52] [INFO] [0;36m(APIServer pid=432)[0;0m INFO 01-20 10:49:52 [cache.py:206] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor.
[2026-01-20 10:49:52] [INFO] [0;36m(APIServer pid=432)[0;0m INFO 01-20 10:49:52 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.
[2026-01-20 10:49:52] [INFO] [0;36m(APIServer pid=432)[0;0m INFO 01-20 10:49:52 [vllm.py:618] Asynchronous scheduling is enabled.
[2026-01-20 10:49:52] [INFO] [0;36m(APIServer pid=432)[0;0m INFO 01-20 10:49:52 [vllm.py:625] Disabling NCCL for DP synchronization when using async scheduling.
[2026-01-20 10:49:54] [WARNING] [0;36m(APIServer pid=432)[0;0m WARNING 01-20 10:49:54 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[2026-01-20 10:50:02] [INFO] [0;36m(EngineCore_DP0 pid=601)[0;0m INFO 01-20 10:50:02 [core.py:96] Initializing a V1 LLM engine (v0.14.0rc2.dev159+g05dc4bfab) with config: model='/mnt/i/AI-Chat/models/ZhipuAI/GLM-4.7-Flash/', speculative_config=None, tokenizer='/mnt/i/AI-Chat/models/ZhipuAI/GLM-4.7-Flash/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=128000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=fp8, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='glm45', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=VLLM-MODEL, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}
[2026-01-20 10:50:02] [WARNING] [0;36m(EngineCore_DP0 pid=601)[0;0m WARNING 01-20 10:50:02 [multiproc_executor.py:897] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[2026-01-20 10:50:10] [INFO] INFO 01-20 10:50:09 [parallel_state.py:1212] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:53235 backend=nccl
[2026-01-20 10:50:10] [INFO] INFO 01-20 10:50:09 [parallel_state.py:1212] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:53235 backend=nccl
[2026-01-20 10:50:11] [INFO] INFO 01-20 10:50:10 [pynccl.py:111] vLLM is using nccl==2.27.5
[2026-01-20 10:50:12] [WARNING] WARNING 01-20 10:50:11 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-20 10:50:12] [WARNING] WARNING 01-20 10:50:11 [symm_mem.py:67] SymmMemCommunicator: Device capability 12.0 not supported, communicator is not available.
[2026-01-20 10:50:12] [WARNING] WARNING 01-20 10:50:11 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2026-01-20 10:50:12] [WARNING] WARNING 01-20 10:50:11 [custom_all_reduce.py:165] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.
[2026-01-20 10:50:12] [INFO] INFO 01-20 10:50:11 [parallel_state.py:1423] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
[2026-01-20 10:50:12] [INFO] INFO 01-20 10:50:11 [parallel_state.py:1423] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[2026-01-20 10:50:12] [WARNING] WARNING 01-20 10:50:11 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[2026-01-20 10:50:12] [WARNING] WARNING 01-20 10:50:11 [interface.py:470] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
[2026-01-20 10:50:13] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m INFO 01-20 10:50:12 [gpu_model_runner.py:3825] Starting to load model /mnt/i/AI-Chat/models/ZhipuAI/GLM-4.7-Flash/...
[2026-01-20 10:50:13] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m INFO 01-20 10:50:12 [cuda.py:351] Using FLASHINFER attention backend out of potential backends: ('FLASHINFER', 'TRITON_ATTN')
[2026-01-20 10:50:14] [INFO] [0;36m(Worker_TP1_EP1 pid=717)[0;0m INFO 01-20 10:50:13 [layer.py:472] [EP Rank 1/2] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 32/64. Experts local to global index map: 0->32, 1->33, 2->34, 3->35, 4->36, 5->37, 6->38, 7->39, 8->40, 9->41, 10->42, 11->43, 12->44, 13->45, 14->46, 15->47, 16->48, 17->49, 18->50, 19->51, 20->52, 21->53, 22->54, 23->55, 24->56, 25->57, 26->58, 27->59, 28->60, 29->61, 30->62, 31->63.
[2026-01-20 10:50:14] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m INFO 01-20 10:50:13 [layer.py:472] [EP Rank 0/2] Expert parallelism is enabled. Expert placement strategy: linear. Local/global number of experts: 32/64. Experts local to global index map: 0->0, 1->1, 2->2, 3->3, 4->4, 5->5, 6->6, 7->7, 8->8, 9->9, 10->10, 11->11, 12->12, 13->13, 14->14, 15->15, 16->16, 17->17, 18->18, 19->19, 20->20, 21->21, 22->22, 23->23, 24->24, 25->25, 26->26, 27->27, 28->28, 29->29, 30->30, 31->31.
[2026-01-20 10:50:14] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m INFO 01-20 10:50:13 [unquantized.py:82] FlashInfer CUTLASS MoE is available for EP but not enabled, consider setting VLLM_USE_FLASHINFER_MOE_FP16=1 to enable it.
[2026-01-20 10:50:14] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m INFO 01-20 10:50:13 [unquantized.py:103] Using TRITON backend for Unquantized MoE
[2026-01-20 10:50:14] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:50:14] [INFO] Loading safetensors checkpoint shards:   0% Completed | 0/48 [00:00<?, ?it/s]
[2026-01-20 10:50:17] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:50:17] [INFO] Loading safetensors checkpoint shards:   2% Completed | 1/48 [00:02<01:55,  2.46s/it]
[2026-01-20 10:50:20] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:50:20] [INFO] Loading safetensors checkpoint shards:   4% Completed | 2/48 [00:06<02:26,  3.18s/it]
[2026-01-20 10:50:23] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:50:23] [INFO] Loading safetensors checkpoint shards:   6% Completed | 3/48 [00:09<02:32,  3.40s/it]
[2026-01-20 10:50:27] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:50:27] [INFO] Loading safetensors checkpoint shards:   8% Completed | 4/48 [00:13<02:37,  3.58s/it]
[2026-01-20 10:50:31] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:50:31] [INFO] Loading safetensors checkpoint shards:  10% Completed | 5/48 [00:17<02:39,  3.70s/it]
[2026-01-20 10:50:34] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:50:34] [INFO] Loading safetensors checkpoint shards:  12% Completed | 6/48 [00:21<02:35,  3.70s/it]
[2026-01-20 10:50:38] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:50:38] [INFO] Loading safetensors checkpoint shards:  15% Completed | 7/48 [00:23<02:08,  3.13s/it]
[2026-01-20 10:50:41] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:50:41] [INFO] Loading safetensors checkpoint shards:  17% Completed | 8/48 [00:26<02:11,  3.29s/it]
[2026-01-20 10:50:45] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:50:45] [INFO] Loading safetensors checkpoint shards:  19% Completed | 9/48 [00:30<02:09,  3.32s/it]
[2026-01-20 10:50:48] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:50:48] [INFO] Loading safetensors checkpoint shards:  21% Completed | 10/48 [00:33<02:09,  3.42s/it]
[2026-01-20 10:50:51] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:50:51] [INFO] Loading safetensors checkpoint shards:  23% Completed | 11/48 [00:37<02:09,  3.50s/it]
[2026-01-20 10:50:55] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:50:55] [INFO] Loading safetensors checkpoint shards:  25% Completed | 12/48 [00:41<02:07,  3.53s/it]
[2026-01-20 10:50:58] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:50:58] [INFO] Loading safetensors checkpoint shards:  27% Completed | 13/48 [00:45<02:07,  3.65s/it]
[2026-01-20 10:51:02] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:51:02] [INFO] Loading safetensors checkpoint shards:  29% Completed | 14/48 [00:48<02:06,  3.71s/it]
[2026-01-20 10:51:06] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:51:06] [INFO] Loading safetensors checkpoint shards:  31% Completed | 15/48 [00:52<02:04,  3.77s/it]
[2026-01-20 10:51:10] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:51:10] [INFO] Loading safetensors checkpoint shards:  33% Completed | 16/48 [00:55<01:45,  3.29s/it]
[2026-01-20 10:51:13] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:51:13] [INFO] Loading safetensors checkpoint shards:  35% Completed | 17/48 [00:59<01:49,  3.52s/it]
[2026-01-20 10:51:17] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:51:17] [INFO] Loading safetensors checkpoint shards:  38% Completed | 18/48 [01:03<01:50,  3.68s/it]
[2026-01-20 10:51:21] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:51:21] [INFO] Loading safetensors checkpoint shards:  40% Completed | 19/48 [01:07<01:48,  3.75s/it]
[2026-01-20 10:51:24] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:51:24] [INFO] Loading safetensors checkpoint shards:  42% Completed | 20/48 [01:10<01:46,  3.80s/it]
[2026-01-20 10:51:28] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:51:28] [INFO] Loading safetensors checkpoint shards:  44% Completed | 21/48 [01:14<01:43,  3.85s/it]
[2026-01-20 10:51:32] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:51:32] [INFO] Loading safetensors checkpoint shards:  46% Completed | 22/48 [01:18<01:40,  3.86s/it]
[2026-01-20 10:51:35] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:51:35] [INFO] Loading safetensors checkpoint shards:  48% Completed | 23/48 [01:22<01:36,  3.84s/it]
[2026-01-20 10:51:39] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:51:39] [INFO] Loading safetensors checkpoint shards:  50% Completed | 24/48 [01:24<01:17,  3.21s/it]
[2026-01-20 10:51:43] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:51:43] [INFO] Loading safetensors checkpoint shards:  52% Completed | 25/48 [01:28<01:17,  3.36s/it]
[2026-01-20 10:51:46] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:51:46] [INFO] Loading safetensors checkpoint shards:  54% Completed | 26/48 [01:31<01:15,  3.45s/it]
[2026-01-20 10:51:50] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:51:50] [INFO] Loading safetensors checkpoint shards:  56% Completed | 27/48 [01:35<01:13,  3.51s/it]
[2026-01-20 10:51:53] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:51:53] [INFO] Loading safetensors checkpoint shards:  58% Completed | 28/48 [01:39<01:11,  3.56s/it]
[2026-01-20 10:51:57] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:51:57] [INFO] Loading safetensors checkpoint shards:  60% Completed | 29/48 [01:42<01:08,  3.60s/it]
[2026-01-20 10:52:00] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:52:00] [INFO] Loading safetensors checkpoint shards:  62% Completed | 30/48 [01:46<01:05,  3.66s/it]
[2026-01-20 10:52:04] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:52:04] [INFO] Loading safetensors checkpoint shards:  65% Completed | 31/48 [01:50<01:02,  3.70s/it]
[2026-01-20 10:52:07] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:52:07] [INFO] Loading safetensors checkpoint shards:  67% Completed | 32/48 [01:54<00:59,  3.71s/it]
[2026-01-20 10:52:11] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:52:11] [INFO] Loading safetensors checkpoint shards:  69% Completed | 33/48 [01:56<00:48,  3.20s/it]
[2026-01-20 10:52:14] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:52:14] [INFO] Loading safetensors checkpoint shards:  71% Completed | 34/48 [01:59<00:47,  3.38s/it]
[2026-01-20 10:52:18] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:52:18] [INFO] Loading safetensors checkpoint shards:  73% Completed | 35/48 [02:03<00:45,  3.54s/it]
[2026-01-20 10:52:22] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:52:22] [INFO] Loading safetensors checkpoint shards:  75% Completed | 36/48 [02:07<00:43,  3.61s/it]
[2026-01-20 10:52:25] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:52:25] [INFO] Loading safetensors checkpoint shards:  77% Completed | 37/48 [02:11<00:40,  3.69s/it]
[2026-01-20 10:52:29] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:52:29] [INFO] Loading safetensors checkpoint shards:  79% Completed | 38/48 [02:15<00:37,  3.76s/it]
[2026-01-20 10:52:33] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:52:33] [INFO] Loading safetensors checkpoint shards:  81% Completed | 39/48 [02:19<00:34,  3.80s/it]
[2026-01-20 10:52:36] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:52:36] [INFO] Loading safetensors checkpoint shards:  83% Completed | 40/48 [02:23<00:30,  3.81s/it]
[2026-01-20 10:52:40] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:52:40] [INFO] Loading safetensors checkpoint shards:  85% Completed | 41/48 [02:26<00:26,  3.84s/it]
[2026-01-20 10:52:44] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:52:44] [INFO] Loading safetensors checkpoint shards:  88% Completed | 42/48 [02:29<00:19,  3.32s/it]
[2026-01-20 10:52:47] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:52:47] [INFO] Loading safetensors checkpoint shards:  90% Completed | 43/48 [02:32<00:17,  3.47s/it]
[2026-01-20 10:52:51] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:52:51] [INFO] Loading safetensors checkpoint shards:  92% Completed | 44/48 [02:36<00:14,  3.59s/it]
[2026-01-20 10:52:55] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:52:55] [INFO] Loading safetensors checkpoint shards:  94% Completed | 45/48 [02:40<00:11,  3.68s/it]
[2026-01-20 10:52:58] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:52:58] [INFO] Loading safetensors checkpoint shards:  96% Completed | 46/48 [02:44<00:07,  3.70s/it]
[2026-01-20 10:53:03] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:53:03] [INFO] Loading safetensors checkpoint shards:  98% Completed | 47/48 [02:49<00:04,  4.20s/it]
[2026-01-20 10:53:03] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:53:03] [INFO] Loading safetensors checkpoint shards: 100% Completed | 48/48 [02:49<00:00,  2.99s/it]
[2026-01-20 10:53:03] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:53:03] [INFO] Loading safetensors checkpoint shards: 100% Completed | 48/48 [02:49<00:00,  3.54s/it]
[2026-01-20 10:53:03] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m
[2026-01-20 10:53:03] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m INFO 01-20 10:53:03 [default_loader.py:291] Loading weights took 178.91 seconds
[2026-01-20 10:53:04] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m INFO 01-20 10:53:04 [gpu_model_runner.py:3922] Model loading took 28.07 GiB memory and 180.308625 seconds
[2026-01-20 10:53:20] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m INFO 01-20 10:53:19 [backends.py:644] Using cache directory: /home/wuwen-ubu/.cache/vllm/torch_compile_cache/a31f817b79/rank_0_0/backbone for vLLM's torch.compile
[2026-01-20 10:53:20] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m INFO 01-20 10:53:19 [backends.py:704] Dynamo bytecode transform time: 12.67 s
[2026-01-20 10:53:36] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m INFO 01-20 10:53:36 [backends.py:261] Cache the graph of compile range (1, 8192) for later use
[2026-01-20 10:53:36] [INFO] [0;36m(Worker_TP1_EP1 pid=717)[0;0m INFO 01-20 10:53:36 [backends.py:261] Cache the graph of compile range (1, 8192) for later use
[2026-01-20 10:53:40] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m /home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:312: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
[2026-01-20 10:53:40] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m   warnings.warn(
[2026-01-20 10:53:40] [INFO] [0;36m(Worker_TP1_EP1 pid=717)[0;0m /home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:312: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
[2026-01-20 10:53:40] [INFO] [0;36m(Worker_TP1_EP1 pid=717)[0;0m   warnings.warn(
[2026-01-20 10:53:42] [WARNING] [0;36m(Worker_TP0_EP0 pid=716)[0;0m WARNING 01-20 10:53:41 [fused_moe.py:1084] Using default MoE config. Performance might be sub-optimal! Config file not found at /home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/configs/E=32,N=1536,device_name=NVIDIA_RTX_PRO_6000_Blackwell_Workstation_Edition.json
[2026-01-20 10:54:03] [INFO] [0;36m(EngineCore_DP0 pid=601)[0;0m INFO 01-20 10:54:03 [shm_broadcast.py:542] No available shared memory broadcast block found in 60 seconds. This typically happens when some processes are hanging or doing some time-consuming work (e.g. compilation, weight/kv cache quantization).
[2026-01-20 10:54:54] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m INFO 01-20 10:54:54 [backends.py:278] Compiling a graph for compile range (1, 8192) takes 83.39 s
[2026-01-20 10:54:54] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m INFO 01-20 10:54:54 [monitor.py:34] torch.compile takes 96.06 s in total
[2026-01-20 10:54:56] [INFO] [0;36m(Worker_TP0_EP0 pid=716)[0;0m INFO 01-20 10:54:56 [gpu_worker.py:355] Available KV cache memory: 56.19 GiB
[2026-01-20 10:54:56] [INFO] [0;36m(EngineCore_DP0 pid=601)[0;0m INFO 01-20 10:54:56 [kv_cache_utils.py:1307] GPU KV cache size: 250,704 tokens
[2026-01-20 10:54:56] [INFO] [0;36m(EngineCore_DP0 pid=601)[0;0m INFO 01-20 10:54:56 [kv_cache_utils.py:1312] Maximum concurrency for 128,000 tokens per request: 1.96x
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP0_EP0 pid=716)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839] WorkerProc hit an exception.
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP0_EP0 pid=716)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839] Traceback (most recent call last):
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP0_EP0 pid=716)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 834, in worker_busy_loop
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP0_EP0 pid=716)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]     output = func(*args, **kwargs)
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP0_EP0 pid=716)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]              ^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP0_EP0 pid=716)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 320, in initialize_from_config
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP0_EP0 pid=716)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP0_EP0 pid=716)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP0_EP0 pid=716)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 411, in initialize_from_config
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP0_EP0 pid=716)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]     self.model_runner.initialize_kv_cache(kv_cache_config)
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP0_EP0 pid=716)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5669, in initialize_kv_cache
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP0_EP0 pid=716)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]     kv_caches = self.initialize_kv_cache_tensors(
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP0_EP0 pid=716)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP0_EP0 pid=716)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5594, in initialize_kv_cache_tensors
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP0_EP0 pid=716)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP0_EP0 pid=716)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP0_EP0 pid=716)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5362, in _allocate_kv_cache_tensors
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP0_EP0 pid=716)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]     tensor = torch.zeros(
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP0_EP0 pid=716)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]              ^^^^^^^^^^^^
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP0_EP0 pid=716)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.20 GiB. GPU 0 has a total capacity of 95.59 GiB of which 17.61 GiB is free. Process 717 has 17179869184.00 GiB memory in use. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 75.06 GiB is allocated by PyTorch, and 118.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP0_EP0 pid=716)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839] Traceback (most recent call last):
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP0_EP0 pid=716)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 834, in worker_busy_loop
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP0_EP0 pid=716)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]     output = func(*args, **kwargs)
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP0_EP0 pid=716)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]              ^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP0_EP0 pid=716)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 320, in initialize_from_config
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP0_EP0 pid=716)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP0_EP0 pid=716)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP0_EP0 pid=716)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 411, in initialize_from_config
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP0_EP0 pid=716)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]     self.model_runner.initialize_kv_cache(kv_cache_config)
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP0_EP0 pid=716)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5669, in initialize_kv_cache
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP0_EP0 pid=716)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]     kv_caches = self.initialize_kv_cache_tensors(
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP0_EP0 pid=716)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP0_EP0 pid=716)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5594, in initialize_kv_cache_tensors
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP0_EP0 pid=716)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP0_EP0 pid=716)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP0_EP0 pid=716)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5362, in _allocate_kv_cache_tensors
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP0_EP0 pid=716)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]     tensor = torch.zeros(
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP0_EP0 pid=716)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]              ^^^^^^^^^^^^
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP0_EP0 pid=716)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.20 GiB. GPU 0 has a total capacity of 95.59 GiB of which 17.61 GiB is free. Process 717 has 17179869184.00 GiB memory in use. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 75.06 GiB is allocated by PyTorch, and 118.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP0_EP0 pid=716)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP1_EP1 pid=717)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839] WorkerProc hit an exception.
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP1_EP1 pid=717)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839] Traceback (most recent call last):
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP1_EP1 pid=717)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 834, in worker_busy_loop
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP1_EP1 pid=717)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]     output = func(*args, **kwargs)
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP1_EP1 pid=717)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]              ^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP1_EP1 pid=717)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 320, in initialize_from_config
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP1_EP1 pid=717)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP1_EP1 pid=717)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP1_EP1 pid=717)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 411, in initialize_from_config
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP1_EP1 pid=717)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]     self.model_runner.initialize_kv_cache(kv_cache_config)
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP1_EP1 pid=717)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5669, in initialize_kv_cache
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP1_EP1 pid=717)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]     kv_caches = self.initialize_kv_cache_tensors(
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP1_EP1 pid=717)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP1_EP1 pid=717)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5594, in initialize_kv_cache_tensors
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP1_EP1 pid=717)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP1_EP1 pid=717)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP1_EP1 pid=717)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5362, in _allocate_kv_cache_tensors
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP1_EP1 pid=717)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]     tensor = torch.zeros(
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP1_EP1 pid=717)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]              ^^^^^^^^^^^^
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP1_EP1 pid=717)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.20 GiB. GPU 1 has a total capacity of 95.59 GiB of which 18.81 GiB is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Process 716 has 17179869184.00 GiB memory in use. Of the allocated memory 73.87 GiB is allocated by PyTorch, and 116.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP1_EP1 pid=717)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839] Traceback (most recent call last):
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP1_EP1 pid=717)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 834, in worker_busy_loop
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP1_EP1 pid=717)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]     output = func(*args, **kwargs)
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP1_EP1 pid=717)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]              ^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP1_EP1 pid=717)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py", line 320, in initialize_from_config
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP1_EP1 pid=717)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]     self.worker.initialize_from_config(kv_cache_config)  # type: ignore
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP1_EP1 pid=717)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP1_EP1 pid=717)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py", line 411, in initialize_from_config
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP1_EP1 pid=717)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]     self.model_runner.initialize_kv_cache(kv_cache_config)
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP1_EP1 pid=717)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5669, in initialize_kv_cache
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP1_EP1 pid=717)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]     kv_caches = self.initialize_kv_cache_tensors(
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP1_EP1 pid=717)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP1_EP1 pid=717)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5594, in initialize_kv_cache_tensors
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP1_EP1 pid=717)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]     kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP1_EP1 pid=717)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP1_EP1 pid=717)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py", line 5362, in _allocate_kv_cache_tensors
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP1_EP1 pid=717)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]     tensor = torch.zeros(
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP1_EP1 pid=717)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]              ^^^^^^^^^^^^
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP1_EP1 pid=717)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.20 GiB. GPU 1 has a total capacity of 95.59 GiB of which 18.81 GiB is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Process 716 has 17179869184.00 GiB memory in use. Of the allocated memory 73.87 GiB is allocated by PyTorch, and 116.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[2026-01-20 10:54:57] [ERROR] [0;36m(Worker_TP1_EP1 pid=717)[0;0m ERROR 01-20 10:54:56 [multiproc_executor.py:839]
[2026-01-20 10:54:57] [ERROR] [0;36m(EngineCore_DP0 pid=601)[0;0m ERROR 01-20 10:54:56 [core.py:935] EngineCore failed to start.
[2026-01-20 10:54:57] [ERROR] [0;36m(EngineCore_DP0 pid=601)[0;0m ERROR 01-20 10:54:56 [core.py:935] Traceback (most recent call last):
[2026-01-20 10:54:57] [ERROR] [0;36m(EngineCore_DP0 pid=601)[0;0m ERROR 01-20 10:54:56 [core.py:935]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-20 10:54:57] [ERROR] [0;36m(EngineCore_DP0 pid=601)[0;0m ERROR 01-20 10:54:56 [core.py:935]     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-20 10:54:57] [ERROR] [0;36m(EngineCore_DP0 pid=601)[0;0m ERROR 01-20 10:54:56 [core.py:935]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 10:54:57] [ERROR] [0;36m(EngineCore_DP0 pid=601)[0;0m ERROR 01-20 10:54:56 [core.py:935]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-20 10:54:57] [ERROR] [0;36m(EngineCore_DP0 pid=601)[0;0m ERROR 01-20 10:54:56 [core.py:935]     super().__init__(
[2026-01-20 10:54:57] [ERROR] [0;36m(EngineCore_DP0 pid=601)[0;0m ERROR 01-20 10:54:56 [core.py:935]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 112, in __init__
[2026-01-20 10:54:57] [ERROR] [0;36m(EngineCore_DP0 pid=601)[0;0m ERROR 01-20 10:54:56 [core.py:935]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[2026-01-20 10:54:57] [ERROR] [0;36m(EngineCore_DP0 pid=601)[0;0m ERROR 01-20 10:54:56 [core.py:935]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 10:54:57] [ERROR] [0;36m(EngineCore_DP0 pid=601)[0;0m ERROR 01-20 10:54:56 [core.py:935]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 269, in _initialize_kv_caches
[2026-01-20 10:54:57] [ERROR] [0;36m(EngineCore_DP0 pid=601)[0;0m ERROR 01-20 10:54:56 [core.py:935]     self.model_executor.initialize_from_config(kv_cache_configs)
[2026-01-20 10:54:57] [ERROR] [0;36m(EngineCore_DP0 pid=601)[0;0m ERROR 01-20 10:54:56 [core.py:935]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[2026-01-20 10:54:57] [ERROR] [0;36m(EngineCore_DP0 pid=601)[0;0m ERROR 01-20 10:54:56 [core.py:935]     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[2026-01-20 10:54:57] [ERROR] [0;36m(EngineCore_DP0 pid=601)[0;0m ERROR 01-20 10:54:56 [core.py:935]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 374, in collective_rpc
[2026-01-20 10:54:57] [ERROR] [0;36m(EngineCore_DP0 pid=601)[0;0m ERROR 01-20 10:54:56 [core.py:935]     return aggregate(get_response())
[2026-01-20 10:54:57] [ERROR] [0;36m(EngineCore_DP0 pid=601)[0;0m ERROR 01-20 10:54:56 [core.py:935]                      ^^^^^^^^^^^^^^
[2026-01-20 10:54:57] [ERROR] [0;36m(EngineCore_DP0 pid=601)[0;0m ERROR 01-20 10:54:56 [core.py:935]   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 357, in get_response
[2026-01-20 10:54:57] [ERROR] [0;36m(EngineCore_DP0 pid=601)[0;0m ERROR 01-20 10:54:56 [core.py:935]     raise RuntimeError(
[2026-01-20 10:54:57] [ERROR] [0;36m(EngineCore_DP0 pid=601)[0;0m ERROR 01-20 10:54:56 [core.py:935] RuntimeError: Worker failed with error 'CUDA out of memory. Tried to allocate 1.20 GiB. GPU 0 has a total capacity of 95.59 GiB of which 17.61 GiB is free. Process 717 has 17179869184.00 GiB memory in use. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 75.06 GiB is allocated by PyTorch, and 118.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)', please check the stack trace above for the root cause
[2026-01-20 10:55:01] [ERROR] [0;36m(EngineCore_DP0 pid=601)[0;0m ERROR 01-20 10:55:01 [multiproc_executor.py:246] Worker proc VllmWorker-1 died unexpectedly, shutting down executor.
[2026-01-20 10:55:02] [INFO] [0;36m(EngineCore_DP0 pid=601)[0;0m Process EngineCore_DP0:
[2026-01-20 10:55:02] [ERROR] [0;36m(EngineCore_DP0 pid=601)[0;0m Traceback (most recent call last):
[2026-01-20 10:55:02] [INFO] [0;36m(EngineCore_DP0 pid=601)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[2026-01-20 10:55:02] [INFO] [0;36m(EngineCore_DP0 pid=601)[0;0m     self.run()
[2026-01-20 10:55:02] [INFO] [0;36m(EngineCore_DP0 pid=601)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/multiprocessing/process.py", line 108, in run
[2026-01-20 10:55:02] [INFO] [0;36m(EngineCore_DP0 pid=601)[0;0m     self._target(*self._args, **self._kwargs)
[2026-01-20 10:55:02] [INFO] [0;36m(EngineCore_DP0 pid=601)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 939, in run_engine_core
[2026-01-20 10:55:02] [INFO] [0;36m(EngineCore_DP0 pid=601)[0;0m     raise e
[2026-01-20 10:55:02] [INFO] [0;36m(EngineCore_DP0 pid=601)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 926, in run_engine_core
[2026-01-20 10:55:02] [INFO] [0;36m(EngineCore_DP0 pid=601)[0;0m     engine_core = EngineCoreProc(*args, engine_index=dp_rank, **kwargs)
[2026-01-20 10:55:02] [INFO] [0;36m(EngineCore_DP0 pid=601)[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 10:55:02] [INFO] [0;36m(EngineCore_DP0 pid=601)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 691, in __init__
[2026-01-20 10:55:02] [INFO] [0;36m(EngineCore_DP0 pid=601)[0;0m     super().__init__(
[2026-01-20 10:55:02] [INFO] [0;36m(EngineCore_DP0 pid=601)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 112, in __init__
[2026-01-20 10:55:02] [INFO] [0;36m(EngineCore_DP0 pid=601)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[2026-01-20 10:55:02] [INFO] [0;36m(EngineCore_DP0 pid=601)[0;0m                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 10:55:02] [INFO] [0;36m(EngineCore_DP0 pid=601)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 269, in _initialize_kv_caches
[2026-01-20 10:55:02] [INFO] [0;36m(EngineCore_DP0 pid=601)[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)
[2026-01-20 10:55:02] [INFO] [0;36m(EngineCore_DP0 pid=601)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/executor/abstract.py", line 115, in initialize_from_config
[2026-01-20 10:55:02] [INFO] [0;36m(EngineCore_DP0 pid=601)[0;0m     self.collective_rpc("initialize_from_config", args=(kv_cache_configs,))
[2026-01-20 10:55:02] [INFO] [0;36m(EngineCore_DP0 pid=601)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 374, in collective_rpc
[2026-01-20 10:55:02] [INFO] [0;36m(EngineCore_DP0 pid=601)[0;0m     return aggregate(get_response())
[2026-01-20 10:55:02] [INFO] [0;36m(EngineCore_DP0 pid=601)[0;0m                      ^^^^^^^^^^^^^^
[2026-01-20 10:55:02] [INFO] [0;36m(EngineCore_DP0 pid=601)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/executor/multiproc_executor.py", line 357, in get_response
[2026-01-20 10:55:02] [INFO] [0;36m(EngineCore_DP0 pid=601)[0;0m     raise RuntimeError(
[2026-01-20 10:55:02] [INFO] [0;36m(EngineCore_DP0 pid=601)[0;0m RuntimeError: Worker failed with error 'CUDA out of memory. Tried to allocate 1.20 GiB. GPU 0 has a total capacity of 95.59 GiB of which 17.61 GiB is free. Process 717 has 17179869184.00 GiB memory in use. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 75.06 GiB is allocated by PyTorch, and 118.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)', please check the stack trace above for the root cause
[2026-01-20 10:55:04] [ERROR] [0;36m(APIServer pid=432)[0;0m Traceback (most recent call last):
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/bin/vllm", line 7, in <module>
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m     sys.exit(main())
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m              ^^^^^^
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/cli/main.py", line 73, in main
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m     args.dispatch_function(args)
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/cli/serve.py", line 60, in cmd
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m     uvloop.run(run_server(args))
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 96, in run
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m     return __asyncio.run(
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m            ^^^^^^^^^^^^^^
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/asyncio/runners.py", line 195, in run
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m     return runner.run(main)
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m            ^^^^^^^^^^^^^^^^
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/asyncio/runners.py", line 118, in run
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m     return self._loop.run_until_complete(task)
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m   File "uvloop/loop.pyx", line 1518, in uvloop.loop.Loop.run_until_complete
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/uvloop/__init__.py", line 48, in wrapper
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m     return await main
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m            ^^^^^^^^^^
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 919, in run_server
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m     await run_server_worker(listen_address, sock, args, **uvicorn_kwargs)
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 938, in run_server_worker
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m     async with build_async_engine_client(
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m     return await anext(self.gen)
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 146, in build_async_engine_client
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m     async with build_async_engine_client_from_engine_args(
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/contextlib.py", line 210, in __aenter__
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m     return await anext(self.gen)
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m            ^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/entrypoints/openai/api_server.py", line 187, in build_async_engine_client_from_engine_args
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m     async_llm = AsyncLLM.from_vllm_config(
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 202, in from_vllm_config
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m     return cls(
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m            ^^^^
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 129, in __init__
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m     self.engine_core = EngineCoreClient.make_async_mp_client(
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 122, in make_async_mp_client
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m     return AsyncMPClient(*client_args)
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 819, in __init__
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m     super().__init__(
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 479, in __init__
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m     with launch_core_engines(vllm_config, executor_class, log_stats) as (
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/contextlib.py", line 144, in __exit__
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m     next(self.gen)
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 918, in launch_core_engines
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m     wait_for_engine_startup(
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m   File "/home/wuwen-ubu/miniconda3/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/utils.py", line 977, in wait_for_engine_startup
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m     raise RuntimeError(
[2026-01-20 10:55:04] [INFO] [0;36m(APIServer pid=432)[0;0m RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
[2026-01-20 10:55:06] [INFO] nvitop监控已启动
